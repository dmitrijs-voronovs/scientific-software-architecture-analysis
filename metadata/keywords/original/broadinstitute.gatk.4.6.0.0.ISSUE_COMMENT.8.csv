id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/gatk/pull/4571#issuecomment-814231696:192,Testability,test,testing,192,"Hello - I'm writing to check on these PRs again. I appreciate these are not GATK's priority, but they're blocking #6973 and we've been stuck for a long time here. If there is additional work, testing or anything needed on this I am happy to offer my time if there's anything that would help get this done.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4571#issuecomment-814231696
https://github.com/broadinstitute/gatk/pull/4571#issuecomment-822691252:977,Testability,test,tests,977,"@cmnbroad @droazen Good morning - I'm writing to follow up again. I appreciate that this not GATK's main priority, but I'm at the point where I need to either unblock this or fork GATK and publish another artifact so we can move our projects forward. The crux of what I'm trying to achieve with these changes is for our VariantQC tool; however, I'm bumping into several situations where MultiVariantWalkers need to be able to determine the FeatureInput source of the variants. I recognize that this and my original PR #6973 has taken a non-trivial amount of @cmnbroad time, but we have basically been stalled with an otherwise approved PR since Feb 8. The PRs blocking that PR are this one and the related #7021. They both involve creating a way to connect VariantContext to FeatureInput - a capability that would benefit the GATK engine and I have been told by @cmnbroad you're interested in having. It seems like the primary problem associated with these changes is ensuring tests and VariantContext comparison code still works, since VariantContexts are going to tend to report a source. I dont know your internal conversations, so I'm guessing based on what's written in github. As I've said, I'd like to do whatever I can to get these changes into a form that takes as little of your effort as possible. . While this particular PR seems close, there is clearly some cleanup needed from what's there now, including code review from @lbergelson that no one ever fixed. Would it help if I put together #4571 and #7021 into a new branch where I also work through associated test changes and try to get this into one concise piece of code to review? Basically try to put everything together to be the minimal amount of work needed on your side? I havent heard anything one way or the other from GATK staff as to whether this would actually be helpful or not. Are there higher order design decisions that need to be considered that I'm not seeing? . I completely understand your time constraints - I'm ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4571#issuecomment-822691252
https://github.com/broadinstitute/gatk/pull/4571#issuecomment-822691252:1575,Testability,test,test,1575," publish another artifact so we can move our projects forward. The crux of what I'm trying to achieve with these changes is for our VariantQC tool; however, I'm bumping into several situations where MultiVariantWalkers need to be able to determine the FeatureInput source of the variants. I recognize that this and my original PR #6973 has taken a non-trivial amount of @cmnbroad time, but we have basically been stalled with an otherwise approved PR since Feb 8. The PRs blocking that PR are this one and the related #7021. They both involve creating a way to connect VariantContext to FeatureInput - a capability that would benefit the GATK engine and I have been told by @cmnbroad you're interested in having. It seems like the primary problem associated with these changes is ensuring tests and VariantContext comparison code still works, since VariantContexts are going to tend to report a source. I dont know your internal conversations, so I'm guessing based on what's written in github. As I've said, I'd like to do whatever I can to get these changes into a form that takes as little of your effort as possible. . While this particular PR seems close, there is clearly some cleanup needed from what's there now, including code review from @lbergelson that no one ever fixed. Would it help if I put together #4571 and #7021 into a new branch where I also work through associated test changes and try to get this into one concise piece of code to review? Basically try to put everything together to be the minimal amount of work needed on your side? I havent heard anything one way or the other from GATK staff as to whether this would actually be helpful or not. Are there higher order design decisions that need to be considered that I'm not seeing? . I completely understand your time constraints - I'm just trying to figure out some solution that let's this go forward. Again, if we cant unblock this soon I'm going to probably fork GATK and need to start working off that project. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4571#issuecomment-822691252
https://github.com/broadinstitute/gatk/pull/4571#issuecomment-822691252:1358,Usability,clear,clearly,1358," publish another artifact so we can move our projects forward. The crux of what I'm trying to achieve with these changes is for our VariantQC tool; however, I'm bumping into several situations where MultiVariantWalkers need to be able to determine the FeatureInput source of the variants. I recognize that this and my original PR #6973 has taken a non-trivial amount of @cmnbroad time, but we have basically been stalled with an otherwise approved PR since Feb 8. The PRs blocking that PR are this one and the related #7021. They both involve creating a way to connect VariantContext to FeatureInput - a capability that would benefit the GATK engine and I have been told by @cmnbroad you're interested in having. It seems like the primary problem associated with these changes is ensuring tests and VariantContext comparison code still works, since VariantContexts are going to tend to report a source. I dont know your internal conversations, so I'm guessing based on what's written in github. As I've said, I'd like to do whatever I can to get these changes into a form that takes as little of your effort as possible. . While this particular PR seems close, there is clearly some cleanup needed from what's there now, including code review from @lbergelson that no one ever fixed. Would it help if I put together #4571 and #7021 into a new branch where I also work through associated test changes and try to get this into one concise piece of code to review? Basically try to put everything together to be the minimal amount of work needed on your side? I havent heard anything one way or the other from GATK staff as to whether this would actually be helpful or not. Are there higher order design decisions that need to be considered that I'm not seeing? . I completely understand your time constraints - I'm just trying to figure out some solution that let's this go forward. Again, if we cant unblock this soon I'm going to probably fork GATK and need to start working off that project. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4571#issuecomment-822691252
https://github.com/broadinstitute/gatk/issues/4572#issuecomment-376892039:276,Testability,test,tests,276,"Thanks @sooheelee. The code is definitely not correctly reconciling the accumulated variants/blocks that are remaining when traversal ends. I have a fix that resolves it, at least for this case, but this needs some more analysis and I'm going to have to write some additional tests to be sure that its correct.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4572#issuecomment-376892039
https://github.com/broadinstitute/gatk/issues/4572#issuecomment-376893744:92,Testability,test,test,92,Great to hear there is a fix so soon @cmnbroad. Let me know if I can help out with creating test data.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4572#issuecomment-376893744
https://github.com/broadinstitute/gatk/issues/4572#issuecomment-376894999:54,Availability,error,error,54,"Fyi @cmnbroad, the researcher has clarified that this error occurs for other primary assembly contigs, e.g. chr1. See their clarification [here](https://gatkforums.broadinstitute.org/gatk/discussion/comment/47226#Comment_47226).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4572#issuecomment-376894999
https://github.com/broadinstitute/gatk/issues/4572#issuecomment-383096422:224,Deployability,release,release,224,"The fix for this issue is merged, but there are a couple of PRs for related issues (https://github.com/broadinstitute/gatk/pull/4680 and https://github.com/broadinstitute/gatk/pull/4681) that should be merged before we do a release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4572#issuecomment-383096422
https://github.com/broadinstitute/gatk/issues/4573#issuecomment-375935539:125,Deployability,release,release,125,"Based on community interest we did this in PR #4522. Although it has been merged into master we haven't put out a new tagged release since then, so it's not yet in the gatk4 docker.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4573#issuecomment-375935539
https://github.com/broadinstitute/gatk/issues/4576#issuecomment-376201246:56,Testability,test,test,56,"In other region, -stand-call-conf 0/10 works well in my test.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4576#issuecomment-376201246
https://github.com/broadinstitute/gatk/issues/4578#issuecomment-376937140:9,Availability,Error,Error,9,"Hi. Same Error on 4.0.3.0; ```; java -jar /usr/hpc-bio/gatk/gatk-package-4.0.3.0-local.jar Mutect2 --verbosity WARNING -R /usr/bio-ref/GRCh38.p0.dnaref/dnaref.fa --germline-resource /usr/bio-ref/GRCh38.p0.dnaref/common.vcf --max-reads-per-alignment-start 100 -L X -I /biowrk/BaseSpace/bam.bwa/HiSeqX-PCR-free-v2.5-NA12878/md.bam -tumor HiSeqX-PCR-free-v2.5-NA12878 -O mutect2.tumor-only.vcf; 23:50:01.301 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; [March 28, 2018 11:50:04 PM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=2354577408; java.lang.IndexOutOfBoundsException: Index: 0, Size: 0; at java.util.ArrayList.rangeCheck(ArrayList.java:657); at java.util.ArrayList.get(ArrayList.java:433); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.isActive(Mutect2Engine.java:316); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:159); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:135); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-376937140
https://github.com/broadinstitute/gatk/issues/4578#issuecomment-376937140:963,Performance,load,loadNextAssemblyRegion,963,"Hi. Same Error on 4.0.3.0; ```; java -jar /usr/hpc-bio/gatk/gatk-package-4.0.3.0-local.jar Mutect2 --verbosity WARNING -R /usr/bio-ref/GRCh38.p0.dnaref/dnaref.fa --germline-resource /usr/bio-ref/GRCh38.p0.dnaref/common.vcf --max-reads-per-alignment-start 100 -L X -I /biowrk/BaseSpace/bam.bwa/HiSeqX-PCR-free-v2.5-NA12878/md.bam -tumor HiSeqX-PCR-free-v2.5-NA12878 -O mutect2.tumor-only.vcf; 23:50:01.301 WARN IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; [March 28, 2018 11:50:04 PM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=2354577408; java.lang.IndexOutOfBoundsException: Index: 0, Size: 0; at java.util.ArrayList.rangeCheck(ArrayList.java:657); at java.util.ArrayList.get(ArrayList.java:433); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.isActive(Mutect2Engine.java:316); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:159); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:135); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-376937140
https://github.com/broadinstitute/gatk/issues/4578#issuecomment-377400654:169,Testability,test,test,169,"sed 's/CAF/AF/g' dbsnp.vcf > dbsnp_for_m2.vcf does NOT work.; because CAF is diff from AF in format, not just name. Mutect2 works well without --germline-resource in my test case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-377400654
https://github.com/broadinstitute/gatk/issues/4578#issuecomment-561699783:1029,Availability,down,down,1029,"blem when running GATK-4.1.2.0, the weird thing is it finished successfully for some samples. . My command is:. `toolPath/gatk --java-options ""-Xmx30G -Djava.io.tmpdir=./tmp"" Mutect2 -I C150589TF.bam -I C150589NF.bam -normal C150589NF -R /path/to/reference/gatk/GRCh38.d1.vd1.fa --panel-of-normals /path/to/mutect2/pon/chr16.vcf.gz --germline-resource /path/to/gnomAD/gnomad.genomes.r3.0.sites.chr16.vcf.bgz -L chr16 -O unfiltered-chr16.vcf >mutect2-chr16.log 2>mutect2-chr16.err`. The last lines of mutect2-chr16.err:; ```; 08:40:14.455 INFO ProgressMeter - chr16:34593354 22.7 132470 5832.0; 08:41:07.915 INFO ProgressMeter - chr16:34595109 23.6 132480 5612.3; 08:42:02.710 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.735608267; 08:42:02.710 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 609.8425236940001; 08:42:02.710 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 142.16 sec; 08:42:02.711 INFO Mutect2 - Shutting down engine; [December 4, 2019 8:42:02 AM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 24.59 minutes.; Runtime.totalMemory()=2076049408; java.lang.IndexOutOfBoundsException: Index: 0, Size: 0; at java.util.ArrayList.rangeCheck(ArrayList.java:653); at java.util.ArrayList.get(ArrayList.java:429); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.lambda$getGermlineAltAlleleFrequencies$31(SomaticGenotypingEngine.java:350); at java.util.stream.ReferencePipeline$6$1.accept(ReferencePipeline.java:244); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); at java.util.stream.DoublePipeline.toArray(Doub",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-561699783
https://github.com/broadinstitute/gatk/issues/4578#issuecomment-561699783:1791,Integrability,wrap,wrapAndCopyInto,1791,"FO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 609.8425236940001; 08:42:02.710 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 142.16 sec; 08:42:02.711 INFO Mutect2 - Shutting down engine; [December 4, 2019 8:42:02 AM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 24.59 minutes.; Runtime.totalMemory()=2076049408; java.lang.IndexOutOfBoundsException: Index: 0, Size: 0; at java.util.ArrayList.rangeCheck(ArrayList.java:653); at java.util.ArrayList.get(ArrayList.java:429); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.lambda$getGermlineAltAlleleFrequencies$31(SomaticGenotypingEngine.java:350); at java.util.stream.ReferencePipeline$6$1.accept(ReferencePipeline.java:244); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:506); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getGermlineAltAlleleFrequencies(SomaticGenotypingEngine.java:352); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getNegativeLogPopulationAFAnnotation(SomaticGenotypingEngine.java:335); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:141); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:250); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:324); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); at org.broadinstitute.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-561699783
https://github.com/broadinstitute/gatk/issues/4578#issuecomment-561699783:496,Testability,log,log,496,"Hi @davidbenjamin . I meet a similar problem when running GATK-4.1.2.0, the weird thing is it finished successfully for some samples. . My command is:. `toolPath/gatk --java-options ""-Xmx30G -Djava.io.tmpdir=./tmp"" Mutect2 -I C150589TF.bam -I C150589NF.bam -normal C150589NF -R /path/to/reference/gatk/GRCh38.d1.vd1.fa --panel-of-normals /path/to/mutect2/pon/chr16.vcf.gz --germline-resource /path/to/gnomAD/gnomad.genomes.r3.0.sites.chr16.vcf.bgz -L chr16 -O unfiltered-chr16.vcf >mutect2-chr16.log 2>mutect2-chr16.err`. The last lines of mutect2-chr16.err:; ```; 08:40:14.455 INFO ProgressMeter - chr16:34593354 22.7 132470 5832.0; 08:41:07.915 INFO ProgressMeter - chr16:34595109 23.6 132480 5612.3; 08:42:02.710 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 0.735608267; 08:42:02.710 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 609.8425236940001; 08:42:02.710 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 142.16 sec; 08:42:02.711 INFO Mutect2 - Shutting down engine; [December 4, 2019 8:42:02 AM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 24.59 minutes.; Runtime.totalMemory()=2076049408; java.lang.IndexOutOfBoundsException: Index: 0, Size: 0; at java.util.ArrayList.rangeCheck(ArrayList.java:653); at java.util.ArrayList.get(ArrayList.java:429); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.lambda$getGermlineAltAlleleFrequencies$31(SomaticGenotypingEngine.java:350); at java.util.stream.ReferencePipeline$6$1.accept(ReferencePipeline.java:244); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); at java",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-561699783
https://github.com/broadinstitute/gatk/issues/4578#issuecomment-561699783:3516,Testability,test,testing,3516,"actPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:506); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getGermlineAltAlleleFrequencies(SomaticGenotypingEngine.java:352); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getNegativeLogPopulationAFAnnotation(SomaticGenotypingEngine.java:335); at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:141); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:250); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:324); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:281); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. Then, I try to run 3 jobs for testing: 1) without --panel-of-normals and --germline-resource; 2) without --panel-of-normals; 3) without --germline-resource. All three jobs finished success. Can you help me solve the problem? . Thanks in davance!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-561699783
https://github.com/broadinstitute/gatk/issues/4578#issuecomment-616557568:606,Availability,down,down,606,"I am having the same problem.; The last lines read as follow:; ```; 12:15:53.217 INFO ProgressMeter - chr3:73018046 285.5 2140110 7497.0; 12:16:03.395 INFO ProgressMeter - chr3:73445027 285.6 2141720 7498.2; 12:16:13.407 INFO ProgressMeter - chr3:75481384 285.8 2149100 7519.7; 12:16:16.285 INFO VectorLoglessPairHMM - Time spent in setup for JNI call : 4.453178435; 12:16:16.286 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 2290.3264339380003; 12:16:16.286 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 4541.79 sec; 12:16:16.289 INFO Mutect2 - Shutting down engine; [April 20, 2020 12:16:16 PM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 285.86 minutes.; Runtime.totalMemory()=8561098752; java.lang.IndexOutOfBoundsException: Index: 0, Size: 0; 	at java.util.ArrayList.rangeCheck(ArrayList.java:657); 	at java.util.ArrayList.get(ArrayList.java:433); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.lambda$getGermlineAltAlleleFrequencies$26(SomaticGenotypingEngine.java:339); 	at java.util.stream.ReferencePipeline$6$1.accept(ReferencePipeline.java:244); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546); 	at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); 	at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:508); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getGermlineAltAlleleFrequencies(SomaticGenotypingEngine.java:341); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getNegativeLogPopulationAFAnnotation(SomaticGenotypingEngine.java:324); 	at org.broadinstitute.hellbender.tools.walkers.mutect",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-616557568
https://github.com/broadinstitute/gatk/issues/4578#issuecomment-616557568:3089,Availability,down,downloaded,3089,"t2Engine.callRegion(Mutect2Engine.java:251); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:320); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:281); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); 	at org.broadinstitute.hellbender.Main.main(Main.java:292); ```; I downloaded the file from gnomAD repository (""All chromosomes VCF""), but had to do some changes, as follow:; 1. Lifted over to hg19, by using [b37tohg19.chain](https://github.com/broadgsa/gatk/blob/master/public/chainFiles/b37tohg19.chain) by using:; ```; sudo java -jar /picard-2.21.3/picard.jar LiftoverVcf I=gnomad.exomes.r2.1.1.sites.vcf.bgz \; O=gnomad.exomes_hg19.r2.1.1.sites.vcf.bgz \; CHAIN=b37tohg19.chain REJECT=rejected_variants.vcf \; R=/reference/hg19/hg19.fa \; ALLOW_MISSING_FIELDS_IN_HEADER=true \; MAX_RECORDS_IN_RAM=100000; ```; 2. Got rid of the [chrN_random tables](http://genome.ucsc.edu/FAQ/FAQdownloads#download10) by manually deleting them from the header of ```ExAC_hg19.r1.sites.vep.vcf.gz``` and creating a new file with just the new header, ```header_hg19_Ion.vcf```; and. 3. Fixed the header by using:; ```; java -jar picard.jar FixVcfHeader I=/gatk_data/reference/gnomAD/gnomad.exomes_hg19.r2.1.1.sites.vcf.bgz \; O=/gatk_data/reference/gnomAD/gnomad.exomes_hg19_Ion.r2.1.1.sites.vcf.bgz \; HEADER=/gatk_data/referen",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-616557568
https://github.com/broadinstitute/gatk/issues/4578#issuecomment-616557568:4226,Availability,error,error,4226,"stitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:281); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); 	at org.broadinstitute.hellbender.Main.main(Main.java:292); ```; I downloaded the file from gnomAD repository (""All chromosomes VCF""), but had to do some changes, as follow:; 1. Lifted over to hg19, by using [b37tohg19.chain](https://github.com/broadgsa/gatk/blob/master/public/chainFiles/b37tohg19.chain) by using:; ```; sudo java -jar /picard-2.21.3/picard.jar LiftoverVcf I=gnomad.exomes.r2.1.1.sites.vcf.bgz \; O=gnomad.exomes_hg19.r2.1.1.sites.vcf.bgz \; CHAIN=b37tohg19.chain REJECT=rejected_variants.vcf \; R=/reference/hg19/hg19.fa \; ALLOW_MISSING_FIELDS_IN_HEADER=true \; MAX_RECORDS_IN_RAM=100000; ```; 2. Got rid of the [chrN_random tables](http://genome.ucsc.edu/FAQ/FAQdownloads#download10) by manually deleting them from the header of ```ExAC_hg19.r1.sites.vep.vcf.gz``` and creating a new file with just the new header, ```header_hg19_Ion.vcf```; and. 3. Fixed the header by using:; ```; java -jar picard.jar FixVcfHeader I=/gatk_data/reference/gnomAD/gnomad.exomes_hg19.r2.1.1.sites.vcf.bgz \; O=/gatk_data/reference/gnomAD/gnomad.exomes_hg19_Ion.r2.1.1.sites.vcf.bgz \; HEADER=/gatk_data/reference/gnomAD/header_hg19_Ion.vcf \; MAX_RECORDS_IN_RAM=70000; ```. I am bit lost on where the error might be, Any help is greatly appreciated.; Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-616557568
https://github.com/broadinstitute/gatk/issues/4578#issuecomment-616557568:1375,Integrability,wrap,wrapAndCopyInto,1375,"M - Total compute time in PairHMM computeLogLikelihoods() : 2290.3264339380003; 12:16:16.286 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 4541.79 sec; 12:16:16.289 INFO Mutect2 - Shutting down engine; [April 20, 2020 12:16:16 PM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 285.86 minutes.; Runtime.totalMemory()=8561098752; java.lang.IndexOutOfBoundsException: Index: 0, Size: 0; 	at java.util.ArrayList.rangeCheck(ArrayList.java:657); 	at java.util.ArrayList.get(ArrayList.java:433); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.lambda$getGermlineAltAlleleFrequencies$26(SomaticGenotypingEngine.java:339); 	at java.util.stream.ReferencePipeline$6$1.accept(ReferencePipeline.java:244); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546); 	at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); 	at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:508); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getGermlineAltAlleleFrequencies(SomaticGenotypingEngine.java:341); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getNegativeLogPopulationAFAnnotation(SomaticGenotypingEngine.java:324); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:146); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:251); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:320); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); 	at org.broad",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-616557568
https://github.com/broadinstitute/gatk/issues/4578#issuecomment-681608709:3029,Availability,error,error,3029,Reader.query(BAMFileReader.java:612); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:533); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:405); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:125); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:66); at org.broadinstitute.hellbender.engine.ReadsDataSource.prepareIteratorsForTraversal(ReadsDataSource.java:416); at org.broadinstitute.hellbender.engine.ReadsDataSource.iterator(ReadsDataSource.java:342); at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:134); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:86); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); ```. and my command:; ```; gatk-4.1.7.0/gatk Mutect2 -force-active -R ~/hg19/gatk_bundle/ucsc.hg19.fasta \; -I bam/T.BQSR.reheader.bam --alleles xx.vcf -O gga.vcf -L total.site.bed -ip 200; ```; Maybe the bam file is broken? I have no idea how to debug with the error messages showed above.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-681608709
https://github.com/broadinstitute/gatk/issues/4578#issuecomment-681608709:3035,Integrability,message,messages,3035,Reader.query(BAMFileReader.java:612); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:533); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:405); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:125); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:66); at org.broadinstitute.hellbender.engine.ReadsDataSource.prepareIteratorsForTraversal(ReadsDataSource.java:416); at org.broadinstitute.hellbender.engine.ReadsDataSource.iterator(ReadsDataSource.java:342); at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:134); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:86); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); ```. and my command:; ```; gatk-4.1.7.0/gatk Mutect2 -force-active -R ~/hg19/gatk_bundle/ucsc.hg19.fasta \; -I bam/T.BQSR.reheader.bam --alleles xx.vcf -O gga.vcf -L total.site.bed -ip 200; ```; Maybe the bam file is broken? I have no idea how to debug with the error messages showed above.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-681608709
https://github.com/broadinstitute/gatk/issues/4578#issuecomment-681608709:1367,Performance,load,loadNextIterator,1367,5); at htsjdk.samtools.MemoryMappedFileBuffer.readBytes(MemoryMappedFileBuffer.java:34); at htsjdk.samtools.AbstractBAMFileIndex.readBytes(AbstractBAMFileIndex.java:439); at htsjdk.samtools.AbstractBAMFileIndex.verifyIndexMagicNumber(AbstractBAMFileIndex.java:376); at htsjdk.samtools.AbstractBAMFileIndex.<init>(AbstractBAMFileIndex.java:70); at htsjdk.samtools.AbstractBAMFileIndex.<init>(AbstractBAMFileIndex.java:64); at htsjdk.samtools.CachingBAMFileIndex.<init>(CachingBAMFileIndex.java:56); at htsjdk.samtools.BAMFileReader.getIndex(BAMFileReader.java:418); at htsjdk.samtools.BAMFileReader.createIndexIterator(BAMFileReader.java:952); at htsjdk.samtools.BAMFileReader.query(BAMFileReader.java:612); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:533); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:405); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:125); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:66); at org.broadinstitute.hellbender.engine.ReadsDataSource.prepareIteratorsForTraversal(ReadsDataSource.java:416); at org.broadinstitute.hellbender.engine.ReadsDataSource.iterator(ReadsDataSource.java:342); at org.broadinstitute.hellbender.engine.MultiIntervalLocalReadShard.iterator(MultiIntervalLocalReadShard.java:134); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.<init>(AssemblyRegionIterator.java:86); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:188); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4578#issuecomment-681608709
https://github.com/broadinstitute/gatk/issues/4580#issuecomment-592146189:78,Modifiability,inherit,inherit,78,"There has been no activity on this for two years, and the two classes already inherit from different superclasses, and the current ""has a"" implementation avoids code duplication nicely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4580#issuecomment-592146189
https://github.com/broadinstitute/gatk/issues/4580#issuecomment-592146189:154,Safety,avoid,avoids,154,"There has been no activity on this for two years, and the two classes already inherit from different superclasses, and the current ""has a"" implementation avoids code duplication nicely.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4580#issuecomment-592146189
https://github.com/broadinstitute/gatk/issues/4582#issuecomment-417439030:226,Deployability,patch,patch,226,"We determined that dbSNP should have the `all` data. Individual data sources will be versioned as well as the Broad-provided packages. Broad-packaged data sources will be semantic versioned (see https://semver.org/) with the `patch` version corresponding to the date of release and any `decorators` on the data. A decorator is an arbitrary string that adds information to the version. For instance, two data source releases may contain several overlapping data sources and some use-case specific ones (such as in the germline and somatic uses of the tool). For these two data source releases, they will have the same version number, but with different decorators. . Concretely: ; `funcotator_dataSources.v1.4.20180829g.tar.gz` - germline data sources; `funcotator_dataSources.v1.4.20180829s.tar.gz` - somatic data sources",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4582#issuecomment-417439030
https://github.com/broadinstitute/gatk/issues/4582#issuecomment-417439030:270,Deployability,release,release,270,"We determined that dbSNP should have the `all` data. Individual data sources will be versioned as well as the Broad-provided packages. Broad-packaged data sources will be semantic versioned (see https://semver.org/) with the `patch` version corresponding to the date of release and any `decorators` on the data. A decorator is an arbitrary string that adds information to the version. For instance, two data source releases may contain several overlapping data sources and some use-case specific ones (such as in the germline and somatic uses of the tool). For these two data source releases, they will have the same version number, but with different decorators. . Concretely: ; `funcotator_dataSources.v1.4.20180829g.tar.gz` - germline data sources; `funcotator_dataSources.v1.4.20180829s.tar.gz` - somatic data sources",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4582#issuecomment-417439030
https://github.com/broadinstitute/gatk/issues/4582#issuecomment-417439030:415,Deployability,release,releases,415,"We determined that dbSNP should have the `all` data. Individual data sources will be versioned as well as the Broad-provided packages. Broad-packaged data sources will be semantic versioned (see https://semver.org/) with the `patch` version corresponding to the date of release and any `decorators` on the data. A decorator is an arbitrary string that adds information to the version. For instance, two data source releases may contain several overlapping data sources and some use-case specific ones (such as in the germline and somatic uses of the tool). For these two data source releases, they will have the same version number, but with different decorators. . Concretely: ; `funcotator_dataSources.v1.4.20180829g.tar.gz` - germline data sources; `funcotator_dataSources.v1.4.20180829s.tar.gz` - somatic data sources",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4582#issuecomment-417439030
https://github.com/broadinstitute/gatk/issues/4582#issuecomment-417439030:583,Deployability,release,releases,583,"We determined that dbSNP should have the `all` data. Individual data sources will be versioned as well as the Broad-provided packages. Broad-packaged data sources will be semantic versioned (see https://semver.org/) with the `patch` version corresponding to the date of release and any `decorators` on the data. A decorator is an arbitrary string that adds information to the version. For instance, two data source releases may contain several overlapping data sources and some use-case specific ones (such as in the germline and somatic uses of the tool). For these two data source releases, they will have the same version number, but with different decorators. . Concretely: ; `funcotator_dataSources.v1.4.20180829g.tar.gz` - germline data sources; `funcotator_dataSources.v1.4.20180829s.tar.gz` - somatic data sources",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4582#issuecomment-417439030
https://github.com/broadinstitute/gatk/pull/4584#issuecomment-376279904:1565,Deployability,pipeline,pipelines,1565,=======; Files 1062 1062 ; Lines 61677 61676 -1 ; Branches 9983 9984 +1 ; ==============================================; - Hits 49243 49076 -167 ; - Misses 8539 8709 +170 ; + Partials 3895 3891 -4; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4584?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ngine/spark/datasources/ReferenceTwoBitSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVmZXJlbmNlVHdvQml0U291cmNlLmphdmE=) | `90.909% <75%> (-9.091%)` | `6 <2> (-1)` | |; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-75.51%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-9%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4584#issuecomment-376279904
https://github.com/broadinstitute/gatk/pull/4584#issuecomment-376279904:1881,Testability,test,test,1881, | Complexity Δ | |; |---|---|---|---|; | [...ngine/spark/datasources/ReferenceTwoBitSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVmZXJlbmNlVHdvQml0U291cmNlLmphdmE=) | `90.909% <75%> (-9.091%)` | `6 <2> (-1)` | |; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-75.51%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-9%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <0%> (-22.807%)` | `0% <0%> (-2%)` | |; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4584#issuecomment-376279904
https://github.com/broadinstitute/gatk/pull/4584#issuecomment-376279904:2747,Testability,test,test,2747,GFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-9%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <0%> (-22.807%)` | `0% <0%> (-2%)` | |; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L01pbmlDbHVzdGVyVXRpbHMuamF2YQ==) | `78.947% <0%> (-10.526%)` | `6% <0%> (-1%)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `89.394% <0%> (-3.03%)` | `61% <0%> (-2%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `61.94% <0%> (-2.985%)` | `30% <0%> (-3%)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4584#issuecomment-376279904
https://github.com/broadinstitute/gatk/pull/4584#issuecomment-376279904:3330,Testability,test,test,3330,ff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-9%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <0%> (-22.807%)` | `0% <0%> (-2%)` | |; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L01pbmlDbHVzdGVyVXRpbHMuamF2YQ==) | `78.947% <0%> (-10.526%)` | `6% <0%> (-1%)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `89.394% <0%> (-3.03%)` | `61% <0%> (-2%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `61.94% <0%> (-2.985%)` | `30% <0%> (-3%)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `79.487% <0%> (-2.564%)` | `44% <0%> (ø)` | |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/4584/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4584#issuecomment-376279904
https://github.com/broadinstitute/gatk/pull/4584#issuecomment-376283397:88,Performance,perform,performance,88,"@tomwhite When you get a chance, could you please test this branch to check whether the performance regression you reported earlier is resolved?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4584#issuecomment-376283397
https://github.com/broadinstitute/gatk/pull/4584#issuecomment-376283397:50,Testability,test,test,50,"@tomwhite When you get a chance, could you please test this branch to check whether the performance regression you reported earlier is resolved?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4584#issuecomment-376283397
https://github.com/broadinstitute/gatk/pull/4590#issuecomment-376423720:1246,Deployability,pipeline,pipelines,1246,e3860943bcccd44fea40ce3?src=pr&el=desc) will **decrease** coverage by `0.266%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #4590 +/- ##; ==============================================; - Coverage 79.84% 79.574% -0.266% ; + Complexity 16958 16911 -47 ; ==============================================; Files 1062 1062 ; Lines 61677 61677 ; Branches 9983 9983 ; ==============================================; - Hits 49243 49079 -164 ; - Misses 8539 8708 +169 ; + Partials 3895 3890 -5; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4590?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-75.51%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-9%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4590#issuecomment-376423720
https://github.com/broadinstitute/gatk/pull/4590#issuecomment-376423720:1562,Testability,test,test,1562,=======================; Files 1062 1062 ; Lines 61677 61677 ; Branches 9983 9983 ; ==============================================; - Hits 49243 49079 -164 ; - Misses 8539 8708 +169 ; + Partials 3895 3890 -5; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4590?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-75.51%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-9%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <0%> (-22.807%)` | `0% <0%> (-2%)` | |; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4590#issuecomment-376423720
https://github.com/broadinstitute/gatk/pull/4590#issuecomment-376423720:2428,Testability,test,test,2428,GFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-9%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <0%> (-22.807%)` | `0% <0%> (-2%)` | |; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L01pbmlDbHVzdGVyVXRpbHMuamF2YQ==) | `78.947% <0%> (-10.526%)` | `6% <0%> (-1%)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `89.394% <0%> (-3.03%)` | `61% <0%> (-2%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `61.94% <0%> (-2.985%)` | `30% <0%> (-3%)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4590#issuecomment-376423720
https://github.com/broadinstitute/gatk/pull/4590#issuecomment-376423720:3011,Testability,test,test,3011,1haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-9%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <0%> (-22.807%)` | `0% <0%> (-2%)` | |; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L01pbmlDbHVzdGVyVXRpbHMuamF2YQ==) | `78.947% <0%> (-10.526%)` | `6% <0%> (-1%)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `89.394% <0%> (-3.03%)` | `61% <0%> (-2%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `61.94% <0%> (-2.985%)` | `30% <0%> (-3%)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `79.487% <0%> (-2.564%)` | `44% <0%> (ø)` | |; | [...te/hellbender/engine/spark/VariantWalkerSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvVmFyaWFudFdhbGtlclNwYXJrLmphdmE=) | `72.34% <0%> (-2.128%)` | `14% <0%> (ø)` | |; | ... and [2 more](https://codecov.io/gh/broadinstitute/gatk/pull/4590/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4590#issuecomment-376423720
https://github.com/broadinstitute/gatk/issues/4591#issuecomment-376503318:92,Integrability,depend,dependencies,92,"@aderzelle The CNN tool requires some Python packages - you may not have the correct set of dependencies. There are instructions [here](https://github.com/broadinstitute/gatk#requirements) on how to establish the required environment via the Conda package manager. See the 3rd bullet point in the ""Optional but recommended"" section. Hope that helps.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4591#issuecomment-376503318
https://github.com/broadinstitute/gatk/issues/4592#issuecomment-376591406:5,Availability,error,error,5,This error seems to be associated with the FeatureReader that reads VCFs off the GCS - any thoughts about how GenomicsDB should deal with this?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592#issuecomment-376591406
https://github.com/broadinstitute/gatk/issues/4592#issuecomment-376597795:243,Availability,error,error,243,"@kgururaj I think we just need to check the catch blocks in `GenomicsDBImport.getFeatureReadersInParallel()`, and get them to propagate the underlying exceptions properly (instead of just the top-level exception), so that we don't lose useful error messages like this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592#issuecomment-376597795
https://github.com/broadinstitute/gatk/issues/4592#issuecomment-376597795:249,Integrability,message,messages,249,"@kgururaj I think we just need to check the catch blocks in `GenomicsDBImport.getFeatureReadersInParallel()`, and get them to propagate the underlying exceptions properly (instead of just the top-level exception), so that we don't lose useful error messages like this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592#issuecomment-376597795
https://github.com/broadinstitute/gatk/issues/4592#issuecomment-580901350:54,Availability,error,error,54,"I don't feel strongly about this anymore, and now the error should be searchable from this ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4592#issuecomment-580901350
https://github.com/broadinstitute/gatk/pull/4593#issuecomment-376567190:1257,Usability,Simpl,SimpleKeyXsvFuncotationFactory,1257,fed7b20b77?src=pr&el=desc) will **decrease** coverage by `0.002%`.; > The diff coverage is `77.901%`. ```diff; @@ Coverage Diff @@; ## master #4593 +/- ##; ===============================================; - Coverage 79.839% 79.837% -0.002% ; - Complexity 16958 16995 +37 ; ===============================================; Files 1062 1063 +1 ; Lines 61680 61802 +122 ; Branches 9983 10008 +25 ; ===============================================; + Hits 49245 49341 +96 ; - Misses 8540 8556 +16 ; - Partials 3895 3905 +10; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4593?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [.../tools/funcotator/dataSources/DataSourceUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4593/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL0RhdGFTb3VyY2VVdGlscy5qYXZh) | `68.047% <0%> (-5.201%)` | `26 <0> (ø)` | |; | [...ataSources/xsv/SimpleKeyXsvFuncotationFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4593/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL3hzdi9TaW1wbGVLZXlYc3ZGdW5jb3RhdGlvbkZhY3RvcnkuamF2YQ==) | `87.5% <100%> (ø)` | `26 <2> (+1)` | :arrow_up: |; | [...ataSources/xsv/LocatableXsvFuncotationFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4593/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL3hzdi9Mb2NhdGFibGVYc3ZGdW5jb3RhdGlvbkZhY3RvcnkuamF2YQ==) | `83.871% <100%> (ø)` | `26 <2> (ø)` | :arrow_down: |; | [...ncotator/mafOutput/MafOutputRendererConstants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4593/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL21hZk91dHB1dC9NYWZPdXRwdXRSZW5kZXJlckNvbnN0YW50cy5qYXZh) | `98.98% <100%> (ø)` | `1 <0> (ø)` | :arrow_down: |; | [...r/dataSou,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4593#issuecomment-376567190
https://github.com/broadinstitute/gatk/issues/4596#issuecomment-379875734:152,Availability,error,error,152,"@davidbenjamin I also found there are two `preemptible_attempts = preemptible_attempts` lines in line 181 and 187. Having these lines wouldn't cause an error, but I just wanted to let you know.; https://github.com/broadinstitute/gatk/blob/b5720472fe025270e410379885cd58fed7b20b77/scripts/mutect2_wdl/mutect2.wdl#L181-L187",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4596#issuecomment-379875734
https://github.com/broadinstitute/gatk/pull/4602#issuecomment-379487429:2245,Usability,Simpl,SimpleNovelAdjacencyInterpreter,2245,ntReInterpreterSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4602/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL0NweFZhcmlhbnRSZUludGVycHJldGVyU3BhcmsuamF2YQ==) | `0% <0%> (ø)` | `0 <0> (?)` | |; | [.../DiscoverVariantsFromContigAlignmentsSAMSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4602/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvRGlzY292ZXJWYXJpYW50c0Zyb21Db250aWdBbGlnbm1lbnRzU0FNU3BhcmsuamF2YQ==) | `83.333% <100%> (+0.14%)` | `30 <0> (ø)` | :arrow_down: |; | [...s/spark/sv/discovery/SvDiscoveryInputMetaData.java](https://codecov.io/gh/broadinstitute/gatk/pull/4602/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvU3ZEaXNjb3ZlcnlJbnB1dE1ldGFEYXRhLmphdmE=) | `100% <100%> (ø)` | `7 <5> (+5)` | :arrow_up: |; | [...ery/inference/SimpleNovelAdjacencyInterpreter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4602/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1NpbXBsZU5vdmVsQWRqYWNlbmN5SW50ZXJwcmV0ZXIuamF2YQ==) | `74.667% <100%> (+0.694%)` | `11 <1> (ø)` | :arrow_down: |; | [.../sv/discovery/inference/CpxVariantInterpreter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4602/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL0NweFZhcmlhbnRJbnRlcnByZXRlci5qYXZh) | `68.382% <50%> (ø)` | `25 <1> (ø)` | :arrow_down: |; | [...iscoverFromLocalAssemblyContigAlignmentsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4602/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvU3ZEaXNjb3ZlckZyb21Mb2NhbEFzc2VtYmx5Q29udGlnQWxpZ25tZW50c1NwYXJrLmphdmE=) | `77.982% <86.364%> (+1.14%)` | `2 <2> (,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602#issuecomment-379487429
https://github.com/broadinstitute/gatk/pull/4602#issuecomment-386752928:26,Modifiability,refactor,refactored,26,"@mwalker174 Hi Mark, I've refactored the code significantly, would you take a look again? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602#issuecomment-386752928
https://github.com/broadinstitute/gatk/pull/4602#issuecomment-387119743:62,Testability,test,tests,62,@SHuang-Broad Thanks the code looks much better. Next step is tests then?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602#issuecomment-387119743
https://github.com/broadinstitute/gatk/pull/4602#issuecomment-387223011:41,Testability,test,testing,41,@mwalker174 ; absolutely. I'll begin the testing process. Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602#issuecomment-387223011
https://github.com/broadinstitute/gatk/pull/4602#issuecomment-389343644:123,Deployability,pipeline,pipeline,123,"@mwalker174 ; Hi Mark, I've finished writing the tests and would you please check again?; Here's the log running the whole pipeline (the number of simple variants extracted is approximately 1.5X the number of complex variants):. ```; .... below is output for complex variants only; 23:09:25.288 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 1334 variants.; 23:09:25.288 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 1334; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - INV: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - INS: 0; ..... below is output from this tool; 23:09:48.167 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 688 variants.; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - INV: 1; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 125; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - INS: 562; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 0; 23:09:48.215 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 1555 variants.; 23:09:48.216 INFO StructuralVariationDiscoveryPipelineSpark - INV: 21; 23:09:48.216 INFO StructuralVariati",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602#issuecomment-389343644
https://github.com/broadinstitute/gatk/pull/4602#issuecomment-389343644:49,Testability,test,tests,49,"@mwalker174 ; Hi Mark, I've finished writing the tests and would you please check again?; Here's the log running the whole pipeline (the number of simple variants extracted is approximately 1.5X the number of complex variants):. ```; .... below is output for complex variants only; 23:09:25.288 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 1334 variants.; 23:09:25.288 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 1334; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - INV: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - INS: 0; ..... below is output from this tool; 23:09:48.167 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 688 variants.; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - INV: 1; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 125; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - INS: 562; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 0; 23:09:48.215 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 1555 variants.; 23:09:48.216 INFO StructuralVariationDiscoveryPipelineSpark - INV: 21; 23:09:48.216 INFO StructuralVariati",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602#issuecomment-389343644
https://github.com/broadinstitute/gatk/pull/4602#issuecomment-389343644:101,Testability,log,log,101,"@mwalker174 ; Hi Mark, I've finished writing the tests and would you please check again?; Here's the log running the whole pipeline (the number of simple variants extracted is approximately 1.5X the number of complex variants):. ```; .... below is output for complex variants only; 23:09:25.288 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 1334 variants.; 23:09:25.288 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 1334; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - INV: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - INS: 0; ..... below is output from this tool; 23:09:48.167 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 688 variants.; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - INV: 1; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 125; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - INS: 562; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 0; 23:09:48.215 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 1555 variants.; 23:09:48.216 INFO StructuralVariationDiscoveryPipelineSpark - INV: 21; 23:09:48.216 INFO StructuralVariati",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602#issuecomment-389343644
https://github.com/broadinstitute/gatk/pull/4602#issuecomment-389343644:147,Usability,simpl,simple,147,"@mwalker174 ; Hi Mark, I've finished writing the tests and would you please check again?; Here's the log running the whole pipeline (the number of simple variants extracted is approximately 1.5X the number of complex variants):. ```; .... below is output for complex variants only; 23:09:25.288 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 1334 variants.; 23:09:25.288 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 1334; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - INV: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 0; 23:09:25.289 INFO StructuralVariationDiscoveryPipelineSpark - INS: 0; ..... below is output from this tool; 23:09:48.167 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 688 variants.; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - INV: 1; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 125; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - INS: 562; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 23:09:48.168 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 0; 23:09:48.215 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 1555 variants.; 23:09:48.216 INFO StructuralVariationDiscoveryPipelineSpark - INV: 21; 23:09:48.216 INFO StructuralVariati",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602#issuecomment-389343644
https://github.com/broadinstitute/gatk/pull/4602#issuecomment-391559827:181,Usability,simpl,simple,181,"@mwalker174 ; Thanks!. To answer you question about the END==POS insertion variants, . First quote the spec; > For precise variants, END is POS + length of REF allele - 1,. Now for simple insertions, the REF allele is a single base allele, which by the above definition forces be equal to POS. Second, if you look at the 4th variant (insertion) on page 11 of the spec version 4.2, END == POS. So I'm following the VCF spec. It's a little ambiguous as the spec doesn't give any example for replacements, i.e. some ref bases are replaced by other bases, so; * when the ref sequence being replaced is <50bp, I emit ""fat insertion"", as documented here. ; * when the replaced region is >49 bp, a DEL call is emitted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602#issuecomment-391559827
https://github.com/broadinstitute/gatk/issues/4603#issuecomment-376946968:591,Deployability,release,release,591,"@magicDGS The GATK versioning scheme is not related to the API -- it is targeted at end users rather than projects using GATK as a library. Here's a slide that explains it:. <img width=""824"" alt=""gatk_versioning"" src=""https://user-images.githubusercontent.com/798637/38042254-e5bb85a4-3281-11e8-8d83-017bb6b73fda.png"">. As the slide mentions, we have given some thought to supplementing the main version number with an ""API version number"", but we'd have to more clearly define what constitutes the official public API for the GATK before doing so. On a side note, now that we're in general release it may be easier for you to get PRs for things like new walker types merged into the GATK proper, particularly if they are fairly self-contained and don't involve refactoring lots of engine classes. I was planning to ask whether you wanted to resurrect your `SlidingWindowWalker` PR at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4603#issuecomment-376946968
https://github.com/broadinstitute/gatk/issues/4603#issuecomment-376946968:762,Modifiability,refactor,refactoring,762,"@magicDGS The GATK versioning scheme is not related to the API -- it is targeted at end users rather than projects using GATK as a library. Here's a slide that explains it:. <img width=""824"" alt=""gatk_versioning"" src=""https://user-images.githubusercontent.com/798637/38042254-e5bb85a4-3281-11e8-8d83-017bb6b73fda.png"">. As the slide mentions, we have given some thought to supplementing the main version number with an ""API version number"", but we'd have to more clearly define what constitutes the official public API for the GATK before doing so. On a side note, now that we're in general release it may be easier for you to get PRs for things like new walker types merged into the GATK proper, particularly if they are fairly self-contained and don't involve refactoring lots of engine classes. I was planning to ask whether you wanted to resurrect your `SlidingWindowWalker` PR at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4603#issuecomment-376946968
https://github.com/broadinstitute/gatk/issues/4603#issuecomment-376946968:463,Usability,clear,clearly,463,"@magicDGS The GATK versioning scheme is not related to the API -- it is targeted at end users rather than projects using GATK as a library. Here's a slide that explains it:. <img width=""824"" alt=""gatk_versioning"" src=""https://user-images.githubusercontent.com/798637/38042254-e5bb85a4-3281-11e8-8d83-017bb6b73fda.png"">. As the slide mentions, we have given some thought to supplementing the main version number with an ""API version number"", but we'd have to more clearly define what constitutes the official public API for the GATK before doing so. On a side note, now that we're in general release it may be easier for you to get PRs for things like new walker types merged into the GATK proper, particularly if they are fairly self-contained and don't involve refactoring lots of engine classes. I was planning to ask whether you wanted to resurrect your `SlidingWindowWalker` PR at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4603#issuecomment-376946968
https://github.com/broadinstitute/gatk/issues/4604#issuecomment-376905093:99,Deployability,integrat,integrated,99,"@jamesemery I think this is because the annotation plugin, which has the pedigree arg, hasn't been integrated with the tools yet (second part of https://github.com/broadinstitute/gatk/issues/3287) ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4604#issuecomment-376905093
https://github.com/broadinstitute/gatk/issues/4604#issuecomment-376905093:99,Integrability,integrat,integrated,99,"@jamesemery I think this is because the annotation plugin, which has the pedigree arg, hasn't been integrated with the tools yet (second part of https://github.com/broadinstitute/gatk/issues/3287) ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4604#issuecomment-376905093
https://github.com/broadinstitute/gatk/issues/4604#issuecomment-376905093:51,Modifiability,plugin,plugin,51,"@jamesemery I think this is because the annotation plugin, which has the pedigree arg, hasn't been integrated with the tools yet (second part of https://github.com/broadinstitute/gatk/issues/3287) ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4604#issuecomment-376905093
https://github.com/broadinstitute/gatk/issues/4604#issuecomment-376909187:89,Deployability,integrat,integrated,89,@cmnbroad That is correct. Once upon a time it was waiting on the VariantAnnotator to be integrated before advancing on it but now its being held in place by newtons first law. I will take a look at updating the branch.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4604#issuecomment-376909187
https://github.com/broadinstitute/gatk/issues/4604#issuecomment-376909187:89,Integrability,integrat,integrated,89,@cmnbroad That is correct. Once upon a time it was waiting on the VariantAnnotator to be integrated before advancing on it but now its being held in place by newtons first law. I will take a look at updating the branch.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4604#issuecomment-376909187
https://github.com/broadinstitute/gatk/pull/4608#issuecomment-377048280:1293,Usability,Simpl,SimpleKeyXsvFuncotationFactory,1293,crease** coverage by `0.093%`.; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## master #4608 +/- ##; ===============================================; + Coverage 79.853% 79.946% +0.093% ; - Complexity 17052 17237 +185 ; ===============================================; Files 1067 1067 ; Lines 62029 62497 +468 ; Branches 10037 10157 +120 ; ===============================================; + Hits 49532 49964 +432 ; - Misses 8585 8597 +12 ; - Partials 3912 3936 +24; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4608?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...dataSources/gencode/GencodeFuncotationFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4608/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL2dlbmNvZGUvR2VuY29kZUZ1bmNvdGF0aW9uRmFjdG9yeS5qYXZh) | `81.34% <100%> (+0.615%)` | `144 <0> (+4)` | :arrow_up: |; | [...ataSources/xsv/SimpleKeyXsvFuncotationFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4608/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL3hzdi9TaW1wbGVLZXlYc3ZGdW5jb3RhdGlvbkZhY3RvcnkuamF2YQ==) | `88.722% <0%> (+1.222%)` | `47% <0%> (+21%)` | :arrow_up: |; | [...org/broadinstitute/hellbender/engine/GATKTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/4608/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvR0FUS1Rvb2wuamF2YQ==) | `92.705% <0%> (+1.401%)` | `186% <0%> (+93%)` | :arrow_up: |; | [...tools/funcotator/DataSourceFuncotationFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4608/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0RhdGFTb3VyY2VGdW5jb3RhdGlvbkZhY3RvcnkuamF2YQ==) | `91.837% <0%> (+1.837%)` | `22% <0%> (+5%)` | :arrow_up: |; | [...ools/funcotator/FuncotatorArgumentDefinitions.java](htt,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4608#issuecomment-377048280
https://github.com/broadinstitute/gatk/issues/4610#issuecomment-377297997:141,Energy Efficiency,reduce,reduce,141,"This was an oversight on our part that we'll fix. You should also note, however, that we have a branch coming soon that will *significantly* reduce the size of the main GATK docker image (by several GB).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4610#issuecomment-377297997
https://github.com/broadinstitute/gatk/issues/4612#issuecomment-404327557:316,Deployability,pipeline,pipeline,316,"@AxVE Sorry, I totally missed this ticket. BWA-Spark requires a paired-ended file to be queryname sorted or grouped. It sounds like it's missing that check upfront. . We've improved SortReadFileSpark now. It's now called SortSamSpark and can sort into query-name order. . Doing the fastq -> sam as part of the spark pipeline would be a great addition, but we currently don't have a great way to read fastq files into spark so it's not a trivial addition.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4612#issuecomment-404327557
https://github.com/broadinstitute/gatk/issues/4612#issuecomment-405072736:232,Deployability,integrat,integrated,232,"For the FASTQ input in Hadoop (and thus, Spark), there is a small library called [FastDoop](https://www.ncbi.nlm.nih.gov/pubmed/28093410) that I found time ago. Today it looks like the page is not working, but this could be (maybe) integrated into Hadoop-BAM/disq and it can help reading FASTQ from Spark",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4612#issuecomment-405072736
https://github.com/broadinstitute/gatk/issues/4612#issuecomment-405072736:232,Integrability,integrat,integrated,232,"For the FASTQ input in Hadoop (and thus, Spark), there is a small library called [FastDoop](https://www.ncbi.nlm.nih.gov/pubmed/28093410) that I found time ago. Today it looks like the page is not working, but this could be (maybe) integrated into Hadoop-BAM/disq and it can help reading FASTQ from Spark",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4612#issuecomment-405072736
https://github.com/broadinstitute/gatk/issues/4612#issuecomment-405114805:109,Integrability,wrap,wrap,109,"In ADAM, we have well maintained Hadoop InputFormats for both normal and interleaved FASTQ. Additionally, we wrap these InputFormats in Spark-friendly APIs (exposed in Scala, Java, Python, and R) that add validation and standard transformations. GATK already has a dependency on ADAM, so...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4612#issuecomment-405114805
https://github.com/broadinstitute/gatk/issues/4612#issuecomment-405114805:265,Integrability,depend,dependency,265,"In ADAM, we have well maintained Hadoop InputFormats for both normal and interleaved FASTQ. Additionally, we wrap these InputFormats in Spark-friendly APIs (exposed in Scala, Java, Python, and R) that add validation and standard transformations. GATK already has a dependency on ADAM, so...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4612#issuecomment-405114805
https://github.com/broadinstitute/gatk/issues/4612#issuecomment-405114805:157,Security,expose,exposed,157,"In ADAM, we have well maintained Hadoop InputFormats for both normal and interleaved FASTQ. Additionally, we wrap these InputFormats in Spark-friendly APIs (exposed in Scala, Java, Python, and R) that add validation and standard transformations. GATK already has a dependency on ADAM, so...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4612#issuecomment-405114805
https://github.com/broadinstitute/gatk/issues/4612#issuecomment-405114805:205,Security,validat,validation,205,"In ADAM, we have well maintained Hadoop InputFormats for both normal and interleaved FASTQ. Additionally, we wrap these InputFormats in Spark-friendly APIs (exposed in Scala, Java, Python, and R) that add validation and standard transformations. GATK already has a dependency on ADAM, so...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4612#issuecomment-405114805
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377359265:172,Deployability,integrat,integration,172,@lbergelson We were just talking about this. Thanks to @skwalker's HaplotypeCaller tie out I think it's close. Question for the engine team: this is going to break several integration tests. What would you suggest doing?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377359265
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377359265:172,Integrability,integrat,integration,172,@lbergelson We were just talking about this. Thanks to @skwalker's HaplotypeCaller tie out I think it's close. Question for the engine team: this is going to break several integration tests. What would you suggest doing?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377359265
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377359265:184,Testability,test,tests,184,@lbergelson We were just talking about this. Thanks to @skwalker's HaplotypeCaller tie out I think it's close. Question for the engine team: this is going to break several integration tests. What would you suggest doing?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377359265
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377526021:334,Deployability,release,release,334,"The other issue is that making newQual the default is going to introduce batch effects. @vdauwera Can correct me if I'm wrong, but the party line used to be ""use the same version for all of your samples or we're not responsible"". I do want to do it, I just don't know how we make it as obvious as possible. Would this justify a minor release? Or we could put one of those scary red warnings in the GenotypeGVCFs log for a few releases?. @davidbenjamin is still going to look into one last spanning deletion issue and we'll run a bunch of samples through HaplotypeCaller (since they share the same AlleleFrequencyCalculator) to make sure nothing chokes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377526021
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377526021:426,Deployability,release,releases,426,"The other issue is that making newQual the default is going to introduce batch effects. @vdauwera Can correct me if I'm wrong, but the party line used to be ""use the same version for all of your samples or we're not responsible"". I do want to do it, I just don't know how we make it as obvious as possible. Would this justify a minor release? Or we could put one of those scary red warnings in the GenotypeGVCFs log for a few releases?. @davidbenjamin is still going to look into one last spanning deletion issue and we'll run a bunch of samples through HaplotypeCaller (since they share the same AlleleFrequencyCalculator) to make sure nothing chokes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377526021
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377526021:412,Testability,log,log,412,"The other issue is that making newQual the default is going to introduce batch effects. @vdauwera Can correct me if I'm wrong, but the party line used to be ""use the same version for all of your samples or we're not responsible"". I do want to do it, I just don't know how we make it as obvious as possible. Would this justify a minor release? Or we could put one of those scary red warnings in the GenotypeGVCFs log for a few releases?. @davidbenjamin is still going to look into one last spanning deletion issue and we'll run a bunch of samples through HaplotypeCaller (since they share the same AlleleFrequencyCalculator) to make sure nothing chokes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377526021
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377549195:86,Deployability,release,release,86,"@davidbenjamin I do think we should throw this switch soon, perhaps even for the next release, provided we keep the old code path intact for now. There are two things that could affect the exact timing of the PR:. 1. I have a major `HaplotypeCaller` testing branch still in flight that I'd prefer to get merged before we throw the switch. I'd expect this to hit master sometime next week. 2. There has been some talk of a supplemental evaluation to the palantir HC tie-out for the exomes on the cloud pipeline. I defer to @ldgauthier as to whether this change should wait until after that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377549195
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377549195:501,Deployability,pipeline,pipeline,501,"@davidbenjamin I do think we should throw this switch soon, perhaps even for the next release, provided we keep the old code path intact for now. There are two things that could affect the exact timing of the PR:. 1. I have a major `HaplotypeCaller` testing branch still in flight that I'd prefer to get merged before we throw the switch. I'd expect this to hit master sometime next week. 2. There has been some talk of a supplemental evaluation to the palantir HC tie-out for the exomes on the cloud pipeline. I defer to @ldgauthier as to whether this change should wait until after that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377549195
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377549195:250,Testability,test,testing,250,"@davidbenjamin I do think we should throw this switch soon, perhaps even for the next release, provided we keep the old code path intact for now. There are two things that could affect the exact timing of the PR:. 1. I have a major `HaplotypeCaller` testing branch still in flight that I'd prefer to get merged before we throw the switch. I'd expect this to hit master sometime next week. 2. There has been some talk of a supplemental evaluation to the palantir HC tie-out for the exomes on the cloud pipeline. I defer to @ldgauthier as to whether this change should wait until after that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377549195
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377566539:239,Testability,test,test,239,"If we keep the old code in tact as you propose then the first phase could be to swap the default from old to new and make `-oldQual` an argument, then it won't affect any potential HC tie-outs. David B. is going to do the 1000 exome scale test -- is that what you were thinking of as the supplemental evaluation? Or do we still need something for FE?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-377566539
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380136134:0,Deployability,Update,Update,0,"Update: the 1000 (okay, 856 to be precise) exomes test passed without a hiccup. After David R's big HC test PR goes in I will address this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380136134
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380136134:50,Testability,test,test,50,"Update: the 1000 (okay, 856 to be precise) exomes test passed without a hiccup. After David R's big HC test PR goes in I will address this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380136134
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380136134:103,Testability,test,test,103,"Update: the 1000 (okay, 856 to be precise) exomes test passed without a hiccup. After David R's big HC test PR goes in I will address this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380136134
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380785661:336,Deployability,pipeline,pipeline,336,"Thanks for the additional info! To clarify, though, was any validation done on the GVCF outputs themselves to check concordance vs. GATK3? If not, can I suggest that we do such a validation? Being able to say that we have 856 validated runs of the GATK4 HC on exome data would greatly bolster the case for including it in the new exome pipeline.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380785661
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380785661:60,Security,validat,validation,60,"Thanks for the additional info! To clarify, though, was any validation done on the GVCF outputs themselves to check concordance vs. GATK3? If not, can I suggest that we do such a validation? Being able to say that we have 856 validated runs of the GATK4 HC on exome data would greatly bolster the case for including it in the new exome pipeline.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380785661
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380785661:179,Security,validat,validation,179,"Thanks for the additional info! To clarify, though, was any validation done on the GVCF outputs themselves to check concordance vs. GATK3? If not, can I suggest that we do such a validation? Being able to say that we have 856 validated runs of the GATK4 HC on exome data would greatly bolster the case for including it in the new exome pipeline.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380785661
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380785661:226,Security,validat,validated,226,"Thanks for the additional info! To clarify, though, was any validation done on the GVCF outputs themselves to check concordance vs. GATK3? If not, can I suggest that we do such a validation? Being able to say that we have 856 validated runs of the GATK4 HC on exome data would greatly bolster the case for including it in the new exome pipeline.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380785661
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380786232:178,Availability,down,downsampling,178,"Also @davidbenjamin: would you mind sending me the 5 exomes with the longest runtimes? I want to do a few experiments to check runtime vs. GATK3 on those samples, and see if the downsampling needs to be made more aggressive in 4.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380786232
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981:101,Availability,robust,robustness,101,No validation here. I was satisfied with the validation from the Palantir report and using this as a robustness test to show that GATK4 HC isn't going to fall over. I have a matched list of GVCFs here: /humgen/gsa-hpprojects/dev/gauthier/scratch/newQualHC/check.list @skwalker could you adapt your analysis to run with this list? I'll need to give you a different jar for the GenotypeGVCFs step on my GVCFs since the annotation format is outdated.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981:287,Energy Efficiency,adapt,adapt,287,No validation here. I was satisfied with the validation from the Palantir report and using this as a robustness test to show that GATK4 HC isn't going to fall over. I have a matched list of GVCFs here: /humgen/gsa-hpprojects/dev/gauthier/scratch/newQualHC/check.list @skwalker could you adapt your analysis to run with this list? I'll need to give you a different jar for the GenotypeGVCFs step on my GVCFs since the annotation format is outdated.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981:287,Modifiability,adapt,adapt,287,No validation here. I was satisfied with the validation from the Palantir report and using this as a robustness test to show that GATK4 HC isn't going to fall over. I have a matched list of GVCFs here: /humgen/gsa-hpprojects/dev/gauthier/scratch/newQualHC/check.list @skwalker could you adapt your analysis to run with this list? I'll need to give you a different jar for the GenotypeGVCFs step on my GVCFs since the annotation format is outdated.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981:3,Security,validat,validation,3,No validation here. I was satisfied with the validation from the Palantir report and using this as a robustness test to show that GATK4 HC isn't going to fall over. I have a matched list of GVCFs here: /humgen/gsa-hpprojects/dev/gauthier/scratch/newQualHC/check.list @skwalker could you adapt your analysis to run with this list? I'll need to give you a different jar for the GenotypeGVCFs step on my GVCFs since the annotation format is outdated.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981:45,Security,validat,validation,45,No validation here. I was satisfied with the validation from the Palantir report and using this as a robustness test to show that GATK4 HC isn't going to fall over. I have a matched list of GVCFs here: /humgen/gsa-hpprojects/dev/gauthier/scratch/newQualHC/check.list @skwalker could you adapt your analysis to run with this list? I'll need to give you a different jar for the GenotypeGVCFs step on my GVCFs since the annotation format is outdated.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981:112,Testability,test,test,112,No validation here. I was satisfied with the validation from the Palantir report and using this as a robustness test to show that GATK4 HC isn't going to fall over. I have a matched list of GVCFs here: /humgen/gsa-hpprojects/dev/gauthier/scratch/newQualHC/check.list @skwalker could you adapt your analysis to run with this list? I'll need to give you a different jar for the GenotypeGVCFs step on my GVCFs since the annotation format is outdated.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-380822981
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-381170947:44,Energy Efficiency,adapt,adapted,44,I imagine that @skwalker's scripts could be adapted for the task -- I'll try to set up a meeting with her next week to discuss.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-381170947
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-381170947:44,Modifiability,adapt,adapted,44,I imagine that @skwalker's scripts could be adapted for the task -- I'll try to set up a meeting with her next week to discuss.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-381170947
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-385440763:128,Deployability,pipeline,pipeline,128,@ldgauthier Are these results sufficient to convince you to use the GATK4 `HaplotypeCaller` in the upcoming exomes on the cloud pipeline? Is it worth setting up a meeting later this week to go over @skwalker 's results in further detail?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-385440763
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-385668755:127,Availability,echo,echoing,127,"I still want @davidbenjamin to double check the * handling in his code (i.e. we shouldn't count it as a real allele since it's echoing an upstream event that's already been evaluated -- otherwise we could have a lot of high quality * only variants where the triggering SNP is too low quality, which would be silly). @skwalker I would like to do one more thing to check on the allele-specific annotations. Can you genotype all David B.'s GATK4 GVCFs together using GenomicsDBImport? We can use this to test #3707 on a larger callset and then compare the AS vs. non-AS output at biallelic sites.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-385668755
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-385668755:501,Testability,test,test,501,"I still want @davidbenjamin to double check the * handling in his code (i.e. we shouldn't count it as a real allele since it's echoing an upstream event that's already been evaluated -- otherwise we could have a lot of high quality * only variants where the triggering SNP is too low quality, which would be silly). @skwalker I would like to do one more thing to check on the allele-specific annotations. Can you genotype all David B.'s GATK4 GVCFs together using GenomicsDBImport? We can use this to test #3707 on a larger callset and then compare the AS vs. non-AS output at biallelic sites.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-385668755
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-393155293:254,Availability,down,down,254,"Thanks for the plots @skwalker ! To clarify, this is the AS- vs non-AS- annotations from the same callset at biallelic sites, right? I'm not surprised there are some differences, but it's hard to tell how significant they are at full alpha. Can you turn down the alpha on the geom_point or add some contours?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-393155293
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-393211539:103,Energy Efficiency,green,green,103,"#4801 addressed the spanning deletion issue with new qual, so I'm ready to go ahead and do this once a green light is given. @droazen you wanted to wait for your HC tests branch to go in, right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-393211539
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-393211539:165,Testability,test,tests,165,"#4801 addressed the spanning deletion issue with new qual, so I'm ready to go ahead and do this once a green light is given. @droazen you wanted to wait for your HC tests branch to go in, right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-393211539
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-393219775:32,Deployability,RELEASE,RELEASE,32,...at which point we will write RELEASE NOTES with warnings about batch effects in BIG LETTERS!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-393219775
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-393280404:172,Deployability,release,release,172,"@davidbenjamin Yes, that's correct -- the HC testing branch is coming soon, and I think we should be able to get both that branch and the switch to newQual in for the next release (assuming that there are no outstanding objections from anyone else on the switch).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-393280404
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-393280404:45,Testability,test,testing,45,"@davidbenjamin Yes, that's correct -- the HC testing branch is coming soon, and I think we should be able to get both that branch and the switch to newQual in for the next release (assuming that there are no outstanding objections from anyone else on the switch).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-393280404
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-393584192:3124,Usability,Clear,Clearly,3124,"lues for a even-length list (see Utils::getMedianValue). So the non-AS rank sums end up getting the better of the two values, which aren't even derived from the data they're supposed to represent. Alleles for the win! (The values are slightly different because the AS values get `floor`ed to the nearest tenth as part of the annotation.). Raw data for QD(34.34) vs AS_QD (2.83):; `X 13681128 . G A,C,<NON_REF> . . AS_RAW_BaseQRankSum=|-1.900,1,-1.800,1||;AS_RAW_MQRankSum=|-5.800,1,0.300,1||;AS_RAW_ReadPosRankSum=|1.000,1,1.200,1||;AS_SB_TABLE=99,68|2,2|35,20|0,0;; BaseQRankSum=-1.783e+00;MQRankSum=0.302;RAW_MQ=800193.00;ReadPosRankSum=1.29`; Genotypes with alt reads here are:; `GT:AD:PL`; `./.:53,2,0,0:0,116,1821,159,1827,1870,159,1827,1870,1870; ./.:0,0,55,0:1947,1947,1947,165,165,0,1947,1947,165,1947; ./.:114,2,0,0:0,299,3972,342,3978,4021,342,3978,4021,4021`; In the genotyped VC the A gets dropped and C gets AC=2 (one homVar). Again, we're seeing representation of the dropped alt only in homRef samples. The non-AS rank sum behavior here sucks too, but we're after QD. Unfortunately, most of the work for QD gets done inside GenotypeGVCFs, so I had to debug. AS_QD uses the per-allele probability of AC=0 (so it allows for AC>0 for other alts), which is the same as the probability of all samples being homRef if there's only one alt. In cases like this where there's another alt with some weight in the PLs (however small) then those probabilities will not be the same. Honestly I'm surprised by how different they are -- -188.9 for the log10(P(all homRef)) vs -15.55 for the log10(P(high quality alt AC = 0)). In summary, these data show that the discrepancies in a small subset of cases are expected because biallelics will be the same, but those discrepant sites are not true biallelics. I will also look into the AS_QD calc for multi-allelics. Clearly from the cloud around 30 the AS_QD is maxing out in some cases. This is my opportunity to fix that and finally remove the jitter.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-393584192
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-395427599:158,Usability,learn,learn,158,"This difference of 20 makes sense theoretically. The old qual always has a heterozygosity prior of 1 in 1000, whereas new qual starts there but is willing to learn a different allele frequency. Very hand-wavingly, a difference of 20 or so means that new qual has learned an allele frequency in the ballpark of 1 in 10 (since log_10 1/10 = log_10 1/1000 + 2 and 2 --> 20 in phred scale). So basically, they only squeak by the old threshold because new qual has learned the *artifact* allele frequency as a true allele frequency. :thumbsup: for `stand-call-conf 30`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-395427599
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-395427599:263,Usability,learn,learned,263,"This difference of 20 makes sense theoretically. The old qual always has a heterozygosity prior of 1 in 1000, whereas new qual starts there but is willing to learn a different allele frequency. Very hand-wavingly, a difference of 20 or so means that new qual has learned an allele frequency in the ballpark of 1 in 10 (since log_10 1/10 = log_10 1/1000 + 2 and 2 --> 20 in phred scale). So basically, they only squeak by the old threshold because new qual has learned the *artifact* allele frequency as a true allele frequency. :thumbsup: for `stand-call-conf 30`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-395427599
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-395427599:460,Usability,learn,learned,460,"This difference of 20 makes sense theoretically. The old qual always has a heterozygosity prior of 1 in 1000, whereas new qual starts there but is willing to learn a different allele frequency. Very hand-wavingly, a difference of 20 or so means that new qual has learned an allele frequency in the ballpark of 1 in 10 (since log_10 1/10 = log_10 1/1000 + 2 and 2 --> 20 in phred scale). So basically, they only squeak by the old threshold because new qual has learned the *artifact* allele frequency as a true allele frequency. :thumbsup: for `stand-call-conf 30`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-395427599
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-411142016:187,Testability,test,test,187,"Hi @davidbenjamin ,. Here's the combined VCFs from running GenotypeGVCFs on just the first site (1:148004722) with and without the new qual argument:; ```; /dsde/data/skwalker/qual_stuff/test/new_qual_1_148004722.vcf; /dsde/data/skwalker/qual_stuff/test/old_qual_1_148004722.vcf; ```; and the results I get (slightly different from above):. CHROM | POS | ALT | REF | QUAL.old | QUAL.new | QD.old | QD.new | AS_QD.old | AS_QD.new; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 1 | 148004722 | G,T | C | 6968.13 | 6986 | 13.72 | 13.72 | 10.48,8.03 | 25.36,0.49. And the combined VCFs from running GenotypeGVCFs on just the second site (1:104297205) with and without the new qual argument:. ```; /dsde/data/skwalker/qual_stuff/test/new_qual_1_104297205.vcf; /dsde/data/skwalker/qual_stuff/test/old_qual_1_104297205.vcf; ``` ; and the results I get (again slightly different from above):. CHROM | POS | ALT | REF | QUAL.old | QUAL.new | QD.old | QD.new | AS_QD.old | AS_QD.new; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 1 | 104297205 | G | C | 333.16 | 354.06 | 1.47 | 1.56 | 1.47 | 25.36",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-411142016
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-411142016:249,Testability,test,test,249,"Hi @davidbenjamin ,. Here's the combined VCFs from running GenotypeGVCFs on just the first site (1:148004722) with and without the new qual argument:; ```; /dsde/data/skwalker/qual_stuff/test/new_qual_1_148004722.vcf; /dsde/data/skwalker/qual_stuff/test/old_qual_1_148004722.vcf; ```; and the results I get (slightly different from above):. CHROM | POS | ALT | REF | QUAL.old | QUAL.new | QD.old | QD.new | AS_QD.old | AS_QD.new; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 1 | 148004722 | G,T | C | 6968.13 | 6986 | 13.72 | 13.72 | 10.48,8.03 | 25.36,0.49. And the combined VCFs from running GenotypeGVCFs on just the second site (1:104297205) with and without the new qual argument:. ```; /dsde/data/skwalker/qual_stuff/test/new_qual_1_104297205.vcf; /dsde/data/skwalker/qual_stuff/test/old_qual_1_104297205.vcf; ``` ; and the results I get (again slightly different from above):. CHROM | POS | ALT | REF | QUAL.old | QUAL.new | QD.old | QD.new | AS_QD.old | AS_QD.new; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 1 | 104297205 | G | C | 333.16 | 354.06 | 1.47 | 1.56 | 1.47 | 25.36",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-411142016
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-411142016:727,Testability,test,test,727,"Hi @davidbenjamin ,. Here's the combined VCFs from running GenotypeGVCFs on just the first site (1:148004722) with and without the new qual argument:; ```; /dsde/data/skwalker/qual_stuff/test/new_qual_1_148004722.vcf; /dsde/data/skwalker/qual_stuff/test/old_qual_1_148004722.vcf; ```; and the results I get (slightly different from above):. CHROM | POS | ALT | REF | QUAL.old | QUAL.new | QD.old | QD.new | AS_QD.old | AS_QD.new; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 1 | 148004722 | G,T | C | 6968.13 | 6986 | 13.72 | 13.72 | 10.48,8.03 | 25.36,0.49. And the combined VCFs from running GenotypeGVCFs on just the second site (1:104297205) with and without the new qual argument:. ```; /dsde/data/skwalker/qual_stuff/test/new_qual_1_104297205.vcf; /dsde/data/skwalker/qual_stuff/test/old_qual_1_104297205.vcf; ``` ; and the results I get (again slightly different from above):. CHROM | POS | ALT | REF | QUAL.old | QUAL.new | QD.old | QD.new | AS_QD.old | AS_QD.new; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 1 | 104297205 | G | C | 333.16 | 354.06 | 1.47 | 1.56 | 1.47 | 25.36",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-411142016
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-411142016:789,Testability,test,test,789,"Hi @davidbenjamin ,. Here's the combined VCFs from running GenotypeGVCFs on just the first site (1:148004722) with and without the new qual argument:; ```; /dsde/data/skwalker/qual_stuff/test/new_qual_1_148004722.vcf; /dsde/data/skwalker/qual_stuff/test/old_qual_1_148004722.vcf; ```; and the results I get (slightly different from above):. CHROM | POS | ALT | REF | QUAL.old | QUAL.new | QD.old | QD.new | AS_QD.old | AS_QD.new; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 1 | 148004722 | G,T | C | 6968.13 | 6986 | 13.72 | 13.72 | 10.48,8.03 | 25.36,0.49. And the combined VCFs from running GenotypeGVCFs on just the second site (1:104297205) with and without the new qual argument:. ```; /dsde/data/skwalker/qual_stuff/test/new_qual_1_104297205.vcf; /dsde/data/skwalker/qual_stuff/test/old_qual_1_104297205.vcf; ``` ; and the results I get (again slightly different from above):. CHROM | POS | ALT | REF | QUAL.old | QUAL.new | QD.old | QD.new | AS_QD.old | AS_QD.new; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 1 | 104297205 | G | C | 333.16 | 354.06 | 1.47 | 1.56 | 1.47 | 25.36",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-411142016
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-411181357:124,Testability,log,log,124,"The too-high `AS_QD = 25.36` in both cases is coming from a line in new qual where if finite numerical precision leads to a log probability greater than 0, we set the allele-specific qual to be infinite. Then in the `AS_QD` code, this infinity is replace by 30 + jitter = 25.36, as Laura suspected. ; That can't be to hard to fix. I still need to figure out the second case where new qual's AS_QD seems low.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-411181357
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434735854:123,Testability,log,log,123,I have a branch that fixes the `AS_QD` for both of those sites. @ldgauthier It *did* turn out to be numerical stability in log space. @skwalker Could you re-run with `/dsde/working/davidben/new-qual-october-2018/new-qual-10-30-2018.jar`?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434735854
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434761217:346,Testability,log,log,346,"Good news. (Probably?) How do we know there aren't any other numerical; instability issues lurking?. On Wed, Oct 31, 2018 at 11:46 AM David Benjamin <notifications@github.com>; wrote:. > I have a branch that fixes the AS_QD for both of those sites. @ldgauthier; > <https://github.com/ldgauthier> It *did* turn out to be numerical; > stability in log space. @skwalker <https://github.com/skwalker> Could you; > re-run with; > /dsde/working/davidben/new-qual-october-2018/new-qual-10-30-2018.jar?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434735854>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdJEnlX6B-icEDNsVSE9W5arHa6IAks5uqcXOgaJpZM4TA5nv>; > .; >. -- ; Laura Doyle Gauthier, Ph.D.; Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434761217
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434769078:316,Safety,avoid,avoiding,316,"I have a good feeling about numerical instability from this point forward because:. * My terminology was lazy. It's not really ""numerical instability,"" which is a deep and frightening topic, but rather just plain old finite precision, which is not nearly so hydra-headed a problem.; * I learned the general rule for avoiding finite precision problems with a qual score, which is: always calculate probabilities of alleles being absent. Previously I was calculating the probability that samples had an allele and subtracting (in log space) that from 1. The problem with that is that for very good GQs this probability is so closed to 1 that quals can become infinite. In this PR we add up the probabilities of genotypes that don't have the allele, which is small but non-zero and everything works fine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434769078
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434769078:528,Testability,log,log,528,"I have a good feeling about numerical instability from this point forward because:. * My terminology was lazy. It's not really ""numerical instability,"" which is a deep and frightening topic, but rather just plain old finite precision, which is not nearly so hydra-headed a problem.; * I learned the general rule for avoiding finite precision problems with a qual score, which is: always calculate probabilities of alleles being absent. Previously I was calculating the probability that samples had an allele and subtracting (in log space) that from 1. The problem with that is that for very good GQs this probability is so closed to 1 that quals can become infinite. In this PR we add up the probabilities of genotypes that don't have the allele, which is small but non-zero and everything works fine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434769078
https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434769078:287,Usability,learn,learned,287,"I have a good feeling about numerical instability from this point forward because:. * My terminology was lazy. It's not really ""numerical instability,"" which is a deep and frightening topic, but rather just plain old finite precision, which is not nearly so hydra-headed a problem.; * I learned the general rule for avoiding finite precision problems with a qual score, which is: always calculate probabilities of alleles being absent. Previously I was calculating the probability that samples had an allele and subtracting (in log space) that from 1. The problem with that is that for very good GQs this probability is so closed to 1 that quals can become infinite. In this PR we add up the probabilities of genotypes that don't have the allele, which is small but non-zero and everything works fine.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4614#issuecomment-434769078
https://github.com/broadinstitute/gatk/issues/4618#issuecomment-404325991:56,Testability,test,tests,56,"We looked at this and it seems to be a problem with the tests, not the tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4618#issuecomment-404325991
https://github.com/broadinstitute/gatk/pull/4621#issuecomment-378027085:3674,Deployability,Integrat,IntegrationTestSpec,3674,java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9hbGxlbGVzcGVjaWZpYy9BU19TdHJhbmRCaWFzVGVzdC5qYXZh) | `86.179% <0%> (+0.813%)` | `44% <0%> (+1%)` | :arrow_up: |; | [...spark/sv/evidence/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `69.583% <0%> (+0.833%)` | `60% <0%> (+1%)` | :arrow_up: |; | [...utils/test/ReadsPreprocessingPipelineTestData.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1JlYWRzUHJlcHJvY2Vzc2luZ1BpcGVsaW5lVGVzdERhdGEuamF2YQ==) | `0.862% <0%> (+0.862%)` | `1% <0%> (+1%)` | :arrow_up: |; | [...kers/haplotypecaller/ReferenceConfidenceModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9SZWZlcmVuY2VDb25maWRlbmNlTW9kZWwuamF2YQ==) | `93.194% <0%> (+1.047%)` | `61% <0%> (+2%)` | :arrow_up: |; | [...ute/hellbender/utils/test/IntegrationTestSpec.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0ludGVncmF0aW9uVGVzdFNwZWMuamF2YQ==) | `73.118% <0%> (+1.075%)` | `25% <0%> (ø)` | :arrow_down: |; | [...e/hellbender/tools/spark/sv/utils/SVFileUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi91dGlscy9TVkZpbGVVdGlscy5qYXZh) | `29.67% <0%> (+1.099%)` | `7% <0%> (ø)` | :arrow_down: |; | ... and [506 more](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4621#issuecomment-378027085
https://github.com/broadinstitute/gatk/pull/4621#issuecomment-378027085:3674,Integrability,Integrat,IntegrationTestSpec,3674,java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9hbGxlbGVzcGVjaWZpYy9BU19TdHJhbmRCaWFzVGVzdC5qYXZh) | `86.179% <0%> (+0.813%)` | `44% <0%> (+1%)` | :arrow_up: |; | [...spark/sv/evidence/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `69.583% <0%> (+0.833%)` | `60% <0%> (+1%)` | :arrow_up: |; | [...utils/test/ReadsPreprocessingPipelineTestData.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1JlYWRzUHJlcHJvY2Vzc2luZ1BpcGVsaW5lVGVzdERhdGEuamF2YQ==) | `0.862% <0%> (+0.862%)` | `1% <0%> (+1%)` | :arrow_up: |; | [...kers/haplotypecaller/ReferenceConfidenceModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9SZWZlcmVuY2VDb25maWRlbmNlTW9kZWwuamF2YQ==) | `93.194% <0%> (+1.047%)` | `61% <0%> (+2%)` | :arrow_up: |; | [...ute/hellbender/utils/test/IntegrationTestSpec.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0ludGVncmF0aW9uVGVzdFNwZWMuamF2YQ==) | `73.118% <0%> (+1.075%)` | `25% <0%> (ø)` | :arrow_down: |; | [...e/hellbender/tools/spark/sv/utils/SVFileUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi91dGlscy9TVkZpbGVVdGlscy5qYXZh) | `29.67% <0%> (+1.099%)` | `7% <0%> (ø)` | :arrow_down: |; | ... and [506 more](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4621#issuecomment-378027085
https://github.com/broadinstitute/gatk/pull/4621#issuecomment-378027085:2990,Testability,test,test,2990,entInterval.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvYWxpZ25tZW50L0FsaWdubWVudEludGVydmFsLmphdmE=) | `92.803% <100%> (+54.167%)` | `83 <0> (+59)` | :arrow_up: |; | [...rs/annotator/allelespecific/AS\_StrandBiasTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9hbGxlbGVzcGVjaWZpYy9BU19TdHJhbmRCaWFzVGVzdC5qYXZh) | `86.179% <0%> (+0.813%)` | `44% <0%> (+1%)` | :arrow_up: |; | [...spark/sv/evidence/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `69.583% <0%> (+0.833%)` | `60% <0%> (+1%)` | :arrow_up: |; | [...utils/test/ReadsPreprocessingPipelineTestData.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1JlYWRzUHJlcHJvY2Vzc2luZ1BpcGVsaW5lVGVzdERhdGEuamF2YQ==) | `0.862% <0%> (+0.862%)` | `1% <0%> (+1%)` | :arrow_up: |; | [...kers/haplotypecaller/ReferenceConfidenceModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9SZWZlcmVuY2VDb25maWRlbmNlTW9kZWwuamF2YQ==) | `93.194% <0%> (+1.047%)` | `61% <0%> (+2%)` | :arrow_up: |; | [...ute/hellbender/utils/test/IntegrationTestSpec.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0ludGVncmF0aW9uVGVzdFNwZWMuamF2YQ==) | `73.118% <0%> (+1.075%)` | `25% <0%> (ø)` | :arrow_down: |; | [...e/hellbender/tools/spark/sv/utils/SVFileUtils.java](,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4621#issuecomment-378027085
https://github.com/broadinstitute/gatk/pull/4621#issuecomment-378027085:3669,Testability,test,test,3669,java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9hbGxlbGVzcGVjaWZpYy9BU19TdHJhbmRCaWFzVGVzdC5qYXZh) | `86.179% <0%> (+0.813%)` | `44% <0%> (+1%)` | :arrow_up: |; | [...spark/sv/evidence/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `69.583% <0%> (+0.833%)` | `60% <0%> (+1%)` | :arrow_up: |; | [...utils/test/ReadsPreprocessingPipelineTestData.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1JlYWRzUHJlcHJvY2Vzc2luZ1BpcGVsaW5lVGVzdERhdGEuamF2YQ==) | `0.862% <0%> (+0.862%)` | `1% <0%> (+1%)` | :arrow_up: |; | [...kers/haplotypecaller/ReferenceConfidenceModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9SZWZlcmVuY2VDb25maWRlbmNlTW9kZWwuamF2YQ==) | `93.194% <0%> (+1.047%)` | `61% <0%> (+2%)` | :arrow_up: |; | [...ute/hellbender/utils/test/IntegrationTestSpec.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0ludGVncmF0aW9uVGVzdFNwZWMuamF2YQ==) | `73.118% <0%> (+1.075%)` | `25% <0%> (ø)` | :arrow_down: |; | [...e/hellbender/tools/spark/sv/utils/SVFileUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi91dGlscy9TVkZpbGVVdGlscy5qYXZh) | `29.67% <0%> (+1.099%)` | `7% <0%> (ø)` | :arrow_down: |; | ... and [506 more](https://codecov.io/gh/broadinstitute/gatk/pull/4621/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4621#issuecomment-378027085
https://github.com/broadinstitute/gatk/pull/4622#issuecomment-378055518:33,Modifiability,refactor,refactoring,33,It looks like there's some minor refactoring in your new graph handler. I'm not a real stickler about sneaking that in but just want to check it was intentional.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4622#issuecomment-378055518
https://github.com/broadinstitute/gatk/pull/4622#issuecomment-378226435:190,Deployability,upgrade,upgrade,190,"Thanks, Mark. Yes, intentional. We moved a couple of methods that we more naturally methods on the assembled objects rather than statics in client code at Chris' suggestion. Part of the API upgrade, from my point of view.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4622#issuecomment-378226435
https://github.com/broadinstitute/gatk/issues/4626#issuecomment-378718128:564,Deployability,install,installed,564,"Thanks much for this. Unfortunately running as root isn't an option since external CWL runners (like bunny, Toil, cwltool and hopefully Cromwell soon) make the decisions about the username to use and will try to mirror the runner with the external user to match user permissions on the output files. Also people trust it more when you're not trying to run as root (hence, not wanting to mess with `/etc/passwd` in the Docker container for fix Spark as well). This was using the bcbio-vc Docker image (https://github.com/bcbio/bcbio_docker#docker-images) with gatk installed via bioconda, but I don't think is image specific unless you're specifically doing something in your images to work around the problem which is doesn't sound like. Is there any chance to tweak Spark to make it less picky/dependent on the user? I'm not enough of a Spark expert to know if this is work-aroundable in a reasonable way?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-378718128
https://github.com/broadinstitute/gatk/issues/4626#issuecomment-378718128:795,Integrability,depend,dependent,795,"Thanks much for this. Unfortunately running as root isn't an option since external CWL runners (like bunny, Toil, cwltool and hopefully Cromwell soon) make the decisions about the username to use and will try to mirror the runner with the external user to match user permissions on the output files. Also people trust it more when you're not trying to run as root (hence, not wanting to mess with `/etc/passwd` in the Docker container for fix Spark as well). This was using the bcbio-vc Docker image (https://github.com/bcbio/bcbio_docker#docker-images) with gatk installed via bioconda, but I don't think is image specific unless you're specifically doing something in your images to work around the problem which is doesn't sound like. Is there any chance to tweak Spark to make it less picky/dependent on the user? I'm not enough of a Spark expert to know if this is work-aroundable in a reasonable way?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-378718128
https://github.com/broadinstitute/gatk/issues/4626#issuecomment-379060749:615,Modifiability,config,configuring,615,"@chapmanb Do you have any control over the `docker run` command? If you do, you could mount `/etc/passwd`/`/etc/shadow` as external resources into the container (as described here, for example: `https://stackoverflow.com/questions/33013444/can-not-add-new-user-in-docker-container-with-mounted-etc-passwd-and-etc-shado`) . This seems slightly less awful than the workarounds you linked to above. It seems to me, however, that running as a user not present in `/etc/passwd` could cause quite a few things to fail, not just Spark. Perhaps you should file a bug report against these CWL runners? They should really be configuring the runtime environment in the container properly for the username they force you to run under! If they are trying to mirror the external user within the container, they should probably mount the external `passwd` file into the container on your behalf when executing `docker run`. @tomwhite Do you happen to know of a workaround on the Spark side to prevent it from doing a username lookup?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-379060749
https://github.com/broadinstitute/gatk/issues/4626#issuecomment-379226226:124,Modifiability,variab,variable,124,@chapmanb As far as I know Spark (and Hadoop) needs the user name to submit a job. Can you set the `SPARK_USER` environment variable to the user you would like to use (even if it is not in /etc/passwd)? You should be able to pass it to Docker with the `-e` option. (I haven't tried this to see if it works.),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-379226226
https://github.com/broadinstitute/gatk/issues/4626#issuecomment-380123018:220,Safety,avoid,avoid,220,"David, thanks for the CWL suggestion. As far as I know most CWL runners don't attempt to edit or mount the internal container `/etc/passwd` unfortunately. They do try to match with the external user outside of Docker to avoid file permission issues. Does Cromwell deal with this problem? We're actively looking to make more use of Cromwell for CWL runs. If we could make that happen that would resolve a lot of issues and I could leave my workaround for other non-conforming callers. Tom, that is a great suggestion and I thought would work as well but we do this (https://github.com/bcbio/bcbio-nextgen/blob/bd03e259877d410045468046a949f6b9724605c5/bcbio/broad/__init__.py#L152) and Spark/Hadoop still wants to look up the user in `/etc/passwd` even if missing. If there is a way to skip that being present I'm happy to tweak that as well. Thanks so much for all this discussion and suggestions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-380123018
https://github.com/broadinstitute/gatk/issues/4626#issuecomment-381642529:265,Availability,error,errors,265,"Thanks for the thoughts. Singularity is definitely awesome and I'm hoping to support it as an alternative choice to Docker for local HPC clusters where we won't require equivalent root permissions to run. So it helps avoid some of the potential external permission errors by creating a potentially cleaner path to running. Unfortunately it doesn't deal with the underlying issue of needing to map users inside of the containers so that Spark is happy with them. Having something more lightweight than needing user updates in the internal `/etc/passwd` would also help with potential issues on other container enginer (Singularity, rkt).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-381642529
https://github.com/broadinstitute/gatk/issues/4626#issuecomment-381642529:514,Deployability,update,updates,514,"Thanks for the thoughts. Singularity is definitely awesome and I'm hoping to support it as an alternative choice to Docker for local HPC clusters where we won't require equivalent root permissions to run. So it helps avoid some of the potential external permission errors by creating a potentially cleaner path to running. Unfortunately it doesn't deal with the underlying issue of needing to map users inside of the containers so that Spark is happy with them. Having something more lightweight than needing user updates in the internal `/etc/passwd` would also help with potential issues on other container enginer (Singularity, rkt).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-381642529
https://github.com/broadinstitute/gatk/issues/4626#issuecomment-381642529:217,Safety,avoid,avoid,217,"Thanks for the thoughts. Singularity is definitely awesome and I'm hoping to support it as an alternative choice to Docker for local HPC clusters where we won't require equivalent root permissions to run. So it helps avoid some of the potential external permission errors by creating a potentially cleaner path to running. Unfortunately it doesn't deal with the underlying issue of needing to map users inside of the containers so that Spark is happy with them. Having something more lightweight than needing user updates in the internal `/etc/passwd` would also help with potential issues on other container enginer (Singularity, rkt).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-381642529
https://github.com/broadinstitute/gatk/issues/4626#issuecomment-382483335:32,Deployability,configurat,configuration,32,"@chapmanb Singularity's default configuration has a line ""config passwd = yes"" and that will create a user entry in the /etc/passwd automatically. So it I understand the issue, spark would automatically find the user running the container in the /etc/passwd file.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-382483335
https://github.com/broadinstitute/gatk/issues/4626#issuecomment-382483335:32,Modifiability,config,configuration,32,"@chapmanb Singularity's default configuration has a line ""config passwd = yes"" and that will create a user entry in the /etc/passwd automatically. So it I understand the issue, spark would automatically find the user running the container in the /etc/passwd file.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-382483335
https://github.com/broadinstitute/gatk/issues/4626#issuecomment-382483335:58,Modifiability,config,config,58,"@chapmanb Singularity's default configuration has a line ""config passwd = yes"" and that will create a user entry in the /etc/passwd automatically. So it I understand the issue, spark would automatically find the user running the container in the /etc/passwd file.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4626#issuecomment-382483335
https://github.com/broadinstitute/gatk/issues/4629#issuecomment-379365531:28,Testability,log,log,28,"@mbabadi Note that the GATK log output now goes to stderr, not stdout, to enable pipelining for select tools.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4629#issuecomment-379365531
https://github.com/broadinstitute/gatk/issues/4629#issuecomment-379385291:287,Testability,log,logger,287,"@droazen I understand that -- the issue is that `ScriptExecutor` does not print either stdout or stderr from the external process unless debug is enabled (cf. `ScriptExecutor.java` ln. 92):. ```; //if debug is enabled, output the stdout and stderr, otherwise capture it to a buffer; if (logger.isDebugEnabled()) {; processSettings.getStdoutSettings().printStandard(true);; processSettings.getStderrSettings().printStandard(true);; } else {; processSettings.getStdoutSettings().setBufferSize(8192);; processSettings.getStderrSettings().setBufferSize(8192);; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4629#issuecomment-379385291
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973:229,Deployability,integrat,integrating,229,"Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. . After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. . If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. . David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973:521,Deployability,integrat,integrate,521,"Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. . After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. . If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. . David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973:896,Deployability,integrat,integration,896,"Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. . After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. . If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. . David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973:967,Deployability,integrat,integrating,967,"Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. . After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. . If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. . David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973:504,Energy Efficiency,efficient,efficient,504,"Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. . After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. . If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. . David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973:575,Energy Efficiency,adapt,adapt,575,"Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. . After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. . If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. . David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973:229,Integrability,integrat,integrating,229,"Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. . After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. . If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. . David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973:521,Integrability,integrat,integrate,521,"Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. . After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. . If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. . David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973:896,Integrability,integrat,integration,896,"Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. . After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. . If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. . David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973:967,Integrability,integrat,integrating,967,"Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. . After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. . If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. . David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973:331,Modifiability,enhance,enhancement,331,"Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. . After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. . If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. . David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973:575,Modifiability,adapt,adapt,575,"Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. . After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. . If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. . David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973:1049,Safety,avoid,avoid,1049,"Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. . After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. . If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. . David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-403101973
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349:931,Deployability,integrat,integrating,931,"Dear Geraldine and David,. Sorry for my delayed reply. I am busy with graduation in recent days, and will work on visa application for the further postdoc position at Harvard Medical School. Thank you for the interests in our tool, MosaicHunter. The option you suggest looks great. Do you mean that I should establish the GATK 4 developing environment and develop the MosaicHunterFilter tool? I may do that when I have some time. I found the document of GATK 4 at https://github.com/broadinstitute/gatk. Do you have any further advices?. Best regards,; Adam Yongxin Ye; Center for Bioinformatics; Peking University. At 2018-07-07 01:43:05, ""Geraldine Van der Auwera"" <notifications@github.com> wrote:. Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349:1221,Deployability,integrat,integrate,1221," tool, MosaicHunter. The option you suggest looks great. Do you mean that I should establish the GATK 4 developing environment and develop the MosaicHunterFilter tool? I may do that when I have some time. I found the document of GATK 4 at https://github.com/broadinstitute/gatk. Do you have any further advices?. Best regards,; Adam Yongxin Ye; Center for Bioinformatics; Peking University. At 2018-07-07 01:43:05, ""Geraldine Van der Auwera"" <notifications@github.com> wrote:. Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349:1594,Deployability,integrat,integration,1594," tool, MosaicHunter. The option you suggest looks great. Do you mean that I should establish the GATK 4 developing environment and develop the MosaicHunterFilter tool? I may do that when I have some time. I found the document of GATK 4 at https://github.com/broadinstitute/gatk. Do you have any further advices?. Best regards,; Adam Yongxin Ye; Center for Bioinformatics; Peking University. At 2018-07-07 01:43:05, ""Geraldine Van der Auwera"" <notifications@github.com> wrote:. Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349:1665,Deployability,integrat,integrating,1665," tool, MosaicHunter. The option you suggest looks great. Do you mean that I should establish the GATK 4 developing environment and develop the MosaicHunterFilter tool? I may do that when I have some time. I found the document of GATK 4 at https://github.com/broadinstitute/gatk. Do you have any further advices?. Best regards,; Adam Yongxin Ye; Center for Bioinformatics; Peking University. At 2018-07-07 01:43:05, ""Geraldine Van der Auwera"" <notifications@github.com> wrote:. Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349:1204,Energy Efficiency,efficient,efficient,1204," tool, MosaicHunter. The option you suggest looks great. Do you mean that I should establish the GATK 4 developing environment and develop the MosaicHunterFilter tool? I may do that when I have some time. I found the document of GATK 4 at https://github.com/broadinstitute/gatk. Do you have any further advices?. Best regards,; Adam Yongxin Ye; Center for Bioinformatics; Peking University. At 2018-07-07 01:43:05, ""Geraldine Van der Auwera"" <notifications@github.com> wrote:. Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349:1275,Energy Efficiency,adapt,adapt,1275," tool, MosaicHunter. The option you suggest looks great. Do you mean that I should establish the GATK 4 developing environment and develop the MosaicHunterFilter tool? I may do that when I have some time. I found the document of GATK 4 at https://github.com/broadinstitute/gatk. Do you have any further advices?. Best regards,; Adam Yongxin Ye; Center for Bioinformatics; Peking University. At 2018-07-07 01:43:05, ""Geraldine Van der Auwera"" <notifications@github.com> wrote:. Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349:931,Integrability,integrat,integrating,931,"Dear Geraldine and David,. Sorry for my delayed reply. I am busy with graduation in recent days, and will work on visa application for the further postdoc position at Harvard Medical School. Thank you for the interests in our tool, MosaicHunter. The option you suggest looks great. Do you mean that I should establish the GATK 4 developing environment and develop the MosaicHunterFilter tool? I may do that when I have some time. I found the document of GATK 4 at https://github.com/broadinstitute/gatk. Do you have any further advices?. Best regards,; Adam Yongxin Ye; Center for Bioinformatics; Peking University. At 2018-07-07 01:43:05, ""Geraldine Van der Auwera"" <notifications@github.com> wrote:. Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349:1221,Integrability,integrat,integrate,1221," tool, MosaicHunter. The option you suggest looks great. Do you mean that I should establish the GATK 4 developing environment and develop the MosaicHunterFilter tool? I may do that when I have some time. I found the document of GATK 4 at https://github.com/broadinstitute/gatk. Do you have any further advices?. Best regards,; Adam Yongxin Ye; Center for Bioinformatics; Peking University. At 2018-07-07 01:43:05, ""Geraldine Van der Auwera"" <notifications@github.com> wrote:. Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349:1594,Integrability,integrat,integration,1594," tool, MosaicHunter. The option you suggest looks great. Do you mean that I should establish the GATK 4 developing environment and develop the MosaicHunterFilter tool? I may do that when I have some time. I found the document of GATK 4 at https://github.com/broadinstitute/gatk. Do you have any further advices?. Best regards,; Adam Yongxin Ye; Center for Bioinformatics; Peking University. At 2018-07-07 01:43:05, ""Geraldine Van der Auwera"" <notifications@github.com> wrote:. Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349:1665,Integrability,integrat,integrating,1665," tool, MosaicHunter. The option you suggest looks great. Do you mean that I should establish the GATK 4 developing environment and develop the MosaicHunterFilter tool? I may do that when I have some time. I found the document of GATK 4 at https://github.com/broadinstitute/gatk. Do you have any further advices?. Best regards,; Adam Yongxin Ye; Center for Bioinformatics; Peking University. At 2018-07-07 01:43:05, ""Geraldine Van der Auwera"" <notifications@github.com> wrote:. Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349:1033,Modifiability,enhance,enhancement,1033,"ly. I am busy with graduation in recent days, and will work on visa application for the further postdoc position at Harvard Medical School. Thank you for the interests in our tool, MosaicHunter. The option you suggest looks great. Do you mean that I should establish the GATK 4 developing environment and develop the MosaicHunterFilter tool? I may do that when I have some time. I found the document of GATK 4 at https://github.com/broadinstitute/gatk. Do you have any further advices?. Best regards,; Adam Yongxin Ye; Center for Bioinformatics; Peking University. At 2018-07-07 01:43:05, ""Geraldine Van der Auwera"" <notifications@github.com> wrote:. Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an opt",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349:1275,Modifiability,adapt,adapt,1275," tool, MosaicHunter. The option you suggest looks great. Do you mean that I should establish the GATK 4 developing environment and develop the MosaicHunterFilter tool? I may do that when I have some time. I found the document of GATK 4 at https://github.com/broadinstitute/gatk. Do you have any further advices?. Best regards,; Adam Yongxin Ye; Center for Bioinformatics; Peking University. At 2018-07-07 01:43:05, ""Geraldine Van der Auwera"" <notifications@github.com> wrote:. Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349:1747,Safety,avoid,avoid,1747," tool, MosaicHunter. The option you suggest looks great. Do you mean that I should establish the GATK 4 developing environment and develop the MosaicHunterFilter tool? I may do that when I have some time. I found the document of GATK 4 at https://github.com/broadinstitute/gatk. Do you have any further advices?. Best regards,; Adam Yongxin Ye; Center for Bioinformatics; Peking University. At 2018-07-07 01:43:05, ""Geraldine Van der Auwera"" <notifications@github.com> wrote:. Hi @Yyx2626, I'm Geraldine, you may remember me from the Beijing training. It was great visiting your team! I'm sorry it took me so long to follow up on this discussion, and I want to thank you again for reaching out to us about integrating the tool that you developed into GATK. We are certainly very interested in providing this enhancement to the research community, and we are now ready to talk about the next steps. After examining your paper and the source code in Github, we think that the most efficient way to integrate the functionality you developed would be to adapt the filtering parts of your tool to run on the output of Mutect2. So this would be a standalone tool that you would run after Mutect2, much like the current FilterMutectCalls tool. If the results are comparable to your current tool, then we would take that into the official distribution of GATK. If somehow that integration does not yield satisfactory results, then we would look at integrating the entire tool, though we're hoping it won't be necessary, so we can avoid maintaining duplicate functionality for some of the boilerplate data transformations. David @davidbenjamin can provide some advice on how to implement this in GATK4; in brief you would need to write some code that applies the filters you developed to a variant context. Let us know if this is an option you'd like to explore; we'd be happy to help. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404104349
https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404109319:215,Usability,guid,guidance,215,"Congratulations on your graduation! And good luck with your application, I know from personal experience it is a hassle, but it’s worth it!. Yes, that is what we are proposing. @davidbenjamin can give you some more guidance to get you started. . Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4632#issuecomment-404109319
https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380130398:593,Availability,down,down,593,"Hi @jjfarrell,. It's hard to know what might be going wrong in these files. Can you describe how you are running `StructuralVariationDiscoveryPipelineSpark` -- on a local Spark cluster, on GCS dataproc, or in Spark local mode? Can you provide the command line?. Is there anything you can identify as being different about the failing files, maybe from other WGS metrics: higher coverage, high duplicate rate or chimera rate, etc? Are these human germline or cancer samples?. One initial thought could be that something is running out of memory when processing these samples, or getting bogged down in garbage collection. You could try increasing the parameters you give for `--driver-memory`, `--executor-memory`, or `--conf spark.yarn.executor.memoryOverhead`. There may be other Spark parameters you could try adjusting as well. Our default scripts run with these Spark options on a GCS Dataproc cluster:. ```; -- \; --spark-runner GCS \; --cluster ""${CLUSTER_NAME}"" \; --num-executors ${NUM_EXECUTORS} \; --driver-memory 30G \; --executor-memory 30G \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; ```. You could try increasing those memory values (if you have the resources) and see if that helps.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380130398
https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380130398:1164,Availability,heartbeat,heartbeatInterval,1164,"Hi @jjfarrell,. It's hard to know what might be going wrong in these files. Can you describe how you are running `StructuralVariationDiscoveryPipelineSpark` -- on a local Spark cluster, on GCS dataproc, or in Spark local mode? Can you provide the command line?. Is there anything you can identify as being different about the failing files, maybe from other WGS metrics: higher coverage, high duplicate rate or chimera rate, etc? Are these human germline or cancer samples?. One initial thought could be that something is running out of memory when processing these samples, or getting bogged down in garbage collection. You could try increasing the parameters you give for `--driver-memory`, `--executor-memory`, or `--conf spark.yarn.executor.memoryOverhead`. There may be other Spark parameters you could try adjusting as well. Our default scripts run with these Spark options on a GCS Dataproc cluster:. ```; -- \; --spark-runner GCS \; --cluster ""${CLUSTER_NAME}"" \; --num-executors ${NUM_EXECUTORS} \; --driver-memory 30G \; --executor-memory 30G \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; ```. You could try increasing those memory values (if you have the resources) and see if that helps.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380130398
https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380130398:1127,Safety,timeout,timeout,1127,"Hi @jjfarrell,. It's hard to know what might be going wrong in these files. Can you describe how you are running `StructuralVariationDiscoveryPipelineSpark` -- on a local Spark cluster, on GCS dataproc, or in Spark local mode? Can you provide the command line?. Is there anything you can identify as being different about the failing files, maybe from other WGS metrics: higher coverage, high duplicate rate or chimera rate, etc? Are these human germline or cancer samples?. One initial thought could be that something is running out of memory when processing these samples, or getting bogged down in garbage collection. You could try increasing the parameters you give for `--driver-memory`, `--executor-memory`, or `--conf spark.yarn.executor.memoryOverhead`. There may be other Spark parameters you could try adjusting as well. Our default scripts run with these Spark options on a GCS Dataproc cluster:. ```; -- \; --spark-runner GCS \; --cluster ""${CLUSTER_NAME}"" \; --num-executors ${NUM_EXECUTORS} \; --driver-memory 30G \; --executor-memory 30G \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; ```. You could try increasing those memory values (if you have the resources) and see if that helps.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380130398
https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314:344,Availability,down,down,344,"I agree with all Chris has said, and think that it's very likely that you're running out of memory on the executors. You might try cutting back on --num-executors, and bumping up --executor-memory.; If you can figure out your adapter sequence, you can specify that as --adaptor-sequence, and sometimes that helps with this stage.; We're laying down asphalt, and you're driving on the hot pavement just behind us. Thanks for trying out this tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314
https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314:226,Energy Efficiency,adapt,adapter,226,"I agree with all Chris has said, and think that it's very likely that you're running out of memory on the executors. You might try cutting back on --num-executors, and bumping up --executor-memory.; If you can figure out your adapter sequence, you can specify that as --adaptor-sequence, and sometimes that helps with this stage.; We're laying down asphalt, and you're driving on the hot pavement just behind us. Thanks for trying out this tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314
https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314:270,Energy Efficiency,adapt,adaptor-sequence,270,"I agree with all Chris has said, and think that it's very likely that you're running out of memory on the executors. You might try cutting back on --num-executors, and bumping up --executor-memory.; If you can figure out your adapter sequence, you can specify that as --adaptor-sequence, and sometimes that helps with this stage.; We're laying down asphalt, and you're driving on the hot pavement just behind us. Thanks for trying out this tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314
https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314:226,Integrability,adapter,adapter,226,"I agree with all Chris has said, and think that it's very likely that you're running out of memory on the executors. You might try cutting back on --num-executors, and bumping up --executor-memory.; If you can figure out your adapter sequence, you can specify that as --adaptor-sequence, and sometimes that helps with this stage.; We're laying down asphalt, and you're driving on the hot pavement just behind us. Thanks for trying out this tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314
https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314:226,Modifiability,adapt,adapter,226,"I agree with all Chris has said, and think that it's very likely that you're running out of memory on the executors. You might try cutting back on --num-executors, and bumping up --executor-memory.; If you can figure out your adapter sequence, you can specify that as --adaptor-sequence, and sometimes that helps with this stage.; We're laying down asphalt, and you're driving on the hot pavement just behind us. Thanks for trying out this tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314
https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314:270,Modifiability,adapt,adaptor-sequence,270,"I agree with all Chris has said, and think that it's very likely that you're running out of memory on the executors. You might try cutting back on --num-executors, and bumping up --executor-memory.; If you can figure out your adapter sequence, you can specify that as --adaptor-sequence, and sometimes that helps with this stage.; We're laying down asphalt, and you're driving on the hot pavement just behind us. Thanks for trying out this tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-380144314
https://github.com/broadinstitute/gatk/issues/4635#issuecomment-381441151:935,Availability,heartbeat,heartbeatInterval,935,"Thanks for the suggestions! The SV jobs are all running fine with no hanging after increasing the memory. The commandline below completed on 100 30x crams without any issues. . ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///user/farrell/adni/sv/$SAMPLE.contig-sam-file\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///user/farrell/$CENTER/sv/$SAMPLE.sv.vcf \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode cluster \; --executor-memory 60G\; --driver-memory 40g\; --num-executors 12\; --executor-cores 4\; --files $REF.img,GRCh38_ignored_kmers.txt \; --name ""$SAMPLE"" --conf spark.yarn.submit.waitAppCompletion=false\; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-381441151
https://github.com/broadinstitute/gatk/issues/4635#issuecomment-381441151:603,Deployability,deploy,deploy-mode,603,"Thanks for the suggestions! The SV jobs are all running fine with no hanging after increasing the memory. The commandline below completed on 100 30x crams without any issues. . ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///user/farrell/adni/sv/$SAMPLE.contig-sam-file\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///user/farrell/$CENTER/sv/$SAMPLE.sv.vcf \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode cluster \; --executor-memory 60G\; --driver-memory 40g\; --num-executors 12\; --executor-cores 4\; --files $REF.img,GRCh38_ignored_kmers.txt \; --name ""$SAMPLE"" --conf spark.yarn.submit.waitAppCompletion=false\; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-381441151
https://github.com/broadinstitute/gatk/issues/4635#issuecomment-381441151:898,Safety,timeout,timeout,898,"Thanks for the suggestions! The SV jobs are all running fine with no hanging after increasing the memory. The commandline below completed on 100 30x crams without any issues. . ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///user/farrell/adni/sv/$SAMPLE.contig-sam-file\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///user/farrell/$CENTER/sv/$SAMPLE.sv.vcf \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode cluster \; --executor-memory 60G\; --driver-memory 40g\; --num-executors 12\; --executor-cores 4\; --files $REF.img,GRCh38_ignored_kmers.txt \; --name ""$SAMPLE"" --conf spark.yarn.submit.waitAppCompletion=false\; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635#issuecomment-381441151
https://github.com/broadinstitute/gatk/issues/4637#issuecomment-439434139:92,Availability,mask,mask,92,"@bhanugandham This message itself is harmless and won't affect results directly, but it can mask warning/error messages coming from Jexl (such as missing variable names!). I wouldn't expect a user to see it when running from a normal installation, and I'm not sure why it seems to be intermittent. I'll need to do some debugging.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4637#issuecomment-439434139
https://github.com/broadinstitute/gatk/issues/4637#issuecomment-439434139:105,Availability,error,error,105,"@bhanugandham This message itself is harmless and won't affect results directly, but it can mask warning/error messages coming from Jexl (such as missing variable names!). I wouldn't expect a user to see it when running from a normal installation, and I'm not sure why it seems to be intermittent. I'll need to do some debugging.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4637#issuecomment-439434139
https://github.com/broadinstitute/gatk/issues/4637#issuecomment-439434139:234,Deployability,install,installation,234,"@bhanugandham This message itself is harmless and won't affect results directly, but it can mask warning/error messages coming from Jexl (such as missing variable names!). I wouldn't expect a user to see it when running from a normal installation, and I'm not sure why it seems to be intermittent. I'll need to do some debugging.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4637#issuecomment-439434139
https://github.com/broadinstitute/gatk/issues/4637#issuecomment-439434139:19,Integrability,message,message,19,"@bhanugandham This message itself is harmless and won't affect results directly, but it can mask warning/error messages coming from Jexl (such as missing variable names!). I wouldn't expect a user to see it when running from a normal installation, and I'm not sure why it seems to be intermittent. I'll need to do some debugging.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4637#issuecomment-439434139
https://github.com/broadinstitute/gatk/issues/4637#issuecomment-439434139:111,Integrability,message,messages,111,"@bhanugandham This message itself is harmless and won't affect results directly, but it can mask warning/error messages coming from Jexl (such as missing variable names!). I wouldn't expect a user to see it when running from a normal installation, and I'm not sure why it seems to be intermittent. I'll need to do some debugging.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4637#issuecomment-439434139
https://github.com/broadinstitute/gatk/issues/4637#issuecomment-439434139:154,Modifiability,variab,variable,154,"@bhanugandham This message itself is harmless and won't affect results directly, but it can mask warning/error messages coming from Jexl (such as missing variable names!). I wouldn't expect a user to see it when running from a normal installation, and I'm not sure why it seems to be intermittent. I'll need to do some debugging.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4637#issuecomment-439434139
https://github.com/broadinstitute/gatk/pull/4638#issuecomment-460611755:27,Deployability,release,release,27,I'll try to roll out a new release on maven central asap after latest merges.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4638#issuecomment-460611755
https://github.com/broadinstitute/gatk/issues/4641#issuecomment-380086823:500,Deployability,update,updated,500,"+1 from me too. This is a problem with tools that read from genomics DB; when run on large sample sets. On Mon, Apr 9, 2018, 5:06 PM jamesemery <notifications@github.com> wrote:. > I have noticed that running print reads with a stringent filter which I; > expect to only return a handful of reads results in the progress meter; > never printing any progress. This makes it look like the gatk has hung; > despite the fact it is chugging away and filtering every read it passes; > over. This should be updated to include an indication of how many reads; > have been filtered. Additionally, it should be improved to use a second; > thread to make periodic updates based on execution time incase the tool; > really has hung in order to make it clearer to the user what is going on.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4641>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLWElMXIsQZBXUpJLA6XHlVP-qd6ks5tm801gaJpZM4TNOh8>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4641#issuecomment-380086823
https://github.com/broadinstitute/gatk/issues/4641#issuecomment-380086823:653,Deployability,update,updates,653,"+1 from me too. This is a problem with tools that read from genomics DB; when run on large sample sets. On Mon, Apr 9, 2018, 5:06 PM jamesemery <notifications@github.com> wrote:. > I have noticed that running print reads with a stringent filter which I; > expect to only return a handful of reads results in the progress meter; > never printing any progress. This makes it look like the gatk has hung; > despite the fact it is chugging away and filtering every read it passes; > over. This should be updated to include an indication of how many reads; > have been filtered. Additionally, it should be improved to use a second; > thread to make periodic updates based on execution time incase the tool; > really has hung in order to make it clearer to the user what is going on.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4641>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLWElMXIsQZBXUpJLA6XHlVP-qd6ks5tm801gaJpZM4TNOh8>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4641#issuecomment-380086823
https://github.com/broadinstitute/gatk/issues/4641#issuecomment-380086823:321,Energy Efficiency,meter,meter,321,"+1 from me too. This is a problem with tools that read from genomics DB; when run on large sample sets. On Mon, Apr 9, 2018, 5:06 PM jamesemery <notifications@github.com> wrote:. > I have noticed that running print reads with a stringent filter which I; > expect to only return a handful of reads results in the progress meter; > never printing any progress. This makes it look like the gatk has hung; > despite the fact it is chugging away and filtering every read it passes; > over. This should be updated to include an indication of how many reads; > have been filtered. Additionally, it should be improved to use a second; > thread to make periodic updates based on execution time incase the tool; > really has hung in order to make it clearer to the user what is going on.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4641>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLWElMXIsQZBXUpJLA6XHlVP-qd6ks5tm801gaJpZM4TNOh8>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4641#issuecomment-380086823
https://github.com/broadinstitute/gatk/issues/4641#issuecomment-380086823:740,Usability,clear,clearer,740,"+1 from me too. This is a problem with tools that read from genomics DB; when run on large sample sets. On Mon, Apr 9, 2018, 5:06 PM jamesemery <notifications@github.com> wrote:. > I have noticed that running print reads with a stringent filter which I; > expect to only return a handful of reads results in the progress meter; > never printing any progress. This makes it look like the gatk has hung; > despite the fact it is chugging away and filtering every read it passes; > over. This should be updated to include an indication of how many reads; > have been filtered. Additionally, it should be improved to use a second; > thread to make periodic updates based on execution time incase the tool; > really has hung in order to make it clearer to the user what is going on.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4641>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLWElMXIsQZBXUpJLA6XHlVP-qd6ks5tm801gaJpZM4TNOh8>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4641#issuecomment-380086823
https://github.com/broadinstitute/gatk/pull/4643#issuecomment-386413917:0,Availability,ping,ping,0,ping @chandrans,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4643#issuecomment-386413917
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-385486423:97,Testability,test,testing,97,Fix for https://github.com/broadinstitute/gatk/issues/4716 - @cwhelan please use this branch for testing. You should be able to re-use the pre-created array - the bug was in the query part only.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-385486423
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-385525908:41,Testability,test,test,41,"@kgururaj Thanks, this branch passes the test case that was failing for me before.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-385525908
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519:31,Deployability,integrat,integration,31,"There is currently one failing integration test:. ```; completedicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals FAILED; java.lang.AssertionError: actual is longer than expected with at least one additional element: [VC null @ chr20:17970000 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={} GT=htsjdk.variant.bcf2.BCF2Codec$LazyData@134b79ab filters=; at org.testng.Assert.fail(Assert.java:93); at org.broadinstitute.hellbender.utils.test.BaseTest.assertCondition(BaseTest.java:395); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.lambda$checkGenomicsDBAgainstExpected$8(GenomicsDBImportIntegrationTest.java:326); at java.util.ArrayList.forEach(ArrayList.java:1249); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.checkGenomicsDBAgainstExpected(GenomicsDBImportIntegrationTest.java:319); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBAgainstCombineGVCFs(GenomicsDBImportIntegrationTest.java:166); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals(GenomicsDBImportIntegrationTest.java:107); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519:31,Integrability,integrat,integration,31,"There is currently one failing integration test:. ```; completedicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals FAILED; java.lang.AssertionError: actual is longer than expected with at least one additional element: [VC null @ chr20:17970000 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={} GT=htsjdk.variant.bcf2.BCF2Codec$LazyData@134b79ab filters=; at org.testng.Assert.fail(Assert.java:93); at org.broadinstitute.hellbender.utils.test.BaseTest.assertCondition(BaseTest.java:395); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.lambda$checkGenomicsDBAgainstExpected$8(GenomicsDBImportIntegrationTest.java:326); at java.util.ArrayList.forEach(ArrayList.java:1249); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.checkGenomicsDBAgainstExpected(GenomicsDBImportIntegrationTest.java:319); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBAgainstCombineGVCFs(GenomicsDBImportIntegrationTest.java:166); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals(GenomicsDBImportIntegrationTest.java:107); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519:43,Testability,test,test,43,"There is currently one failing integration test:. ```; completedicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals FAILED; java.lang.AssertionError: actual is longer than expected with at least one additional element: [VC null @ chr20:17970000 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={} GT=htsjdk.variant.bcf2.BCF2Codec$LazyData@134b79ab filters=; at org.testng.Assert.fail(Assert.java:93); at org.broadinstitute.hellbender.utils.test.BaseTest.assertCondition(BaseTest.java:395); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.lambda$checkGenomicsDBAgainstExpected$8(GenomicsDBImportIntegrationTest.java:326); at java.util.ArrayList.forEach(ArrayList.java:1249); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.checkGenomicsDBAgainstExpected(GenomicsDBImportIntegrationTest.java:319); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBAgainstCombineGVCFs(GenomicsDBImportIntegrationTest.java:166); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals(GenomicsDBImportIntegrationTest.java:107); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519:102,Testability,test,testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals,102,"There is currently one failing integration test:. ```; completedicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals FAILED; java.lang.AssertionError: actual is longer than expected with at least one additional element: [VC null @ chr20:17970000 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={} GT=htsjdk.variant.bcf2.BCF2Codec$LazyData@134b79ab filters=; at org.testng.Assert.fail(Assert.java:93); at org.broadinstitute.hellbender.utils.test.BaseTest.assertCondition(BaseTest.java:395); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.lambda$checkGenomicsDBAgainstExpected$8(GenomicsDBImportIntegrationTest.java:326); at java.util.ArrayList.forEach(ArrayList.java:1249); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.checkGenomicsDBAgainstExpected(GenomicsDBImportIntegrationTest.java:319); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBAgainstCombineGVCFs(GenomicsDBImportIntegrationTest.java:166); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals(GenomicsDBImportIntegrationTest.java:107); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519:190,Testability,Assert,AssertionError,190,"There is currently one failing integration test:. ```; completedicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals FAILED; java.lang.AssertionError: actual is longer than expected with at least one additional element: [VC null @ chr20:17970000 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={} GT=htsjdk.variant.bcf2.BCF2Codec$LazyData@134b79ab filters=; at org.testng.Assert.fail(Assert.java:93); at org.broadinstitute.hellbender.utils.test.BaseTest.assertCondition(BaseTest.java:395); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.lambda$checkGenomicsDBAgainstExpected$8(GenomicsDBImportIntegrationTest.java:326); at java.util.ArrayList.forEach(ArrayList.java:1249); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.checkGenomicsDBAgainstExpected(GenomicsDBImportIntegrationTest.java:319); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBAgainstCombineGVCFs(GenomicsDBImportIntegrationTest.java:166); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals(GenomicsDBImportIntegrationTest.java:107); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519:421,Testability,test,testng,421,"There is currently one failing integration test:. ```; completedicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals FAILED; java.lang.AssertionError: actual is longer than expected with at least one additional element: [VC null @ chr20:17970000 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={} GT=htsjdk.variant.bcf2.BCF2Codec$LazyData@134b79ab filters=; at org.testng.Assert.fail(Assert.java:93); at org.broadinstitute.hellbender.utils.test.BaseTest.assertCondition(BaseTest.java:395); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.lambda$checkGenomicsDBAgainstExpected$8(GenomicsDBImportIntegrationTest.java:326); at java.util.ArrayList.forEach(ArrayList.java:1249); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.checkGenomicsDBAgainstExpected(GenomicsDBImportIntegrationTest.java:319); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBAgainstCombineGVCFs(GenomicsDBImportIntegrationTest.java:166); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals(GenomicsDBImportIntegrationTest.java:107); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519:428,Testability,Assert,Assert,428,"There is currently one failing integration test:. ```; completedicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals FAILED; java.lang.AssertionError: actual is longer than expected with at least one additional element: [VC null @ chr20:17970000 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={} GT=htsjdk.variant.bcf2.BCF2Codec$LazyData@134b79ab filters=; at org.testng.Assert.fail(Assert.java:93); at org.broadinstitute.hellbender.utils.test.BaseTest.assertCondition(BaseTest.java:395); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.lambda$checkGenomicsDBAgainstExpected$8(GenomicsDBImportIntegrationTest.java:326); at java.util.ArrayList.forEach(ArrayList.java:1249); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.checkGenomicsDBAgainstExpected(GenomicsDBImportIntegrationTest.java:319); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBAgainstCombineGVCFs(GenomicsDBImportIntegrationTest.java:166); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals(GenomicsDBImportIntegrationTest.java:107); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519:440,Testability,Assert,Assert,440,"There is currently one failing integration test:. ```; completedicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals FAILED; java.lang.AssertionError: actual is longer than expected with at least one additional element: [VC null @ chr20:17970000 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={} GT=htsjdk.variant.bcf2.BCF2Codec$LazyData@134b79ab filters=; at org.testng.Assert.fail(Assert.java:93); at org.broadinstitute.hellbender.utils.test.BaseTest.assertCondition(BaseTest.java:395); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.lambda$checkGenomicsDBAgainstExpected$8(GenomicsDBImportIntegrationTest.java:326); at java.util.ArrayList.forEach(ArrayList.java:1249); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.checkGenomicsDBAgainstExpected(GenomicsDBImportIntegrationTest.java:319); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBAgainstCombineGVCFs(GenomicsDBImportIntegrationTest.java:166); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals(GenomicsDBImportIntegrationTest.java:107); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519:496,Testability,test,test,496,"There is currently one failing integration test:. ```; completedicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals FAILED; java.lang.AssertionError: actual is longer than expected with at least one additional element: [VC null @ chr20:17970000 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={} GT=htsjdk.variant.bcf2.BCF2Codec$LazyData@134b79ab filters=; at org.testng.Assert.fail(Assert.java:93); at org.broadinstitute.hellbender.utils.test.BaseTest.assertCondition(BaseTest.java:395); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.lambda$checkGenomicsDBAgainstExpected$8(GenomicsDBImportIntegrationTest.java:326); at java.util.ArrayList.forEach(ArrayList.java:1249); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.checkGenomicsDBAgainstExpected(GenomicsDBImportIntegrationTest.java:319); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBAgainstCombineGVCFs(GenomicsDBImportIntegrationTest.java:166); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals(GenomicsDBImportIntegrationTest.java:107); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519:510,Testability,assert,assertCondition,510,"There is currently one failing integration test:. ```; completedicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals FAILED; java.lang.AssertionError: actual is longer than expected with at least one additional element: [VC null @ chr20:17970000 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={} GT=htsjdk.variant.bcf2.BCF2Codec$LazyData@134b79ab filters=; at org.testng.Assert.fail(Assert.java:93); at org.broadinstitute.hellbender.utils.test.BaseTest.assertCondition(BaseTest.java:395); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.lambda$checkGenomicsDBAgainstExpected$8(GenomicsDBImportIntegrationTest.java:326); at java.util.ArrayList.forEach(ArrayList.java:1249); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.checkGenomicsDBAgainstExpected(GenomicsDBImportIntegrationTest.java:319); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBAgainstCombineGVCFs(GenomicsDBImportIntegrationTest.java:166); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals(GenomicsDBImportIntegrationTest.java:107); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519:1002,Testability,test,testGenomicsDBAgainstCombineGVCFs,1002,"There is currently one failing integration test:. ```; completedicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals FAILED; java.lang.AssertionError: actual is longer than expected with at least one additional element: [VC null @ chr20:17970000 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={} GT=htsjdk.variant.bcf2.BCF2Codec$LazyData@134b79ab filters=; at org.testng.Assert.fail(Assert.java:93); at org.broadinstitute.hellbender.utils.test.BaseTest.assertCondition(BaseTest.java:395); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.lambda$checkGenomicsDBAgainstExpected$8(GenomicsDBImportIntegrationTest.java:326); at java.util.ArrayList.forEach(ArrayList.java:1249); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.checkGenomicsDBAgainstExpected(GenomicsDBImportIntegrationTest.java:319); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBAgainstCombineGVCFs(GenomicsDBImportIntegrationTest.java:166); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals(GenomicsDBImportIntegrationTest.java:107); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519:1161,Testability,test,testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals,1161,"There is currently one failing integration test:. ```; completedicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals FAILED; java.lang.AssertionError: actual is longer than expected with at least one additional element: [VC null @ chr20:17970000 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={} GT=htsjdk.variant.bcf2.BCF2Codec$LazyData@134b79ab filters=; at org.testng.Assert.fail(Assert.java:93); at org.broadinstitute.hellbender.utils.test.BaseTest.assertCondition(BaseTest.java:395); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.lambda$checkGenomicsDBAgainstExpected$8(GenomicsDBImportIntegrationTest.java:326); at java.util.ArrayList.forEach(ArrayList.java:1249); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.checkGenomicsDBAgainstExpected(GenomicsDBImportIntegrationTest.java:319); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBAgainstCombineGVCFs(GenomicsDBImportIntegrationTest.java:166); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithMultipleIntervals(GenomicsDBImportIntegrationTest.java:107); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386368519
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386378020:16,Availability,failure,failure,16,@droazen - that failure is related to the question I sent Louis and James - I'll forward you the same email,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-386378020
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387760986:8,Testability,test,testing,8,"Are you testing with a single chromosome interval or multiple intervals? If a single interval, there should be no difference since a single thread will be used.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387760986
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542:294,Deployability,configurat,configuration,294,"@lbergelson sorry for my late response. I'm currently on vacations but I will try to respond (with some delay) any question.; So, what I'm seeing here (using Github web without having a proper dev env) is that for each interval, it's going to call (in parallel) sample reader function from the configuration =>; ```; final Map<String, FeatureReader<VariantContext>> sampleToReaderMap =; this.config.sampleToReaderMapCreator().apply(; this.config.getSampleNameToVcfPath(), updatedBatchSize, index); ; ```; That's is the first difference from previous implementation. If whatever you have in that function consume lots of memory, that's an issue.; Regarding the thread pool, I'm not seeing it's being starved by chromosome parallel import but it might use extra memory to execute since there is a high load of threads use due to the number of parallel imports.; Worker threads can execute only one task at the time, but the ForkJoinPool doesn’t create a separate thread for every single subtask. Instead, each thread in the pool has its own double-ended queue (or deque, pronounced deck) **which stores tasks**. Those are the two things I'm seeing right now without having the chance to debug :(.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542:472,Deployability,update,updatedBatchSize,472,"@lbergelson sorry for my late response. I'm currently on vacations but I will try to respond (with some delay) any question.; So, what I'm seeing here (using Github web without having a proper dev env) is that for each interval, it's going to call (in parallel) sample reader function from the configuration =>; ```; final Map<String, FeatureReader<VariantContext>> sampleToReaderMap =; this.config.sampleToReaderMapCreator().apply(; this.config.getSampleNameToVcfPath(), updatedBatchSize, index); ; ```; That's is the first difference from previous implementation. If whatever you have in that function consume lots of memory, that's an issue.; Regarding the thread pool, I'm not seeing it's being starved by chromosome parallel import but it might use extra memory to execute since there is a high load of threads use due to the number of parallel imports.; Worker threads can execute only one task at the time, but the ForkJoinPool doesn’t create a separate thread for every single subtask. Instead, each thread in the pool has its own double-ended queue (or deque, pronounced deck) **which stores tasks**. Those are the two things I'm seeing right now without having the chance to debug :(.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542:294,Modifiability,config,configuration,294,"@lbergelson sorry for my late response. I'm currently on vacations but I will try to respond (with some delay) any question.; So, what I'm seeing here (using Github web without having a proper dev env) is that for each interval, it's going to call (in parallel) sample reader function from the configuration =>; ```; final Map<String, FeatureReader<VariantContext>> sampleToReaderMap =; this.config.sampleToReaderMapCreator().apply(; this.config.getSampleNameToVcfPath(), updatedBatchSize, index); ; ```; That's is the first difference from previous implementation. If whatever you have in that function consume lots of memory, that's an issue.; Regarding the thread pool, I'm not seeing it's being starved by chromosome parallel import but it might use extra memory to execute since there is a high load of threads use due to the number of parallel imports.; Worker threads can execute only one task at the time, but the ForkJoinPool doesn’t create a separate thread for every single subtask. Instead, each thread in the pool has its own double-ended queue (or deque, pronounced deck) **which stores tasks**. Those are the two things I'm seeing right now without having the chance to debug :(.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542:392,Modifiability,config,config,392,"@lbergelson sorry for my late response. I'm currently on vacations but I will try to respond (with some delay) any question.; So, what I'm seeing here (using Github web without having a proper dev env) is that for each interval, it's going to call (in parallel) sample reader function from the configuration =>; ```; final Map<String, FeatureReader<VariantContext>> sampleToReaderMap =; this.config.sampleToReaderMapCreator().apply(; this.config.getSampleNameToVcfPath(), updatedBatchSize, index); ; ```; That's is the first difference from previous implementation. If whatever you have in that function consume lots of memory, that's an issue.; Regarding the thread pool, I'm not seeing it's being starved by chromosome parallel import but it might use extra memory to execute since there is a high load of threads use due to the number of parallel imports.; Worker threads can execute only one task at the time, but the ForkJoinPool doesn’t create a separate thread for every single subtask. Instead, each thread in the pool has its own double-ended queue (or deque, pronounced deck) **which stores tasks**. Those are the two things I'm seeing right now without having the chance to debug :(.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542:439,Modifiability,config,config,439,"@lbergelson sorry for my late response. I'm currently on vacations but I will try to respond (with some delay) any question.; So, what I'm seeing here (using Github web without having a proper dev env) is that for each interval, it's going to call (in parallel) sample reader function from the configuration =>; ```; final Map<String, FeatureReader<VariantContext>> sampleToReaderMap =; this.config.sampleToReaderMapCreator().apply(; this.config.getSampleNameToVcfPath(), updatedBatchSize, index); ; ```; That's is the first difference from previous implementation. If whatever you have in that function consume lots of memory, that's an issue.; Regarding the thread pool, I'm not seeing it's being starved by chromosome parallel import but it might use extra memory to execute since there is a high load of threads use due to the number of parallel imports.; Worker threads can execute only one task at the time, but the ForkJoinPool doesn’t create a separate thread for every single subtask. Instead, each thread in the pool has its own double-ended queue (or deque, pronounced deck) **which stores tasks**. Those are the two things I'm seeing right now without having the chance to debug :(.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542:800,Performance,load,load,800,"@lbergelson sorry for my late response. I'm currently on vacations but I will try to respond (with some delay) any question.; So, what I'm seeing here (using Github web without having a proper dev env) is that for each interval, it's going to call (in parallel) sample reader function from the configuration =>; ```; final Map<String, FeatureReader<VariantContext>> sampleToReaderMap =; this.config.sampleToReaderMapCreator().apply(; this.config.getSampleNameToVcfPath(), updatedBatchSize, index); ; ```; That's is the first difference from previous implementation. If whatever you have in that function consume lots of memory, that's an issue.; Regarding the thread pool, I'm not seeing it's being starved by chromosome parallel import but it might use extra memory to execute since there is a high load of threads use due to the number of parallel imports.; Worker threads can execute only one task at the time, but the ForkJoinPool doesn’t create a separate thread for every single subtask. Instead, each thread in the pool has its own double-ended queue (or deque, pronounced deck) **which stores tasks**. Those are the two things I'm seeing right now without having the chance to debug :(.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542:1052,Performance,queue,queue,1052,"@lbergelson sorry for my late response. I'm currently on vacations but I will try to respond (with some delay) any question.; So, what I'm seeing here (using Github web without having a proper dev env) is that for each interval, it's going to call (in parallel) sample reader function from the configuration =>; ```; final Map<String, FeatureReader<VariantContext>> sampleToReaderMap =; this.config.sampleToReaderMapCreator().apply(; this.config.getSampleNameToVcfPath(), updatedBatchSize, index); ; ```; That's is the first difference from previous implementation. If whatever you have in that function consume lots of memory, that's an issue.; Regarding the thread pool, I'm not seeing it's being starved by chromosome parallel import but it might use extra memory to execute since there is a high load of threads use due to the number of parallel imports.; Worker threads can execute only one task at the time, but the ForkJoinPool doesn’t create a separate thread for every single subtask. Instead, each thread in the pool has its own double-ended queue (or deque, pronounced deck) **which stores tasks**. Those are the two things I'm seeing right now without having the chance to debug :(.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387810542
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387934906:362,Energy Efficiency,consumption,consumption,362,"* Added a command line parameter to control the number of intervals to import in parallel - default 1; * This doesn't affect Louis' case since he was importing a single interval.; * I ran the old and new versions of GATK on 1000 samples with a batch size of 20. Verified that at most 20 readers are open at a given time (GNU/Linux open file descriptors). Memory consumption difference was less than 10%.; * @cwhelan (edit, sorry @lbergelson ) was the import command run with a list of VCF files in the command line or with the --sample-name-map argument?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-387934906
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3239,Availability,down,down,3239,"s; 16:27:48.951 INFO GenomicsDBImport - Initializing engine; 16:28:03.989 INFO IntervalArgumentCollection - Processing 29812 bp from intervals; 16:28:03.994 INFO GenomicsDBImport - Done initializing engine; Created workspace /humgen/gsa-hpprojects/dev/gauthier/reblockGVCF/forkTest; 16:28:04.155 INFO GenomicsDBImport - Vid Map JSON file will be written to forkTest/vidmap.json; 16:28:04.155 INFO GenomicsDBImport - Callset Map JSON file will be written to forkTest/callset.json; 16:28:04.156 INFO GenomicsDBImport - Complete VCF Header will be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importe",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:2126,Deployability,patch,patch,2126,"FO GenomicsDBImport - Start Date/Time: May 4, 2018 4:27:48 PM EDT; 16:27:48.950 INFO GenomicsDBImport - ------------------------------------------------------------; 16:27:48.950 INFO GenomicsDBImport - ------------------------------------------------------------; 16:27:48.950 INFO GenomicsDBImport - HTSJDK Version: 2.14.3; 16:27:48.951 INFO GenomicsDBImport - Picard Version: 2.18.1; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:27:48.951 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:27:48.951 INFO GenomicsDBImport - Inflater: IntelInflater; 16:27:48.951 INFO GenomicsDBImport - GCS max retries/reopens: 20; 16:27:48.951 INFO GenomicsDBImport - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 16:27:48.951 INFO GenomicsDBImport - Initializing engine; 16:28:03.989 INFO IntervalArgumentCollection - Processing 29812 bp from intervals; 16:28:03.994 INFO GenomicsDBImport - Done initializing engine; Created workspace /humgen/gsa-hpprojects/dev/gauthier/reblockGVCF/forkTest; 16:28:04.155 INFO GenomicsDBImport - Vid Map JSON file will be written to forkTest/vidmap.json; 16:28:04.155 INFO GenomicsDBImport - Callset Map JSON file will be written to forkTest/callset.json; 16:28:04.156 INFO GenomicsDBImport - Complete VCF Header will be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:416,Performance,Load,Loading,416,"@kgururaj I ran with batch size 50, using a sample map, 5 reader threads:. `java -jar GATK_GDBfork.jar GenomicsDBImport --genomicsdb-workspace-path forkTest --batch-size 50 -L chr20:45840744-45870555 --sample-name-map gnarly_reblocked_all.sample_map --reader-threads 5`. That sample map has 80K genomes because that's the project I'm working on now. Log was as follows:; ```; 16:27:48.831 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/humgen/gsa-hpprojects/dev/gauthier/reblockGVCF/GATK_GDBfork.jar!/com/intel/gkl/nati; ve/libgkl_compression.so; 16:27:48.947 INFO GenomicsDBImport - ------------------------------------------------------------; 16:27:48.948 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.3.0-24-g8804e16-SNAPSHOT; 16:27:48.948 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:27:48.949 INFO GenomicsDBImport - Executing as gauthier@gsa5.broadinstitute.org on Linux v2.6.32-642.15.1.el6.x86_64 amd64; 16:27:48.949 INFO GenomicsDBImport - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 16:27:48.950 INFO GenomicsDBImport - Start Date/Time: May 4, 2018 4:27:48 PM EDT; 16:27:48.950 INFO GenomicsDBImport - ------------------------------------------------------------; 16:27:48.950 INFO GenomicsDBImport - ------------------------------------------------------------; 16:27:48.950 INFO GenomicsDBImport - HTSJDK Version: 2.14.3; 16:27:48.951 INFO GenomicsDBImport - Picard Version: 2.18.1; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:27:48.951 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:27:48.951 INFO GenomicsDBImport - Inflater: IntelInflater; 16",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3426,Performance,concurren,concurrent,3426,"itializing engine; Created workspace /humgen/gsa-hpprojects/dev/gauthier/reblockGVCF/forkTest; 16:28:04.155 INFO GenomicsDBImport - Vid Map JSON file will be written to forkTest/vidmap.json; 16:28:04.155 INFO GenomicsDBImport - Callset Map JSON file will be written to forkTest/callset.json; 16:28:04.156 INFO GenomicsDBImport - Complete VCF Header will be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.in",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3516,Performance,concurren,concurrent,3516,"est; 16:28:04.155 INFO GenomicsDBImport - Vid Map JSON file will be written to forkTest/vidmap.json; 16:28:04.155 INFO GenomicsDBImport - Callset Map JSON file will be written to forkTest/callset.json; 16:28:04.156 INFO GenomicsDBImport - Complete VCF Header will be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3603,Performance,concurren,concurrent,3603,"/vidmap.json; 16:28:04.155 INFO GenomicsDBImport - Callset Map JSON file will be written to forkTest/callset.json; 16:28:04.156 INFO GenomicsDBImport - Complete VCF Header will be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(Geno",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3692,Performance,concurren,concurrent,3692,"to forkTest/callset.json; 16:28:04.156 INFO GenomicsDBImport - Complete VCF Header will be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:358); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(Geno",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3780,Performance,concurren,concurrent,3780,"be written to forkTest/vcfheader.vcf; 16:28:04.156 INFO GenomicsDBImport - Importing to array - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:358); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$nul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3869,Performance,concurren,concurrent,3869,"rray - forkTest/genomicsdb_array; 16:28:04.158 INFO ProgressMeter - Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:358); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:598); at com.intel.genomicsdb.importer.GenomicsDBImporter$$La",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:3937,Performance,concurren,concurrent,3937,"Starting traversal; 16:28:04.158 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:358); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:598); at com.intel.genomicsdb.importer.GenomicsDBImporter$$Lambda$58/15335646.get(Unknown Source); at java.util.concurrent.Comple",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:4017,Performance,concurren,concurrent,4017,"d Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:358); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:598); at com.intel.genomicsdb.importer.GenomicsDBImporter$$Lambda$58/15335646.get(Unknown Source); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 5 more; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:4089,Performance,concurren,concurrent,4089,"d Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:358); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:598); at com.intel.genomicsdb.importer.GenomicsDBImporter$$Lambda$58/15335646.get(Unknown Source); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 5 more; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:4926,Performance,concurren,concurrent,4926,"d Minutes Batches Processed Batches/Minute; 16:28:05.198 INFO GenomicsDBImport - Starting batch input file preload; 16:29:23.571 INFO GenomicsDBImport - Finished batch preload; 16:48:46.140 INFO GenomicsDBImport - Shutting down engine; [May 4, 2018 4:48:46 PM EDT] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 20.96 minutes.; Runtime.totalMemory()=22281715712; java.util.concurrent.CompletionException: java.lang.OutOfMemoryError: Java heap space; at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273); at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592); at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582); at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.OutOfMemoryError: Java heap space; at com.intel.genomicsdb.importer.SilentByteBufferStream.<init>(SilentByteBufferStream.java:55); at com.intel.genomicsdb.importer.GenomicsDBImporterStreamWrapper.<init>(GenomicsDBImporterStreamWrapper.java:70); at com.intel.genomicsdb.importer.GenomicsDBImporter.addBufferStream(GenomicsDBImporter.java:397); at com.intel.genomicsdb.importer.GenomicsDBImporter.addSortedVariantContextIterator(GenomicsDBImporter.java:358); at com.intel.genomicsdb.importer.GenomicsDBImporter.<init>(GenomicsDBImporter.java:167); at com.intel.genomicsdb.importer.GenomicsDBImporter.lambda$null$2(GenomicsDBImporter.java:598); at com.intel.genomicsdb.importer.GenomicsDBImporter$$Lambda$58/15335646.get(Unknown Source); at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590); ... 5 more; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572:350,Testability,Log,Log,350,"@kgururaj I ran with batch size 50, using a sample map, 5 reader threads:. `java -jar GATK_GDBfork.jar GenomicsDBImport --genomicsdb-workspace-path forkTest --batch-size 50 -L chr20:45840744-45870555 --sample-name-map gnarly_reblocked_all.sample_map --reader-threads 5`. That sample map has 80K genomes because that's the project I'm working on now. Log was as follows:; ```; 16:27:48.831 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/humgen/gsa-hpprojects/dev/gauthier/reblockGVCF/GATK_GDBfork.jar!/com/intel/gkl/nati; ve/libgkl_compression.so; 16:27:48.947 INFO GenomicsDBImport - ------------------------------------------------------------; 16:27:48.948 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.3.0-24-g8804e16-SNAPSHOT; 16:27:48.948 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:27:48.949 INFO GenomicsDBImport - Executing as gauthier@gsa5.broadinstitute.org on Linux v2.6.32-642.15.1.el6.x86_64 amd64; 16:27:48.949 INFO GenomicsDBImport - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 16:27:48.950 INFO GenomicsDBImport - Start Date/Time: May 4, 2018 4:27:48 PM EDT; 16:27:48.950 INFO GenomicsDBImport - ------------------------------------------------------------; 16:27:48.950 INFO GenomicsDBImport - ------------------------------------------------------------; 16:27:48.950 INFO GenomicsDBImport - HTSJDK Version: 2.14.3; 16:27:48.951 INFO GenomicsDBImport - Picard Version: 2.18.1; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:27:48.951 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:27:48.951 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:27:48.951 INFO GenomicsDBImport - Inflater: IntelInflater; 16",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388079572
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388183296:150,Availability,failure,failures,150,"80k is what I had easy access to and what I'm the most invested in; benchmarking right now. Master does fine with the same params. It's slow,; but no failures. We decided to split into 1000 shards (Eric is convinced; that there's a substantial startup cost per shard so we do better in total; cpu-hours on fewer shards) and each of those takes about 24 hours. On Thu, May 10, 2018, 11:09 AM Louis Bergelson <notifications@github.com>; wrote:. > @ldgauthier <https://github.com/ldgauthier> You're running 80k? Does that; > run using the current master version of GATK? I assumed you were rerunning; > a 20k shard with the same settings we had used for the 20k.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388082988>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLdlQoWlC8kjRvJJermDYEjltVUFks5txFgigaJpZM4TOtSm>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388183296
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388183296:23,Security,access,access,23,"80k is what I had easy access to and what I'm the most invested in; benchmarking right now. Master does fine with the same params. It's slow,; but no failures. We decided to split into 1000 shards (Eric is convinced; that there's a substantial startup cost per shard so we do better in total; cpu-hours on fewer shards) and each of those takes about 24 hours. On Thu, May 10, 2018, 11:09 AM Louis Bergelson <notifications@github.com>; wrote:. > @ldgauthier <https://github.com/ldgauthier> You're running 80k? Does that; > run using the current master version of GATK? I assumed you were rerunning; > a 20k shard with the same settings we had used for the 20k.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388082988>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLdlQoWlC8kjRvJJermDYEjltVUFks5txFgigaJpZM4TOtSm>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388183296
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388183296:68,Testability,benchmark,benchmarking,68,"80k is what I had easy access to and what I'm the most invested in; benchmarking right now. Master does fine with the same params. It's slow,; but no failures. We decided to split into 1000 shards (Eric is convinced; that there's a substantial startup cost per shard so we do better in total; cpu-hours on fewer shards) and each of those takes about 24 hours. On Thu, May 10, 2018, 11:09 AM Louis Bergelson <notifications@github.com>; wrote:. > @ldgauthier <https://github.com/ldgauthier> You're running 80k? Does that; > run using the current master version of GATK? I assumed you were rerunning; > a 20k shard with the same settings we had used for the 20k.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388082988>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLdlQoWlC8kjRvJJermDYEjltVUFks5txFgigaJpZM4TOtSm>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388183296
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388221783:98,Energy Efficiency,allocate,allocated,98,\* Hangs head in shame *; I made a mistake in the buffer size computation in the Java side - over allocated .; Fixed now - consumes approximately the same amount of memory now,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-388221783
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-389687052:16,Testability,test,test,16,Modified the CI test that compares with CombineGVCFs so that the interval(s) queried does/do not hit the issue discussed on Monday,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-389687052
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-389699780:2162,Deployability,pipeline,pipelines,2162,) | `0% <ø> (ø)` | `0 <0> (ø)` | :arrow_down: |; | [...institute/hellbender/engine/FeatureDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2UuamF2YQ==) | `75.796% <84.211%> (-0.593%)` | `44 <1> (-1)` | |; | [.../hellbender/tools/genomicsdb/GenomicsDBImport.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJJbXBvcnQuamF2YQ==) | `80.083% <92.157%> (+4.325%)` | `52 <11> (-1)` | :arrow_down: |; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-74.257%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (-30%)` | `1% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-389699780
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-389699780:2478,Testability,test,test,2478,1%> (-0.593%)` | `44 <1> (-1)` | |; | [.../hellbender/tools/genomicsdb/GenomicsDBImport.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJJbXBvcnQuamF2YQ==) | `80.083% <92.157%> (+4.325%)` | `52 <11> (-1)` | :arrow_down: |; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-74.257%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (-30%)` | `1% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-10%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-389699780
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-389699780:3653,Testability,test,test,3653,i90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-74.257%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (-30%)` | `1% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-10%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <0%> (-22.807%)` | `0% <0%> (-2%)` | |; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L01pbmlDbHVzdGVyVXRpbHMuamF2YQ==) | `78.947% <0%> (-10.526%)` | `6% <0%> (-1%)` | |; | ... and [6 more](https://codecov.io/gh/broadinstitute/gatk/pull/4645/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-389699780
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-390979702:75,Availability,fault,fault,75,"@lbergelson I just realized I didn't specify `-new-qual` so the hang is my fault. Trying again. (This is a process because it takes 15 hours to import 80K samples, even if it's just over 4Kb.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-390979702
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-391354963:72,Availability,fault,fault,72,"Good news! With the exception of the commandline in the header (also my fault) my two outputs from 4.0.3.0-27-g173ed25 vs. 4.0.4.0-30-gec36b13 for chr20:1265769-1269692 on ~80K samples are BITWISE EQUIVALENT! Runtime was also pretty similar. Admittedly I didn't compare memory usage, but I trust Karthik's assessment and I feel good giving this my :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-391354963
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-394427475:55,Testability,test,test-related,55,"@kgururaj Looks like the only remaining TODOs here are test-related. Before we can merge, we need:. 1. Better tests for the case of multiple, non-adjacent intervals (in particular, a test case involving multiple intervals from different contigs, if that is supported, or a test showing that a sensible exception is thrown if it's not supported).; 2. A test covering the issue reported in #4716; 3. Test cases covering the new sites-only query support (#3688) and support for retrieving the GT field",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-394427475
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-394427475:110,Testability,test,tests,110,"@kgururaj Looks like the only remaining TODOs here are test-related. Before we can merge, we need:. 1. Better tests for the case of multiple, non-adjacent intervals (in particular, a test case involving multiple intervals from different contigs, if that is supported, or a test showing that a sensible exception is thrown if it's not supported).; 2. A test covering the issue reported in #4716; 3. Test cases covering the new sites-only query support (#3688) and support for retrieving the GT field",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-394427475
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-394427475:183,Testability,test,test,183,"@kgururaj Looks like the only remaining TODOs here are test-related. Before we can merge, we need:. 1. Better tests for the case of multiple, non-adjacent intervals (in particular, a test case involving multiple intervals from different contigs, if that is supported, or a test showing that a sensible exception is thrown if it's not supported).; 2. A test covering the issue reported in #4716; 3. Test cases covering the new sites-only query support (#3688) and support for retrieving the GT field",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-394427475
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-394427475:273,Testability,test,test,273,"@kgururaj Looks like the only remaining TODOs here are test-related. Before we can merge, we need:. 1. Better tests for the case of multiple, non-adjacent intervals (in particular, a test case involving multiple intervals from different contigs, if that is supported, or a test showing that a sensible exception is thrown if it's not supported).; 2. A test covering the issue reported in #4716; 3. Test cases covering the new sites-only query support (#3688) and support for retrieving the GT field",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-394427475
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-394427475:352,Testability,test,test,352,"@kgururaj Looks like the only remaining TODOs here are test-related. Before we can merge, we need:. 1. Better tests for the case of multiple, non-adjacent intervals (in particular, a test case involving multiple intervals from different contigs, if that is supported, or a test showing that a sensible exception is thrown if it's not supported).; 2. A test covering the issue reported in #4716; 3. Test cases covering the new sites-only query support (#3688) and support for retrieving the GT field",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-394427475
https://github.com/broadinstitute/gatk/pull/4645#issuecomment-394427475:398,Testability,Test,Test,398,"@kgururaj Looks like the only remaining TODOs here are test-related. Before we can merge, we need:. 1. Better tests for the case of multiple, non-adjacent intervals (in particular, a test case involving multiple intervals from different contigs, if that is supported, or a test showing that a sensible exception is thrown if it's not supported).; 2. A test covering the issue reported in #4716; 3. Test cases covering the new sites-only query support (#3688) and support for retrieving the GT field",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4645#issuecomment-394427475
https://github.com/broadinstitute/gatk/pull/4646#issuecomment-383718666:235,Deployability,pipeline,pipeline,235,@TedBrookings I implemented these suggestions of yours:. - replace CLUSTER_NAME with SANITIZED_BAM in the results directory; - allow output directory to be overridden by setting SV_OUTPUT_DIR; - rename `runWholePipeline` to `run-whole-pipeline`. I also found and fixed two other bugs:. - added a `set -f` to the create cluster script to avoid having bash expand the wildcard glob for that step (this caused a problem if a file matching the pattern was present locally); - changed how the copy results script got cluster info so that it parses the result header (this fixes a problem when using preemptible workers because the number of columns in the results of the `gcloud dataproc clusters list` command had a different number of columns. Can you give these changes a quick re-review?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4646#issuecomment-383718666
https://github.com/broadinstitute/gatk/pull/4646#issuecomment-383718666:337,Safety,avoid,avoid,337,@TedBrookings I implemented these suggestions of yours:. - replace CLUSTER_NAME with SANITIZED_BAM in the results directory; - allow output directory to be overridden by setting SV_OUTPUT_DIR; - rename `runWholePipeline` to `run-whole-pipeline`. I also found and fixed two other bugs:. - added a `set -f` to the create cluster script to avoid having bash expand the wildcard glob for that step (this caused a problem if a file matching the pattern was present locally); - changed how the copy results script got cluster info so that it parses the result header (this fixes a problem when using preemptible workers because the number of columns in the results of the `gcloud dataproc clusters list` command had a different number of columns. Can you give these changes a quick re-review?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4646#issuecomment-383718666
https://github.com/broadinstitute/gatk/issues/4647#issuecomment-380465668:331,Availability,down,down,331,"The MacArthur lab has dealt with this by using our PID/PGT tags and post processing. If we did it for them it would probably be appreciated. Full disclosure: we don't have 100% sensitivity to adjacent phased SNPs with PID/PGT because the phasing heuristic requires cis-phase variants to be on ALL the same haplotypes, which breaks down occasionally in messy regions and some other scenarios.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4647#issuecomment-380465668
https://github.com/broadinstitute/gatk/issues/4647#issuecomment-380521889:83,Safety,risk,risk,83,"@sooheelee I don't see, at least not immediately, how artifacts would be more of a risk than they are already. If anything I could see this making it a bit harder for false positives to get by.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4647#issuecomment-380521889
https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:29,Availability,error,error,29,"HG00731 fails with a similar error:. ```; 18/04/11 16:08:40 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 98.0 in stage 42.0 (TID 49620, cwhelan-hg00731-cram-samtools-bam-feature-w-4.c.broad-dsde-methods.internal, executor 43): java.lang.IllegalArgumentException: Invalid interval. Contig:chr6 start:34662153 end:34662143; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575
https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:82,Energy Efficiency,schedul,scheduler,82,"HG00731 fails with a similar error:. ```; 18/04/11 16:08:40 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 98.0 in stage 42.0 (TID 49620, cwhelan-hg00731-cram-samtools-bam-feature-w-4.c.broad-dsde-methods.internal, executor 43): java.lang.IllegalArgumentException: Invalid interval. Contig:chr6 start:34662153 end:34662143; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575
https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:1892,Energy Efficiency,schedul,scheduler,1892,.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575
https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:1971,Energy Efficiency,schedul,scheduler,1971,.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575
https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:2050,Energy Efficiency,schedul,scheduler,2050,.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575
https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:2173,Performance,concurren,concurrent,2173,.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575
https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:2257,Performance,concurren,concurrent,2257,.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575
https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:380,Security,validat,validateArg,380,"HG00731 fails with a similar error:. ```; 18/04/11 16:08:40 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 98.0 in stage 42.0 (TID 49620, cwhelan-hg00731-cram-samtools-bam-feature-w-4.c.broad-dsde-methods.internal, executor 43): java.lang.IllegalArgumentException: Invalid interval. Contig:chr6 start:34662153 end:34662143; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575
https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:463,Security,validat,validatePositions,463,"HG00731 fails with a similar error:. ```; 18/04/11 16:08:40 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 98.0 in stage 42.0 (TID 49620, cwhelan-hg00731-cram-samtools-bam-feature-w-4.c.broad-dsde-methods.internal, executor 43): java.lang.IllegalArgumentException: Invalid interval. Contig:chr6 start:34662153 end:34662143; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575
https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:448,Usability,Simpl,SimpleInterval,448,"HG00731 fails with a similar error:. ```; 18/04/11 16:08:40 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 98.0 in stage 42.0 (TID 49620, cwhelan-hg00731-cram-samtools-bam-feature-w-4.c.broad-dsde-methods.internal, executor 43): java.lang.IllegalArgumentException: Invalid interval. Contig:chr6 start:34662153 end:34662143; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575
https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:481,Usability,Simpl,SimpleInterval,481,"HG00731 fails with a similar error:. ```; 18/04/11 16:08:40 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 98.0 in stage 42.0 (TID 49620, cwhelan-hg00731-cram-samtools-bam-feature-w-4.c.broad-dsde-methods.internal, executor 43): java.lang.IllegalArgumentException: Invalid interval. Contig:chr6 start:34662153 end:34662143; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575
https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:545,Usability,Simpl,SimpleInterval,545,"HG00731 fails with a similar error:. ```; 18/04/11 16:08:40 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 98.0 in stage 42.0 (TID 49620, cwhelan-hg00731-cram-samtools-bam-feature-w-4.c.broad-dsde-methods.internal, executor 43): java.lang.IllegalArgumentException: Invalid interval. Contig:chr6 start:34662153 end:34662143; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575
https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575:567,Usability,Simpl,SimpleInterval,567,"HG00731 fails with a similar error:. ```; 18/04/11 16:08:40 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 98.0 in stage 42.0 (TID 49620, cwhelan-hg00731-cram-samtools-bam-feature-w-4.c.broad-dsde-methods.internal, executor 43): java.lang.IllegalArgumentException: Invalid interval. Contig:chr6 start:34662153 end:34662143; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:163); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648#issuecomment-380510575
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-380816090:227,Deployability,integrat,integration,227,"I'd rather not change the default HC behavior, but this is pretty exciting because we can lay the argument about porting ReadBackedPhasing to rest. It would be good to do a comparison with RBP -- can you take a look at the RBP integration tests from GATK3 to see if there was a MNP test there?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-380816090
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-380816090:227,Integrability,integrat,integration,227,"I'd rather not change the default HC behavior, but this is pretty exciting because we can lay the argument about porting ReadBackedPhasing to rest. It would be good to do a comparison with RBP -- can you take a look at the RBP integration tests from GATK3 to see if there was a MNP test there?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-380816090
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-380816090:239,Testability,test,tests,239,"I'd rather not change the default HC behavior, but this is pretty exciting because we can lay the argument about porting ReadBackedPhasing to rest. It would be good to do a comparison with RBP -- can you take a look at the RBP integration tests from GATK3 to see if there was a MNP test there?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-380816090
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-380816090:282,Testability,test,test,282,"I'd rather not change the default HC behavior, but this is pretty exciting because we can lay the argument about porting ReadBackedPhasing to rest. It would be good to do a comparison with RBP -- can you take a look at the RBP integration tests from GATK3 to see if there was a MNP test there?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-380816090
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-380867582:22,Testability,test,tests,22,"@ldgauthier the GATK3 tests have two bams relevant to MNPs. One has two unphased SNPs 3 bases apart; the other has two phased adjacent SNPs ie a DNP. That's it as far as I can tell. I think I ought to cook up some synthetic reads for a nice test. By the way, should add a MNP merging distance option as in ReadBackedPhasing? Currently, for example, the code I wrote can't make a MNP out of ACT -> GCA.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-380867582
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-380867582:241,Testability,test,test,241,"@ldgauthier the GATK3 tests have two bams relevant to MNPs. One has two unphased SNPs 3 bases apart; the other has two phased adjacent SNPs ie a DNP. That's it as far as I can tell. I think I ought to cook up some synthetic reads for a nice test. By the way, should add a MNP merging distance option as in ReadBackedPhasing? Currently, for example, the code I wrote can't make a MNP out of ACT -> GCA.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-380867582
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-380871983:176,Testability,test,tests,176,"I only want adjacent bases, so I am okay. On Thu, Apr 12, 2018, 12:34 David Benjamin <notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> the GATK3 tests have two bams; > relevant to MNPs. One has two unphased SNPs 3 bases apart; the other has; > two phased adjacent SNPs ie a DNP. That's it as far as I can tell. I think; > I ought to cook up some synthetic reads for a nice test.; >; > By the way, should add a MNP merging distance option as in; > ReadBackedPhasing? Currently, for example, the code I wrote can't make a; > MNP out of ACT -> GCA.; >; > —; > You are receiving this because your review was requested.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4650#issuecomment-380867582>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk7itJIetc3iY3pptj9ljoscsExcqks5tn4IngaJpZM4TRJxM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-380871983
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-380871983:404,Testability,test,test,404,"I only want adjacent bases, so I am okay. On Thu, Apr 12, 2018, 12:34 David Benjamin <notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> the GATK3 tests have two bams; > relevant to MNPs. One has two unphased SNPs 3 bases apart; the other has; > two phased adjacent SNPs ie a DNP. That's it as far as I can tell. I think; > I ought to cook up some synthetic reads for a nice test.; >; > By the way, should add a MNP merging distance option as in; > ReadBackedPhasing? Currently, for example, the code I wrote can't make a; > MNP out of ACT -> GCA.; >; > —; > You are receiving this because your review was requested.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4650#issuecomment-380867582>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk7itJIetc3iY3pptj9ljoscsExcqks5tn4IngaJpZM4TRJxM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-380871983
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-381171433:88,Integrability,depend,depending,88,Actually ACT -> GCA would be useful because they could potentially be in the same codon depending on the reading frame. Is that an easy feature to add?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-381171433
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-381206366:22,Availability,down,downstream,22,"@ldgauthier There are downstream tools that are going to choke on that for sure. If we can add another flag to control this as well, I am okay.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-381206366
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-383357001:80,Testability,test,tests,80,@ldgauthier @LeeTL1220 I put in an parameter for the MNP spacing and a bunch of tests.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-383357001
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-383357042:21,Testability,test,tests,21,. . . and one of the tests is sadistic.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-383357042
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-384473840:23,Deployability,integrat,integration,23,@ldgauthier I added an integration test for GVCF mode and it works fine: the alleles are as expected with the addition of `<NON REF>`. I'm now going to investigate how the MNPs interact with CombineGVCFs and GenotypeGVCFs.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-384473840
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-384473840:23,Integrability,integrat,integration,23,@ldgauthier I added an integration test for GVCF mode and it works fine: the alleles are as expected with the addition of `<NON REF>`. I'm now going to investigate how the MNPs interact with CombineGVCFs and GenotypeGVCFs.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-384473840
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-384473840:35,Testability,test,test,35,@ldgauthier I added an integration test for GVCF mode and it works fine: the alleles are as expected with the addition of `<NON REF>`. I'm now going to investigate how the MNPs interact with CombineGVCFs and GenotypeGVCFs.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-384473840
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-384836262:175,Modifiability,inherit,inherit,175,"@ldgauthier Some parts of taking splitting MNPs at the end of HaplotypeCaller are easy: breaking eg one DNP at position n into a SNP at n and a SNP at n + 1, letting the SNPs inherit the PLs, AF, and AD (okay, this isn't quite right because a read might end in the middle of the MNP, but close enough) of the parent MNP. . . but the general problem of splitting annotations seems like it might be too tricky. I'm leaning toward instead just modifying `AssemblyBasedCallerGenotypingEngine.phaseCalls()`. It seems that this phasing relies very heavily on perfect phasing or anti-phasing and that even one questionable haplotype with incorrect phasing can spoil things. I would guess that we could improve the phasing by making some simple guess as to which haplotypes are real. Basically, the problem is that while HaplotypeCaller imposes ploidy on alleles, it does not do so on haplotypes, and so phasing information is diluted. With your permission I would like to merge this PR and open a new issue for improving `phaseCalls`. After all, the issue is fixed in M2, and HC now has a perfectly good MNP mode, with the caveat that it doesn't interact nicely with GVCF mode.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-384836262
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-384836262:730,Usability,simpl,simple,730,"@ldgauthier Some parts of taking splitting MNPs at the end of HaplotypeCaller are easy: breaking eg one DNP at position n into a SNP at n and a SNP at n + 1, letting the SNPs inherit the PLs, AF, and AD (okay, this isn't quite right because a read might end in the middle of the MNP, but close enough) of the parent MNP. . . but the general problem of splitting annotations seems like it might be too tricky. I'm leaning toward instead just modifying `AssemblyBasedCallerGenotypingEngine.phaseCalls()`. It seems that this phasing relies very heavily on perfect phasing or anti-phasing and that even one questionable haplotype with incorrect phasing can spoil things. I would guess that we could improve the phasing by making some simple guess as to which haplotypes are real. Basically, the problem is that while HaplotypeCaller imposes ploidy on alleles, it does not do so on haplotypes, and so phasing information is diluted. With your permission I would like to merge this PR and open a new issue for improving `phaseCalls`. After all, the issue is fixed in M2, and HC now has a perfectly good MNP mode, with the caveat that it doesn't interact nicely with GVCF mode.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-384836262
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-387833314:218,Availability,avail,available,218,There's enough work with the annotation handling to justify this being a separate task for the HaplotypeCaller side. Let's just turn your new phasing off for HaplotypeCaller GVCF mode. I'm still interested in it being available for single-sample because it would be awesome for clinical.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-387833314
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-387866601:87,Availability,error,error,87,"@ldgauthier As the PR stands mnps are off by default for HC. Should I have it throw an error if they are turned on in GVCF mode, and should I turn it on by default in non-GVCF mode?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-387866601
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-388080057:39,Availability,Error,Error,39,Let's do off by default for all modes. Error if MNPs and GVCF mode.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-388080057
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-388163049:40,Availability,Error,Error,40,>Let's do off by default for all modes. Error if MNPs and GVCF mode. @ldgauthier Done and done. @LeeTL1220 I need for your sign-off as well.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-388163049
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-396290602:1018,Safety,unsafe,unsafe,1018,"@davidbenjamin & @ldgauthier Sorry for commenting on a closed/merged PR but I wasn't sure where else to take the discussion. If there's a more appropriate place please redirect me!. First off, this is very cool and I'm so glad to see this making it's way into HC/M2! It's super helpful for functional annotation/clinical interpretation. Thanks for working on this!. I had two thoughts which maybe belong as separate issues, but I figured I'd raise them here first and see what you thought:. 1. It would actually be useful to be able to combine this behavior with GVCF mode in some cases. I understand all the caveats about merging and joint-genotyping when this has been done, but there are use cases for single-sample calling where both GCVF and MNP mode combined would be useful. E.g. in a clinical setting it's very useful to have the GVCF with the reference blocks, and also call MNPs as MNPs. There would be no merging in this case. Any chance this could be allowed, perhaps with a warning or requirement that `--unsafe` be on?; 2. IIRC RBP would also phase combinations of `indel-SNP` and `indel-indel` in addition to `SNP-SNP`. I'm curious how hard it would be to apply the same grouping logic across indels as well? I tried to read the code in the PR, but honestly I don't think I understand the ramifications of including indels sufficiently well. Would there be any conceptual objections or road-blocks to doing this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-396290602
https://github.com/broadinstitute/gatk/pull/4650#issuecomment-396290602:1195,Testability,log,logic,1195,"@davidbenjamin & @ldgauthier Sorry for commenting on a closed/merged PR but I wasn't sure where else to take the discussion. If there's a more appropriate place please redirect me!. First off, this is very cool and I'm so glad to see this making it's way into HC/M2! It's super helpful for functional annotation/clinical interpretation. Thanks for working on this!. I had two thoughts which maybe belong as separate issues, but I figured I'd raise them here first and see what you thought:. 1. It would actually be useful to be able to combine this behavior with GVCF mode in some cases. I understand all the caveats about merging and joint-genotyping when this has been done, but there are use cases for single-sample calling where both GCVF and MNP mode combined would be useful. E.g. in a clinical setting it's very useful to have the GVCF with the reference blocks, and also call MNPs as MNPs. There would be no merging in this case. Any chance this could be allowed, perhaps with a warning or requirement that `--unsafe` be on?; 2. IIRC RBP would also phase combinations of `indel-SNP` and `indel-indel` in addition to `SNP-SNP`. I'm curious how hard it would be to apply the same grouping logic across indels as well? I tried to read the code in the PR, but honestly I don't think I understand the ramifications of including indels sufficiently well. Would there be any conceptual objections or road-blocks to doing this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4650#issuecomment-396290602
https://github.com/broadinstitute/gatk/issues/4651#issuecomment-381279453:317,Deployability,update,update,317,"@magicDGS My take on these: I think the read filter plugin descriptor shouldn't be removed, since it also does filter merging, header propagation, etc. which need to be done even if you want to disable user command line control. (Also, if you remove it I would expect you'd get an NPE). So I would say that we should update the doc to say that you shouldn't remove it from the list, and should override `makeReadFilter` in the case where you want finer control. I know we talked a lot about the transformer issue a while back; I think the intention was that we would do the integration with the rest of the tool types as part of https://github.com/broadinstitute/gatk/issues/2160, but @droazen may recall differently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4651#issuecomment-381279453
https://github.com/broadinstitute/gatk/issues/4651#issuecomment-381279453:574,Deployability,integrat,integration,574,"@magicDGS My take on these: I think the read filter plugin descriptor shouldn't be removed, since it also does filter merging, header propagation, etc. which need to be done even if you want to disable user command line control. (Also, if you remove it I would expect you'd get an NPE). So I would say that we should update the doc to say that you shouldn't remove it from the list, and should override `makeReadFilter` in the case where you want finer control. I know we talked a lot about the transformer issue a while back; I think the intention was that we would do the integration with the rest of the tool types as part of https://github.com/broadinstitute/gatk/issues/2160, but @droazen may recall differently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4651#issuecomment-381279453
https://github.com/broadinstitute/gatk/issues/4651#issuecomment-381279453:574,Integrability,integrat,integration,574,"@magicDGS My take on these: I think the read filter plugin descriptor shouldn't be removed, since it also does filter merging, header propagation, etc. which need to be done even if you want to disable user command line control. (Also, if you remove it I would expect you'd get an NPE). So I would say that we should update the doc to say that you shouldn't remove it from the list, and should override `makeReadFilter` in the case where you want finer control. I know we talked a lot about the transformer issue a while back; I think the intention was that we would do the integration with the rest of the tool types as part of https://github.com/broadinstitute/gatk/issues/2160, but @droazen may recall differently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4651#issuecomment-381279453
https://github.com/broadinstitute/gatk/issues/4651#issuecomment-381279453:52,Modifiability,plugin,plugin,52,"@magicDGS My take on these: I think the read filter plugin descriptor shouldn't be removed, since it also does filter merging, header propagation, etc. which need to be done even if you want to disable user command line control. (Also, if you remove it I would expect you'd get an NPE). So I would say that we should update the doc to say that you shouldn't remove it from the list, and should override `makeReadFilter` in the case where you want finer control. I know we talked a lot about the transformer issue a while back; I think the intention was that we would do the integration with the rest of the tool types as part of https://github.com/broadinstitute/gatk/issues/2160, but @droazen may recall differently.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4651#issuecomment-381279453
https://github.com/broadinstitute/gatk/pull/4652#issuecomment-381172945:3467,Testability,test,test,3467,| [...decs/xsvLocatableTable/XsvLocatableTableCodec.java](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb2RlY3MveHN2TG9jYXRhYmxlVGFibGUvWHN2TG9jYXRhYmxlVGFibGVDb2RlYy5qYXZh) | `78.947% <0%> (-3.769%)` | `69% <0%> (+9%)` | |; | [...itute/hellbender/tools/walkers/bqsr/ApplyBQSR.java](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQXBwbHlCUVNSLmphdmE=) | `90.909% <0%> (-0.758%)` | `11% <0%> (+5%)` | |; | [...lkers/ReferenceConfidenceVariantContextMerger.java](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL1JlZmVyZW5jZUNvbmZpZGVuY2VWYXJpYW50Q29udGV4dE1lcmdlci5qYXZh) | `94.979% <0%> (-0.278%)` | `69% <0%> (-2%)` | |; | [...lugin/DefaultGATKReadFilterArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vRGVmYXVsdEdBVEtSZWFkRmlsdGVyQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `100% <0%> (ø)` | `8% <0%> (+4%)` | :arrow_up: |; | [...llbender/utils/test/SimpleIntervalTestFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1NpbXBsZUludGVydmFsVGVzdEZhY3RvcnkuamF2YQ==) | `0% <0%> (ø)` | `0% <0%> (?)` | |; | [.../annotatedregion/SimpleAnnotatedGenomicRegion.java](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL2Fubm90YXRlZHJlZ2lvbi9TaW1wbGVBbm5vdGF0ZWRHZW5vbWljUmVnaW9uLmphdmE=) | `82.222% <0%> (ø)` | `14% <0%> (?)` | |; | ... and [11 more](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4652#issuecomment-381172945
https://github.com/broadinstitute/gatk/pull/4652#issuecomment-381172945:3472,Usability,Simpl,SimpleIntervalTestFactory,3472,| [...decs/xsvLocatableTable/XsvLocatableTableCodec.java](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb2RlY3MveHN2TG9jYXRhYmxlVGFibGUvWHN2TG9jYXRhYmxlVGFibGVDb2RlYy5qYXZh) | `78.947% <0%> (-3.769%)` | `69% <0%> (+9%)` | |; | [...itute/hellbender/tools/walkers/bqsr/ApplyBQSR.java](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQXBwbHlCUVNSLmphdmE=) | `90.909% <0%> (-0.758%)` | `11% <0%> (+5%)` | |; | [...lkers/ReferenceConfidenceVariantContextMerger.java](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL1JlZmVyZW5jZUNvbmZpZGVuY2VWYXJpYW50Q29udGV4dE1lcmdlci5qYXZh) | `94.979% <0%> (-0.278%)` | `69% <0%> (-2%)` | |; | [...lugin/DefaultGATKReadFilterArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vRGVmYXVsdEdBVEtSZWFkRmlsdGVyQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `100% <0%> (ø)` | `8% <0%> (+4%)` | :arrow_up: |; | [...llbender/utils/test/SimpleIntervalTestFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1NpbXBsZUludGVydmFsVGVzdEZhY3RvcnkuamF2YQ==) | `0% <0%> (ø)` | `0% <0%> (?)` | |; | [.../annotatedregion/SimpleAnnotatedGenomicRegion.java](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL2Fubm90YXRlZHJlZ2lvbi9TaW1wbGVBbm5vdGF0ZWRHZW5vbWljUmVnaW9uLmphdmE=) | `82.222% <0%> (ø)` | `14% <0%> (?)` | |; | ... and [11 more](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4652#issuecomment-381172945
https://github.com/broadinstitute/gatk/pull/4652#issuecomment-381172945:3759,Usability,Simpl,SimpleAnnotatedGenomicRegion,3759,| [...decs/xsvLocatableTable/XsvLocatableTableCodec.java](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb2RlY3MveHN2TG9jYXRhYmxlVGFibGUvWHN2TG9jYXRhYmxlVGFibGVDb2RlYy5qYXZh) | `78.947% <0%> (-3.769%)` | `69% <0%> (+9%)` | |; | [...itute/hellbender/tools/walkers/bqsr/ApplyBQSR.java](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQXBwbHlCUVNSLmphdmE=) | `90.909% <0%> (-0.758%)` | `11% <0%> (+5%)` | |; | [...lkers/ReferenceConfidenceVariantContextMerger.java](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL1JlZmVyZW5jZUNvbmZpZGVuY2VWYXJpYW50Q29udGV4dE1lcmdlci5qYXZh) | `94.979% <0%> (-0.278%)` | `69% <0%> (-2%)` | |; | [...lugin/DefaultGATKReadFilterArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vRGVmYXVsdEdBVEtSZWFkRmlsdGVyQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `100% <0%> (ø)` | `8% <0%> (+4%)` | :arrow_up: |; | [...llbender/utils/test/SimpleIntervalTestFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1NpbXBsZUludGVydmFsVGVzdEZhY3RvcnkuamF2YQ==) | `0% <0%> (ø)` | `0% <0%> (?)` | |; | [.../annotatedregion/SimpleAnnotatedGenomicRegion.java](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL2Fubm90YXRlZHJlZ2lvbi9TaW1wbGVBbm5vdGF0ZWRHZW5vbWljUmVnaW9uLmphdmE=) | `82.222% <0%> (ø)` | `14% <0%> (?)` | |; | ... and [11 more](https://codecov.io/gh/broadinstitute/gatk/pull/4652/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4652#issuecomment-381172945
https://github.com/broadinstitute/gatk/pull/4652#issuecomment-383583002:52,Availability,down,down,52,@lucidtronix This looks good now. Can you squash it down to one commit and rebase and then we can merge once the tests pass.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4652#issuecomment-383583002
https://github.com/broadinstitute/gatk/pull/4652#issuecomment-383583002:113,Testability,test,tests,113,@lucidtronix This looks good now. Can you squash it down to one commit and rebase and then we can merge once the tests pass.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4652#issuecomment-383583002
https://github.com/broadinstitute/gatk/pull/4653#issuecomment-382141714:3766,Testability,test,test,3766,<0%> (?)` | |; | [...r/tools/walkers/mutect/Mutect2FilteringEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/4653/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyRmlsdGVyaW5nRW5naW5lLmphdmE=) | `82.993% <0%> (+0.116%)` | `40% <0%> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/utils/read/ReadUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4653/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL1JlYWRVdGlscy5qYXZh) | `79.753% <0%> (+0.146%)` | `184% <0%> (-1%)` | :arrow_down: |; | [...ls/copynumber/utils/CombineSegmentBreakpoints.java](https://codecov.io/gh/broadinstitute/gatk/pull/4653/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL0NvbWJpbmVTZWdtZW50QnJlYWtwb2ludHMuamF2YQ==) | `91.597% <0%> (+0.585%)` | `35% <0%> (+8%)` | :arrow_up: |; | [...ava/org/broadinstitute/hellbender/utils/Utils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4653/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9VdGlscy5qYXZh) | `81.527% <0%> (+1.092%)` | `139% <0%> (-1%)` | :arrow_down: |; | [...ataSources/xsv/LocatableXsvFuncotationFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4653/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL3hzdi9Mb2NhdGFibGVYc3ZGdW5jb3RhdGlvbkZhY3RvcnkuamF2YQ==) | `86.916% <0%> (+1.731%)` | `29% <0%> (+5%)` | :arrow_up: |; | [...itute/hellbender/utils/test/SamAssertionUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4653/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1NhbUFzc2VydGlvblV0aWxzLmphdmE=) | `75.281% <0%> (+2.46%)` | `40% <0%> (-6%)` | :arrow_down: |; | ... and [1 more](https://codecov.io/gh/broadinstitute/gatk/pull/4653/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4653#issuecomment-382141714
https://github.com/broadinstitute/gatk/pull/4653#issuecomment-382141714:1843,Usability,Simpl,SimpleAnnotatedGenomicRegion,1843,ll/4653?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...decs/xsvLocatableTable/XsvLocatableTableCodec.java](https://codecov.io/gh/broadinstitute/gatk/pull/4653/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb2RlY3MveHN2TG9jYXRhYmxlVGFibGUvWHN2TG9jYXRhYmxlVGFibGVDb2RlYy5qYXZh) | `77.512% <0%> (-5.204%)` | `67% <0%> (+7%)` | |; | [...g/broadinstitute/hellbender/utils/io/Resource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4653/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pby9SZXNvdXJjZS5qYXZh) | `55.263% <0%> (-0.292%)` | `7% <0%> (+1%)` | |; | [...itute/hellbender/tools/walkers/bqsr/ApplyBQSR.java](https://codecov.io/gh/broadinstitute/gatk/pull/4653/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQXBwbHlCUVNSLmphdmE=) | `91.667% <0%> (ø)` | `6% <0%> (ø)` | :arrow_down: |; | [.../annotatedregion/SimpleAnnotatedGenomicRegion.java](https://codecov.io/gh/broadinstitute/gatk/pull/4653/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL2Fubm90YXRlZHJlZ2lvbi9TaW1wbGVBbm5vdGF0ZWRHZW5vbWljUmVnaW9uLmphdmE=) | `75.556% <0%> (ø)` | `10% <0%> (?)` | |; | [...r/tools/walkers/mutect/Mutect2FilteringEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/4653/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyRmlsdGVyaW5nRW5naW5lLmphdmE=) | `82.993% <0%> (+0.116%)` | `40% <0%> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/utils/read/ReadUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4653/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL1JlYWRVdGlscy5qYXZh) | `79.753% <0%> (+0.146%)` | `184% <0%> (-1%)` | :arrow_down: |; | [...ls/copynumber/utils/CombineSegmentBreakpoints.java](https://codecov.io,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4653#issuecomment-382141714
https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188:163,Modifiability,config,config,163,"@jamesemery Great, thanks for checking. Could you do a review pass on this when you get a chance? It's not clear that the approach taken here of sending the owner config file around is what we want....it seems like instead we need a way to load the owner config from the launcher script itself.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188
https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188:255,Modifiability,config,config,255,"@jamesemery Great, thanks for checking. Could you do a review pass on this when you get a chance? It's not clear that the approach taken here of sending the owner config file around is what we want....it seems like instead we need a way to load the owner config from the launcher script itself.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188
https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188:240,Performance,load,load,240,"@jamesemery Great, thanks for checking. Could you do a review pass on this when you get a chance? It's not clear that the approach taken here of sending the owner config file around is what we want....it seems like instead we need a way to load the owner config from the launcher script itself.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188
https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188:107,Usability,clear,clear,107,"@jamesemery Great, thanks for checking. Could you do a review pass on this when you get a chance? It's not clear that the approach taken here of sending the owner config file around is what we want....it seems like instead we need a way to load the owner config from the launcher script itself.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4653#issuecomment-420055188
https://github.com/broadinstitute/gatk/pull/4656#issuecomment-380909365:1546,Deployability,pipeline,pipelines,1546,===============================; Files 1074 1080 +6 ; Lines 62907 64036 +1129 ; Branches 10181 10471 +290 ; ==============================================; + Hits 50225 51322 +1097 ; - Misses 8701 8704 +3 ; - Partials 3981 4010 +29; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4656?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ava/org/broadinstitute/hellbender/utils/Utils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4656/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9VdGlscy5qYXZh) | `80.241% <0%> (-0.194%)` | `142 <2> (+2)` | |; | [...ections/MarkDuplicatesSparkArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4656/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL2FyZ3VtZW50Y29sbGVjdGlvbnMvTWFya0R1cGxpY2F0ZXNTcGFya0FyZ3VtZW50Q29sbGVjdGlvbi5qYXZh) | `100% <100%> (ø)` | `1 <1> (?)` | |; | [...nder/tools/spark/pipelines/ReadsPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4656/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `89.13% <100%> (-0.231%)` | `12 <0> (ø)` | |; | [...k/pipelines/BwaAndMarkDuplicatesPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4656/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQndhQW5kTWFya0R1cGxpY2F0ZXNQaXBlbGluZVNwYXJrLmphdmE=) | `77.778% <100%> (-1.17%)` | `4 <0> (ø)` | |; | [...s/read/markduplicates/sparkrecords/PairedEnds.java](https://codecov.io/gh/broadinstitute/gatk/pull/4656/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9QYWlyZWRFbmRzLmphdmE=) | `100% <100%> (ø)` | `1 <1> (?)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4656#issuecomment-380909365
https://github.com/broadinstitute/gatk/pull/4656#issuecomment-380909365:1836,Deployability,pipeline,pipelines,1836,656?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ava/org/broadinstitute/hellbender/utils/Utils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4656/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9VdGlscy5qYXZh) | `80.241% <0%> (-0.194%)` | `142 <2> (+2)` | |; | [...ections/MarkDuplicatesSparkArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4656/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL2FyZ3VtZW50Y29sbGVjdGlvbnMvTWFya0R1cGxpY2F0ZXNTcGFya0FyZ3VtZW50Q29sbGVjdGlvbi5qYXZh) | `100% <100%> (ø)` | `1 <1> (?)` | |; | [...nder/tools/spark/pipelines/ReadsPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4656/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `89.13% <100%> (-0.231%)` | `12 <0> (ø)` | |; | [...k/pipelines/BwaAndMarkDuplicatesPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4656/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQndhQW5kTWFya0R1cGxpY2F0ZXNQaXBlbGluZVNwYXJrLmphdmE=) | `77.778% <100%> (-1.17%)` | `4 <0> (ø)` | |; | [...s/read/markduplicates/sparkrecords/PairedEnds.java](https://codecov.io/gh/broadinstitute/gatk/pull/4656/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9QYWlyZWRFbmRzLmphdmE=) | `100% <100%> (ø)` | `1 <1> (?)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4656/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `82.051% <100%> (ø)` | `44 <5> (ø)` | :arrow_down: |; | [...ils/read/markduplicates/sparkrecords/Fragment.java](https://codecov.io/gh/broa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4656#issuecomment-380909365
https://github.com/broadinstitute/gatk/issues/4657#issuecomment-380989518:102,Availability,error,errors,102,Hi @cmnbroad - I removed a space in the path and re-ran ValidateVariants. It went through throwing no errors. Many thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4657#issuecomment-380989518
https://github.com/broadinstitute/gatk/issues/4657#issuecomment-380989518:56,Security,Validat,ValidateVariants,56,Hi @cmnbroad - I removed a space in the path and re-ran ValidateVariants. It went through throwing no errors. Many thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4657#issuecomment-380989518
https://github.com/broadinstitute/gatk/issues/4659#issuecomment-382305931:163,Integrability,message,message,163,Because I am stupid and don't know how to read numbers (I saw 4.3 instead of 4.6): actually the list that I did was based on the 4.6 changelog. I edited title and message.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4659#issuecomment-382305931
https://github.com/broadinstitute/gatk/issues/4659#issuecomment-382412315:198,Testability,log,log,198,"Heh, no problem, it's easy to make a mistake like that. I have [a branch](https://github.com/broadinstitute/gatk/tree/lb_update_to_gradle_4.6) that I've working to get 4.6 on. Gradle 4+ changes the log output and it interacts badly with travis. I've been trying to figure out the solution, but I keep running into different dumb typos and things so it's going slowly. I think it's pretty close now though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4659#issuecomment-382412315
https://github.com/broadinstitute/gatk/issues/4660#issuecomment-382000269:63,Integrability,interface,interface,63,"@magicDGS We already plan to add the common CommandLineProgram interface as described in (broadinstitute/barclay#127). I think that makes a lot more sense than adding more new methods for this. The existing PR for it is out of date though, since there is more and more common functionality that could be pushed up into the common base (beta/experimental decoration, tool registry, etc.).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4660#issuecomment-382000269
https://github.com/broadinstitute/gatk/issues/4660#issuecomment-382002792:274,Deployability,update,updated,274,"I agree with the common functionality, but the PR aims to set the less constraints as possible for the custom implementations. Let me know here or in https://github.com/broadinstitute/barclay/issues/127 the functionality to be common in the base class, and I can work on an updated PR for it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4660#issuecomment-382002792
https://github.com/broadinstitute/gatk/issues/4660#issuecomment-384224234:171,Modifiability,refactor,refactoring,171,"@cmnbroad - what is the timeframe for a common CLP class? Because in the meantime it would be nice to be able to add picard tools, and anyway the common CLP would require refactoring in `Main`. So is it too much problem if I add a temporary method to add single picard tools?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4660#issuecomment-384224234
https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408867631:20,Availability,error,error,20,I've just seen this error too when running `genome_reads-pipeline_hdfs.sh` from https://github.com/broadinstitute/gatk/tree/master/scripts/spark_eval. It's odd that we get an `ArrayIndexOutOfBoundsException` even though there's a `ensureCapacity` call in the previous block...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408867631
https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408872373:364,Availability,failure,failures,364,"It could be a synchronization issue. E.g. if thread A is asking for an alleleCount of 3 and and thread B an alleleCount of 4, then thread A could grow the array to 3 after thread B grows it to 4 (meaning the array is grown to size 4 but then set back to size 3), but before thread B reads position 4. This read will then fail. BTW I was seeing quite a lot of task failures, around 10%.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408872373
https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408872373:14,Integrability,synchroniz,synchronization,14,"It could be a synchronization issue. E.g. if thread A is asking for an alleleCount of 3 and and thread B an alleleCount of 4, then thread A could grow the array to 3 after thread B grows it to 4 (meaning the array is grown to size 4 but then set back to size 3), but before thread B reads position 4. This read will then fail. BTW I was seeing quite a lot of task failures, around 10%.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408872373
https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230:3317,Energy Efficiency,schedul,scheduler,3317,reamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230
https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230:3389,Energy Efficiency,schedul,scheduler,3389,reamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230
https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230:1499,Integrability,Wrap,WrappingSpliterator,1499,ls.java:283); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.AlleleSubsettingUtils.subsetAlleles(AlleleSubsettingUtils.java:92); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:296); 	at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:210); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:162); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:596); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$1(HaplotypeCallerSpark.java:282); 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029); 	a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230
https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230:1841,Integrability,Wrap,WrappingSpliterator,1841,lateGenotypes(GenotypingEngine.java:210); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerGenotypingEngine.assignGenotypeLikelihoods(HaplotypeCallerGenotypingEngine.java:162); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:596); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$1(HaplotypeCallerSpark.java:282); 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	a,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230
https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230:1999,Integrability,Wrap,Wrappers,1999,peCallerGenotypingEngine.java:162); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:596); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$1(HaplotypeCallerSpark.java:282); 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrRe,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230
https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230:2033,Integrability,Wrap,Wrappers,2033,ine.java:162); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:596); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.lambda$regionToVariants$1(HaplotypeCallerSpark.java:282); 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1812); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230
https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230:3514,Performance,concurren,concurrent,3514,reamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230
https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230:3599,Performance,concurren,concurrent,3599,reamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661#issuecomment-408874230
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-383235501:2207,Usability,Simpl,SimpleSVType,2207,` | :arrow_down: |; | [...ender/tools/spark/sv/utils/GATKSVVCFConstants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4663/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi91dGlscy9HQVRLU1ZWQ0ZDb25zdGFudHMuamF2YQ==) | `75% <ø> (ø)` | `1 <0> (ø)` | :arrow_down: |; | [...tructuralVariationDiscoveryArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4663/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TdHJ1Y3R1cmFsVmFyaWF0aW9uRGlzY292ZXJ5QXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `97.297% <ø> (ø)` | `0 <0> (ø)` | :arrow_down: |; | [.../tools/spark/sv/discovery/BreakEndVariantType.java](https://codecov.io/gh/broadinstitute/gatk/pull/4663/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvQnJlYWtFbmRWYXJpYW50VHlwZS5qYXZh) | `0% <0%> (ø)` | `0 <0> (ø)` | :arrow_down: |; | [...lbender/tools/spark/sv/discovery/SimpleSVType.java](https://codecov.io/gh/broadinstitute/gatk/pull/4663/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvU2ltcGxlU1ZUeXBlLmphdmE=) | `86.667% <100%> (+0.226%)` | `3 <1> (+1)` | :arrow_up: |; | [...der/tools/spark/sv/utils/GATKSVVCFHeaderLines.java](https://codecov.io/gh/broadinstitute/gatk/pull/4663/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi91dGlscy9HQVRLU1ZWQ0ZIZWFkZXJMaW5lcy5qYXZh) | `82.813% <100%> (+0.273%)` | `7 <0> (ø)` | :arrow_down: |; | [...pleNovelAdjacencyAndChimericAlignmentEvidence.java](https://codecov.io/gh/broadinstitute/gatk/pull/4663/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1NpbXBsZU5vdmVsQWRqYWNlbmN5QW5kQ2hpbWVyaWNBbGlnbm1lbnRFdmlkZW5jZS5qYXZh) | `87.5% <100%> (+63.176%)` | `10 <2> (+5)` | :arrow_up: |; | [...ry/inference/CpxVar,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-383235501
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199:1058,Energy Efficiency,schedul,scheduler,1058,lVariationDiscoveryPipelineSpark - Used 3957 evidence target links to annotate assembled breakpoints; 00:47:25.226 INFO StructuralVariationDiscoveryPipelineSpark - Called 336 imprecise deletion variants; 00:47:25.264 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 9651 variants.; 00:47:25.280 INFO StructuralVariationDiscoveryPipelineSpark - INV: 281; 00:47:25.281 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 5630; 00:47:25.281 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 2248; 00:47:25.281 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1492; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 18/04/27 00:47:28 WARN org.apache.spark.scheduler.TaskSetManager: Stage 19 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 00:47:33 WARN org.apache.spark.scheduler.TaskSetManager: Stage 20 contains a task of very large size (2685 KB). The maximum recommended task size is 100 KB.; 00:47:39.417 INFO StructuralVariationDiscoveryPipelineSpark - Processing 323040 raw alignments from 254678 contigs.; 18/04/27 00:47:39 WARN org.apache.spark.scheduler.TaskSetManager: Stage 22 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 00:47:47.083 INFO StructuralVariationDiscoveryPipelineSpark - Filtering on MQ left 225280 contigs.; 00:47:48.135 INFO StructuralVariationDiscoveryPipelineSpark - 23702 contigs with chimeric alignments potentially giving SV signals.; 18/04/27 00:47:49 WARN org.apache.spark.scheduler.TaskSetManager: Stage 32 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 00:47:57 WARN org.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199:1225,Energy Efficiency,schedul,scheduler,1225,led 336 imprecise deletion variants; 00:47:25.264 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 9651 variants.; 00:47:25.280 INFO StructuralVariationDiscoveryPipelineSpark - INV: 281; 00:47:25.281 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 5630; 00:47:25.281 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 2248; 00:47:25.281 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1492; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 18/04/27 00:47:28 WARN org.apache.spark.scheduler.TaskSetManager: Stage 19 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 00:47:33 WARN org.apache.spark.scheduler.TaskSetManager: Stage 20 contains a task of very large size (2685 KB). The maximum recommended task size is 100 KB.; 00:47:39.417 INFO StructuralVariationDiscoveryPipelineSpark - Processing 323040 raw alignments from 254678 contigs.; 18/04/27 00:47:39 WARN org.apache.spark.scheduler.TaskSetManager: Stage 22 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 00:47:47.083 INFO StructuralVariationDiscoveryPipelineSpark - Filtering on MQ left 225280 contigs.; 00:47:48.135 INFO StructuralVariationDiscoveryPipelineSpark - 23702 contigs with chimeric alignments potentially giving SV signals.; 18/04/27 00:47:49 WARN org.apache.spark.scheduler.TaskSetManager: Stage 32 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 00:47:57 WARN org.apache.spark.scheduler.TaskSetManager: Stage 33 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 00:48:04 WARN org.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199:1509,Energy Efficiency,schedul,scheduler,1509,INFO StructuralVariationDiscoveryPipelineSpark - DUP: 2248; 00:47:25.281 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1492; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 18/04/27 00:47:28 WARN org.apache.spark.scheduler.TaskSetManager: Stage 19 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 00:47:33 WARN org.apache.spark.scheduler.TaskSetManager: Stage 20 contains a task of very large size (2685 KB). The maximum recommended task size is 100 KB.; 00:47:39.417 INFO StructuralVariationDiscoveryPipelineSpark - Processing 323040 raw alignments from 254678 contigs.; 18/04/27 00:47:39 WARN org.apache.spark.scheduler.TaskSetManager: Stage 22 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 00:47:47.083 INFO StructuralVariationDiscoveryPipelineSpark - Filtering on MQ left 225280 contigs.; 00:47:48.135 INFO StructuralVariationDiscoveryPipelineSpark - 23702 contigs with chimeric alignments potentially giving SV signals.; 18/04/27 00:47:49 WARN org.apache.spark.scheduler.TaskSetManager: Stage 32 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 00:47:57 WARN org.apache.spark.scheduler.TaskSetManager: Stage 33 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 00:48:04 WARN org.apache.spark.scheduler.TaskSetManager: Stage 34 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 00:48:13.094 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 15964 variants.; 00:48:13.107 INFO StructuralVariationDiscoveryPipeline,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199:1909,Energy Efficiency,schedul,scheduler,1909,veryPipelineSpark - BND_INV55: 0; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 18/04/27 00:47:28 WARN org.apache.spark.scheduler.TaskSetManager: Stage 19 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 00:47:33 WARN org.apache.spark.scheduler.TaskSetManager: Stage 20 contains a task of very large size (2685 KB). The maximum recommended task size is 100 KB.; 00:47:39.417 INFO StructuralVariationDiscoveryPipelineSpark - Processing 323040 raw alignments from 254678 contigs.; 18/04/27 00:47:39 WARN org.apache.spark.scheduler.TaskSetManager: Stage 22 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 00:47:47.083 INFO StructuralVariationDiscoveryPipelineSpark - Filtering on MQ left 225280 contigs.; 00:47:48.135 INFO StructuralVariationDiscoveryPipelineSpark - 23702 contigs with chimeric alignments potentially giving SV signals.; 18/04/27 00:47:49 WARN org.apache.spark.scheduler.TaskSetManager: Stage 32 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 00:47:57 WARN org.apache.spark.scheduler.TaskSetManager: Stage 33 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 00:48:04 WARN org.apache.spark.scheduler.TaskSetManager: Stage 34 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 00:48:13.094 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 15964 variants.; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 6920; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 264; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 268; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 4680; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 487; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - INS: 3345; 00,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199:2076,Energy Efficiency,schedul,scheduler,2076,r: Stage 19 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 00:47:33 WARN org.apache.spark.scheduler.TaskSetManager: Stage 20 contains a task of very large size (2685 KB). The maximum recommended task size is 100 KB.; 00:47:39.417 INFO StructuralVariationDiscoveryPipelineSpark - Processing 323040 raw alignments from 254678 contigs.; 18/04/27 00:47:39 WARN org.apache.spark.scheduler.TaskSetManager: Stage 22 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 00:47:47.083 INFO StructuralVariationDiscoveryPipelineSpark - Filtering on MQ left 225280 contigs.; 00:47:48.135 INFO StructuralVariationDiscoveryPipelineSpark - 23702 contigs with chimeric alignments potentially giving SV signals.; 18/04/27 00:47:49 WARN org.apache.spark.scheduler.TaskSetManager: Stage 32 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 00:47:57 WARN org.apache.spark.scheduler.TaskSetManager: Stage 33 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 00:48:04 WARN org.apache.spark.scheduler.TaskSetManager: Stage 34 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 00:48:13.094 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 15964 variants.; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 6920; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 264; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 268; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 4680; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 487; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - INS: 3345; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - INV: 0; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 00:48:13.107 INFO Structu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199:2243,Energy Efficiency,schedul,scheduler,2243,r: Stage 20 contains a task of very large size (2685 KB). The maximum recommended task size is 100 KB.; 00:47:39.417 INFO StructuralVariationDiscoveryPipelineSpark - Processing 323040 raw alignments from 254678 contigs.; 18/04/27 00:47:39 WARN org.apache.spark.scheduler.TaskSetManager: Stage 22 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 00:47:47.083 INFO StructuralVariationDiscoveryPipelineSpark - Filtering on MQ left 225280 contigs.; 00:47:48.135 INFO StructuralVariationDiscoveryPipelineSpark - 23702 contigs with chimeric alignments potentially giving SV signals.; 18/04/27 00:47:49 WARN org.apache.spark.scheduler.TaskSetManager: Stage 32 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 00:47:57 WARN org.apache.spark.scheduler.TaskSetManager: Stage 33 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 00:48:04 WARN org.apache.spark.scheduler.TaskSetManager: Stage 34 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 00:48:13.094 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 15964 variants.; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 6920; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 264; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 268; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 4680; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 487; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - INS: 3345; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - INV: 0; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 00:48:47.955 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 1334 variants.; 00:48:47.956 INFO StructuralVaria,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199:4924,Energy Efficiency,schedul,scheduler,4924,lVariationDiscoveryPipelineSpark - Used 3957 evidence target links to annotate assembled breakpoints; 02:20:10.082 INFO StructuralVariationDiscoveryPipelineSpark - Called 336 imprecise deletion variants; 02:20:10.121 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 9651 variants.; 02:20:10.138 INFO StructuralVariationDiscoveryPipelineSpark - INV: 281; 02:20:10.138 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 5630; 02:20:10.138 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 2248; 02:20:10.138 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1492; 02:20:10.147 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 02:20:10.148 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 02:20:10.148 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 02:20:10.148 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 02:20:10.148 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 18/04/27 02:20:12 WARN org.apache.spark.scheduler.TaskSetManager: Stage 19 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 02:20:18 WARN org.apache.spark.scheduler.TaskSetManager: Stage 20 contains a task of very large size (2599 KB). The maximum recommended task size is 100 KB.; 02:20:24.460 INFO StructuralVariationDiscoveryPipelineSpark - Processing 323040 raw alignments from 254678 contigs.; 18/04/27 02:20:24 WARN org.apache.spark.scheduler.TaskSetManager: Stage 22 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 02:20:32.040 INFO StructuralVariationDiscoveryPipelineSpark - Filtering on MQ left 225280 contigs.; 02:20:33.079 INFO StructuralVariationDiscoveryPipelineSpark - 23841 contigs with chimeric alignments potentially giving SV signals.; 18/04/27 02:20:34 WARN org.apache.spark.scheduler.TaskSetManager: Stage 32 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 02:20:42 WARN org.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199:5091,Energy Efficiency,schedul,scheduler,5091,led 336 imprecise deletion variants; 02:20:10.121 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 9651 variants.; 02:20:10.138 INFO StructuralVariationDiscoveryPipelineSpark - INV: 281; 02:20:10.138 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 5630; 02:20:10.138 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 2248; 02:20:10.138 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1492; 02:20:10.147 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 02:20:10.148 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 02:20:10.148 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 02:20:10.148 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 02:20:10.148 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 18/04/27 02:20:12 WARN org.apache.spark.scheduler.TaskSetManager: Stage 19 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 02:20:18 WARN org.apache.spark.scheduler.TaskSetManager: Stage 20 contains a task of very large size (2599 KB). The maximum recommended task size is 100 KB.; 02:20:24.460 INFO StructuralVariationDiscoveryPipelineSpark - Processing 323040 raw alignments from 254678 contigs.; 18/04/27 02:20:24 WARN org.apache.spark.scheduler.TaskSetManager: Stage 22 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 02:20:32.040 INFO StructuralVariationDiscoveryPipelineSpark - Filtering on MQ left 225280 contigs.; 02:20:33.079 INFO StructuralVariationDiscoveryPipelineSpark - 23841 contigs with chimeric alignments potentially giving SV signals.; 18/04/27 02:20:34 WARN org.apache.spark.scheduler.TaskSetManager: Stage 32 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 02:20:42 WARN org.apache.spark.scheduler.TaskSetManager: Stage 33 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 02:20:49 WARN org.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199:5375,Energy Efficiency,schedul,scheduler,5375,INFO StructuralVariationDiscoveryPipelineSpark - DUP: 2248; 02:20:10.138 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1492; 02:20:10.147 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 02:20:10.148 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 02:20:10.148 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 02:20:10.148 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 02:20:10.148 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 18/04/27 02:20:12 WARN org.apache.spark.scheduler.TaskSetManager: Stage 19 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 02:20:18 WARN org.apache.spark.scheduler.TaskSetManager: Stage 20 contains a task of very large size (2599 KB). The maximum recommended task size is 100 KB.; 02:20:24.460 INFO StructuralVariationDiscoveryPipelineSpark - Processing 323040 raw alignments from 254678 contigs.; 18/04/27 02:20:24 WARN org.apache.spark.scheduler.TaskSetManager: Stage 22 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 02:20:32.040 INFO StructuralVariationDiscoveryPipelineSpark - Filtering on MQ left 225280 contigs.; 02:20:33.079 INFO StructuralVariationDiscoveryPipelineSpark - 23841 contigs with chimeric alignments potentially giving SV signals.; 18/04/27 02:20:34 WARN org.apache.spark.scheduler.TaskSetManager: Stage 32 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 02:20:42 WARN org.apache.spark.scheduler.TaskSetManager: Stage 33 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 02:20:49 WARN org.apache.spark.scheduler.TaskSetManager: Stage 34 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 02:20:58.973 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 15659 variants.; 02:20:58.991 INFO StructuralVariationDiscoveryPipeline,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199:5775,Energy Efficiency,schedul,scheduler,5775,veryPipelineSpark - BND_INV55: 0; 02:20:10.148 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 18/04/27 02:20:12 WARN org.apache.spark.scheduler.TaskSetManager: Stage 19 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 02:20:18 WARN org.apache.spark.scheduler.TaskSetManager: Stage 20 contains a task of very large size (2599 KB). The maximum recommended task size is 100 KB.; 02:20:24.460 INFO StructuralVariationDiscoveryPipelineSpark - Processing 323040 raw alignments from 254678 contigs.; 18/04/27 02:20:24 WARN org.apache.spark.scheduler.TaskSetManager: Stage 22 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 02:20:32.040 INFO StructuralVariationDiscoveryPipelineSpark - Filtering on MQ left 225280 contigs.; 02:20:33.079 INFO StructuralVariationDiscoveryPipelineSpark - 23841 contigs with chimeric alignments potentially giving SV signals.; 18/04/27 02:20:34 WARN org.apache.spark.scheduler.TaskSetManager: Stage 32 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 02:20:42 WARN org.apache.spark.scheduler.TaskSetManager: Stage 33 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 02:20:49 WARN org.apache.spark.scheduler.TaskSetManager: Stage 34 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 02:20:58.973 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 15659 variants.; 02:20:58.991 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 6920; 02:20:58.991 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 264; 02:20:58.991 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 268; 02:20:58.991 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 5019; 02:20:58.991 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1793; 02:20:58.991 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1395; 0,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199:5942,Energy Efficiency,schedul,scheduler,5942,r: Stage 19 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 02:20:18 WARN org.apache.spark.scheduler.TaskSetManager: Stage 20 contains a task of very large size (2599 KB). The maximum recommended task size is 100 KB.; 02:20:24.460 INFO StructuralVariationDiscoveryPipelineSpark - Processing 323040 raw alignments from 254678 contigs.; 18/04/27 02:20:24 WARN org.apache.spark.scheduler.TaskSetManager: Stage 22 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 02:20:32.040 INFO StructuralVariationDiscoveryPipelineSpark - Filtering on MQ left 225280 contigs.; 02:20:33.079 INFO StructuralVariationDiscoveryPipelineSpark - 23841 contigs with chimeric alignments potentially giving SV signals.; 18/04/27 02:20:34 WARN org.apache.spark.scheduler.TaskSetManager: Stage 32 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 02:20:42 WARN org.apache.spark.scheduler.TaskSetManager: Stage 33 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 02:20:49 WARN org.apache.spark.scheduler.TaskSetManager: Stage 34 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 02:20:58.973 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 15659 variants.; 02:20:58.991 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 6920; 02:20:58.991 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 264; 02:20:58.991 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 268; 02:20:58.991 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 5019; 02:20:58.991 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1793; 02:20:58.991 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1395; 02:20:58.992 INFO StructuralVariationDiscoveryPipelineSpark - INV: 0; 02:20:58.992 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 02:20:58.992 INFO Struct,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199:6109,Energy Efficiency,schedul,scheduler,6109,r: Stage 20 contains a task of very large size (2599 KB). The maximum recommended task size is 100 KB.; 02:20:24.460 INFO StructuralVariationDiscoveryPipelineSpark - Processing 323040 raw alignments from 254678 contigs.; 18/04/27 02:20:24 WARN org.apache.spark.scheduler.TaskSetManager: Stage 22 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 02:20:32.040 INFO StructuralVariationDiscoveryPipelineSpark - Filtering on MQ left 225280 contigs.; 02:20:33.079 INFO StructuralVariationDiscoveryPipelineSpark - 23841 contigs with chimeric alignments potentially giving SV signals.; 18/04/27 02:20:34 WARN org.apache.spark.scheduler.TaskSetManager: Stage 32 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 02:20:42 WARN org.apache.spark.scheduler.TaskSetManager: Stage 33 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 02:20:49 WARN org.apache.spark.scheduler.TaskSetManager: Stage 34 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 02:20:58.973 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 15659 variants.; 02:20:58.991 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 6920; 02:20:58.991 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 264; 02:20:58.991 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 268; 02:20:58.991 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 5019; 02:20:58.991 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1793; 02:20:58.991 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1395; 02:20:58.992 INFO StructuralVariationDiscoveryPipelineSpark - INV: 0; 02:20:58.992 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 02:20:58.992 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 02:21:46.603 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 1334 variants.; 02:21:46.604 INFO StructuralVari,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199:26,Testability,log,log,26,Feature (this) branch run log; ```; 00:47:25.107 INFO StructuralVariationDiscoveryPipelineSpark - Used 3957 evidence target links to annotate assembled breakpoints; 00:47:25.226 INFO StructuralVariationDiscoveryPipelineSpark - Called 336 imprecise deletion variants; 00:47:25.264 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 9651 variants.; 00:47:25.280 INFO StructuralVariationDiscoveryPipelineSpark - INV: 281; 00:47:25.281 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 5630; 00:47:25.281 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 2248; 00:47:25.281 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1492; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 00:47:25.291 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 18/04/27 00:47:28 WARN org.apache.spark.scheduler.TaskSetManager: Stage 19 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 18/04/27 00:47:33 WARN org.apache.spark.scheduler.TaskSetManager: Stage 20 contains a task of very large size (2685 KB). The maximum recommended task size is 100 KB.; 00:47:39.417 INFO StructuralVariationDiscoveryPipelineSpark - Processing 323040 raw alignments from 254678 contigs.; 18/04/27 00:47:39 WARN org.apache.spark.scheduler.TaskSetManager: Stage 22 contains a task of very large size (2262 KB). The maximum recommended task size is 100 KB.; 00:47:47.083 INFO StructuralVariationDiscoveryPipelineSpark - Filtering on MQ left 225280 contigs.; 00:47:48.135 INFO StructuralVariationDiscoveryPipelineSpark - 23702 contigs with chimeric alignments potentially giving SV signals.; 18/04/27 00:47:49 WARN org.apache.spark.scheduler.TaskSetManager: Stage 32 contains a task of very large size (2262 KB). The maximum,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199:3892,Testability,log,log,3892,eSpark - INS: 3345; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - INV: 0; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 00:48:13.107 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 00:48:47.955 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 1334 variants.; 00:48:47.956 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 1334; 00:48:47.956 INFO StructuralVariationDiscoveryPipelineSpark - INV: 0; 00:48:47.956 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 00:48:47.956 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 00:48:47.956 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 00:48:47.956 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 00:48:47.956 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 0; 00:48:47.956 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 0; 00:48:47.956 INFO StructuralVariationDiscoveryPipelineSpark - INS: 0; ```. Master branch run log; ```; 02:20:09.897 INFO StructuralVariationDiscoveryPipelineSpark - Used 3957 evidence target links to annotate assembled breakpoints; 02:20:10.082 INFO StructuralVariationDiscoveryPipelineSpark - Called 336 imprecise deletion variants; 02:20:10.121 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 9651 variants.; 02:20:10.138 INFO StructuralVariationDiscoveryPipelineSpark - INV: 281; 02:20:10.138 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 5630; 02:20:10.138 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 2248; 02:20:10.138 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1492; 02:20:10.147 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 0; 02:20:10.148 INFO StructuralVariationDiscoveryPipelineSpark - DUP_INV: 0; 02:20:10.148 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 0; 02:20:10.148 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 0; 02:20:10.148 INFO StructuralVariationDiscoveryPipelineSpark - CPX: 0; 18/04/27,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-384848199
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:2730,Deployability,update,update,2730,"tually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) for future use. Comment by @cwhelan ; > I'm a bit confused by this comment: this method is still being called in several places, so how is it obsolete?. Reply by @SHUANG-Broad (also copied to update the ""TODO"" message; > this predicate is currently used in two places (excluding appearance in comments): `BreakpointComplications.IntraChrStrandSwitchBreakpointComplications`, where it is use to test if the input simple chimera indicates an inverse tandem duplication and trigger the logic for inferring duplicated region; and `BreakpointsInference.IntraChrStrandSwitchBreakpointInference`, where it is used for breakpoint inference. The problem is, the contig will not even be sent here, because `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` defines a simple chimera that has strand switch and the two alignments overlaps on reference as ""incomplete"", so in practice the two uses are not going to be triggered. But when we come back later and see what can be extracted from such ""incomplete"" contigs, these code could be useful again. So it is kept. ------------; ### On the problem of writing out SAM records of ""Unknown"" contigs efficiently. First round comment by @cwhelan ; > This seems like a very inef",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:3703,Energy Efficiency,efficient,efficiently,3703,"o how is it obsolete?. Reply by @SHUANG-Broad (also copied to update the ""TODO"" message; > this predicate is currently used in two places (excluding appearance in comments): `BreakpointComplications.IntraChrStrandSwitchBreakpointComplications`, where it is use to test if the input simple chimera indicates an inverse tandem duplication and trigger the logic for inferring duplicated region; and `BreakpointsInference.IntraChrStrandSwitchBreakpointInference`, where it is used for breakpoint inference. The problem is, the contig will not even be sent here, because `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` defines a simple chimera that has strand switch and the two alignments overlaps on reference as ""incomplete"", so in practice the two uses are not going to be triggered. But when we come back later and see what can be extracted from such ""incomplete"" contigs, these code could be useful again. So it is kept. ------------; ### On the problem of writing out SAM records of ""Unknown"" contigs efficiently. First round comment by @cwhelan ; > This seems like a very inefficient way to write these three files. You end up calling collect on the RDD three different times and then traversing the local collection three times. Why not make a map of contig name to bam file, collect the rdd once, and then traverse the local collection once, writing each read to the appropriate bam file from the map?. Second round comment by @cwhelan ; > This is a better but you are still collecting the RDD and passing over the collection three times. What I meant by my original suggestion was this: Why not make the map go the other way, ie make a Map<String, ReasonForAlignmentClassificationFailure> that maps contig names to their reasons? Then make a Map<ReasonForAlignmentClassificationFailure, SAMFileWriter> with three entries. Then you only have to iterate over the collection of reads once to write everything out (you just look up the writer for each entry). Reply ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:2134,Integrability,message,message,2134,"hat I'm thinking about, is two pass:; one pass for splitting them up into the 3 classes,; then another pass on each of those 3 RDD's to turn them into VariantContext's.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) for future use. Comment by @cwhelan ; > I'm a bit confused by this comment: this method is still being called in several places, so how is it obsolete?. Reply by @SHUANG-Broad (also copied to update the ""TODO"" message; > this predicate is currently used in two places (excluding appearance in comments): `BreakpointComplications.IntraChrStrandSwitchBreakpointComplications`, where it is use to test if the input simple chimera indicates an inverse tandem duplication and trigger the logic for inferring duplicated region; and `BreakpointsInference.IntraChrStrandSwitchBreakpointInference`, where",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:2748,Integrability,message,message,2748,"tually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) for future use. Comment by @cwhelan ; > I'm a bit confused by this comment: this method is still being called in several places, so how is it obsolete?. Reply by @SHUANG-Broad (also copied to update the ""TODO"" message; > this predicate is currently used in two places (excluding appearance in comments): `BreakpointComplications.IntraChrStrandSwitchBreakpointComplications`, where it is use to test if the input simple chimera indicates an inverse tandem duplication and trigger the logic for inferring duplicated region; and `BreakpointsInference.IntraChrStrandSwitchBreakpointInference`, where it is used for breakpoint inference. The problem is, the contig will not even be sent here, because `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` defines a simple chimera that has strand switch and the two alignments overlaps on reference as ""incomplete"", so in practice the two uses are not going to be triggered. But when we come back later and see what can be extracted from such ""incomplete"" contigs, these code could be useful again. So it is kept. ------------; ### On the problem of writing out SAM records of ""Unknown"" contigs efficiently. First round comment by @cwhelan ; > This seems like a very inef",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:1548,Modifiability,inherit,inherit,1548,"o the right processing methods in a single pass over the RDD. Reply by @SHuang-Broad. > I tried to fix it in this PR, but that seems to be a big task,; and probably is impossible to achieve in a single pass,; because currently each class of contig ends up producing a different type of object; (3 general classes: simple -> SimpleNovelAdjacency, complex -> ComplexVariantCanonicalRepresentation, and unknown -> SAM records of the contigs); and a groupBy() operation is necessary in the middle using these objects as keys; due to the fact that different contigs may produce the same variant; So what I'm thinking about, is two pass:; one pass for splitting them up into the 3 classes,; then another pass on each of those 3 RDD's to turn them into VariantContext's.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:1977,Modifiability,inherit,inheritance,1977,"3 general classes: simple -> SimpleNovelAdjacency, complex -> ComplexVariantCanonicalRepresentation, and unknown -> SAM records of the contigs); and a groupBy() operation is necessary in the middle using these objects as keys; due to the fact that different contigs may produce the same variant; So what I'm thinking about, is two pass:; one pass for splitting them up into the 3 classes,; then another pass on each of those 3 RDD's to turn them into VariantContext's.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) for future use. Comment by @cwhelan ; > I'm a bit confused by this comment: this method is still being called in several places, so how is it obsolete?. Reply by @SHUANG-Broad (also copied to update the ""TODO"" message; > this predicate is currently used in two places (excluding appearance in com",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:146,Performance,perform,performance,146,"For record keeping, as the comments and replies may be buried in the many commits. ------------; ### On the problem of too many splits of RDD and performance concerns. Initial comment by @cwhelan :; > I'm starting to really not like this approach of splitting up the RDD into lots of smaller RDDs for later processing. It seems inefficient to me: it launches tons of different Spark stages each of which has a bunch of overhead. Perhaps not in this PR, but I think it would be better to classify the contigs on the fly and dispatch them to the right processing methods in a single pass over the RDD. Reply by @SHuang-Broad. > I tried to fix it in this PR, but that seems to be a big task,; and probably is impossible to achieve in a single pass,; because currently each class of contig ends up producing a different type of object; (3 general classes: simple -> SimpleNovelAdjacency, complex -> ComplexVariantCanonicalRepresentation, and unknown -> SAM records of the contigs); and a groupBy() operation is necessary in the middle using these objects as keys; due to the fact that different contigs may produce the same variant; So what I'm thinking about, is two pass:; one pass for splitting them up into the 3 classes,; then another pass on each of those 3 RDD's to turn them into VariantContext's.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:2932,Testability,test,test,2932," inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) for future use. Comment by @cwhelan ; > I'm a bit confused by this comment: this method is still being called in several places, so how is it obsolete?. Reply by @SHUANG-Broad (also copied to update the ""TODO"" message; > this predicate is currently used in two places (excluding appearance in comments): `BreakpointComplications.IntraChrStrandSwitchBreakpointComplications`, where it is use to test if the input simple chimera indicates an inverse tandem duplication and trigger the logic for inferring duplicated region; and `BreakpointsInference.IntraChrStrandSwitchBreakpointInference`, where it is used for breakpoint inference. The problem is, the contig will not even be sent here, because `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` defines a simple chimera that has strand switch and the two alignments overlaps on reference as ""incomplete"", so in practice the two uses are not going to be triggered. But when we come back later and see what can be extracted from such ""incomplete"" contigs, these code could be useful again. So it is kept. ------------; ### On the problem of writing out SAM records of ""Unknown"" contigs efficiently. First round comment by @cwhelan ; > This seems like a very inefficient way to write these three files. You end up calling collect on the RDD three different times and then traversing the local collection three times. Why not make a map of contig name to bam fil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:3021,Testability,log,logic,3021," inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) for future use. Comment by @cwhelan ; > I'm a bit confused by this comment: this method is still being called in several places, so how is it obsolete?. Reply by @SHUANG-Broad (also copied to update the ""TODO"" message; > this predicate is currently used in two places (excluding appearance in comments): `BreakpointComplications.IntraChrStrandSwitchBreakpointComplications`, where it is use to test if the input simple chimera indicates an inverse tandem duplication and trigger the logic for inferring duplicated region; and `BreakpointsInference.IntraChrStrandSwitchBreakpointInference`, where it is used for breakpoint inference. The problem is, the contig will not even be sent here, because `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` defines a simple chimera that has strand switch and the two alignments overlaps on reference as ""incomplete"", so in practice the two uses are not going to be triggered. But when we come back later and see what can be extracted from such ""incomplete"" contigs, these code could be useful again. So it is kept. ------------; ### On the problem of writing out SAM records of ""Unknown"" contigs efficiently. First round comment by @cwhelan ; > This seems like a very inefficient way to write these three files. You end up calling collect on the RDD three different times and then traversing the local collection three times. Why not make a map of contig name to bam fil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:852,Usability,simpl,simple,852,"For record keeping, as the comments and replies may be buried in the many commits. ------------; ### On the problem of too many splits of RDD and performance concerns. Initial comment by @cwhelan :; > I'm starting to really not like this approach of splitting up the RDD into lots of smaller RDDs for later processing. It seems inefficient to me: it launches tons of different Spark stages each of which has a bunch of overhead. Perhaps not in this PR, but I think it would be better to classify the contigs on the fly and dispatch them to the right processing methods in a single pass over the RDD. Reply by @SHuang-Broad. > I tried to fix it in this PR, but that seems to be a big task,; and probably is impossible to achieve in a single pass,; because currently each class of contig ends up producing a different type of object; (3 general classes: simple -> SimpleNovelAdjacency, complex -> ComplexVariantCanonicalRepresentation, and unknown -> SAM records of the contigs); and a groupBy() operation is necessary in the middle using these objects as keys; due to the fact that different contigs may produce the same variant; So what I'm thinking about, is two pass:; one pass for splitting them up into the 3 classes,; then another pass on each of those 3 RDD's to turn them into VariantContext's.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:862,Usability,Simpl,SimpleNovelAdjacency,862,"For record keeping, as the comments and replies may be buried in the many commits. ------------; ### On the problem of too many splits of RDD and performance concerns. Initial comment by @cwhelan :; > I'm starting to really not like this approach of splitting up the RDD into lots of smaller RDDs for later processing. It seems inefficient to me: it launches tons of different Spark stages each of which has a bunch of overhead. Perhaps not in this PR, but I think it would be better to classify the contigs on the fly and dispatch them to the right processing methods in a single pass over the RDD. Reply by @SHuang-Broad. > I tried to fix it in this PR, but that seems to be a big task,; and probably is impossible to achieve in a single pass,; because currently each class of contig ends up producing a different type of object; (3 general classes: simple -> SimpleNovelAdjacency, complex -> ComplexVariantCanonicalRepresentation, and unknown -> SAM records of the contigs); and a groupBy() operation is necessary in the middle using these objects as keys; due to the fact that different contigs may produce the same variant; So what I'm thinking about, is two pass:; one pass for splitting them up into the 3 classes,; then another pass on each of those 3 RDD's to turn them into VariantContext's.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:2076,Usability,Simpl,SimpleChimera,2076,"jects as keys; due to the fact that different contigs may produce the same variant; So what I'm thinking about, is two pass:; one pass for splitting them up into the 3 classes,; then another pass on each of those 3 RDD's to turn them into VariantContext's.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) for future use. Comment by @cwhelan ; > I'm a bit confused by this comment: this method is still being called in several places, so how is it obsolete?. Reply by @SHUANG-Broad (also copied to update the ""TODO"" message; > this predicate is currently used in two places (excluding appearance in comments): `BreakpointComplications.IntraChrStrandSwitchBreakpointComplications`, where it is use to test if the input simple chimera indicates an inverse tandem duplication and trigger the logic for inferring dupl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:2441,Usability,Simpl,SimpleChimera,2441,"s.; Any better idea?. Reply by @cwhelan ; > That would be better, and yeah you don't have to do it in this PR.; In theory you could make the keys for the groupByKey() (ie NovelAdjacencyAndAltHaplotype, CpxVariantCanonicalRepresentation, right?) all inherit from the same superclass and do a single group by, couldn't you? Then you could do everything in a single pass. Reply by @SHuang-Broad; > Yes, that is what I'm planning but I'm not sure yet about how to approach that (I actually tried it, before putting in the above comment, and quickly ran into the problem of mixing Java serialization and Kryo serialization, so a larger re-structuring might be needed, and not just a inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) for future use. Comment by @cwhelan ; > I'm a bit confused by this comment: this method is still being called in several places, so how is it obsolete?. Reply by @SHUANG-Broad (also copied to update the ""TODO"" message; > this predicate is currently used in two places (excluding appearance in comments): `BreakpointComplications.IntraChrStrandSwitchBreakpointComplications`, where it is use to test if the input simple chimera indicates an inverse tandem duplication and trigger the logic for inferring duplicated region; and `BreakpointsInference.IntraChrStrandSwitchBreakpointInference`, where it is used for breakpoint inference. The problem is, the contig will not even be sent here, because `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:2950,Usability,simpl,simple,2950," inheritance structure). ------------; ### On the problem of having a confusing TODO for ; `boolean SimpleChimera.isCandidateInvertedDuplication()`. The todo message. > TODO: 5/5/18 Note that the use of the following predicate is currently obsoleted by; {@link AssemblyContigWithFineTunedAlignments#hasIncompletePictureFromTwoAlignments()}; because the contigs with this alignment signature is classified as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) for future use. Comment by @cwhelan ; > I'm a bit confused by this comment: this method is still being called in several places, so how is it obsolete?. Reply by @SHUANG-Broad (also copied to update the ""TODO"" message; > this predicate is currently used in two places (excluding appearance in comments): `BreakpointComplications.IntraChrStrandSwitchBreakpointComplications`, where it is use to test if the input simple chimera indicates an inverse tandem duplication and trigger the logic for inferring duplicated region; and `BreakpointsInference.IntraChrStrandSwitchBreakpointInference`, where it is used for breakpoint inference. The problem is, the contig will not even be sent here, because `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` defines a simple chimera that has strand switch and the two alignments overlaps on reference as ""incomplete"", so in practice the two uses are not going to be triggered. But when we come back later and see what can be extracted from such ""incomplete"" contigs, these code could be useful again. So it is kept. ------------; ### On the problem of writing out SAM records of ""Unknown"" contigs efficiently. First round comment by @cwhelan ; > This seems like a very inefficient way to write these three files. You end up calling collect on the RDD three different times and then traversing the local collection three times. Why not make a map of contig name to bam fil",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030
https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030:3324,Usability,simpl,simple,3324,"ied as ""incomplete"",; hence will NOT sent here for constructing SimpleChimera's.; But we may want to keep the code (and related code in BreakpointComplications) for future use. Comment by @cwhelan ; > I'm a bit confused by this comment: this method is still being called in several places, so how is it obsolete?. Reply by @SHUANG-Broad (also copied to update the ""TODO"" message; > this predicate is currently used in two places (excluding appearance in comments): `BreakpointComplications.IntraChrStrandSwitchBreakpointComplications`, where it is use to test if the input simple chimera indicates an inverse tandem duplication and trigger the logic for inferring duplicated region; and `BreakpointsInference.IntraChrStrandSwitchBreakpointInference`, where it is used for breakpoint inference. The problem is, the contig will not even be sent here, because `AssemblyContigWithFineTunedAlignments.hasIncompletePictureFromTwoAlignments()` defines a simple chimera that has strand switch and the two alignments overlaps on reference as ""incomplete"", so in practice the two uses are not going to be triggered. But when we come back later and see what can be extracted from such ""incomplete"" contigs, these code could be useful again. So it is kept. ------------; ### On the problem of writing out SAM records of ""Unknown"" contigs efficiently. First round comment by @cwhelan ; > This seems like a very inefficient way to write these three files. You end up calling collect on the RDD three different times and then traversing the local collection three times. Why not make a map of contig name to bam file, collect the rdd once, and then traverse the local collection once, writing each read to the appropriate bam file from the map?. Second round comment by @cwhelan ; > This is a better but you are still collecting the RDD and passing over the collection three times. What I meant by my original suggestion was this: Why not make the map go the other way, ie make a Map<String, ReasonForAlignmentClassi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4663#issuecomment-387899030
https://github.com/broadinstitute/gatk/pull/4664#issuecomment-381982668:1255,Deployability,pipeline,pipelines,1255,bb71c154c6f087?src=pr&el=desc) will **decrease** coverage by `0.25%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #4664 +/- ##; ==============================================; - Coverage 79.823% 79.573% -0.25% ; + Complexity 17328 17280 -48 ; ==============================================; Files 1075 1074 -1 ; Lines 62917 62907 -10 ; Branches 10181 10181 ; ==============================================; - Hits 50222 50057 -165 ; - Misses 8714 8875 +161 ; + Partials 3981 3975 -6; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4664?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-74.257%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-10%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4664#issuecomment-381982668
https://github.com/broadinstitute/gatk/pull/4664#issuecomment-381982668:1571,Testability,test,test,1571,=============; Files 1075 1074 -1 ; Lines 62917 62907 -10 ; Branches 10181 10181 ; ==============================================; - Hits 50222 50057 -165 ; - Misses 8714 8875 +161 ; + Partials 3981 3975 -6; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4664?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-74.257%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-10%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <0%> (-22.807%)` | `0% <0%> (-2%)` | |; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4664#issuecomment-381982668
https://github.com/broadinstitute/gatk/pull/4664#issuecomment-381982668:2438,Testability,test,test,2438,Fyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-10%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <0%> (-22.807%)` | `0% <0%> (-2%)` | |; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L01pbmlDbHVzdGVyVXRpbHMuamF2YQ==) | `78.947% <0%> (-10.526%)` | `6% <0%> (-1%)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `89.394% <0%> (-3.03%)` | `61% <0%> (-2%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `61.481% <0%> (-2.963%)` | `30% <0%> (-3%)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4664#issuecomment-381982668
https://github.com/broadinstitute/gatk/pull/4664#issuecomment-381982668:3021,Testability,test,test,3021,aW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-10%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <0%> (-22.807%)` | `0% <0%> (-2%)` | |; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L01pbmlDbHVzdGVyVXRpbHMuamF2YQ==) | `78.947% <0%> (-10.526%)` | `6% <0%> (-1%)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `89.394% <0%> (-3.03%)` | `61% <0%> (-2%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `61.481% <0%> (-2.963%)` | `30% <0%> (-3%)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `79.487% <0%> (-2.564%)` | `44% <0%> (ø)` | |; | [...te/hellbender/engine/spark/VariantWalkerSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvVmFyaWFudFdhbGtlclNwYXJrLmphdmE=) | `72.34% <0%> (-2.128%)` | `14% <0%> (ø)` | |; | ... and [2 more](https://codecov.io/gh/broadinstitute/gatk/pull/4664/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4664#issuecomment-381982668
https://github.com/broadinstitute/gatk/issues/4665#issuecomment-382367333:19,Integrability,depend,depends,19,"The GATK framework depends on the WellformedReadFilter, and individual tools use additional filters for various tool-specific reasons. `--disable-tool-default-read-filters` is a pretty big hammer and should probably be labeled an `@Advanced` argument. I can't speak for Mutect specifically, but I wouldn't expect tools to run and/or produce meaningful results if all read filters are completely disabled.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-382367333
https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:73,Availability,error,error,73,"Hi! I have the same issue as @chandrans.; When I run Mutect2 this is the error:; `(gatk) root@d387db9e4351:/Desktop# gatk Mutect2 -R /Desktop/UCSC_hg19_genome.fasta -I /Desktop/HP0049.bam -O /Desktop/HP0049.vcf.g; Using GATK jar /gatk/gatk-package-4.1.1.0-local.ja; Running:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.1.0-local.jar Mutect2 -R /Desktop/UCSC_hg19_genome.fasta -I /Desktop/HP0049.bam -O /Desktop/HP0049.vcf.g; 08:27:06.032 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.s; Apr 23, 2019 8:27:10 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngin; INFO: Failed to detect whether we are running on Google Compute Engine. 08:27:10.882 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.883 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.1.; 08:27:10.883 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/. 08:27:10.884 INFO Mutect2 - Executing as root@d387db9e4351 on Linux v4.9.125-linuxkit amd64. 08:27:10.884 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12. 08:27:10.885 INFO Mutect2 - Start Date/Time: April 23, 2019 8:27:05 AM UT; 08:27:10.885 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.886 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.887 INFO Mutect2 - HTSJDK Version: 2.19.; 08:27:10.887 INFO Mutect2 - Picard Version: 2.19.; 08:27:10.887 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2. 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136
https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:2815,Availability,Avail,Available,2815,"10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : fals; 08:27:10.888 INFO Mutect2 - Deflater: IntelDeflate; 08:27:10.889 INFO Mutect2 - Inflater: IntelInflate; 08:27:10.889 INFO Mutect2 - GCS max retries/reopens: 2; 08:27:10.889 INFO Mutect2 - Requester pays: disable; 08:27:10.889 INFO Mutect2 - Initializing engin; 08:27:11.333 INFO Mutect2 - Done initializing engin; 08:27:11.381 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.s; 08:27:11.383 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.s; 08:27:11.426 INFO **IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHM**; 08:27:11.427 INFO IntelPairHmm - Available threads: 4; 08:27:11.428 INFO IntelPairHmm - Requested threads: 4; 08:27:11.428 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementatio; 08:27:11.432 INFO Mutect2 - Shutting down engin; [April 23, 2019 8:27:11 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=190840832; java.lang.IllegalArgumentException: samples cannot be empt; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.ReferenceConfidenceModel.<init>(ReferenceConfidenceModel.java:116); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticReferenceConfidenceModel.<init>(SomaticReferenceConfidenceModel.java:38); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.<init>(Mutect2Engine.java:149); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.onTraversalStart(Mutect2.java:286); 	at org.br",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136
https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:3035,Availability,down,down,3035,"TRIBBLE : fals; 08:27:10.888 INFO Mutect2 - Deflater: IntelDeflate; 08:27:10.889 INFO Mutect2 - Inflater: IntelInflate; 08:27:10.889 INFO Mutect2 - GCS max retries/reopens: 2; 08:27:10.889 INFO Mutect2 - Requester pays: disable; 08:27:10.889 INFO Mutect2 - Initializing engin; 08:27:11.333 INFO Mutect2 - Done initializing engin; 08:27:11.381 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.s; 08:27:11.383 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.s; 08:27:11.426 INFO **IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHM**; 08:27:11.427 INFO IntelPairHmm - Available threads: 4; 08:27:11.428 INFO IntelPairHmm - Requested threads: 4; 08:27:11.428 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementatio; 08:27:11.432 INFO Mutect2 - Shutting down engin; [April 23, 2019 8:27:11 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=190840832; java.lang.IllegalArgumentException: samples cannot be empt; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.ReferenceConfidenceModel.<init>(ReferenceConfidenceModel.java:116); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticReferenceConfidenceModel.<init>(SomaticReferenceConfidenceModel.java:38); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.<init>(Mutect2Engine.java:149); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.onTraversalStart(Mutect2.java:286); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:982); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136
https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:610,Performance,Load,Loading,610,"Hi! I have the same issue as @chandrans.; When I run Mutect2 this is the error:; `(gatk) root@d387db9e4351:/Desktop# gatk Mutect2 -R /Desktop/UCSC_hg19_genome.fasta -I /Desktop/HP0049.bam -O /Desktop/HP0049.vcf.g; Using GATK jar /gatk/gatk-package-4.1.1.0-local.ja; Running:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.1.0-local.jar Mutect2 -R /Desktop/UCSC_hg19_genome.fasta -I /Desktop/HP0049.bam -O /Desktop/HP0049.vcf.g; 08:27:06.032 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.s; Apr 23, 2019 8:27:10 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngin; INFO: Failed to detect whether we are running on Google Compute Engine. 08:27:10.882 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.883 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.1.; 08:27:10.883 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/. 08:27:10.884 INFO Mutect2 - Executing as root@d387db9e4351 on Linux v4.9.125-linuxkit amd64. 08:27:10.884 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12. 08:27:10.885 INFO Mutect2 - Start Date/Time: April 23, 2019 8:27:05 AM UT; 08:27:10.885 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.886 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.887 INFO Mutect2 - HTSJDK Version: 2.19.; 08:27:10.887 INFO Mutect2 - Picard Version: 2.19.; 08:27:10.887 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2. 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136
https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:2415,Performance,Load,Loading,2415,"tect2 - Start Date/Time: April 23, 2019 8:27:05 AM UT; 08:27:10.885 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.886 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.887 INFO Mutect2 - HTSJDK Version: 2.19.; 08:27:10.887 INFO Mutect2 - Picard Version: 2.19.; 08:27:10.887 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2. 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : fals; 08:27:10.888 INFO Mutect2 - Deflater: IntelDeflate; 08:27:10.889 INFO Mutect2 - Inflater: IntelInflate; 08:27:10.889 INFO Mutect2 - GCS max retries/reopens: 2; 08:27:10.889 INFO Mutect2 - Requester pays: disable; 08:27:10.889 INFO Mutect2 - Initializing engin; 08:27:11.333 INFO Mutect2 - Done initializing engin; 08:27:11.381 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.s; 08:27:11.383 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.s; 08:27:11.426 INFO **IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHM**; 08:27:11.427 INFO IntelPairHmm - Available threads: 4; 08:27:11.428 INFO IntelPairHmm - Requested threads: 4; 08:27:11.428 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementatio; 08:27:11.432 INFO Mutect2 - Shutting down engin; [April 23, 2019 8:27:11 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=190840832; java.lang.IllegalArgumentException: samples cannot be empt; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.Refer",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136
https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:2568,Performance,Load,Loading,2568," INFO Mutect2 - -----------------------------------------------------------; 08:27:10.887 INFO Mutect2 - HTSJDK Version: 2.19.; 08:27:10.887 INFO Mutect2 - Picard Version: 2.19.; 08:27:10.887 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2. 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : fals; 08:27:10.888 INFO Mutect2 - Deflater: IntelDeflate; 08:27:10.889 INFO Mutect2 - Inflater: IntelInflate; 08:27:10.889 INFO Mutect2 - GCS max retries/reopens: 2; 08:27:10.889 INFO Mutect2 - Requester pays: disable; 08:27:10.889 INFO Mutect2 - Initializing engin; 08:27:11.333 INFO Mutect2 - Done initializing engin; 08:27:11.381 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.s; 08:27:11.383 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.s; 08:27:11.426 INFO **IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHM**; 08:27:11.427 INFO IntelPairHmm - Available threads: 4; 08:27:11.428 INFO IntelPairHmm - Requested threads: 4; 08:27:11.428 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementatio; 08:27:11.432 INFO Mutect2 - Shutting down engin; [April 23, 2019 8:27:11 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=190840832; java.lang.IllegalArgumentException: samples cannot be empt; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.ReferenceConfidenceModel.<init>(ReferenceConfidenceModel.java:116); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticReferenceConfidenceModel.<init>",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136
https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:2937,Performance,multi-thread,multi-threaded,2937,"ITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : fals; 08:27:10.888 INFO Mutect2 - Deflater: IntelDeflate; 08:27:10.889 INFO Mutect2 - Inflater: IntelInflate; 08:27:10.889 INFO Mutect2 - GCS max retries/reopens: 2; 08:27:10.889 INFO Mutect2 - Requester pays: disable; 08:27:10.889 INFO Mutect2 - Initializing engin; 08:27:11.333 INFO Mutect2 - Done initializing engin; 08:27:11.381 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.s; 08:27:11.383 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.s; 08:27:11.426 INFO **IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHM**; 08:27:11.427 INFO IntelPairHmm - Available threads: 4; 08:27:11.428 INFO IntelPairHmm - Requested threads: 4; 08:27:11.428 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementatio; 08:27:11.432 INFO Mutect2 - Shutting down engin; [April 23, 2019 8:27:11 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=190840832; java.lang.IllegalArgumentException: samples cannot be empt; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.ReferenceConfidenceModel.<init>(ReferenceConfidenceModel.java:116); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticReferenceConfidenceModel.<init>(SomaticReferenceConfidenceModel.java:38); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.<init>(Mutect2Engine.java:149); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.onTraversalStart(Mutect2.java:286); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:982); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136
https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:863,Safety,detect,detect,863,"Hi! I have the same issue as @chandrans.; When I run Mutect2 this is the error:; `(gatk) root@d387db9e4351:/Desktop# gatk Mutect2 -R /Desktop/UCSC_hg19_genome.fasta -I /Desktop/HP0049.bam -O /Desktop/HP0049.vcf.g; Using GATK jar /gatk/gatk-package-4.1.1.0-local.ja; Running:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.1.0-local.jar Mutect2 -R /Desktop/UCSC_hg19_genome.fasta -I /Desktop/HP0049.bam -O /Desktop/HP0049.vcf.g; 08:27:06.032 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.s; Apr 23, 2019 8:27:10 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngin; INFO: Failed to detect whether we are running on Google Compute Engine. 08:27:10.882 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.883 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.1.; 08:27:10.883 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/. 08:27:10.884 INFO Mutect2 - Executing as root@d387db9e4351 on Linux v4.9.125-linuxkit amd64. 08:27:10.884 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12. 08:27:10.885 INFO Mutect2 - Start Date/Time: April 23, 2019 8:27:05 AM UT; 08:27:10.885 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.886 INFO Mutect2 - -----------------------------------------------------------; 08:27:10.887 INFO Mutect2 - HTSJDK Version: 2.19.; 08:27:10.887 INFO Mutect2 - Picard Version: 2.19.; 08:27:10.887 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2. 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals; 08:27:10.888 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : tru; 08:27:10.888 INFO Mutec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136
https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136:3312,Security,validat,validateArg,3312," 08:27:11.333 INFO Mutect2 - Done initializing engin; 08:27:11.381 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_utils.s; 08:27:11.383 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.s; 08:27:11.426 INFO **IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHM**; 08:27:11.427 INFO IntelPairHmm - Available threads: 4; 08:27:11.428 INFO IntelPairHmm - Requested threads: 4; 08:27:11.428 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementatio; 08:27:11.432 INFO Mutect2 - Shutting down engin; [April 23, 2019 8:27:11 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.09 minutes.; Runtime.totalMemory()=190840832; java.lang.IllegalArgumentException: samples cannot be empt; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.ReferenceConfidenceModel.<init>(ReferenceConfidenceModel.java:116); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticReferenceConfidenceModel.<init>(SomaticReferenceConfidenceModel.java:38); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.<init>(Mutect2Engine.java:149); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.onTraversalStart(Mutect2.java:286); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:982); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-485729136
https://github.com/broadinstitute/gatk/issues/4665#issuecomment-486384034:70,Availability,error,error,70,"@Ismaelfermir The message about ""Flush-to-zero"" is normal, and not an error. The problem is likely that your BAM doesn't have read groups in its header, or if it does, those read groups may lack sample names.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-486384034
https://github.com/broadinstitute/gatk/issues/4665#issuecomment-486384034:18,Integrability,message,message,18,"@Ismaelfermir The message about ""Flush-to-zero"" is normal, and not an error. The problem is likely that your BAM doesn't have read groups in its header, or if it does, those read groups may lack sample names.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665#issuecomment-486384034
https://github.com/broadinstitute/gatk/pull/4666#issuecomment-382129089:1853,Deployability,pipeline,pipelines,1853,=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...lbender/tools/spark/pathseq/PathSeqScoreSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4666/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BhdGhTZXFTY29yZVNwYXJrLmphdmE=) | `57.407% <ø> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...nder/tools/spark/pathseq/PathSeqPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4666/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BhdGhTZXFQaXBlbGluZVNwYXJrLmphdmE=) | `81.25% <ø> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...stitute/hellbender/engine/spark/GATKSparkTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/4666/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1NwYXJrVG9vbC5qYXZh) | `83.495% <100%> (+0.162%)` | `56 <0> (ø)` | :arrow_down: |; | [...k/pipelines/BwaAndMarkDuplicatesPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4666/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQndhQW5kTWFya0R1cGxpY2F0ZXNQaXBlbGluZVNwYXJrLmphdmE=) | `92.593% <100%> (+13.645%)` | `7 <0> (+3)` | :arrow_up: |; | [...ellbender/tools/spark/pathseq/PathSeqBwaSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4666/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BhdGhTZXFCd2FTcGFyay5qYXZh) | `67.391% <100%> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...ender/engine/spark/datasources/ReadsSparkSink.java](https://codecov.io/gh/broadinstitute/gatk/pull/4666/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NpbmsuamF2YQ==) | `73.043% <50%> (-2.411%)` | `26 <2> (+2)` | |; | [...der/tools/walkers/mutect/M2ArgumentCollection.java](https://codecov.io/gh/b,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4666#issuecomment-382129089
https://github.com/broadinstitute/gatk/pull/4666#issuecomment-382129089:3138,Testability,test,test,3138,5kTWFya0R1cGxpY2F0ZXNQaXBlbGluZVNwYXJrLmphdmE=) | `92.593% <100%> (+13.645%)` | `7 <0> (+3)` | :arrow_up: |; | [...ellbender/tools/spark/pathseq/PathSeqBwaSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4666/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BhdGhTZXFCd2FTcGFyay5qYXZh) | `67.391% <100%> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...ender/engine/spark/datasources/ReadsSparkSink.java](https://codecov.io/gh/broadinstitute/gatk/pull/4666/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NpbmsuamF2YQ==) | `73.043% <50%> (-2.411%)` | `26 <2> (+2)` | |; | [...der/tools/walkers/mutect/M2ArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4666/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NMkFyZ3VtZW50Q29sbGVjdGlvbi5qYXZh) | `96.875% <0%> (-3.125%)` | `3% <0%> (+2%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4666/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `62.921% <0%> (-1.523%)` | `49% <0%> (+16%)` | |; | [...hellbender/utils/read/markduplicates/ReadsKey.java](https://codecov.io/gh/broadinstitute/gatk/pull/4666/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL1JlYWRzS2V5LmphdmE=) | `93.103% <0%> (-1.341%)` | `15% <0%> (+6%)` | |; | [...itute/hellbender/tools/walkers/bqsr/ApplyBQSR.java](https://codecov.io/gh/broadinstitute/gatk/pull/4666/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQXBwbHlCUVNSLmphdmE=) | `90.909% <0%> (-0.758%)` | `11% <0%> (+5%)` | |; | ... and [56 more](https://codecov.io/gh/broadinstitute/gatk/pull/4666/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4666#issuecomment-382129089
https://github.com/broadinstitute/gatk/pull/4666#issuecomment-387097386:26,Testability,test,test,26,@jamesemery Apparently my test doesn't work on travis. Gaaah.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4666#issuecomment-387097386
https://github.com/broadinstitute/gatk/issues/4668#issuecomment-389265736:38,Deployability,toggle,toggle,38,"This should be an option that you can toggle, but should default to ignoring transcript versions. With the toggle will need to double-check Gencode as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4668#issuecomment-389265736
https://github.com/broadinstitute/gatk/issues/4668#issuecomment-389265736:107,Deployability,toggle,toggle,107,"This should be an option that you can toggle, but should default to ignoring transcript versions. With the toggle will need to double-check Gencode as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4668#issuecomment-389265736
https://github.com/broadinstitute/gatk/issues/4668#issuecomment-416679277:27,Deployability,toggle,toggle,27,Create new comparators and toggle between the two.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4668#issuecomment-416679277
https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382309887:90,Deployability,install,installed,90,"Do you have the [build requirements](https://github.com/broadinstitute/gatk#requirements) installed? Concretely, [git lfs](https://git-lfs.github.com/) might not be there and thus the command is failing. This is just a guess...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382309887
https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382363528:98,Availability,down,download,98,"> How can I install GATK4 successful?. If you just want to install (as opposed to build), you can download a [release .zip or .tar](https://github.com/broadinstitute/gatk/releases) directly. If you want to build, then as @magicDGS suggested, you need to have git-lfs installed. If you already have that, I would try manually running the command `git lfs pull --include src/main/resources/large` from the root of the repo clone and see if you get a more detailed failure message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382363528
https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382363528:462,Availability,failure,failure,462,"> How can I install GATK4 successful?. If you just want to install (as opposed to build), you can download a [release .zip or .tar](https://github.com/broadinstitute/gatk/releases) directly. If you want to build, then as @magicDGS suggested, you need to have git-lfs installed. If you already have that, I would try manually running the command `git lfs pull --include src/main/resources/large` from the root of the repo clone and see if you get a more detailed failure message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382363528
https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382363528:12,Deployability,install,install,12,"> How can I install GATK4 successful?. If you just want to install (as opposed to build), you can download a [release .zip or .tar](https://github.com/broadinstitute/gatk/releases) directly. If you want to build, then as @magicDGS suggested, you need to have git-lfs installed. If you already have that, I would try manually running the command `git lfs pull --include src/main/resources/large` from the root of the repo clone and see if you get a more detailed failure message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382363528
https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382363528:59,Deployability,install,install,59,"> How can I install GATK4 successful?. If you just want to install (as opposed to build), you can download a [release .zip or .tar](https://github.com/broadinstitute/gatk/releases) directly. If you want to build, then as @magicDGS suggested, you need to have git-lfs installed. If you already have that, I would try manually running the command `git lfs pull --include src/main/resources/large` from the root of the repo clone and see if you get a more detailed failure message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382363528
https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382363528:110,Deployability,release,release,110,"> How can I install GATK4 successful?. If you just want to install (as opposed to build), you can download a [release .zip or .tar](https://github.com/broadinstitute/gatk/releases) directly. If you want to build, then as @magicDGS suggested, you need to have git-lfs installed. If you already have that, I would try manually running the command `git lfs pull --include src/main/resources/large` from the root of the repo clone and see if you get a more detailed failure message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382363528
https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382363528:171,Deployability,release,releases,171,"> How can I install GATK4 successful?. If you just want to install (as opposed to build), you can download a [release .zip or .tar](https://github.com/broadinstitute/gatk/releases) directly. If you want to build, then as @magicDGS suggested, you need to have git-lfs installed. If you already have that, I would try manually running the command `git lfs pull --include src/main/resources/large` from the root of the repo clone and see if you get a more detailed failure message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382363528
https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382363528:267,Deployability,install,installed,267,"> How can I install GATK4 successful?. If you just want to install (as opposed to build), you can download a [release .zip or .tar](https://github.com/broadinstitute/gatk/releases) directly. If you want to build, then as @magicDGS suggested, you need to have git-lfs installed. If you already have that, I would try manually running the command `git lfs pull --include src/main/resources/large` from the root of the repo clone and see if you get a more detailed failure message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382363528
https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382363528:470,Integrability,message,message,470,"> How can I install GATK4 successful?. If you just want to install (as opposed to build), you can download a [release .zip or .tar](https://github.com/broadinstitute/gatk/releases) directly. If you want to build, then as @magicDGS suggested, you need to have git-lfs installed. If you already have that, I would try manually running the command `git lfs pull --include src/main/resources/large` from the root of the repo clone and see if you get a more detailed failure message.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382363528
https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382592770:40,Deployability,install,install,40,"THX all!; The problem is that I did not install ""git lfs"" @magicDGS",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4669#issuecomment-382592770
https://github.com/broadinstitute/gatk/pull/4673#issuecomment-382458425:77,Deployability,update,update,77,"To phrase it another way, you imported the new program group, but you didn't update the `programGroup = ` line in the `CommandLineProgramProperties` to refer to it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4673#issuecomment-382458425
https://github.com/broadinstitute/gatk/pull/4673#issuecomment-384447522:68,Deployability,release,release,68,I see. Thank you both. Just fixed it and hope it can go in the next release.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4673#issuecomment-384447522
https://github.com/broadinstitute/gatk/pull/4673#issuecomment-384479161:2179,Usability,Simpl,SimpleNovelAdjacencyInterpreter,2179,) | `83.333% <ø> (ø)` | `4 <0> (ø)` | :arrow_down: |; | [...nder/tools/walkers/vqsr/FilterVariantTranches.java](https://codecov.io/gh/broadinstitute/gatk/pull/4673/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvRmlsdGVyVmFyaWFudFRyYW5jaGVzLmphdmE=) | `76.087% <ø> (ø)` | `5 <0> (ø)` | :arrow_down: |; | [...ellbender/tools/walkers/vqsr/CNNScoreVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4673/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OU2NvcmVWYXJpYW50cy5qYXZh) | `81.69% <ø> (+7.634%)` | `55 <0> (+15)` | :arrow_up: |; | [...hellbender/tools/walkers/vqsr/CNNVariantTrain.java](https://codecov.io/gh/broadinstitute/gatk/pull/4673/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OVmFyaWFudFRyYWluLmphdmE=) | `80.645% <ø> (ø)` | `4 <0> (ø)` | :arrow_down: |; | [...ery/inference/SimpleNovelAdjacencyInterpreter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4673/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1NpbXBsZU5vdmVsQWRqYWNlbmN5SW50ZXJwcmV0ZXIuamF2YQ==) | `35.87% <0%> (-64.13%)` | `7% <0%> (ø)` | |; | [...tractOpticalDuplicateFinderCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/4673/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL0Fic3RyYWN0T3B0aWNhbER1cGxpY2F0ZUZpbmRlckNvbW1hbmRMaW5lUHJvZ3JhbS5qYXZh) | `57.143% <0%> (-42.857%)` | `4% <0%> (ø)` | |; | [...stitute/hellbender/engine/spark/GATKSparkTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/4673/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1NwYXJrVG9vbC5qYXZh) | `48.295% <0%> (-35.038%)` | `56% <0%> (ø)` | |; | [...tools/walkers/haplotypecaller/HaplotypeCaller.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4673#issuecomment-384479161
https://github.com/broadinstitute/gatk/pull/4673#issuecomment-390752790:52,Testability,test,tests,52,@chandrans Can you check to see why this is failing tests now?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4673#issuecomment-390752790
https://github.com/broadinstitute/gatk/pull/4673#issuecomment-390758890:4,Availability,failure,failures,4,The failures look transient so I'm restarting them.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4673#issuecomment-390758890
https://github.com/broadinstitute/gatk/pull/4673#issuecomment-394793224:18,Testability,test,tests,18,"Hey Chris. If the tests all pass now, we should be good to merge. Geraldine said if tests fail, ask you for help :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4673#issuecomment-394793224
https://github.com/broadinstitute/gatk/pull/4673#issuecomment-394793224:84,Testability,test,tests,84,"Hey Chris. If the tests all pass now, we should be good to merge. Geraldine said if tests fail, ask you for help :)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4673#issuecomment-394793224
https://github.com/broadinstitute/gatk/pull/4674#issuecomment-388402050:39,Availability,failure,failure,39,@jamesemery Looks like there is a test failure in this PR now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4674#issuecomment-388402050
https://github.com/broadinstitute/gatk/pull/4674#issuecomment-388402050:34,Testability,test,test,34,@jamesemery Looks like there is a test failure in this PR now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4674#issuecomment-388402050
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:1146,Energy Efficiency,reduce,reducers,1146," compare the result of MarkDuplicates and MarkDuplicatesSpark.; the same input SAM file and the default parameter, the MarkDuplicatesSpark have more data marked as duplicated.; Can you give me any suggest how to debug it, why the Spark version have more data marked?. READ_PAIR_DUPLICATES; **11933661 (MarkDuplicates); 11974162 (MarkDuplicatesSpark)**. Here is the metric file; ```. MarkDuplicatesSpark --output hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates.bam --metrics-file hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates-metrics.txt --input hdfs://wolfpass-aep:9000/user/test/spark_412.bowtie2.bam --spark-master yarn --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES --do-not-mark-unmapped-mates false --read-name-regex <optimized capture of last three ':' separated fields as numeric values> --optical-duplicate-pixel-distance 100 --read-validation-stringency SILENT --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:759,Performance,optimiz,optimized,759," compare the result of MarkDuplicates and MarkDuplicatesSpark.; the same input SAM file and the default parameter, the MarkDuplicatesSpark have more data marked as duplicated.; Can you give me any suggest how to debug it, why the Spark version have more data marked?. READ_PAIR_DUPLICATES; **11933661 (MarkDuplicates); 11974162 (MarkDuplicatesSpark)**. Here is the metric file; ```. MarkDuplicatesSpark --output hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates.bam --metrics-file hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates-metrics.txt --input hdfs://wolfpass-aep:9000/user/test/spark_412.bowtie2.bam --spark-master yarn --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES --do-not-mark-unmapped-mates false --read-name-regex <optimized capture of last three ':' separated fields as numeric values> --optical-duplicate-pixel-distance 100 --read-validation-stringency SILENT --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:2459,Performance,optimiz,optimized,2459,eflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt --MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP 50000 --MAX_FILE_HANDLES_FOR_READ_ENDS_MAP 8000 --SORTING_COLLECTION_SIZE_RATIO 0.25 --TAG_DUPLICATE_SET_MEMBERS false --REMOVE_SEQUENCING_DUPLICATES false --TAGGING_POLICY DontTag --CLEAR_DT true --ADD_PG_TAG_TO_READS true --REMOVE_DUPLICATES false --ASSUME_SORTED false --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --PROGRAM_RECORD_ID MarkDup; licates --PROGRAM_GROUP_NAME MarkDuplicates --READ_NAME_REGEX <optimized capture of last three ':' separated fields as numeric values> --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 --MAX_OPTICAL_DUPLICATE_SET_SIZE 300000 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help f; alse --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false. METRICS CLASS	picard.sam.DuplicationMetrics; LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES	READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11933661	585768	0.22221	06317338. ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:877,Security,validat,validation-stringency,877," compare the result of MarkDuplicates and MarkDuplicatesSpark.; the same input SAM file and the default parameter, the MarkDuplicatesSpark have more data marked as duplicated.; Can you give me any suggest how to debug it, why the Spark version have more data marked?. READ_PAIR_DUPLICATES; **11933661 (MarkDuplicates); 11974162 (MarkDuplicatesSpark)**. Here is the metric file; ```. MarkDuplicatesSpark --output hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates.bam --metrics-file hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates-metrics.txt --input hdfs://wolfpass-aep:9000/user/test/spark_412.bowtie2.bam --spark-master yarn --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES --do-not-mark-unmapped-mates false --read-name-regex <optimized capture of last three ':' separated fields as numeric values> --optical-duplicate-pixel-distance 100 --read-validation-stringency SILENT --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:1065,Security,validat,validation,1065," compare the result of MarkDuplicates and MarkDuplicatesSpark.; the same input SAM file and the default parameter, the MarkDuplicatesSpark have more data marked as duplicated.; Can you give me any suggest how to debug it, why the Spark version have more data marked?. READ_PAIR_DUPLICATES; **11933661 (MarkDuplicates); 11974162 (MarkDuplicatesSpark)**. Here is the metric file; ```. MarkDuplicatesSpark --output hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates.bam --metrics-file hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates-metrics.txt --input hdfs://wolfpass-aep:9000/user/test/spark_412.bowtie2.bam --spark-master yarn --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES --do-not-mark-unmapped-mates false --read-name-regex <optimized capture of last three ':' separated fields as numeric values> --optical-duplicate-pixel-distance 100 --read-validation-stringency SILENT --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt -",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:448,Testability,test,test,448,"I have compare the result of MarkDuplicates and MarkDuplicatesSpark.; the same input SAM file and the default parameter, the MarkDuplicatesSpark have more data marked as duplicated.; Can you give me any suggest how to debug it, why the Spark version have more data marked?. READ_PAIR_DUPLICATES; **11933661 (MarkDuplicates); 11974162 (MarkDuplicatesSpark)**. Here is the metric file; ```. MarkDuplicatesSpark --output hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates.bam --metrics-file hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates-metrics.txt --input hdfs://wolfpass-aep:9000/user/test/spark_412.bowtie2.bam --spark-master yarn --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES --do-not-mark-unmapped-mates false --read-name-regex <optimized capture of last three ':' separated fields as numeric values> --optical-duplicate-pixel-distance 100 --read-validation-stringency SILENT --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:527,Testability,test,test,527,"I have compare the result of MarkDuplicates and MarkDuplicatesSpark.; the same input SAM file and the default parameter, the MarkDuplicatesSpark have more data marked as duplicated.; Can you give me any suggest how to debug it, why the Spark version have more data marked?. READ_PAIR_DUPLICATES; **11933661 (MarkDuplicates); 11974162 (MarkDuplicatesSpark)**. Here is the metric file; ```. MarkDuplicatesSpark --output hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates.bam --metrics-file hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates-metrics.txt --input hdfs://wolfpass-aep:9000/user/test/spark_412.bowtie2.bam --spark-master yarn --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES --do-not-mark-unmapped-mates false --read-name-regex <optimized capture of last three ':' separated fields as numeric values> --optical-duplicate-pixel-distance 100 --read-validation-stringency SILENT --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:607,Testability,test,test,607,"I have compare the result of MarkDuplicates and MarkDuplicatesSpark.; the same input SAM file and the default parameter, the MarkDuplicatesSpark have more data marked as duplicated.; Can you give me any suggest how to debug it, why the Spark version have more data marked?. READ_PAIR_DUPLICATES; **11933661 (MarkDuplicates); 11974162 (MarkDuplicatesSpark)**. Here is the metric file; ```. MarkDuplicatesSpark --output hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates.bam --metrics-file hdfs://wolfpass-aep:9000/user/test/spark_412.MarkDuplicates-metrics.txt --input hdfs://wolfpass-aep:9000/user/test/spark_412.bowtie2.bam --spark-master yarn --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES --do-not-mark-unmapped-mates false --read-name-regex <optimized capture of last three ':' separated fields as numeric values> --optical-duplicate-pixel-distance 100 --read-validation-stringency SILENT --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:1794,Testability,test,test,1794,arated fields as numeric values> --optical-duplicate-pixel-distance 100 --read-validation-stringency SILENT --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt --MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP 50000 --MAX_FILE_HANDLES_FOR_READ_ENDS_MAP 8000 --SORTING_COLLECTION_SIZE_RATIO 0.25 --TAG_DUPLICATE_SET_MEMBERS false --REMOVE_SEQUENCING_DUPLICATES false --TAGGING_POLICY DontTag --CLEAR_DT true --ADD_PG_TAG_TO_READS true --REMOVE_DUPLICATES false --ASSUME_SORTED false --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --PROGRAM_RECORD_ID MarkDup; licates --PROGRAM_GROUP_NAME MarkDuplicates --READ_NAME_REGEX <optimized capture of last three ':' separated fields as numeric values> --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 --MAX_OPTICAL_DUPLICATE_SET_SIZE 300000 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_se,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:1812,Testability,TEST,TEST,1812,arated fields as numeric values> --optical-duplicate-pixel-distance 100 --read-validation-stringency SILENT --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt --MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP 50000 --MAX_FILE_HANDLES_FOR_READ_ENDS_MAP 8000 --SORTING_COLLECTION_SIZE_RATIO 0.25 --TAG_DUPLICATE_SET_MEMBERS false --REMOVE_SEQUENCING_DUPLICATES false --TAGGING_POLICY DontTag --CLEAR_DT true --ADD_PG_TAG_TO_READS true --REMOVE_DUPLICATES false --ASSUME_SORTED false --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --PROGRAM_RECORD_ID MarkDup; licates --PROGRAM_GROUP_NAME MarkDuplicates --READ_NAME_REGEX <optimized capture of last three ':' separated fields as numeric values> --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 --MAX_OPTICAL_DUPLICATE_SET_SIZE 300000 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_se,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:1860,Testability,test,test,1860,--read-validation-stringency SILENT --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt --MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP 50000 --MAX_FILE_HANDLES_FOR_READ_ENDS_MAP 8000 --SORTING_COLLECTION_SIZE_RATIO 0.25 --TAG_DUPLICATE_SET_MEMBERS false --REMOVE_SEQUENCING_DUPLICATES false --TAGGING_POLICY DontTag --CLEAR_DT true --ADD_PG_TAG_TO_READS true --REMOVE_DUPLICATES false --ASSUME_SORTED false --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --PROGRAM_RECORD_ID MarkDup; licates --PROGRAM_GROUP_NAME MarkDuplicates --READ_NAME_REGEX <optimized capture of last three ':' separated fields as numeric values> --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 --MAX_OPTICAL_DUPLICATE_SET_SIZE 300000 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help f; alse --version false --showHidden false --USE_JDK_,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:1878,Testability,TEST,TEST,1878,--read-validation-stringency SILENT --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt --MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP 50000 --MAX_FILE_HANDLES_FOR_READ_ENDS_MAP 8000 --SORTING_COLLECTION_SIZE_RATIO 0.25 --TAG_DUPLICATE_SET_MEMBERS false --REMOVE_SEQUENCING_DUPLICATES false --TAGGING_POLICY DontTag --CLEAR_DT true --ADD_PG_TAG_TO_READS true --REMOVE_DUPLICATES false --ASSUME_SORTED false --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --PROGRAM_RECORD_ID MarkDup; licates --PROGRAM_GROUP_NAME MarkDuplicates --READ_NAME_REGEX <optimized capture of last three ':' separated fields as numeric values> --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 --MAX_OPTICAL_DUPLICATE_SET_SIZE 300000 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help f; alse --version false --showHidden false --USE_JDK_,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:1939,Testability,test,test,1939,ding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt --MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP 50000 --MAX_FILE_HANDLES_FOR_READ_ENDS_MAP 8000 --SORTING_COLLECTION_SIZE_RATIO 0.25 --TAG_DUPLICATE_SET_MEMBERS false --REMOVE_SEQUENCING_DUPLICATES false --TAGGING_POLICY DontTag --CLEAR_DT true --ADD_PG_TAG_TO_READS true --REMOVE_DUPLICATES false --ASSUME_SORTED false --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --PROGRAM_RECORD_ID MarkDup; licates --PROGRAM_GROUP_NAME MarkDuplicates --READ_NAME_REGEX <optimized capture of last three ':' separated fields as numeric values> --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 --MAX_OPTICAL_DUPLICATE_SET_SIZE 300000 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help f; alse --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false. METRICS CLASS	picard.sam.Duplicatio,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905:1957,Testability,TEST,TEST,1957,ding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --bam-partition-size 0 --disable-sequence-dictionary-validation false --add-output-vcf-command-line true --sharded-output false --num-reducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false. METRICS CLASS	org.broadinstitute.hellbender.utils.read.markduplicates.GATKDuplicationMetrics LIBRARY	UNPAIRED_READS_EXAMINED	READ_PAIRS_EXAMINED	SECONDARY_OR_SUPPLEMENTARY_RDS	UNMAPPED_READS	UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES	READ_PAIR_OPTICAL_DUPLICATES	PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE; lib1	173613	53799913	0	7610605	81003	11974162	585768	0.222961	05870713. MarkDuplicates --INPUT /home/test/WGS_pipeline/TEST/output/orig_412.bowtie2.bam --OUTPUT /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates.bam --METRICS_FILE /home/test/WGS_pipeline/TEST/output/orig_412.MarkDuplicates-metrics.txt --MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP 50000 --MAX_FILE_HANDLES_FOR_READ_ENDS_MAP 8000 --SORTING_COLLECTION_SIZE_RATIO 0.25 --TAG_DUPLICATE_SET_MEMBERS false --REMOVE_SEQUENCING_DUPLICATES false --TAGGING_POLICY DontTag --CLEAR_DT true --ADD_PG_TAG_TO_READS true --REMOVE_DUPLICATES false --ASSUME_SORTED false --DUPLICATE_SCORING_STRATEGY SUM_OF_BASE_QUALITIES --PROGRAM_RECORD_ID MarkDup; licates --PROGRAM_GROUP_NAME MarkDuplicates --READ_NAME_REGEX <optimized capture of last three ':' separated fields as numeric values> --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 --MAX_OPTICAL_DUPLICATE_SET_SIZE 300000 --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help f; alse --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false. METRICS CLASS	picard.sam.Duplicatio,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427229905
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427388821:140,Deployability,release,released,140,"Hello, @oldmikeyang I'm in the middle of doing a tie out for MarkDuplicatesSpark right now. I just recently fixed (and it will hopefully be released soon) some counting issues involving the metrics collection (it was over-counting the number of duplicate pairs marked compared to picard) I suspect it is likely that the actual bam output is correct. I will have a branch soon that I would ask you to try markDuplicatesSpark again on and tell me if it's still causing problems, unfortunately an unrelated fix requires a change to go into picard https://github.com/broadinstitute/picard/pull/1230. . I will let you know when the PR is open, as I would love to know if it fixes this mismatch.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427388821
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:274,Availability,failure,failure,274,"> @oldmikeyang Do you get the same result if the inputs and outputs to `MarkDuplicatesSpark` are on the local filesystem rather than HDFS?; > ; > @jamesemery Can you comment?. I just try the MarkDuplicatesSpark from local file system without the HDFS.; the application will failure. It said can't find the file, but the file is on the local file system. ```; ls -la /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; -rw-rw-r--. 1 test test 4668988887 Oct 4 11:27 /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; ```. Here is SPARK information. ```; A USER ERROR has occurred: Failed to read bam header from; /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.sec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:575,Availability,ERROR,ERROR,575,"> @oldmikeyang Do you get the same result if the inputs and outputs to `MarkDuplicatesSpark` are on the local filesystem rather than HDFS?; > ; > @jamesemery Can you comment?. I just try the MarkDuplicatesSpark from local file system without the HDFS.; the application will failure. It said can't find the file, but the file is on the local file system. ```; ls -la /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; -rw-rw-r--. 1 test test 4668988887 Oct 4 11:27 /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; ```. Here is SPARK information. ```; A USER ERROR has occurred: Failed to read bam header from; /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.sec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:5162,Deployability,deploy,deploy,5162,TKSparkTool.runPipeline(GATKSparkTool.java:458); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:635); Caused by: java.io.FileNotFoundException: File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.ha,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:1288,Integrability,protocol,protocolPB,1288,"n't find the file, but the file is on the local file system. ```; ls -la /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; -rw-rw-r--. 1 test test 4668988887 Oct 4 11:27 /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; ```. Here is SPARK information. ```; A USER ERROR has occurred: Failed to read bam header from; /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from /hom",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:1445,Integrability,protocol,protocol,1445,4668988887 Oct 4 11:27 /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; ```. Here is SPARK information. ```; A USER ERROR has occurred: Failed to read bam header from; /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:2951,Integrability,protocol,protocolPB,2951,(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:237); 	at org.broadinstitute.hellbender.engine.spark.GAT,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:3108,Integrability,protocol,protocol,3108,erver.java:2489). ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:237); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:488); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:468,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:5868,Integrability,protocol,protocolPB,5868,hodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:635); Caused by: java.io.FileNotFoundException: File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.j,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:6025,Integrability,protocol,protocol,6025,ccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:635); Caused by: java.io.FileNotFoundException: File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInst,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:9109,Integrability,protocol,protocolPB,9109,.open(DistributedFileSystem.java:312); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.seqdoop.hadoop_bam.util.SAMHeaderReader.readSAMHeaderFrom(SAMHeaderReader.java:51); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:235); 	... 15 more; Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.apache.hadoop.ipc.Client.call(Client.java:1475); 	at org.apache.hadoop.ipc.Client.call(Client.java:1412); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoke,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:9266,Integrability,protocol,protocol,9266,MHeaderFrom(SAMHeaderReader.java:51); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:235); 	... 15 more; Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.apache.hadoop.ipc.Client.call(Client.java:1475); 	at org.apache.hadoop.ipc.Client.call(Client.java:1412); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229); 	at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProt,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:10242,Integrability,protocol,protocolPB,10242,"anslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.apache.hadoop.ipc.Client.call(Client.java:1475); 	at org.apache.hadoop.ipc.Client.call(Client.java:1412); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229); 	at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191); 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102); 	at com.sun.proxy.$Proxy11.getBlockLocations(Unknown Source); 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1226); 	... 28 more; 18/10/06 09:45:36 INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 16, (reason: Shutdown hook called before final status was reported.); ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:1865,Security,secur,security,1865,he.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:1874,Security,Access,AccessController,1874,s.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlo,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:1930,Security,secur,security,1930,2); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:1998,Security,secur,security,1998,.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslato,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:3528,Security,secur,security,3528,he.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:237); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:488); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:468); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:458); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.ja,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:3537,Security,Access,AccessController,3537,s.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:237); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:488); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:468); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:458); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:3593,Security,secur,security,3593,2); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:237); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:488); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:468); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:458); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLinePro,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:3661,Security,secur,security,3661,.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:237); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:488); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:468); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:458); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:6445,Security,secur,security,6445,he.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106); 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73); 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228); 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213); 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(D,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:6454,Security,Access,AccessController,6454,s.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106); 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73); 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228); 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213); 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:6510,Security,secur,security,6510,2); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106); 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73); 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228); 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213); 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1201); 	at org.apache.hadoop.hdfs.DFSInputStream.fe,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:6578,Security,secur,security,6578,.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106); 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73); 	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228); 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213); 	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1201); 	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:306); 	at ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:9686,Security,secur,security,9686,he.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.apache.hadoop.ipc.Client.call(Client.java:1475); 	at org.apache.hadoop.ipc.Client.call(Client.java:1412); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229); 	at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.hadoop.io.retry.RetryInvocation,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:9695,Security,Access,AccessController,9695,s.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.apache.hadoop.ipc.Client.call(Client.java:1475); 	at org.apache.hadoop.ipc.Client.call(Client.java:1412); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229); 	at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invok,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:9751,Security,secur,security,9751,2); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.apache.hadoop.ipc.Client.call(Client.java:1475); 	at org.apache.hadoop.ipc.Client.call(Client.java:1412); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229); 	at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191); 	at org.ap,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:9819,Security,secur,security,9819,.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). 	at org.apache.hadoop.ipc.Client.call(Client.java:1475); 	at org.apache.hadoop.ipc.Client.call(Client.java:1412); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229); 	at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191); 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:372,Testability,test,test,372,"> @oldmikeyang Do you get the same result if the inputs and outputs to `MarkDuplicatesSpark` are on the local filesystem rather than HDFS?; > ; > @jamesemery Can you comment?. I just try the MarkDuplicatesSpark from local file system without the HDFS.; the application will failure. It said can't find the file, but the file is on the local file system. ```; ls -la /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; -rw-rw-r--. 1 test test 4668988887 Oct 4 11:27 /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; ```. Here is SPARK information. ```; A USER ERROR has occurred: Failed to read bam header from; /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.sec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:390,Testability,TEST,TEST,390,"> @oldmikeyang Do you get the same result if the inputs and outputs to `MarkDuplicatesSpark` are on the local filesystem rather than HDFS?; > ; > @jamesemery Can you comment?. I just try the MarkDuplicatesSpark from local file system without the HDFS.; the application will failure. It said can't find the file, but the file is on the local file system. ```; ls -la /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; -rw-rw-r--. 1 test test 4668988887 Oct 4 11:27 /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; ```. Here is SPARK information. ```; A USER ERROR has occurred: Failed to read bam header from; /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.sec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:439,Testability,test,test,439,"> @oldmikeyang Do you get the same result if the inputs and outputs to `MarkDuplicatesSpark` are on the local filesystem rather than HDFS?; > ; > @jamesemery Can you comment?. I just try the MarkDuplicatesSpark from local file system without the HDFS.; the application will failure. It said can't find the file, but the file is on the local file system. ```; ls -la /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; -rw-rw-r--. 1 test test 4668988887 Oct 4 11:27 /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; ```. Here is SPARK information. ```; A USER ERROR has occurred: Failed to read bam header from; /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.sec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:444,Testability,test,test,444,"> @oldmikeyang Do you get the same result if the inputs and outputs to `MarkDuplicatesSpark` are on the local filesystem rather than HDFS?; > ; > @jamesemery Can you comment?. I just try the MarkDuplicatesSpark from local file system without the HDFS.; the application will failure. It said can't find the file, but the file is on the local file system. ```; ls -la /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; -rw-rw-r--. 1 test test 4668988887 Oct 4 11:27 /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; ```. Here is SPARK information. ```; A USER ERROR has occurred: Failed to read bam header from; /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.sec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:478,Testability,test,test,478,"> @oldmikeyang Do you get the same result if the inputs and outputs to `MarkDuplicatesSpark` are on the local filesystem rather than HDFS?; > ; > @jamesemery Can you comment?. I just try the MarkDuplicatesSpark from local file system without the HDFS.; the application will failure. It said can't find the file, but the file is on the local file system. ```; ls -la /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; -rw-rw-r--. 1 test test 4668988887 Oct 4 11:27 /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; ```. Here is SPARK information. ```; A USER ERROR has occurred: Failed to read bam header from; /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.sec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:496,Testability,TEST,TEST,496,"> @oldmikeyang Do you get the same result if the inputs and outputs to `MarkDuplicatesSpark` are on the local filesystem rather than HDFS?; > ; > @jamesemery Can you comment?. I just try the MarkDuplicatesSpark from local file system without the HDFS.; the application will failure. It said can't find the file, but the file is on the local file system. ```; ls -la /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; -rw-rw-r--. 1 test test 4668988887 Oct 4 11:27 /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; ```. Here is SPARK information. ```; A USER ERROR has occurred: Failed to read bam header from; /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.sec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:633,Testability,test,test,633,"> @oldmikeyang Do you get the same result if the inputs and outputs to `MarkDuplicatesSpark` are on the local filesystem rather than HDFS?; > ; > @jamesemery Can you comment?. I just try the MarkDuplicatesSpark from local file system without the HDFS.; the application will failure. It said can't find the file, but the file is on the local file system. ```; ls -la /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; -rw-rw-r--. 1 test test 4668988887 Oct 4 11:27 /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; ```. Here is SPARK information. ```; A USER ERROR has occurred: Failed to read bam header from; /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.sec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:651,Testability,TEST,TEST,651,"> @oldmikeyang Do you get the same result if the inputs and outputs to `MarkDuplicatesSpark` are on the local filesystem rather than HDFS?; > ; > @jamesemery Can you comment?. I just try the MarkDuplicatesSpark from local file system without the HDFS.; the application will failure. It said can't find the file, but the file is on the local file system. ```; ls -la /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; -rw-rw-r--. 1 test test 4668988887 Oct 4 11:27 /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; ```. Here is SPARK information. ```; A USER ERROR has occurred: Failed to read bam header from; /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.sec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:723,Testability,test,test,723,"> @oldmikeyang Do you get the same result if the inputs and outputs to `MarkDuplicatesSpark` are on the local filesystem rather than HDFS?; > ; > @jamesemery Can you comment?. I just try the MarkDuplicatesSpark from local file system without the HDFS.; the application will failure. It said can't find the file, but the file is on the local file system. ```; ls -la /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; -rw-rw-r--. 1 test test 4668988887 Oct 4 11:27 /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; ```. Here is SPARK information. ```; A USER ERROR has occurred: Failed to read bam header from; /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.sec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:741,Testability,TEST,TEST,741,"> @oldmikeyang Do you get the same result if the inputs and outputs to `MarkDuplicatesSpark` are on the local filesystem rather than HDFS?; > ; > @jamesemery Can you comment?. I just try the MarkDuplicatesSpark from local file system without the HDFS.; the application will failure. It said can't find the file, but the file is on the local file system. ```; ls -la /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; -rw-rw-r--. 1 test test 4668988887 Oct 4 11:27 /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; ```. Here is SPARK information. ```; A USER ERROR has occurred: Failed to read bam header from; /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.sec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:2296,Testability,test,test,2296,otocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:2314,Testability,TEST,TEST,2314,otocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:2386,Testability,test,test,2386,erverSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.jav,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:2404,Testability,TEST,TEST,2404,erverSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850); 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793); 	at java.security.AccessController.doPrivileged(Native Method); 	at javax.security.auth.Subject.doAs(Subject.java:422); 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840); 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489). ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException: Failed to read bam header from /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; Caused by:File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.jav,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:5303,Testability,test,test,5303,ommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:635); Caused by: java.io.FileNotFoundException: File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:5321,Testability,TEST,TEST,5321,ommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:635); Caused by: java.io.FileNotFoundException: File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:8544,Testability,test,test,8544,astBlockLength(DFSInputStream.java:306); 	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:272); 	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:264); 	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1526); 	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:304); 	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299); 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); 	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:312); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.seqdoop.hadoop_bam.util.SAMHeaderReader.readSAMHeaderFrom(SAMHeaderReader.java:51); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:235); 	... 15 more; Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294:8562,Testability,TEST,TEST,8562,astBlockLength(DFSInputStream.java:306); 	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:272); 	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:264); 	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1526); 	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:304); 	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299); 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); 	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:312); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.seqdoop.hadoop_bam.util.SAMHeaderReader.readSAMHeaderFrom(SAMHeaderReader.java:51); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:235); 	... 15 more; Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /home/test/WGS_pipeline/TEST/output/spark_412.bowtie2.bam; 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72); 	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62); 	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152); 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1819); 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692); 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381); 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java); 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447); 	at org.apache.hadoop.ipc.RPC$Server.call(RPC,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427537294
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427538451:142,Deployability,release,released,142,"> Hello, @oldmikeyang I'm in the middle of doing a tie out for MarkDuplicatesSpark right now. I just recently fixed (and it will hopefully be released soon) some counting issues involving the metrics collection (it was over-counting the number of duplicate pairs marked compared to picard) I suspect it is likely that the actual bam output is correct. I will have a branch soon that I would ask you to try markDuplicatesSpark again on and tell me if it's still causing problems, unfortunately an unrelated fix requires a change to go into picard [broadinstitute/picard#1230](https://github.com/broadinstitute/picard/pull/1230).; > ; > I will let you know when the PR is open, as I would love to know if it fixes this mismatch. I would like to try it. By the way, I am running the whole SPARK version, MarkDuplicatesSpark + BQSRPipelineSpark + HaplotypeCallerSpark to get the vcf file. ; I found the output of the vcf is different with the GATK standard pipeline MarkDuplicates + BaseRecalibrator + ApplyBQSR + HaplotypeCaller.; The SPARK version is 3% less than the standard pipeline.; So I am debugging where these difference come from.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427538451
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427538451:953,Deployability,pipeline,pipeline,953,"> Hello, @oldmikeyang I'm in the middle of doing a tie out for MarkDuplicatesSpark right now. I just recently fixed (and it will hopefully be released soon) some counting issues involving the metrics collection (it was over-counting the number of duplicate pairs marked compared to picard) I suspect it is likely that the actual bam output is correct. I will have a branch soon that I would ask you to try markDuplicatesSpark again on and tell me if it's still causing problems, unfortunately an unrelated fix requires a change to go into picard [broadinstitute/picard#1230](https://github.com/broadinstitute/picard/pull/1230).; > ; > I will let you know when the PR is open, as I would love to know if it fixes this mismatch. I would like to try it. By the way, I am running the whole SPARK version, MarkDuplicatesSpark + BQSRPipelineSpark + HaplotypeCallerSpark to get the vcf file. ; I found the output of the vcf is different with the GATK standard pipeline MarkDuplicates + BaseRecalibrator + ApplyBQSR + HaplotypeCaller.; The SPARK version is 3% less than the standard pipeline.; So I am debugging where these difference come from.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427538451
https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427538451:1075,Deployability,pipeline,pipeline,1075,"> Hello, @oldmikeyang I'm in the middle of doing a tie out for MarkDuplicatesSpark right now. I just recently fixed (and it will hopefully be released soon) some counting issues involving the metrics collection (it was over-counting the number of duplicate pairs marked compared to picard) I suspect it is likely that the actual bam output is correct. I will have a branch soon that I would ask you to try markDuplicatesSpark again on and tell me if it's still causing problems, unfortunately an unrelated fix requires a change to go into picard [broadinstitute/picard#1230](https://github.com/broadinstitute/picard/pull/1230).; > ; > I will let you know when the PR is open, as I would love to know if it fixes this mismatch. I would like to try it. By the way, I am running the whole SPARK version, MarkDuplicatesSpark + BQSRPipelineSpark + HaplotypeCallerSpark to get the vcf file. ; I found the output of the vcf is different with the GATK standard pipeline MarkDuplicates + BaseRecalibrator + ApplyBQSR + HaplotypeCaller.; The SPARK version is 3% less than the standard pipeline.; So I am debugging where these difference come from.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4675#issuecomment-427538451
https://github.com/broadinstitute/gatk/pull/4676#issuecomment-382512384:9,Testability,test,tests,9,👍 if the tests pass merge,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4676#issuecomment-382512384
https://github.com/broadinstitute/gatk/pull/4677#issuecomment-383092400:2277,Security,validat,validation,2277,e/CpxVariantCanonicalRepresentation.java](https://codecov.io/gh/broadinstitute/gatk/pull/4677/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL0NweFZhcmlhbnRDYW5vbmljYWxSZXByZXNlbnRhdGlvbi5qYXZh) | `78.992% <71.429%> (+0.022%)` | `52 <0> (+2)` | :arrow_up: |; | [...transforms/markduplicates/MarkDuplicatesSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4677/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay90cmFuc2Zvcm1zL21hcmtkdXBsaWNhdGVzL01hcmtEdXBsaWNhdGVzU3BhcmsuamF2YQ==) | `90.909% <0%> (-4.213%)` | `9% <0%> (-6%)` | |; | [...hellbender/tools/walkers/mutect/Mutect2Engine.java](https://codecov.io/gh/broadinstitute/gatk/pull/4677/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyRW5naW5lLmphdmE=) | `87.654% <0%> (-3.39%)` | `50% <0%> (+1%)` | |; | [...tools/spark/validation/CompareDuplicatesSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4677/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay92YWxpZGF0aW9uL0NvbXBhcmVEdXBsaWNhdGVzU3BhcmsuamF2YQ==) | `82.927% <0%> (-1.518%)` | `24% <0%> (ø)` | |; | [...forms/markduplicates/MarkDuplicatesSparkUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4677/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay90cmFuc2Zvcm1zL21hcmtkdXBsaWNhdGVzL01hcmtEdXBsaWNhdGVzU3BhcmtVdGlscy5qYXZh) | `89.5% <0%> (-1.083%)` | `58% <0%> (-9%)` | |; | [...ellbender/tools/walkers/vqsr/CNNScoreVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4677/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OU2NvcmVWYXJpYW50cy5qYXZh) | `74.057% <0%> (-0.829%)` | `40% <0%> (-1%)` | |; | [...r/engine/filters/ReadGroupBlackListReadFilter.java](https://codecov.io/gh/b,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4677#issuecomment-383092400
https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384776520:38,Testability,test,test,38,"It's always difficult to come up with test data for these complex events. What I usually do is to create test data on what I know should work, including imaginable edge cases, and accumulate more edge cases as we run into them. Can you expand a little on what you mean by ; > to have an adversarial function that generates problems from known solutions rather than doing it by hand",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384776520
https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384776520:105,Testability,test,test,105,"It's always difficult to come up with test data for these complex events. What I usually do is to create test data on what I know should work, including imaginable edge cases, and accumulate more edge cases as we run into them. Can you expand a little on what you mean by ; > to have an adversarial function that generates problems from known solutions rather than doing it by hand",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384776520
https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384785429:670,Integrability,rout,route,670,"Like I said, I don't know enough to say if this approach is feasible, but sometimes it's possible to test an algorithm with a problem-generating function. This works well when it's easy to generate self-consistent solutions and test data consistent with a given solution. I was able to do this for some of the interval overlap functions on the python side of my code. Other methods were too complicated (for me to figure out anyway). Let's say you have an algorithm function f_algo that you want to test. You design a function test_data_factory that takes as input either a known solution, or a set of parameters from which it can easily generate a known solution (by a route that is logically distinct from the way f_algo works). Then f_tester generates and returns test_data. The flow looks like; ```; // get either random test solutions or some distinct set that tests particular edge cases; test_params = get_test_params();; test_solution = easy_solution_from_params(test_params);. test_data = test_data_factory(test_params, test_solution);; assert f_algo(test_data) == test_solution;; ```; In some sense you've already done that, but you've personally stepped in as test_data_factory(), and I'm wondering if it would be possible to automate that to get rid of all those numbers in the test code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384785429
https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384785429:101,Testability,test,test,101,"Like I said, I don't know enough to say if this approach is feasible, but sometimes it's possible to test an algorithm with a problem-generating function. This works well when it's easy to generate self-consistent solutions and test data consistent with a given solution. I was able to do this for some of the interval overlap functions on the python side of my code. Other methods were too complicated (for me to figure out anyway). Let's say you have an algorithm function f_algo that you want to test. You design a function test_data_factory that takes as input either a known solution, or a set of parameters from which it can easily generate a known solution (by a route that is logically distinct from the way f_algo works). Then f_tester generates and returns test_data. The flow looks like; ```; // get either random test solutions or some distinct set that tests particular edge cases; test_params = get_test_params();; test_solution = easy_solution_from_params(test_params);. test_data = test_data_factory(test_params, test_solution);; assert f_algo(test_data) == test_solution;; ```; In some sense you've already done that, but you've personally stepped in as test_data_factory(), and I'm wondering if it would be possible to automate that to get rid of all those numbers in the test code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384785429
https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384785429:228,Testability,test,test,228,"Like I said, I don't know enough to say if this approach is feasible, but sometimes it's possible to test an algorithm with a problem-generating function. This works well when it's easy to generate self-consistent solutions and test data consistent with a given solution. I was able to do this for some of the interval overlap functions on the python side of my code. Other methods were too complicated (for me to figure out anyway). Let's say you have an algorithm function f_algo that you want to test. You design a function test_data_factory that takes as input either a known solution, or a set of parameters from which it can easily generate a known solution (by a route that is logically distinct from the way f_algo works). Then f_tester generates and returns test_data. The flow looks like; ```; // get either random test solutions or some distinct set that tests particular edge cases; test_params = get_test_params();; test_solution = easy_solution_from_params(test_params);. test_data = test_data_factory(test_params, test_solution);; assert f_algo(test_data) == test_solution;; ```; In some sense you've already done that, but you've personally stepped in as test_data_factory(), and I'm wondering if it would be possible to automate that to get rid of all those numbers in the test code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384785429
https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384785429:499,Testability,test,test,499,"Like I said, I don't know enough to say if this approach is feasible, but sometimes it's possible to test an algorithm with a problem-generating function. This works well when it's easy to generate self-consistent solutions and test data consistent with a given solution. I was able to do this for some of the interval overlap functions on the python side of my code. Other methods were too complicated (for me to figure out anyway). Let's say you have an algorithm function f_algo that you want to test. You design a function test_data_factory that takes as input either a known solution, or a set of parameters from which it can easily generate a known solution (by a route that is logically distinct from the way f_algo works). Then f_tester generates and returns test_data. The flow looks like; ```; // get either random test solutions or some distinct set that tests particular edge cases; test_params = get_test_params();; test_solution = easy_solution_from_params(test_params);. test_data = test_data_factory(test_params, test_solution);; assert f_algo(test_data) == test_solution;; ```; In some sense you've already done that, but you've personally stepped in as test_data_factory(), and I'm wondering if it would be possible to automate that to get rid of all those numbers in the test code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384785429
https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384785429:684,Testability,log,logically,684,"Like I said, I don't know enough to say if this approach is feasible, but sometimes it's possible to test an algorithm with a problem-generating function. This works well when it's easy to generate self-consistent solutions and test data consistent with a given solution. I was able to do this for some of the interval overlap functions on the python side of my code. Other methods were too complicated (for me to figure out anyway). Let's say you have an algorithm function f_algo that you want to test. You design a function test_data_factory that takes as input either a known solution, or a set of parameters from which it can easily generate a known solution (by a route that is logically distinct from the way f_algo works). Then f_tester generates and returns test_data. The flow looks like; ```; // get either random test solutions or some distinct set that tests particular edge cases; test_params = get_test_params();; test_solution = easy_solution_from_params(test_params);. test_data = test_data_factory(test_params, test_solution);; assert f_algo(test_data) == test_solution;; ```; In some sense you've already done that, but you've personally stepped in as test_data_factory(), and I'm wondering if it would be possible to automate that to get rid of all those numbers in the test code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384785429
https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384785429:825,Testability,test,test,825,"Like I said, I don't know enough to say if this approach is feasible, but sometimes it's possible to test an algorithm with a problem-generating function. This works well when it's easy to generate self-consistent solutions and test data consistent with a given solution. I was able to do this for some of the interval overlap functions on the python side of my code. Other methods were too complicated (for me to figure out anyway). Let's say you have an algorithm function f_algo that you want to test. You design a function test_data_factory that takes as input either a known solution, or a set of parameters from which it can easily generate a known solution (by a route that is logically distinct from the way f_algo works). Then f_tester generates and returns test_data. The flow looks like; ```; // get either random test solutions or some distinct set that tests particular edge cases; test_params = get_test_params();; test_solution = easy_solution_from_params(test_params);. test_data = test_data_factory(test_params, test_solution);; assert f_algo(test_data) == test_solution;; ```; In some sense you've already done that, but you've personally stepped in as test_data_factory(), and I'm wondering if it would be possible to automate that to get rid of all those numbers in the test code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384785429
https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384785429:866,Testability,test,tests,866,"Like I said, I don't know enough to say if this approach is feasible, but sometimes it's possible to test an algorithm with a problem-generating function. This works well when it's easy to generate self-consistent solutions and test data consistent with a given solution. I was able to do this for some of the interval overlap functions on the python side of my code. Other methods were too complicated (for me to figure out anyway). Let's say you have an algorithm function f_algo that you want to test. You design a function test_data_factory that takes as input either a known solution, or a set of parameters from which it can easily generate a known solution (by a route that is logically distinct from the way f_algo works). Then f_tester generates and returns test_data. The flow looks like; ```; // get either random test solutions or some distinct set that tests particular edge cases; test_params = get_test_params();; test_solution = easy_solution_from_params(test_params);. test_data = test_data_factory(test_params, test_solution);; assert f_algo(test_data) == test_solution;; ```; In some sense you've already done that, but you've personally stepped in as test_data_factory(), and I'm wondering if it would be possible to automate that to get rid of all those numbers in the test code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384785429
https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384785429:1046,Testability,assert,assert,1046,"Like I said, I don't know enough to say if this approach is feasible, but sometimes it's possible to test an algorithm with a problem-generating function. This works well when it's easy to generate self-consistent solutions and test data consistent with a given solution. I was able to do this for some of the interval overlap functions on the python side of my code. Other methods were too complicated (for me to figure out anyway). Let's say you have an algorithm function f_algo that you want to test. You design a function test_data_factory that takes as input either a known solution, or a set of parameters from which it can easily generate a known solution (by a route that is logically distinct from the way f_algo works). Then f_tester generates and returns test_data. The flow looks like; ```; // get either random test solutions or some distinct set that tests particular edge cases; test_params = get_test_params();; test_solution = easy_solution_from_params(test_params);. test_data = test_data_factory(test_params, test_solution);; assert f_algo(test_data) == test_solution;; ```; In some sense you've already done that, but you've personally stepped in as test_data_factory(), and I'm wondering if it would be possible to automate that to get rid of all those numbers in the test code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384785429
https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384785429:1290,Testability,test,test,1290,"Like I said, I don't know enough to say if this approach is feasible, but sometimes it's possible to test an algorithm with a problem-generating function. This works well when it's easy to generate self-consistent solutions and test data consistent with a given solution. I was able to do this for some of the interval overlap functions on the python side of my code. Other methods were too complicated (for me to figure out anyway). Let's say you have an algorithm function f_algo that you want to test. You design a function test_data_factory that takes as input either a known solution, or a set of parameters from which it can easily generate a known solution (by a route that is logically distinct from the way f_algo works). Then f_tester generates and returns test_data. The flow looks like; ```; // get either random test solutions or some distinct set that tests particular edge cases; test_params = get_test_params();; test_solution = easy_solution_from_params(test_params);. test_data = test_data_factory(test_params, test_solution);; assert f_algo(test_data) == test_solution;; ```; In some sense you've already done that, but you've personally stepped in as test_data_factory(), and I'm wondering if it would be possible to automate that to get rid of all those numbers in the test code.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384785429
https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384788485:107,Testability,test,tests,107,"Hmm, I see what you mean.; It’s an interesting idea, and I think Valentin does that for some CIGAR related tests; but I think it is difficult to apply in our case without having a “simulator”. I’ll keep this in mind.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4677#issuecomment-384788485
https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382715149:314,Availability,error,error,314,"Hi @stefandiederich,. My guess is that you may have created your GATK conda environment with an older version of the GATK that did not include the changes to the `PloidyModelWriter`. Can you try creating a new conda environment with 4.0.3.0 and re-running?. @vdauwera @sooheelee @chandrans Any idea what the forum error message is about?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382715149
https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382715149:320,Integrability,message,message,320,"Hi @stefandiederich,. My guess is that you may have created your GATK conda environment with an older version of the GATK that did not include the changes to the `PloidyModelWriter`. Can you try creating a new conda environment with 4.0.3.0 and re-running?. @vdauwera @sooheelee @chandrans Any idea what the forum error message is about?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382715149
https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382717819:132,Usability,feedback,feedback,132,"Hi @samuelklee ; yes you are right, I did not recreate the conda environment when updating to 4.0.3.0. I'll try that and give you a feedback. Do you know the state of developement of the germline CNV best practise or a short manual?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382717819
https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382736818:97,Availability,error,error,97,"Hi @samuelklee ; I updated my conda environment and it worked now. ; Sorry again for posting the error in github and not in the gatk forum, hopefully next time it will work. Thanks again; Stefan",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382736818
https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382736818:19,Deployability,update,updated,19,"Hi @samuelklee ; I updated my conda environment and it worked now. ; Sorry again for posting the error in github and not in the gatk forum, hopefully next time it will work. Thanks again; Stefan",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382736818
https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382736948:201,Availability,error,error,201,"As our forum gets updates from Vanilla, sometimes we get new and unexpected glitches. These are typically ironed out fairly quickly. @tmm211 is handling Vanilla forum issues these days. Sorry for your error @stefandiederich.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382736948
https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382736948:18,Deployability,update,updates,18,"As our forum gets updates from Vanilla, sometimes we get new and unexpected glitches. These are typically ironed out fairly quickly. @tmm211 is handling Vanilla forum issues these days. Sorry for your error @stefandiederich.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382736948
https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382777741:318,Availability,error,error,318,"@stefandiederich We are finalizing some hyperparameter optimizations and will have some results and recommendations to share within ~1 month. You may find some preliminary recommendations for WES in this forum thread: https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. Once our optimizations are finalized, @vdauwera and her team will put together a tutorial and other workshop materials---stay tuned!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382777741
https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382777741:55,Performance,optimiz,optimizations,55,"@stefandiederich We are finalizing some hyperparameter optimizations and will have some results and recommendations to share within ~1 month. You may find some preliminary recommendations for WES in this forum thread: https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. Once our optimizations are finalized, @vdauwera and her team will put together a tutorial and other workshop materials---stay tuned!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382777741
https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382777741:334,Performance,optimiz,optimizations,334,"@stefandiederich We are finalizing some hyperparameter optimizations and will have some results and recommendations to share within ~1 month. You may find some preliminary recommendations for WES in this forum thread: https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. Once our optimizations are finalized, @vdauwera and her team will put together a tutorial and other workshop materials---stay tuned!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382777741
https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382777741:451,Performance,tune,tuned,451,"@stefandiederich We are finalizing some hyperparameter optimizations and will have some results and recommendations to share within ~1 month. You may find some preliminary recommendations for WES in this forum thread: https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. Once our optimizations are finalized, @vdauwera and her team will put together a tutorial and other workshop materials---stay tuned!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382777741
https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382873889:19,Deployability,patch,patch,19,@stefandiederich a patch went out to fix this at 10 AM this morning. Can you confirm you are no longer having trouble posting?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382873889
https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382984253:166,Availability,error,error,166,Good morning :-); @samuelklee ; Thanks for the link to the preliminary parameter. I will try them!. @tmm211 ; Just tested the posting function on gatk main page. The error seems to be solved. Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382984253
https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382984253:115,Testability,test,tested,115,Good morning :-); @samuelklee ; Thanks for the link to the preliminary parameter. I will try them!. @tmm211 ; Just tested the posting function on gatk main page. The error seems to be solved. Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4679#issuecomment-382984253
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384300799:37,Availability,error,error,37,"@droazen - sorry for the compilation error, it was just an early optimization. Can you have a look to it? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384300799
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384300799:65,Performance,optimiz,optimization,65,"@droazen - sorry for the compilation error, it was just an early optimization. Can you have a look to it? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384300799
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384608003:2806,Deployability,pipeline,pipelines,2806,JlYWRXYWxrZXIuamF2YQ==) | `83.784% <83.784%> (ø)` | `14 <14> (?)` | |; | [...ute/hellbender/engine/SlidingWindowReadWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvU2xpZGluZ1dpbmRvd1JlYWRXYWxrZXIuamF2YQ==) | `85.294% <85.294%> (ø)` | `10 <10> (?)` | |; | [...e/hellbender/utils/iterators/ShardingIterator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pdGVyYXRvcnMvU2hhcmRpbmdJdGVyYXRvci5qYXZh) | `92.683% <92.683%> (ø)` | `18 <18> (?)` | |; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-74.257%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...ery/inference/SimpleNovelAdjacencyInterpreter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1NpbXBsZU5vdmVsQWRqYWNlbmN5SW50ZXJwcmV0ZXIuamF2YQ==) | `73.973% <0%> (-26.027%)` | `11% <0%> (+4%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/ga,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384608003
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384608003:3122,Testability,test,test,3122,1JlYWRXYWxrZXIuamF2YQ==) | `85.294% <85.294%> (ø)` | `10 <10> (?)` | |; | [...e/hellbender/utils/iterators/ShardingIterator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pdGVyYXRvcnMvU2hhcmRpbmdJdGVyYXRvci5qYXZh) | `92.683% <92.683%> (ø)` | `18 <18> (?)` | |; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-74.257%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...ery/inference/SimpleNovelAdjacencyInterpreter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1NpbXBsZU5vdmVsQWRqYWNlbmN5SW50ZXJwcmV0ZXIuamF2YQ==) | `73.973% <0%> (-26.027%)` | `11% <0%> (+4%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-10%)` | |; | ... and [108 more](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384608003
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384608003:3390,Usability,Simpl,SimpleNovelAdjacencyInterpreter,3390,1JlYWRXYWxrZXIuamF2YQ==) | `85.294% <85.294%> (ø)` | `10 <10> (?)` | |; | [...e/hellbender/utils/iterators/ShardingIterator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pdGVyYXRvcnMvU2hhcmRpbmdJdGVyYXRvci5qYXZh) | `92.683% <92.683%> (ø)` | `18 <18> (?)` | |; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-74.257%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...ery/inference/SimpleNovelAdjacencyInterpreter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1NpbXBsZU5vdmVsQWRqYWNlbmN5SW50ZXJwcmV0ZXIuamF2YQ==) | `73.973% <0%> (-26.027%)` | `11% <0%> (+4%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-10%)` | |; | ... and [108 more](https://codecov.io/gh/broadinstitute/gatk/pull/4682/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384608003
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384608688:46,Availability,error,error,46,"Finally got the test passing in travis (silly error from refactoring previous stuff). Ready to go, @droazen!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384608688
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384608688:57,Modifiability,refactor,refactoring,57,"Finally got the test passing in travis (silly error from refactoring previous stuff). Ready to go, @droazen!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384608688
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384608688:16,Testability,test,test,16,"Finally got the test passing in travis (silly error from refactoring previous stuff). Ready to go, @droazen!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-384608688
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-385910502:9,Availability,ping,ping,9,Friendly ping here @droazen!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-385910502
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-388326562:395,Availability,down,downstream,395,"@droazen - any idea about the time to get into this? It has been 2 years since the original proposal (https://github.com/broadinstitute/gatk/pull/1528) and almost a month for this to be reviewed. I understand that the bandwidth for community-driven development is lower than the one for broad needs - but I need to get non-intrusive functionality like this in an easier way to be shared between downstream projects. I'll probably opt to create a new project for this kind of gatk extensions: general-purpose walkers, common utility for gatk abstractions (e.g., `GATKRead`), etc. In that case, I am open to backport anything that might be useful for your team (and for the rest of the community), but I cannot block my own development due to long-standing PRs. Let me know what do you think about this. I will keep contributing to GATK and proposing new code to be included, but I guess that having a repository with this code available for the community in a versioned and delivered way might be useful.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-388326562
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-388326562:926,Availability,avail,available,926,"@droazen - any idea about the time to get into this? It has been 2 years since the original proposal (https://github.com/broadinstitute/gatk/pull/1528) and almost a month for this to be reviewed. I understand that the bandwidth for community-driven development is lower than the one for broad needs - but I need to get non-intrusive functionality like this in an easier way to be shared between downstream projects. I'll probably opt to create a new project for this kind of gatk extensions: general-purpose walkers, common utility for gatk abstractions (e.g., `GATKRead`), etc. In that case, I am open to backport anything that might be useful for your team (and for the rest of the community), but I cannot block my own development due to long-standing PRs. Let me know what do you think about this. I will keep contributing to GATK and proposing new code to be included, but I guess that having a repository with this code available for the community in a versioned and delivered way might be useful.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-388326562
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389185267:372,Deployability,integrat,integration,372,"@droazen - this is prepared from the next pass. Maybe you can answer the following questions before doing a full second pass:. * Your comment about ""Assert that you get the correct number of shards back"" is tested implicitly, by iterating over the expected shards. Should I really make it explicit?; * Does realistic size reads adds something to the unit test itself? The integration test for the real data (and probably a future PR with other sliding-window implementations) might be enough for the purpose of real data...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389185267
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389185267:372,Integrability,integrat,integration,372,"@droazen - this is prepared from the next pass. Maybe you can answer the following questions before doing a full second pass:. * Your comment about ""Assert that you get the correct number of shards back"" is tested implicitly, by iterating over the expected shards. Should I really make it explicit?; * Does realistic size reads adds something to the unit test itself? The integration test for the real data (and probably a future PR with other sliding-window implementations) might be enough for the purpose of real data...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389185267
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389185267:149,Testability,Assert,Assert,149,"@droazen - this is prepared from the next pass. Maybe you can answer the following questions before doing a full second pass:. * Your comment about ""Assert that you get the correct number of shards back"" is tested implicitly, by iterating over the expected shards. Should I really make it explicit?; * Does realistic size reads adds something to the unit test itself? The integration test for the real data (and probably a future PR with other sliding-window implementations) might be enough for the purpose of real data...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389185267
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389185267:207,Testability,test,tested,207,"@droazen - this is prepared from the next pass. Maybe you can answer the following questions before doing a full second pass:. * Your comment about ""Assert that you get the correct number of shards back"" is tested implicitly, by iterating over the expected shards. Should I really make it explicit?; * Does realistic size reads adds something to the unit test itself? The integration test for the real data (and probably a future PR with other sliding-window implementations) might be enough for the purpose of real data...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389185267
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389185267:355,Testability,test,test,355,"@droazen - this is prepared from the next pass. Maybe you can answer the following questions before doing a full second pass:. * Your comment about ""Assert that you get the correct number of shards back"" is tested implicitly, by iterating over the expected shards. Should I really make it explicit?; * Does realistic size reads adds something to the unit test itself? The integration test for the real data (and probably a future PR with other sliding-window implementations) might be enough for the purpose of real data...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389185267
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389185267:384,Testability,test,test,384,"@droazen - this is prepared from the next pass. Maybe you can answer the following questions before doing a full second pass:. * Your comment about ""Assert that you get the correct number of shards back"" is tested implicitly, by iterating over the expected shards. Should I really make it explicit?; * Does realistic size reads adds something to the unit test itself? The integration test for the real data (and probably a future PR with other sliding-window implementations) might be enough for the purpose of real data...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389185267
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389998139:63,Testability,test,testing,63,"@magicDGS:. 1. Yes, you're right that you're already correctly testing that you get the right number of shards back. No need for an extra assertion. 2. I think that realistically-sized reads (of differing lengths!) does add to the unit test, since it's important to test with reads that overlap extensively with other reads. You'll also want to vary the read lengths to test with reads of different lengths at the same start position, shorter reads that start after longer reads, and any other arrangements that are significant to your `ShardingIterator`. . It shouldn't be too difficult to write a simple method that uses `ArtificialReadUtils` to create small pileups of reads within a given interval (might be worth extracting into `ArtificialReadUtils` itself so that future tests can use it).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389998139
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389998139:138,Testability,assert,assertion,138,"@magicDGS:. 1. Yes, you're right that you're already correctly testing that you get the right number of shards back. No need for an extra assertion. 2. I think that realistically-sized reads (of differing lengths!) does add to the unit test, since it's important to test with reads that overlap extensively with other reads. You'll also want to vary the read lengths to test with reads of different lengths at the same start position, shorter reads that start after longer reads, and any other arrangements that are significant to your `ShardingIterator`. . It shouldn't be too difficult to write a simple method that uses `ArtificialReadUtils` to create small pileups of reads within a given interval (might be worth extracting into `ArtificialReadUtils` itself so that future tests can use it).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389998139
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389998139:236,Testability,test,test,236,"@magicDGS:. 1. Yes, you're right that you're already correctly testing that you get the right number of shards back. No need for an extra assertion. 2. I think that realistically-sized reads (of differing lengths!) does add to the unit test, since it's important to test with reads that overlap extensively with other reads. You'll also want to vary the read lengths to test with reads of different lengths at the same start position, shorter reads that start after longer reads, and any other arrangements that are significant to your `ShardingIterator`. . It shouldn't be too difficult to write a simple method that uses `ArtificialReadUtils` to create small pileups of reads within a given interval (might be worth extracting into `ArtificialReadUtils` itself so that future tests can use it).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389998139
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389998139:266,Testability,test,test,266,"@magicDGS:. 1. Yes, you're right that you're already correctly testing that you get the right number of shards back. No need for an extra assertion. 2. I think that realistically-sized reads (of differing lengths!) does add to the unit test, since it's important to test with reads that overlap extensively with other reads. You'll also want to vary the read lengths to test with reads of different lengths at the same start position, shorter reads that start after longer reads, and any other arrangements that are significant to your `ShardingIterator`. . It shouldn't be too difficult to write a simple method that uses `ArtificialReadUtils` to create small pileups of reads within a given interval (might be worth extracting into `ArtificialReadUtils` itself so that future tests can use it).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389998139
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389998139:370,Testability,test,test,370,"@magicDGS:. 1. Yes, you're right that you're already correctly testing that you get the right number of shards back. No need for an extra assertion. 2. I think that realistically-sized reads (of differing lengths!) does add to the unit test, since it's important to test with reads that overlap extensively with other reads. You'll also want to vary the read lengths to test with reads of different lengths at the same start position, shorter reads that start after longer reads, and any other arrangements that are significant to your `ShardingIterator`. . It shouldn't be too difficult to write a simple method that uses `ArtificialReadUtils` to create small pileups of reads within a given interval (might be worth extracting into `ArtificialReadUtils` itself so that future tests can use it).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389998139
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389998139:778,Testability,test,tests,778,"@magicDGS:. 1. Yes, you're right that you're already correctly testing that you get the right number of shards back. No need for an extra assertion. 2. I think that realistically-sized reads (of differing lengths!) does add to the unit test, since it's important to test with reads that overlap extensively with other reads. You'll also want to vary the read lengths to test with reads of different lengths at the same start position, shorter reads that start after longer reads, and any other arrangements that are significant to your `ShardingIterator`. . It shouldn't be too difficult to write a simple method that uses `ArtificialReadUtils` to create small pileups of reads within a given interval (might be worth extracting into `ArtificialReadUtils` itself so that future tests can use it).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389998139
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389998139:599,Usability,simpl,simple,599,"@magicDGS:. 1. Yes, you're right that you're already correctly testing that you get the right number of shards back. No need for an extra assertion. 2. I think that realistically-sized reads (of differing lengths!) does add to the unit test, since it's important to test with reads that overlap extensively with other reads. You'll also want to vary the read lengths to test with reads of different lengths at the same start position, shorter reads that start after longer reads, and any other arrangements that are significant to your `ShardingIterator`. . It shouldn't be too difficult to write a simple method that uses `ArtificialReadUtils` to create small pileups of reads within a given interval (might be worth extracting into `ArtificialReadUtils` itself so that future tests can use it).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-389998139
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-415849096:61,Testability,test,test,61,Not yet - I didn't have the time to figure out how to do the test utility to create small pile of reads (because I think that there is already similar functionality in `ArtificialReadUtils`.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-415849096
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-418453819:95,Deployability,integrat,integration,95,"@magicDGS In the interest of moving this PR along, perhaps it would be easier to just write an integration test for `ExampleSlidingWindowReadWalker` that uses a large interval from a more realistic bam (such as `CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam` from `large`). This would address most of my concerns, and we could create a ticket to eventually add exhaustive tests with synthetic reads in a future PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-418453819
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-418453819:95,Integrability,integrat,integration,95,"@magicDGS In the interest of moving this PR along, perhaps it would be easier to just write an integration test for `ExampleSlidingWindowReadWalker` that uses a large interval from a more realistic bam (such as `CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam` from `large`). This would address most of my concerns, and we could create a ticket to eventually add exhaustive tests with synthetic reads in a future PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-418453819
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-418453819:107,Testability,test,test,107,"@magicDGS In the interest of moving this PR along, perhaps it would be easier to just write an integration test for `ExampleSlidingWindowReadWalker` that uses a large interval from a more realistic bam (such as `CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam` from `large`). This would address most of my concerns, and we could create a ticket to eventually add exhaustive tests with synthetic reads in a future PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-418453819
https://github.com/broadinstitute/gatk/pull/4682#issuecomment-418453819:366,Testability,test,tests,366,"@magicDGS In the interest of moving this PR along, perhaps it would be easier to just write an integration test for `ExampleSlidingWindowReadWalker` that uses a large interval from a more realistic bam (such as `CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam` from `large`). This would address most of my concerns, and we could create a ticket to eventually add exhaustive tests with synthetic reads in a future PR.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4682#issuecomment-418453819
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382781387:541,Performance,perform,performed,541,"@sooheelee Are you sure that smoothing doesn't always converge before we can see different behavior for your test data? How many iterations of smoothing do you get when you set this parameter to 0, and how many do you get when you set it to 1? (It might be helpful to post a condensed version of the stdout.). This parameter is not meant to be a boolean. If it is set to N > 0, then at most N smoothing iterations will be attempted before the next MCMC fit. However, if convergence is reached before N iterations, then the final MCMC fit is performed. If the parameter is set to 0, then no MCMC refits are performed. In both cases, the total number of allowed smoothing iterations is set by a separate parameter, `maximum-number-of-smoothing-iterations`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382781387
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382781387:606,Performance,perform,performed,606,"@sooheelee Are you sure that smoothing doesn't always converge before we can see different behavior for your test data? How many iterations of smoothing do you get when you set this parameter to 0, and how many do you get when you set it to 1? (It might be helpful to post a condensed version of the stdout.). This parameter is not meant to be a boolean. If it is set to N > 0, then at most N smoothing iterations will be attempted before the next MCMC fit. However, if convergence is reached before N iterations, then the final MCMC fit is performed. If the parameter is set to 0, then no MCMC refits are performed. In both cases, the total number of allowed smoothing iterations is set by a separate parameter, `maximum-number-of-smoothing-iterations`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382781387
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382781387:109,Testability,test,test,109,"@sooheelee Are you sure that smoothing doesn't always converge before we can see different behavior for your test data? How many iterations of smoothing do you get when you set this parameter to 0, and how many do you get when you set it to 1? (It might be helpful to post a condensed version of the stdout.). This parameter is not meant to be a boolean. If it is set to N > 0, then at most N smoothing iterations will be attempted before the next MCMC fit. However, if convergence is reached before N iterations, then the final MCMC fit is performed. If the parameter is set to 0, then no MCMC refits are performed. In both cases, the total number of allowed smoothing iterations is set by a separate parameter, `maximum-number-of-smoothing-iterations`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382781387
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:1760,Testability,log,log,1760,"er of segments before smoothing iteration: 638; 09:40:44.923 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 398; 09:40:44.924 INFO MultidimensionalModeller - Smoothing iteration: 2; 09:40:44.924 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 398; 09:40:44.924 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 398; 09:40:44.924 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:40:45.760 INFO GibbsSampler - Starting MCMC sampling.; 09:40:48.121 INFO GibbsSampler - 25 of 100 samples generated.; 09:40:50.618 INFO GibbsSampler - 50 of 100 samples generated.; 09:40:53.062 INFO GibbsSampler - 75 of 100 samples generated.; 09:40:55.518 INFO GibbsSampler - 100 of 100 samples generated.; 09:40:55.518 INFO GibbsSampler - MCMC sampling complete.; 09:40:55.518 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:40:55.539 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:40:55.871 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599072.084975...; 09:40:55.871 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.150935, biasVariance=0.041480, outlierProbability=0.032502}; 09:40:56.161 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599060.773672...; 09:40:56.161 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.153086, biasVariance=0.035405, outlierProbability=0.036256}; 09:40:56.492 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599057.851664...; 09:40:56.492 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147247, biasVariance=0.033414, outlierProbability=0.038948}; 09:40:56.775 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599057.518701...; 09:40:56.775 INFO AlleleFractionInitializer - AlleleFractionGlobalParameter",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:1873,Testability,log,log,1873,"hing iteration: 398; 09:40:44.924 INFO MultidimensionalModeller - Smoothing iteration: 2; 09:40:44.924 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 398; 09:40:44.924 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 398; 09:40:44.924 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:40:45.760 INFO GibbsSampler - Starting MCMC sampling.; 09:40:48.121 INFO GibbsSampler - 25 of 100 samples generated.; 09:40:50.618 INFO GibbsSampler - 50 of 100 samples generated.; 09:40:53.062 INFO GibbsSampler - 75 of 100 samples generated.; 09:40:55.518 INFO GibbsSampler - 100 of 100 samples generated.; 09:40:55.518 INFO GibbsSampler - MCMC sampling complete.; 09:40:55.518 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:40:55.539 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:40:55.871 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599072.084975...; 09:40:55.871 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.150935, biasVariance=0.041480, outlierProbability=0.032502}; 09:40:56.161 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599060.773672...; 09:40:56.161 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.153086, biasVariance=0.035405, outlierProbability=0.036256}; 09:40:56.492 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599057.851664...; 09:40:56.492 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147247, biasVariance=0.033414, outlierProbability=0.038948}; 09:40:56.775 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599057.518701...; 09:40:56.775 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147247, biasVariance=0.032380, outlierProbability=0.038948}; 09:40:56.888 INFO GibbsSampler - Starting MCMC sam",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:2123,Testability,log,log,2123," after smoothing iteration: 398; 09:40:44.924 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:40:45.760 INFO GibbsSampler - Starting MCMC sampling.; 09:40:48.121 INFO GibbsSampler - 25 of 100 samples generated.; 09:40:50.618 INFO GibbsSampler - 50 of 100 samples generated.; 09:40:53.062 INFO GibbsSampler - 75 of 100 samples generated.; 09:40:55.518 INFO GibbsSampler - 100 of 100 samples generated.; 09:40:55.518 INFO GibbsSampler - MCMC sampling complete.; 09:40:55.518 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:40:55.539 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:40:55.871 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599072.084975...; 09:40:55.871 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.150935, biasVariance=0.041480, outlierProbability=0.032502}; 09:40:56.161 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599060.773672...; 09:40:56.161 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.153086, biasVariance=0.035405, outlierProbability=0.036256}; 09:40:56.492 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599057.851664...; 09:40:56.492 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147247, biasVariance=0.033414, outlierProbability=0.038948}; 09:40:56.775 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599057.518701...; 09:40:56.775 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147247, biasVariance=0.032380, outlierProbability=0.038948}; 09:40:56.888 INFO GibbsSampler - Starting MCMC sampling.; 09:41:03.070 INFO GibbsSampler - 25 of 100 samples generated.; 09:41:09.768 INFO GibbsSampler - 50 of 100 samples generated.; 09:41:16.127 INFO GibbsSampler - 75 of 100 samples generated.; 09:41:22.748 INFO GibbsSampler - 100 of 100 samples g",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:2373,Testability,log,log,2373,"sSampler - 50 of 100 samples generated.; 09:40:53.062 INFO GibbsSampler - 75 of 100 samples generated.; 09:40:55.518 INFO GibbsSampler - 100 of 100 samples generated.; 09:40:55.518 INFO GibbsSampler - MCMC sampling complete.; 09:40:55.518 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:40:55.539 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:40:55.871 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599072.084975...; 09:40:55.871 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.150935, biasVariance=0.041480, outlierProbability=0.032502}; 09:40:56.161 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599060.773672...; 09:40:56.161 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.153086, biasVariance=0.035405, outlierProbability=0.036256}; 09:40:56.492 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599057.851664...; 09:40:56.492 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147247, biasVariance=0.033414, outlierProbability=0.038948}; 09:40:56.775 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599057.518701...; 09:40:56.775 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147247, biasVariance=0.032380, outlierProbability=0.038948}; 09:40:56.888 INFO GibbsSampler - Starting MCMC sampling.; 09:41:03.070 INFO GibbsSampler - 25 of 100 samples generated.; 09:41:09.768 INFO GibbsSampler - 50 of 100 samples generated.; 09:41:16.127 INFO GibbsSampler - 75 of 100 samples generated.; 09:41:22.748 INFO GibbsSampler - 100 of 100 samples generated.; 09:41:22.748 INFO GibbsSampler - MCMC sampling complete.; 09:41:22.792 INFO MultidimensionalModeller - Final number of segments after smoothing: 398; ```. If set to 1, we get:; ```; 09:45:23.658 INFO ModelSegments - Segments written to inc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:2623,Testability,log,log,2623,"imensionalModeller - Fitting allele-fraction model...; 09:40:55.539 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:40:55.871 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599072.084975...; 09:40:55.871 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.150935, biasVariance=0.041480, outlierProbability=0.032502}; 09:40:56.161 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599060.773672...; 09:40:56.161 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.153086, biasVariance=0.035405, outlierProbability=0.036256}; 09:40:56.492 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599057.851664...; 09:40:56.492 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147247, biasVariance=0.033414, outlierProbability=0.038948}; 09:40:56.775 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599057.518701...; 09:40:56.775 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147247, biasVariance=0.032380, outlierProbability=0.038948}; 09:40:56.888 INFO GibbsSampler - Starting MCMC sampling.; 09:41:03.070 INFO GibbsSampler - 25 of 100 samples generated.; 09:41:09.768 INFO GibbsSampler - 50 of 100 samples generated.; 09:41:16.127 INFO GibbsSampler - 75 of 100 samples generated.; 09:41:22.748 INFO GibbsSampler - 100 of 100 samples generated.; 09:41:22.748 INFO GibbsSampler - MCMC sampling complete.; 09:41:22.792 INFO MultidimensionalModeller - Final number of segments after smoothing: 398; ```. If set to 1, we get:; ```; 09:45:23.658 INFO ModelSegments - Segments written to increase_smoothing/T_increase_smoothing_4030_1.modelBegin.seg; 09:45:23.658 INFO MultidimensionalModeller - Writing posterior summaries for copy-ratio global parameters to increase_smoothing/T_increase_smoothing_4030_1.modelBegin.cr.param; 09:45:23.660 ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:4754,Testability,log,log,4754,"T_increase_smoothing_4030_1.modelBegin.af.param; 09:45:23.660 INFO MultidimensionalModeller - Initial number of segments before smoothing: 638; 09:45:23.660 INFO MultidimensionalModeller - Smoothing iteration: 1; 09:45:23.660 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 638; 09:45:23.662 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 398; 09:45:23.663 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:45:24.508 INFO GibbsSampler - Starting MCMC sampling.; 09:45:26.962 INFO GibbsSampler - 25 of 100 samples generated.; 09:45:29.471 INFO GibbsSampler - 50 of 100 samples generated.; 09:45:31.911 INFO GibbsSampler - 75 of 100 samples generated.; 09:45:34.335 INFO GibbsSampler - 100 of 100 samples generated.; 09:45:34.335 INFO GibbsSampler - MCMC sampling complete.; 09:45:34.335 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:45:34.359 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:45:34.724 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599072.084975...; 09:45:34.724 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.150935, biasVariance=0.041480, outlierProbability=0.032502}; 09:45:35.017 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599060.773672...; 09:45:35.018 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.153086, biasVariance=0.035405, outlierProbability=0.036256}; 09:45:35.337 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599057.851664...; 09:45:35.337 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147247, biasVariance=0.033414, outlierProbability=0.038948}; 09:45:35.631 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599057.518701...; 09:45:35.631 INFO AlleleFractionInitializer - AlleleFractionGlobalParameter",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:4867,Testability,log,log,4867,"fore smoothing: 638; 09:45:23.660 INFO MultidimensionalModeller - Smoothing iteration: 1; 09:45:23.660 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 638; 09:45:23.662 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 398; 09:45:23.663 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:45:24.508 INFO GibbsSampler - Starting MCMC sampling.; 09:45:26.962 INFO GibbsSampler - 25 of 100 samples generated.; 09:45:29.471 INFO GibbsSampler - 50 of 100 samples generated.; 09:45:31.911 INFO GibbsSampler - 75 of 100 samples generated.; 09:45:34.335 INFO GibbsSampler - 100 of 100 samples generated.; 09:45:34.335 INFO GibbsSampler - MCMC sampling complete.; 09:45:34.335 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:45:34.359 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:45:34.724 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599072.084975...; 09:45:34.724 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.150935, biasVariance=0.041480, outlierProbability=0.032502}; 09:45:35.017 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599060.773672...; 09:45:35.018 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.153086, biasVariance=0.035405, outlierProbability=0.036256}; 09:45:35.337 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599057.851664...; 09:45:35.337 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147247, biasVariance=0.033414, outlierProbability=0.038948}; 09:45:35.631 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599057.518701...; 09:45:35.631 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147247, biasVariance=0.032380, outlierProbability=0.038948}; 09:45:35.748 INFO GibbsSampler - Starting MCMC sam",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:5117,Testability,log,log,5117," after smoothing iteration: 398; 09:45:23.663 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:45:24.508 INFO GibbsSampler - Starting MCMC sampling.; 09:45:26.962 INFO GibbsSampler - 25 of 100 samples generated.; 09:45:29.471 INFO GibbsSampler - 50 of 100 samples generated.; 09:45:31.911 INFO GibbsSampler - 75 of 100 samples generated.; 09:45:34.335 INFO GibbsSampler - 100 of 100 samples generated.; 09:45:34.335 INFO GibbsSampler - MCMC sampling complete.; 09:45:34.335 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:45:34.359 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:45:34.724 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599072.084975...; 09:45:34.724 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.150935, biasVariance=0.041480, outlierProbability=0.032502}; 09:45:35.017 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599060.773672...; 09:45:35.018 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.153086, biasVariance=0.035405, outlierProbability=0.036256}; 09:45:35.337 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599057.851664...; 09:45:35.337 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147247, biasVariance=0.033414, outlierProbability=0.038948}; 09:45:35.631 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599057.518701...; 09:45:35.631 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147247, biasVariance=0.032380, outlierProbability=0.038948}; 09:45:35.748 INFO GibbsSampler - Starting MCMC sampling.; 09:45:41.926 INFO GibbsSampler - 25 of 100 samples generated.; 09:45:48.545 INFO GibbsSampler - 50 of 100 samples generated.; 09:45:54.915 INFO GibbsSampler - 75 of 100 samples generated.; 09:46:01.577 INFO GibbsSampler - 100 of 100 samples g",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:5367,Testability,log,log,5367,"sSampler - 50 of 100 samples generated.; 09:45:31.911 INFO GibbsSampler - 75 of 100 samples generated.; 09:45:34.335 INFO GibbsSampler - 100 of 100 samples generated.; 09:45:34.335 INFO GibbsSampler - MCMC sampling complete.; 09:45:34.335 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:45:34.359 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:45:34.724 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599072.084975...; 09:45:34.724 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.150935, biasVariance=0.041480, outlierProbability=0.032502}; 09:45:35.017 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599060.773672...; 09:45:35.018 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.153086, biasVariance=0.035405, outlierProbability=0.036256}; 09:45:35.337 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599057.851664...; 09:45:35.337 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147247, biasVariance=0.033414, outlierProbability=0.038948}; 09:45:35.631 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599057.518701...; 09:45:35.631 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147247, biasVariance=0.032380, outlierProbability=0.038948}; 09:45:35.748 INFO GibbsSampler - Starting MCMC sampling.; 09:45:41.926 INFO GibbsSampler - 25 of 100 samples generated.; 09:45:48.545 INFO GibbsSampler - 50 of 100 samples generated.; 09:45:54.915 INFO GibbsSampler - 75 of 100 samples generated.; 09:46:01.577 INFO GibbsSampler - 100 of 100 samples generated.; 09:46:01.577 INFO GibbsSampler - MCMC sampling complete.; 09:46:01.621 INFO MultidimensionalModeller - Smoothing iteration: 2; 09:46:01.621 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 398; 09:46:01.621 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:5617,Testability,log,log,5617,"imensionalModeller - Fitting allele-fraction model...; 09:45:34.359 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:45:34.724 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599072.084975...; 09:45:34.724 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.150935, biasVariance=0.041480, outlierProbability=0.032502}; 09:45:35.017 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599060.773672...; 09:45:35.018 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.153086, biasVariance=0.035405, outlierProbability=0.036256}; 09:45:35.337 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599057.851664...; 09:45:35.337 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147247, biasVariance=0.033414, outlierProbability=0.038948}; 09:45:35.631 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599057.518701...; 09:45:35.631 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147247, biasVariance=0.032380, outlierProbability=0.038948}; 09:45:35.748 INFO GibbsSampler - Starting MCMC sampling.; 09:45:41.926 INFO GibbsSampler - 25 of 100 samples generated.; 09:45:48.545 INFO GibbsSampler - 50 of 100 samples generated.; 09:45:54.915 INFO GibbsSampler - 75 of 100 samples generated.; 09:46:01.577 INFO GibbsSampler - 100 of 100 samples generated.; 09:46:01.577 INFO GibbsSampler - MCMC sampling complete.; 09:46:01.621 INFO MultidimensionalModeller - Smoothing iteration: 2; 09:46:01.621 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 398; 09:46:01.621 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 341; 09:46:01.622 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:46:02.459 INFO GibbsSampler - Starting MCMC sampling.; 09:46:04.896 INFO GibbsSampler - 25 o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:7053,Testability,log,log,7053,"0 samples generated.; 09:46:01.577 INFO GibbsSampler - 100 of 100 samples generated.; 09:46:01.577 INFO GibbsSampler - MCMC sampling complete.; 09:46:01.621 INFO MultidimensionalModeller - Smoothing iteration: 2; 09:46:01.621 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 398; 09:46:01.621 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 341; 09:46:01.622 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:46:02.459 INFO GibbsSampler - Starting MCMC sampling.; 09:46:04.896 INFO GibbsSampler - 25 of 100 samples generated.; 09:46:07.292 INFO GibbsSampler - 50 of 100 samples generated.; 09:46:09.705 INFO GibbsSampler - 75 of 100 samples generated.; 09:46:12.179 INFO GibbsSampler - 100 of 100 samples generated.; 09:46:12.179 INFO GibbsSampler - MCMC sampling complete.; 09:46:12.179 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:46:12.202 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:46:12.546 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599161.730531...; 09:46:12.546 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151592, biasVariance=0.041338, outlierProbability=0.030947}; 09:46:12.859 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599150.005317...; 09:46:12.859 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151592, biasVariance=0.036533, outlierProbability=0.037484}; 09:46:13.165 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599148.402059...; 09:46:13.165 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149236, biasVariance=0.034264, outlierProbability=0.038997}; 09:46:13.451 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599147.957754...; 09:46:13.451 INFO AlleleFractionInitializer - AlleleFractionGlobalParameter",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:7166,Testability,log,log,7166," sampling complete.; 09:46:01.621 INFO MultidimensionalModeller - Smoothing iteration: 2; 09:46:01.621 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 398; 09:46:01.621 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 341; 09:46:01.622 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:46:02.459 INFO GibbsSampler - Starting MCMC sampling.; 09:46:04.896 INFO GibbsSampler - 25 of 100 samples generated.; 09:46:07.292 INFO GibbsSampler - 50 of 100 samples generated.; 09:46:09.705 INFO GibbsSampler - 75 of 100 samples generated.; 09:46:12.179 INFO GibbsSampler - 100 of 100 samples generated.; 09:46:12.179 INFO GibbsSampler - MCMC sampling complete.; 09:46:12.179 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:46:12.202 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:46:12.546 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599161.730531...; 09:46:12.546 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151592, biasVariance=0.041338, outlierProbability=0.030947}; 09:46:12.859 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599150.005317...; 09:46:12.859 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151592, biasVariance=0.036533, outlierProbability=0.037484}; 09:46:13.165 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599148.402059...; 09:46:13.165 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149236, biasVariance=0.034264, outlierProbability=0.038997}; 09:46:13.451 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599147.957754...; 09:46:13.451 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147087, biasVariance=0.033230, outlierProbability=0.038997}; 09:46:13.573 INFO GibbsSampler - Starting MCMC sam",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:7416,Testability,log,log,7416," after smoothing iteration: 341; 09:46:01.622 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:46:02.459 INFO GibbsSampler - Starting MCMC sampling.; 09:46:04.896 INFO GibbsSampler - 25 of 100 samples generated.; 09:46:07.292 INFO GibbsSampler - 50 of 100 samples generated.; 09:46:09.705 INFO GibbsSampler - 75 of 100 samples generated.; 09:46:12.179 INFO GibbsSampler - 100 of 100 samples generated.; 09:46:12.179 INFO GibbsSampler - MCMC sampling complete.; 09:46:12.179 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:46:12.202 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:46:12.546 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599161.730531...; 09:46:12.546 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151592, biasVariance=0.041338, outlierProbability=0.030947}; 09:46:12.859 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599150.005317...; 09:46:12.859 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151592, biasVariance=0.036533, outlierProbability=0.037484}; 09:46:13.165 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599148.402059...; 09:46:13.165 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149236, biasVariance=0.034264, outlierProbability=0.038997}; 09:46:13.451 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599147.957754...; 09:46:13.451 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147087, biasVariance=0.033230, outlierProbability=0.038997}; 09:46:13.573 INFO GibbsSampler - Starting MCMC sampling.; 09:46:19.986 INFO GibbsSampler - 25 of 100 samples generated.; 09:46:26.612 INFO GibbsSampler - 50 of 100 samples generated.; 09:46:33.274 INFO GibbsSampler - 75 of 100 samples generated.; 09:46:39.723 INFO GibbsSampler - 100 of 100 samples g",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:7666,Testability,log,log,7666,"sSampler - 50 of 100 samples generated.; 09:46:09.705 INFO GibbsSampler - 75 of 100 samples generated.; 09:46:12.179 INFO GibbsSampler - 100 of 100 samples generated.; 09:46:12.179 INFO GibbsSampler - MCMC sampling complete.; 09:46:12.179 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:46:12.202 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:46:12.546 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599161.730531...; 09:46:12.546 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151592, biasVariance=0.041338, outlierProbability=0.030947}; 09:46:12.859 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599150.005317...; 09:46:12.859 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151592, biasVariance=0.036533, outlierProbability=0.037484}; 09:46:13.165 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599148.402059...; 09:46:13.165 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149236, biasVariance=0.034264, outlierProbability=0.038997}; 09:46:13.451 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599147.957754...; 09:46:13.451 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147087, biasVariance=0.033230, outlierProbability=0.038997}; 09:46:13.573 INFO GibbsSampler - Starting MCMC sampling.; 09:46:19.986 INFO GibbsSampler - 25 of 100 samples generated.; 09:46:26.612 INFO GibbsSampler - 50 of 100 samples generated.; 09:46:33.274 INFO GibbsSampler - 75 of 100 samples generated.; 09:46:39.723 INFO GibbsSampler - 100 of 100 samples generated.; 09:46:39.723 INFO GibbsSampler - MCMC sampling complete.; 09:46:39.763 INFO MultidimensionalModeller - Smoothing iteration: 3; 09:46:39.763 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 341; 09:46:39.764 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:7916,Testability,log,log,7916,"imensionalModeller - Fitting allele-fraction model...; 09:46:12.202 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:46:12.546 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599161.730531...; 09:46:12.546 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151592, biasVariance=0.041338, outlierProbability=0.030947}; 09:46:12.859 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599150.005317...; 09:46:12.859 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151592, biasVariance=0.036533, outlierProbability=0.037484}; 09:46:13.165 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599148.402059...; 09:46:13.165 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149236, biasVariance=0.034264, outlierProbability=0.038997}; 09:46:13.451 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599147.957754...; 09:46:13.451 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147087, biasVariance=0.033230, outlierProbability=0.038997}; 09:46:13.573 INFO GibbsSampler - Starting MCMC sampling.; 09:46:19.986 INFO GibbsSampler - 25 of 100 samples generated.; 09:46:26.612 INFO GibbsSampler - 50 of 100 samples generated.; 09:46:33.274 INFO GibbsSampler - 75 of 100 samples generated.; 09:46:39.723 INFO GibbsSampler - 100 of 100 samples generated.; 09:46:39.723 INFO GibbsSampler - MCMC sampling complete.; 09:46:39.763 INFO MultidimensionalModeller - Smoothing iteration: 3; 09:46:39.763 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 341; 09:46:39.764 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 326; 09:46:39.764 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:46:40.559 INFO GibbsSampler - Starting MCMC sampling.; 09:46:42.928 INFO GibbsSampler - 25 o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:9352,Testability,log,log,9352,"0 samples generated.; 09:46:39.723 INFO GibbsSampler - 100 of 100 samples generated.; 09:46:39.723 INFO GibbsSampler - MCMC sampling complete.; 09:46:39.763 INFO MultidimensionalModeller - Smoothing iteration: 3; 09:46:39.763 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 341; 09:46:39.764 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 326; 09:46:39.764 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:46:40.559 INFO GibbsSampler - Starting MCMC sampling.; 09:46:42.928 INFO GibbsSampler - 25 of 100 samples generated.; 09:46:45.335 INFO GibbsSampler - 50 of 100 samples generated.; 09:46:47.720 INFO GibbsSampler - 75 of 100 samples generated.; 09:46:50.130 INFO GibbsSampler - 100 of 100 samples generated.; 09:46:50.130 INFO GibbsSampler - MCMC sampling complete.; 09:46:50.130 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:46:50.155 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:46:50.491 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599186.382799...; 09:46:50.491 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151674, biasVariance=0.041597, outlierProbability=0.031043}; 09:46:50.793 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599173.973431...; 09:46:50.793 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151674, biasVariance=0.036521, outlierProbability=0.036858}; 09:46:51.096 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599172.045766...; 09:46:51.096 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149513, biasVariance=0.034385, outlierProbability=0.039399}; 09:46:51.370 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599171.568776...; 09:46:51.370 INFO AlleleFractionInitializer - AlleleFractionGlobalParameter",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:9465,Testability,log,log,9465," sampling complete.; 09:46:39.763 INFO MultidimensionalModeller - Smoothing iteration: 3; 09:46:39.763 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 341; 09:46:39.764 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 326; 09:46:39.764 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:46:40.559 INFO GibbsSampler - Starting MCMC sampling.; 09:46:42.928 INFO GibbsSampler - 25 of 100 samples generated.; 09:46:45.335 INFO GibbsSampler - 50 of 100 samples generated.; 09:46:47.720 INFO GibbsSampler - 75 of 100 samples generated.; 09:46:50.130 INFO GibbsSampler - 100 of 100 samples generated.; 09:46:50.130 INFO GibbsSampler - MCMC sampling complete.; 09:46:50.130 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:46:50.155 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:46:50.491 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599186.382799...; 09:46:50.491 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151674, biasVariance=0.041597, outlierProbability=0.031043}; 09:46:50.793 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599173.973431...; 09:46:50.793 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151674, biasVariance=0.036521, outlierProbability=0.036858}; 09:46:51.096 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599172.045766...; 09:46:51.096 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149513, biasVariance=0.034385, outlierProbability=0.039399}; 09:46:51.370 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599171.568776...; 09:46:51.370 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147363, biasVariance=0.033351, outlierProbability=0.039399}; 09:46:51.487 INFO GibbsSampler - Starting MCMC sam",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:9715,Testability,log,log,9715," after smoothing iteration: 326; 09:46:39.764 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:46:40.559 INFO GibbsSampler - Starting MCMC sampling.; 09:46:42.928 INFO GibbsSampler - 25 of 100 samples generated.; 09:46:45.335 INFO GibbsSampler - 50 of 100 samples generated.; 09:46:47.720 INFO GibbsSampler - 75 of 100 samples generated.; 09:46:50.130 INFO GibbsSampler - 100 of 100 samples generated.; 09:46:50.130 INFO GibbsSampler - MCMC sampling complete.; 09:46:50.130 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:46:50.155 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:46:50.491 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599186.382799...; 09:46:50.491 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151674, biasVariance=0.041597, outlierProbability=0.031043}; 09:46:50.793 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599173.973431...; 09:46:50.793 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151674, biasVariance=0.036521, outlierProbability=0.036858}; 09:46:51.096 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599172.045766...; 09:46:51.096 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149513, biasVariance=0.034385, outlierProbability=0.039399}; 09:46:51.370 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599171.568776...; 09:46:51.370 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147363, biasVariance=0.033351, outlierProbability=0.039399}; 09:46:51.487 INFO GibbsSampler - Starting MCMC sampling.; 09:46:57.921 INFO GibbsSampler - 25 of 100 samples generated.; 09:47:04.479 INFO GibbsSampler - 50 of 100 samples generated.; 09:47:11.286 INFO GibbsSampler - 75 of 100 samples generated.; 09:47:17.895 INFO GibbsSampler - 100 of 100 samples g",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:9965,Testability,log,log,9965,"sSampler - 50 of 100 samples generated.; 09:46:47.720 INFO GibbsSampler - 75 of 100 samples generated.; 09:46:50.130 INFO GibbsSampler - 100 of 100 samples generated.; 09:46:50.130 INFO GibbsSampler - MCMC sampling complete.; 09:46:50.130 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:46:50.155 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:46:50.491 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599186.382799...; 09:46:50.491 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151674, biasVariance=0.041597, outlierProbability=0.031043}; 09:46:50.793 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599173.973431...; 09:46:50.793 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151674, biasVariance=0.036521, outlierProbability=0.036858}; 09:46:51.096 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599172.045766...; 09:46:51.096 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149513, biasVariance=0.034385, outlierProbability=0.039399}; 09:46:51.370 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599171.568776...; 09:46:51.370 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147363, biasVariance=0.033351, outlierProbability=0.039399}; 09:46:51.487 INFO GibbsSampler - Starting MCMC sampling.; 09:46:57.921 INFO GibbsSampler - 25 of 100 samples generated.; 09:47:04.479 INFO GibbsSampler - 50 of 100 samples generated.; 09:47:11.286 INFO GibbsSampler - 75 of 100 samples generated.; 09:47:17.895 INFO GibbsSampler - 100 of 100 samples generated.; 09:47:17.895 INFO GibbsSampler - MCMC sampling complete.; 09:47:17.941 INFO MultidimensionalModeller - Smoothing iteration: 4; 09:47:17.942 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 326; 09:47:17.942 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:10215,Testability,log,log,10215,"imensionalModeller - Fitting allele-fraction model...; 09:46:50.155 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:46:50.491 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599186.382799...; 09:46:50.491 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151674, biasVariance=0.041597, outlierProbability=0.031043}; 09:46:50.793 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599173.973431...; 09:46:50.793 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151674, biasVariance=0.036521, outlierProbability=0.036858}; 09:46:51.096 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599172.045766...; 09:46:51.096 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149513, biasVariance=0.034385, outlierProbability=0.039399}; 09:46:51.370 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599171.568776...; 09:46:51.370 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147363, biasVariance=0.033351, outlierProbability=0.039399}; 09:46:51.487 INFO GibbsSampler - Starting MCMC sampling.; 09:46:57.921 INFO GibbsSampler - 25 of 100 samples generated.; 09:47:04.479 INFO GibbsSampler - 50 of 100 samples generated.; 09:47:11.286 INFO GibbsSampler - 75 of 100 samples generated.; 09:47:17.895 INFO GibbsSampler - 100 of 100 samples generated.; 09:47:17.895 INFO GibbsSampler - MCMC sampling complete.; 09:47:17.941 INFO MultidimensionalModeller - Smoothing iteration: 4; 09:47:17.942 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 326; 09:47:17.942 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 320; 09:47:17.942 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:47:18.756 INFO GibbsSampler - Starting MCMC sampling.; 09:47:21.195 INFO GibbsSampler - 25 o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:11651,Testability,log,log,11651,"0 samples generated.; 09:47:17.895 INFO GibbsSampler - 100 of 100 samples generated.; 09:47:17.895 INFO GibbsSampler - MCMC sampling complete.; 09:47:17.941 INFO MultidimensionalModeller - Smoothing iteration: 4; 09:47:17.942 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 326; 09:47:17.942 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 320; 09:47:17.942 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:47:18.756 INFO GibbsSampler - Starting MCMC sampling.; 09:47:21.195 INFO GibbsSampler - 25 of 100 samples generated.; 09:47:23.597 INFO GibbsSampler - 50 of 100 samples generated.; 09:47:26.031 INFO GibbsSampler - 75 of 100 samples generated.; 09:47:28.533 INFO GibbsSampler - 100 of 100 samples generated.; 09:47:28.533 INFO GibbsSampler - MCMC sampling complete.; 09:47:28.533 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:47:28.553 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:47:28.883 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599193.821455...; 09:47:28.884 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151686, biasVariance=0.041615, outlierProbability=0.031024}; 09:47:29.188 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599181.471228...; 09:47:29.188 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151686, biasVariance=0.036547, outlierProbability=0.036766}; 09:47:29.513 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599179.528974...; 09:47:29.514 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149492, biasVariance=0.034434, outlierProbability=0.039341}; 09:47:29.794 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599179.050215...; 09:47:29.794 INFO AlleleFractionInitializer - AlleleFractionGlobalParameter",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:11764,Testability,log,log,11764," sampling complete.; 09:47:17.941 INFO MultidimensionalModeller - Smoothing iteration: 4; 09:47:17.942 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 326; 09:47:17.942 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 320; 09:47:17.942 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:47:18.756 INFO GibbsSampler - Starting MCMC sampling.; 09:47:21.195 INFO GibbsSampler - 25 of 100 samples generated.; 09:47:23.597 INFO GibbsSampler - 50 of 100 samples generated.; 09:47:26.031 INFO GibbsSampler - 75 of 100 samples generated.; 09:47:28.533 INFO GibbsSampler - 100 of 100 samples generated.; 09:47:28.533 INFO GibbsSampler - MCMC sampling complete.; 09:47:28.533 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:47:28.553 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:47:28.883 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599193.821455...; 09:47:28.884 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151686, biasVariance=0.041615, outlierProbability=0.031024}; 09:47:29.188 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599181.471228...; 09:47:29.188 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151686, biasVariance=0.036547, outlierProbability=0.036766}; 09:47:29.513 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599179.528974...; 09:47:29.514 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149492, biasVariance=0.034434, outlierProbability=0.039341}; 09:47:29.794 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599179.050215...; 09:47:29.794 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147343, biasVariance=0.033400, outlierProbability=0.039341}; 09:47:29.914 INFO GibbsSampler - Starting MCMC sam",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:12014,Testability,log,log,12014," after smoothing iteration: 320; 09:47:17.942 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:47:18.756 INFO GibbsSampler - Starting MCMC sampling.; 09:47:21.195 INFO GibbsSampler - 25 of 100 samples generated.; 09:47:23.597 INFO GibbsSampler - 50 of 100 samples generated.; 09:47:26.031 INFO GibbsSampler - 75 of 100 samples generated.; 09:47:28.533 INFO GibbsSampler - 100 of 100 samples generated.; 09:47:28.533 INFO GibbsSampler - MCMC sampling complete.; 09:47:28.533 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:47:28.553 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:47:28.883 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599193.821455...; 09:47:28.884 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151686, biasVariance=0.041615, outlierProbability=0.031024}; 09:47:29.188 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599181.471228...; 09:47:29.188 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151686, biasVariance=0.036547, outlierProbability=0.036766}; 09:47:29.513 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599179.528974...; 09:47:29.514 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149492, biasVariance=0.034434, outlierProbability=0.039341}; 09:47:29.794 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599179.050215...; 09:47:29.794 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147343, biasVariance=0.033400, outlierProbability=0.039341}; 09:47:29.914 INFO GibbsSampler - Starting MCMC sampling.; 09:47:36.149 INFO GibbsSampler - 25 of 100 samples generated.; 09:47:42.958 INFO GibbsSampler - 50 of 100 samples generated.; 09:47:49.518 INFO GibbsSampler - 75 of 100 samples generated.; 09:47:55.984 INFO GibbsSampler - 100 of 100 samples g",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:12264,Testability,log,log,12264,"sSampler - 50 of 100 samples generated.; 09:47:26.031 INFO GibbsSampler - 75 of 100 samples generated.; 09:47:28.533 INFO GibbsSampler - 100 of 100 samples generated.; 09:47:28.533 INFO GibbsSampler - MCMC sampling complete.; 09:47:28.533 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:47:28.553 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:47:28.883 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599193.821455...; 09:47:28.884 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151686, biasVariance=0.041615, outlierProbability=0.031024}; 09:47:29.188 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599181.471228...; 09:47:29.188 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151686, biasVariance=0.036547, outlierProbability=0.036766}; 09:47:29.513 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599179.528974...; 09:47:29.514 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149492, biasVariance=0.034434, outlierProbability=0.039341}; 09:47:29.794 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599179.050215...; 09:47:29.794 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147343, biasVariance=0.033400, outlierProbability=0.039341}; 09:47:29.914 INFO GibbsSampler - Starting MCMC sampling.; 09:47:36.149 INFO GibbsSampler - 25 of 100 samples generated.; 09:47:42.958 INFO GibbsSampler - 50 of 100 samples generated.; 09:47:49.518 INFO GibbsSampler - 75 of 100 samples generated.; 09:47:55.984 INFO GibbsSampler - 100 of 100 samples generated.; 09:47:55.984 INFO GibbsSampler - MCMC sampling complete.; 09:47:56.024 INFO MultidimensionalModeller - Smoothing iteration: 5; 09:47:56.025 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 320; 09:47:56.025 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:12514,Testability,log,log,12514,"imensionalModeller - Fitting allele-fraction model...; 09:47:28.553 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:47:28.883 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599193.821455...; 09:47:28.884 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151686, biasVariance=0.041615, outlierProbability=0.031024}; 09:47:29.188 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599181.471228...; 09:47:29.188 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.151686, biasVariance=0.036547, outlierProbability=0.036766}; 09:47:29.513 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599179.528974...; 09:47:29.514 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149492, biasVariance=0.034434, outlierProbability=0.039341}; 09:47:29.794 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599179.050215...; 09:47:29.794 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147343, biasVariance=0.033400, outlierProbability=0.039341}; 09:47:29.914 INFO GibbsSampler - Starting MCMC sampling.; 09:47:36.149 INFO GibbsSampler - 25 of 100 samples generated.; 09:47:42.958 INFO GibbsSampler - 50 of 100 samples generated.; 09:47:49.518 INFO GibbsSampler - 75 of 100 samples generated.; 09:47:55.984 INFO GibbsSampler - 100 of 100 samples generated.; 09:47:55.984 INFO GibbsSampler - MCMC sampling complete.; 09:47:56.024 INFO MultidimensionalModeller - Smoothing iteration: 5; 09:47:56.025 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 320; 09:47:56.025 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 315; 09:47:56.025 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:47:56.852 INFO GibbsSampler - Starting MCMC sampling.; 09:47:59.368 INFO GibbsSampler - 25 o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:13950,Testability,log,log,13950,"0 samples generated.; 09:47:55.984 INFO GibbsSampler - 100 of 100 samples generated.; 09:47:55.984 INFO GibbsSampler - MCMC sampling complete.; 09:47:56.024 INFO MultidimensionalModeller - Smoothing iteration: 5; 09:47:56.025 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 320; 09:47:56.025 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 315; 09:47:56.025 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:47:56.852 INFO GibbsSampler - Starting MCMC sampling.; 09:47:59.368 INFO GibbsSampler - 25 of 100 samples generated.; 09:48:01.836 INFO GibbsSampler - 50 of 100 samples generated.; 09:48:04.204 INFO GibbsSampler - 75 of 100 samples generated.; 09:48:06.588 INFO GibbsSampler - 100 of 100 samples generated.; 09:48:06.588 INFO GibbsSampler - MCMC sampling complete.; 09:48:06.589 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:48:06.609 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:48:06.967 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599214.667518...; 09:48:06.967 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152065, biasVariance=0.041090, outlierProbability=0.031116}; 09:48:07.271 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599203.280111...; 09:48:07.271 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152065, biasVariance=0.036781, outlierProbability=0.037210}; 09:48:07.582 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599201.550266...; 09:48:07.582 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149834, biasVariance=0.034708, outlierProbability=0.039514}; 09:48:07.846 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599201.121676...; 09:48:07.846 INFO AlleleFractionInitializer - AlleleFractionGlobalParameter",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:14063,Testability,log,log,14063," sampling complete.; 09:47:56.024 INFO MultidimensionalModeller - Smoothing iteration: 5; 09:47:56.025 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 320; 09:47:56.025 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 315; 09:47:56.025 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:47:56.852 INFO GibbsSampler - Starting MCMC sampling.; 09:47:59.368 INFO GibbsSampler - 25 of 100 samples generated.; 09:48:01.836 INFO GibbsSampler - 50 of 100 samples generated.; 09:48:04.204 INFO GibbsSampler - 75 of 100 samples generated.; 09:48:06.588 INFO GibbsSampler - 100 of 100 samples generated.; 09:48:06.588 INFO GibbsSampler - MCMC sampling complete.; 09:48:06.589 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:48:06.609 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:48:06.967 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599214.667518...; 09:48:06.967 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152065, biasVariance=0.041090, outlierProbability=0.031116}; 09:48:07.271 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599203.280111...; 09:48:07.271 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152065, biasVariance=0.036781, outlierProbability=0.037210}; 09:48:07.582 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599201.550266...; 09:48:07.582 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149834, biasVariance=0.034708, outlierProbability=0.039514}; 09:48:07.846 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599201.121676...; 09:48:07.846 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147684, biasVariance=0.033674, outlierProbability=0.039514}; 09:48:07.961 INFO GibbsSampler - Starting MCMC sam",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:14313,Testability,log,log,14313," after smoothing iteration: 315; 09:47:56.025 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:47:56.852 INFO GibbsSampler - Starting MCMC sampling.; 09:47:59.368 INFO GibbsSampler - 25 of 100 samples generated.; 09:48:01.836 INFO GibbsSampler - 50 of 100 samples generated.; 09:48:04.204 INFO GibbsSampler - 75 of 100 samples generated.; 09:48:06.588 INFO GibbsSampler - 100 of 100 samples generated.; 09:48:06.588 INFO GibbsSampler - MCMC sampling complete.; 09:48:06.589 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:48:06.609 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:48:06.967 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599214.667518...; 09:48:06.967 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152065, biasVariance=0.041090, outlierProbability=0.031116}; 09:48:07.271 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599203.280111...; 09:48:07.271 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152065, biasVariance=0.036781, outlierProbability=0.037210}; 09:48:07.582 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599201.550266...; 09:48:07.582 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149834, biasVariance=0.034708, outlierProbability=0.039514}; 09:48:07.846 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599201.121676...; 09:48:07.846 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147684, biasVariance=0.033674, outlierProbability=0.039514}; 09:48:07.961 INFO GibbsSampler - Starting MCMC sampling.; 09:48:14.260 INFO GibbsSampler - 25 of 100 samples generated.; 09:48:20.656 INFO GibbsSampler - 50 of 100 samples generated.; 09:48:27.049 INFO GibbsSampler - 75 of 100 samples generated.; 09:48:33.639 INFO GibbsSampler - 100 of 100 samples g",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:14563,Testability,log,log,14563,"sSampler - 50 of 100 samples generated.; 09:48:04.204 INFO GibbsSampler - 75 of 100 samples generated.; 09:48:06.588 INFO GibbsSampler - 100 of 100 samples generated.; 09:48:06.588 INFO GibbsSampler - MCMC sampling complete.; 09:48:06.589 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:48:06.609 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:48:06.967 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599214.667518...; 09:48:06.967 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152065, biasVariance=0.041090, outlierProbability=0.031116}; 09:48:07.271 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599203.280111...; 09:48:07.271 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152065, biasVariance=0.036781, outlierProbability=0.037210}; 09:48:07.582 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599201.550266...; 09:48:07.582 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149834, biasVariance=0.034708, outlierProbability=0.039514}; 09:48:07.846 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599201.121676...; 09:48:07.846 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147684, biasVariance=0.033674, outlierProbability=0.039514}; 09:48:07.961 INFO GibbsSampler - Starting MCMC sampling.; 09:48:14.260 INFO GibbsSampler - 25 of 100 samples generated.; 09:48:20.656 INFO GibbsSampler - 50 of 100 samples generated.; 09:48:27.049 INFO GibbsSampler - 75 of 100 samples generated.; 09:48:33.639 INFO GibbsSampler - 100 of 100 samples generated.; 09:48:33.639 INFO GibbsSampler - MCMC sampling complete.; 09:48:33.671 INFO MultidimensionalModeller - Smoothing iteration: 6; 09:48:33.671 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 315; 09:48:33.672 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:14813,Testability,log,log,14813,"imensionalModeller - Fitting allele-fraction model...; 09:48:06.609 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:48:06.967 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599214.667518...; 09:48:06.967 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152065, biasVariance=0.041090, outlierProbability=0.031116}; 09:48:07.271 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599203.280111...; 09:48:07.271 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152065, biasVariance=0.036781, outlierProbability=0.037210}; 09:48:07.582 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599201.550266...; 09:48:07.582 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149834, biasVariance=0.034708, outlierProbability=0.039514}; 09:48:07.846 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599201.121676...; 09:48:07.846 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147684, biasVariance=0.033674, outlierProbability=0.039514}; 09:48:07.961 INFO GibbsSampler - Starting MCMC sampling.; 09:48:14.260 INFO GibbsSampler - 25 of 100 samples generated.; 09:48:20.656 INFO GibbsSampler - 50 of 100 samples generated.; 09:48:27.049 INFO GibbsSampler - 75 of 100 samples generated.; 09:48:33.639 INFO GibbsSampler - 100 of 100 samples generated.; 09:48:33.639 INFO GibbsSampler - MCMC sampling complete.; 09:48:33.671 INFO MultidimensionalModeller - Smoothing iteration: 6; 09:48:33.671 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 315; 09:48:33.672 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 311; 09:48:33.672 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:48:34.470 INFO GibbsSampler - Starting MCMC sampling.; 09:48:36.720 INFO GibbsSampler - 25 o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:16249,Testability,log,log,16249,"0 samples generated.; 09:48:33.639 INFO GibbsSampler - 100 of 100 samples generated.; 09:48:33.639 INFO GibbsSampler - MCMC sampling complete.; 09:48:33.671 INFO MultidimensionalModeller - Smoothing iteration: 6; 09:48:33.671 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 315; 09:48:33.672 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 311; 09:48:33.672 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:48:34.470 INFO GibbsSampler - Starting MCMC sampling.; 09:48:36.720 INFO GibbsSampler - 25 of 100 samples generated.; 09:48:39.070 INFO GibbsSampler - 50 of 100 samples generated.; 09:48:41.417 INFO GibbsSampler - 75 of 100 samples generated.; 09:48:43.749 INFO GibbsSampler - 100 of 100 samples generated.; 09:48:43.749 INFO GibbsSampler - MCMC sampling complete.; 09:48:43.749 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:48:43.771 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:48:44.116 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599214.649782...; 09:48:44.116 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.041049, outlierProbability=0.031143}; 09:48:44.424 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599203.367762...; 09:48:44.424 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.036778, outlierProbability=0.037218}; 09:48:44.714 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599201.638390...; 09:48:44.714 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149830, biasVariance=0.034706, outlierProbability=0.039510}; 09:48:44.991 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599201.210843...; 09:48:44.991 INFO AlleleFractionInitializer - AlleleFractionGlobalParameter",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:16362,Testability,log,log,16362," sampling complete.; 09:48:33.671 INFO MultidimensionalModeller - Smoothing iteration: 6; 09:48:33.671 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 315; 09:48:33.672 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 311; 09:48:33.672 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:48:34.470 INFO GibbsSampler - Starting MCMC sampling.; 09:48:36.720 INFO GibbsSampler - 25 of 100 samples generated.; 09:48:39.070 INFO GibbsSampler - 50 of 100 samples generated.; 09:48:41.417 INFO GibbsSampler - 75 of 100 samples generated.; 09:48:43.749 INFO GibbsSampler - 100 of 100 samples generated.; 09:48:43.749 INFO GibbsSampler - MCMC sampling complete.; 09:48:43.749 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:48:43.771 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:48:44.116 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599214.649782...; 09:48:44.116 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.041049, outlierProbability=0.031143}; 09:48:44.424 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599203.367762...; 09:48:44.424 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.036778, outlierProbability=0.037218}; 09:48:44.714 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599201.638390...; 09:48:44.714 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149830, biasVariance=0.034706, outlierProbability=0.039510}; 09:48:44.991 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599201.210843...; 09:48:44.991 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147681, biasVariance=0.033671, outlierProbability=0.039510}; 09:48:45.104 INFO GibbsSampler - Starting MCMC sam",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:16612,Testability,log,log,16612," after smoothing iteration: 311; 09:48:33.672 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:48:34.470 INFO GibbsSampler - Starting MCMC sampling.; 09:48:36.720 INFO GibbsSampler - 25 of 100 samples generated.; 09:48:39.070 INFO GibbsSampler - 50 of 100 samples generated.; 09:48:41.417 INFO GibbsSampler - 75 of 100 samples generated.; 09:48:43.749 INFO GibbsSampler - 100 of 100 samples generated.; 09:48:43.749 INFO GibbsSampler - MCMC sampling complete.; 09:48:43.749 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:48:43.771 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:48:44.116 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599214.649782...; 09:48:44.116 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.041049, outlierProbability=0.031143}; 09:48:44.424 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599203.367762...; 09:48:44.424 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.036778, outlierProbability=0.037218}; 09:48:44.714 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599201.638390...; 09:48:44.714 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149830, biasVariance=0.034706, outlierProbability=0.039510}; 09:48:44.991 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599201.210843...; 09:48:44.991 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147681, biasVariance=0.033671, outlierProbability=0.039510}; 09:48:45.104 INFO GibbsSampler - Starting MCMC sampling.; 09:48:51.555 INFO GibbsSampler - 25 of 100 samples generated.; 09:48:58.171 INFO GibbsSampler - 50 of 100 samples generated.; 09:49:05.022 INFO GibbsSampler - 75 of 100 samples generated.; 09:49:11.512 INFO GibbsSampler - 100 of 100 samples g",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:16862,Testability,log,log,16862,"sSampler - 50 of 100 samples generated.; 09:48:41.417 INFO GibbsSampler - 75 of 100 samples generated.; 09:48:43.749 INFO GibbsSampler - 100 of 100 samples generated.; 09:48:43.749 INFO GibbsSampler - MCMC sampling complete.; 09:48:43.749 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:48:43.771 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:48:44.116 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599214.649782...; 09:48:44.116 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.041049, outlierProbability=0.031143}; 09:48:44.424 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599203.367762...; 09:48:44.424 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.036778, outlierProbability=0.037218}; 09:48:44.714 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599201.638390...; 09:48:44.714 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149830, biasVariance=0.034706, outlierProbability=0.039510}; 09:48:44.991 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599201.210843...; 09:48:44.991 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147681, biasVariance=0.033671, outlierProbability=0.039510}; 09:48:45.104 INFO GibbsSampler - Starting MCMC sampling.; 09:48:51.555 INFO GibbsSampler - 25 of 100 samples generated.; 09:48:58.171 INFO GibbsSampler - 50 of 100 samples generated.; 09:49:05.022 INFO GibbsSampler - 75 of 100 samples generated.; 09:49:11.512 INFO GibbsSampler - 100 of 100 samples generated.; 09:49:11.512 INFO GibbsSampler - MCMC sampling complete.; 09:49:11.543 INFO MultidimensionalModeller - Smoothing iteration: 7; 09:49:11.543 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 311; 09:49:11.543 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:17112,Testability,log,log,17112,"imensionalModeller - Fitting allele-fraction model...; 09:48:43.771 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:48:44.116 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599214.649782...; 09:48:44.116 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.041049, outlierProbability=0.031143}; 09:48:44.424 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599203.367762...; 09:48:44.424 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.036778, outlierProbability=0.037218}; 09:48:44.714 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599201.638390...; 09:48:44.714 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149830, biasVariance=0.034706, outlierProbability=0.039510}; 09:48:44.991 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599201.210843...; 09:48:44.991 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147681, biasVariance=0.033671, outlierProbability=0.039510}; 09:48:45.104 INFO GibbsSampler - Starting MCMC sampling.; 09:48:51.555 INFO GibbsSampler - 25 of 100 samples generated.; 09:48:58.171 INFO GibbsSampler - 50 of 100 samples generated.; 09:49:05.022 INFO GibbsSampler - 75 of 100 samples generated.; 09:49:11.512 INFO GibbsSampler - 100 of 100 samples generated.; 09:49:11.512 INFO GibbsSampler - MCMC sampling complete.; 09:49:11.543 INFO MultidimensionalModeller - Smoothing iteration: 7; 09:49:11.543 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 311; 09:49:11.543 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 311; 09:49:11.543 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:49:12.363 INFO GibbsSampler - Starting MCMC sampling.; 09:49:14.885 INFO GibbsSampler - 25 o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:18548,Testability,log,log,18548,"0 samples generated.; 09:49:11.512 INFO GibbsSampler - 100 of 100 samples generated.; 09:49:11.512 INFO GibbsSampler - MCMC sampling complete.; 09:49:11.543 INFO MultidimensionalModeller - Smoothing iteration: 7; 09:49:11.543 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 311; 09:49:11.543 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 311; 09:49:11.543 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:49:12.363 INFO GibbsSampler - Starting MCMC sampling.; 09:49:14.885 INFO GibbsSampler - 25 of 100 samples generated.; 09:49:17.255 INFO GibbsSampler - 50 of 100 samples generated.; 09:49:19.604 INFO GibbsSampler - 75 of 100 samples generated.; 09:49:21.938 INFO GibbsSampler - 100 of 100 samples generated.; 09:49:21.939 INFO GibbsSampler - MCMC sampling complete.; 09:49:21.939 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:49:21.958 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:49:22.312 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599214.649782...; 09:49:22.312 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.041049, outlierProbability=0.031143}; 09:49:22.622 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599203.367762...; 09:49:22.622 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.036778, outlierProbability=0.037218}; 09:49:22.933 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599201.638390...; 09:49:22.933 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149830, biasVariance=0.034706, outlierProbability=0.039510}; 09:49:23.216 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599201.210843...; 09:49:23.216 INFO AlleleFractionInitializer - AlleleFractionGlobalParameter",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:18661,Testability,log,log,18661," sampling complete.; 09:49:11.543 INFO MultidimensionalModeller - Smoothing iteration: 7; 09:49:11.543 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 311; 09:49:11.543 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 311; 09:49:11.543 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:49:12.363 INFO GibbsSampler - Starting MCMC sampling.; 09:49:14.885 INFO GibbsSampler - 25 of 100 samples generated.; 09:49:17.255 INFO GibbsSampler - 50 of 100 samples generated.; 09:49:19.604 INFO GibbsSampler - 75 of 100 samples generated.; 09:49:21.938 INFO GibbsSampler - 100 of 100 samples generated.; 09:49:21.939 INFO GibbsSampler - MCMC sampling complete.; 09:49:21.939 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:49:21.958 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:49:22.312 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599214.649782...; 09:49:22.312 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.041049, outlierProbability=0.031143}; 09:49:22.622 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599203.367762...; 09:49:22.622 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.036778, outlierProbability=0.037218}; 09:49:22.933 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599201.638390...; 09:49:22.933 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149830, biasVariance=0.034706, outlierProbability=0.039510}; 09:49:23.216 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599201.210843...; 09:49:23.216 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147681, biasVariance=0.033671, outlierProbability=0.039510}; 09:49:23.323 INFO GibbsSampler - Starting MCMC sam",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:18911,Testability,log,log,18911," after smoothing iteration: 311; 09:49:11.543 INFO MultidimensionalModeller - Fitting copy-ratio model...; 09:49:12.363 INFO GibbsSampler - Starting MCMC sampling.; 09:49:14.885 INFO GibbsSampler - 25 of 100 samples generated.; 09:49:17.255 INFO GibbsSampler - 50 of 100 samples generated.; 09:49:19.604 INFO GibbsSampler - 75 of 100 samples generated.; 09:49:21.938 INFO GibbsSampler - 100 of 100 samples generated.; 09:49:21.939 INFO GibbsSampler - MCMC sampling complete.; 09:49:21.939 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:49:21.958 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:49:22.312 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599214.649782...; 09:49:22.312 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.041049, outlierProbability=0.031143}; 09:49:22.622 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599203.367762...; 09:49:22.622 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.036778, outlierProbability=0.037218}; 09:49:22.933 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599201.638390...; 09:49:22.933 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149830, biasVariance=0.034706, outlierProbability=0.039510}; 09:49:23.216 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599201.210843...; 09:49:23.216 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147681, biasVariance=0.033671, outlierProbability=0.039510}; 09:49:23.323 INFO GibbsSampler - Starting MCMC sampling.; 09:49:29.603 INFO GibbsSampler - 25 of 100 samples generated.; 09:49:36.116 INFO GibbsSampler - 50 of 100 samples generated.; 09:49:42.766 INFO GibbsSampler - 75 of 100 samples generated.; 09:49:49.322 INFO GibbsSampler - 100 of 100 samples g",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:19161,Testability,log,log,19161,"09:49:14.885 INFO GibbsSampler - 25 of 100 samples generated.; 09:49:17.255 INFO GibbsSampler - 50 of 100 samples generated.; 09:49:19.604 INFO GibbsSampler - 75 of 100 samples generated.; 09:49:21.938 INFO GibbsSampler - 100 of 100 samples generated.; 09:49:21.939 INFO GibbsSampler - MCMC sampling complete.; 09:49:21.939 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:49:21.958 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:49:22.312 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599214.649782...; 09:49:22.312 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.041049, outlierProbability=0.031143}; 09:49:22.622 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599203.367762...; 09:49:22.622 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.036778, outlierProbability=0.037218}; 09:49:22.933 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599201.638390...; 09:49:22.933 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149830, biasVariance=0.034706, outlierProbability=0.039510}; 09:49:23.216 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599201.210843...; 09:49:23.216 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147681, biasVariance=0.033671, outlierProbability=0.039510}; 09:49:23.323 INFO GibbsSampler - Starting MCMC sampling.; 09:49:29.603 INFO GibbsSampler - 25 of 100 samples generated.; 09:49:36.116 INFO GibbsSampler - 50 of 100 samples generated.; 09:49:42.766 INFO GibbsSampler - 75 of 100 samples generated.; 09:49:49.322 INFO GibbsSampler - 100 of 100 samples generated.; 09:49:49.322 INFO GibbsSampler - MCMC sampling complete.; 09:49:49.355 INFO MultidimensionalModeller - Final number of segments after smoothing: 311; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732:19411,Testability,log,log,19411,"09:49:14.885 INFO GibbsSampler - 25 of 100 samples generated.; 09:49:17.255 INFO GibbsSampler - 50 of 100 samples generated.; 09:49:19.604 INFO GibbsSampler - 75 of 100 samples generated.; 09:49:21.938 INFO GibbsSampler - 100 of 100 samples generated.; 09:49:21.939 INFO GibbsSampler - MCMC sampling complete.; 09:49:21.939 INFO MultidimensionalModeller - Fitting allele-fraction model...; 09:49:21.958 INFO AlleleFractionInitializer - Initializing allele-fraction model, iterating until log likelihood converges to within 0.500000...; 09:49:22.312 INFO AlleleFractionInitializer - Iteration 1, model log likelihood = -599214.649782...; 09:49:22.312 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.041049, outlierProbability=0.031143}; 09:49:22.622 INFO AlleleFractionInitializer - Iteration 2, model log likelihood = -599203.367762...; 09:49:22.622 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.152095, biasVariance=0.036778, outlierProbability=0.037218}; 09:49:22.933 INFO AlleleFractionInitializer - Iteration 3, model log likelihood = -599201.638390...; 09:49:22.933 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.149830, biasVariance=0.034706, outlierProbability=0.039510}; 09:49:23.216 INFO AlleleFractionInitializer - Iteration 4, model log likelihood = -599201.210843...; 09:49:23.216 INFO AlleleFractionInitializer - AlleleFractionGlobalParameters{meanBias=1.147681, biasVariance=0.033671, outlierProbability=0.039510}; 09:49:23.323 INFO GibbsSampler - Starting MCMC sampling.; 09:49:29.603 INFO GibbsSampler - 25 of 100 samples generated.; 09:49:36.116 INFO GibbsSampler - 50 of 100 samples generated.; 09:49:42.766 INFO GibbsSampler - 75 of 100 samples generated.; 09:49:49.322 INFO GibbsSampler - 100 of 100 samples generated.; 09:49:49.322 INFO GibbsSampler - MCMC sampling complete.; 09:49:49.355 INFO MultidimensionalModeller - Final number of segments after smoothing: 311; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382805732
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382807646:952,Integrability,depend,depend,952,"Yes, that seems to be working as intended. You'll see that in your first log, the following lines indicate that convergence has been reached after 2 iterations:. ````; 09:40:44.924 INFO MultidimensionalModeller - Smoothing iteration: 2; 09:40:44.924 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 398; 09:40:44.924 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 398; ````. Therefore, when you set N >= 3, the behavior is the same as when N = 2. In your second log, only one iteration goes by before we refit again. This refit yields posteriors that are sufficiently different from the approximation used when no refitting is performed so that additional smoothing needs to be performed before convergence. You'll see that in the second case, we end up with fewer segments. Whether or not this smoother result (which takes longer to arrive at, since refitting is expensive) is desired will depend on the goals of the analysis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382807646
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382807646:688,Performance,perform,performed,688,"Yes, that seems to be working as intended. You'll see that in your first log, the following lines indicate that convergence has been reached after 2 iterations:. ````; 09:40:44.924 INFO MultidimensionalModeller - Smoothing iteration: 2; 09:40:44.924 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 398; 09:40:44.924 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 398; ````. Therefore, when you set N >= 3, the behavior is the same as when N = 2. In your second log, only one iteration goes by before we refit again. This refit yields posteriors that are sufficiently different from the approximation used when no refitting is performed so that additional smoothing needs to be performed before convergence. You'll see that in the second case, we end up with fewer segments. Whether or not this smoother result (which takes longer to arrive at, since refitting is expensive) is desired will depend on the goals of the analysis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382807646
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382807646:739,Performance,perform,performed,739,"Yes, that seems to be working as intended. You'll see that in your first log, the following lines indicate that convergence has been reached after 2 iterations:. ````; 09:40:44.924 INFO MultidimensionalModeller - Smoothing iteration: 2; 09:40:44.924 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 398; 09:40:44.924 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 398; ````. Therefore, when you set N >= 3, the behavior is the same as when N = 2. In your second log, only one iteration goes by before we refit again. This refit yields posteriors that are sufficiently different from the approximation used when no refitting is performed so that additional smoothing needs to be performed before convergence. You'll see that in the second case, we end up with fewer segments. Whether or not this smoother result (which takes longer to arrive at, since refitting is expensive) is desired will depend on the goals of the analysis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382807646
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382807646:73,Testability,log,log,73,"Yes, that seems to be working as intended. You'll see that in your first log, the following lines indicate that convergence has been reached after 2 iterations:. ````; 09:40:44.924 INFO MultidimensionalModeller - Smoothing iteration: 2; 09:40:44.924 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 398; 09:40:44.924 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 398; ````. Therefore, when you set N >= 3, the behavior is the same as when N = 2. In your second log, only one iteration goes by before we refit again. This refit yields posteriors that are sufficiently different from the approximation used when no refitting is performed so that additional smoothing needs to be performed before convergence. You'll see that in the second case, we end up with fewer segments. Whether or not this smoother result (which takes longer to arrive at, since refitting is expensive) is desired will depend on the goals of the analysis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382807646
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382807646:523,Testability,log,log,523,"Yes, that seems to be working as intended. You'll see that in your first log, the following lines indicate that convergence has been reached after 2 iterations:. ````; 09:40:44.924 INFO MultidimensionalModeller - Smoothing iteration: 2; 09:40:44.924 INFO MultidimensionalModeller - Number of segments before smoothing iteration: 398; 09:40:44.924 INFO MultidimensionalModeller - Number of segments after smoothing iteration: 398; ````. Therefore, when you set N >= 3, the behavior is the same as when N = 2. In your second log, only one iteration goes by before we refit again. This refit yields posteriors that are sufficiently different from the approximation used when no refitting is performed so that additional smoothing needs to be performed before convergence. You'll see that in the second case, we end up with fewer segments. Whether or not this smoother result (which takes longer to arrive at, since refitting is expensive) is desired will depend on the goals of the analysis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382807646
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382838158:143,Deployability,pipeline,pipeline,143,"@sooheelee By the way, can we avoid using ""ACNV""? Let's just call this tool ModelSegments to distinguish it from the old GATK CNV -> GATK ACNV pipeline.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382838158
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382838158:30,Safety,avoid,avoid,30,"@sooheelee By the way, can we avoid using ""ACNV""? Let's just call this tool ModelSegments to distinguish it from the old GATK CNV -> GATK ACNV pipeline.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382838158
https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382863906:328,Usability,intuit,intuitive,328,"So behavior when disabled (parameter set to 0) and when set to 2 or above gives the same results for my data where smoothing iterations you say plateau at two iterations. When set to 1, the smoothing iterations increase to seven and this is the only other alternative result one can expect for this data. It is somewhat counter-intuitive. But given @samuelklee says this is the expected behavior for this parameter, then I can close this.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4683#issuecomment-382863906
https://github.com/broadinstitute/gatk/issues/4684#issuecomment-566723997:72,Usability,learn,learned,72,"Evaluation is so complicated (as necessary, and correctly so) now as we learned along the way, this ticket is no longer relevant.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4684#issuecomment-566723997
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383172864:88,Availability,error,error,88,"@mmokrejs The 3.8-1 pom is not broken -- I just checked out and built it myself without error. It looks to me like you are building from an unclean working tree with an unresolved merge conflict, since there are conflict markers in your copy of the pom:. ```; 15 <<<<<<< HEAD; 16 <version>3.8-1</version>; 17 =======; 18 <version>3.8-2-SNAPSHOT</version>; 19 >>>>>>> 0450e2531ee021e28bd7c5e92b5ba736d530d9af; ```. You should ensure that you have a clean checkout of `3.8-1` by running `git checkout -f 3.8-1` in your clone.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383172864
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383177094:60,Availability,Down,Downloading,60,"Actually I still have in xterm buffer the source:. ```; >>> Downloading 'https://github.com/broadgsa/gatk/archive/3.8-1.tar.gz'; --2018-04-20 19:21:15-- https://github.com/broadgsa/gatk/archive/3.8-1.tar.gz; Resolving github.com... 192.30.253.112, 192.30.253.113; Connecting to github.com|192.30.253.112|:443... connected.; HTTP request sent, awaiting response... 302 Found; Location: https://codeload.github.com/broadgsa/gatk/tar.gz/3.8-1 [following]; --2018-04-20 19:21:16-- https://codeload.github.com/broadgsa/gatk/tar.gz/3.8-1; Resolving codeload.github.com... 192.30.253.121, 192.30.253.120; Connecting to codeload.github.com|192.30.253.121|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: unspecified [application/x-gzip]; Saving to: '/scratch/usr/portage/distfiles/gatk-3.8.1.tar.gz'. /scratch/usr/portage/distfiles/gatk-3.8.1.tar.gz [ <=> ] 473.08M 1.07MB/s in 8m 44s . 2018-04-20 19:30:01 (924 KB/s) - '/scratch/usr/portage/distfiles/gatk-3.8.1.tar.gz' saved [496061006]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383177094
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514:1489,Availability,ERROR,ERROR,1489,"Receiving objects: 100% (243170/243170), 608.21 MiB | 996.00 KiB/s, done.; Resolving deltas: 100% (130671/130671), done.; From https://github.com/broadgsa/gatk; * [new tag] 3.8-1 -> 3.8-1; * [new tag] 1.0 -> 1.0; git symbolic-ref refs/git-r3/sci-biology/gatk/0/__main__ refs/tags/3.8-1; * Checking out https://github.com/broadgsa/gatk.git to /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; git checkout --quiet refs/tags/3.8-1; GIT NEW branch -->; repository: https://github.com/broadgsa/gatk.git; at the commit: f2ed14489851ff2c00da3dcab9ad0b8f5ccec200; >>> Source unpacked in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven usin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514:1497,Availability,ERROR,ERROR,1497,"Receiving objects: 100% (243170/243170), 608.21 MiB | 996.00 KiB/s, done.; Resolving deltas: 100% (130671/130671), done.; From https://github.com/broadgsa/gatk; * [new tag] 3.8-1 -> 3.8-1; * [new tag] 1.0 -> 1.0; git symbolic-ref refs/git-r3/sci-biology/gatk/0/__main__ refs/tags/3.8-1; * Checking out https://github.com/broadgsa/gatk.git to /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; git checkout --quiet refs/tags/3.8-1; GIT NEW branch -->; repository: https://github.com/broadgsa/gatk.git; at the commit: f2ed14489851ff2c00da3dcab9ad0b8f5ccec200; >>> Source unpacked in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven usin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514:1883,Availability,ERROR,ERROR,1883,"/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; git checkout --quiet refs/tags/3.8-1; GIT NEW branch -->; repository: https://github.com/broadgsa/gatk.git; at the commit: f2ed14489851ff2c00da3dcab9ad0b8f5ccec200; >>> Source unpacked in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. I can",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514:1939,Availability,ERROR,ERROR,1939,"/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; git checkout --quiet refs/tags/3.8-1; GIT NEW branch -->; repository: https://github.com/broadgsa/gatk.git; at the commit: f2ed14489851ff2c00da3dcab9ad0b8f5ccec200; >>> Source unpacked in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. I can",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514:1949,Availability,ERROR,ERROR,1949,"/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; git checkout --quiet refs/tags/3.8-1; GIT NEW branch -->; repository: https://github.com/broadgsa/gatk.git; at the commit: f2ed14489851ff2c00da3dcab9ad0b8f5ccec200; >>> Source unpacked in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. I can",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514:2106,Availability,error,error,2106,"-3.8.1 ...; git checkout --quiet refs/tags/3.8-1; GIT NEW branch -->; repository: https://github.com/broadgsa/gatk.git; at the commit: f2ed14489851ff2c00da3dcab9ad0b8f5ccec200; >>> Source unpacked in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. I cannot say if `git checkout -f` is used, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514:2114,Availability,ERROR,ERROR,2114,"-3.8.1 ...; git checkout --quiet refs/tags/3.8-1; GIT NEW branch -->; repository: https://github.com/broadgsa/gatk.git; at the commit: f2ed14489851ff2c00da3dcab9ad0b8f5ccec200; >>> Source unpacked in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. I cannot say if `git checkout -f` is used, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514:2441,Availability,ERROR,ERROR,2441,"-3.8.1 ...; git checkout --quiet refs/tags/3.8-1; GIT NEW branch -->; repository: https://github.com/broadgsa/gatk.git; at the commit: f2ed14489851ff2c00da3dcab9ad0b8f5ccec200; >>> Source unpacked in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. I cannot say if `git checkout -f` is used, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514:2451,Availability,ERROR,ERROR,2451,"-3.8.1 ...; git checkout --quiet refs/tags/3.8-1; GIT NEW branch -->; repository: https://github.com/broadgsa/gatk.git; at the commit: f2ed14489851ff2c00da3dcab9ad0b8f5ccec200; >>> Source unpacked in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. I cannot say if `git checkout -f` is used, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514:2493,Availability,error,errors,2493,"-3.8.1 ...; git checkout --quiet refs/tags/3.8-1; GIT NEW branch -->; repository: https://github.com/broadgsa/gatk.git; at the commit: f2ed14489851ff2c00da3dcab9ad0b8f5ccec200; >>> Source unpacked in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. I cannot say if `git checkout -f` is used, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514:2536,Availability,ERROR,ERROR,2536,"-3.8.1 ...; git checkout --quiet refs/tags/3.8-1; GIT NEW branch -->; repository: https://github.com/broadgsa/gatk.git; at the commit: f2ed14489851ff2c00da3dcab9ad0b8f5ccec200; >>> Source unpacked in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. I cannot say if `git checkout -f` is used, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514:2608,Availability,ERROR,ERROR,2608,"-3.8.1 ...; git checkout --quiet refs/tags/3.8-1; GIT NEW branch -->; repository: https://github.com/broadgsa/gatk.git; at the commit: f2ed14489851ff2c00da3dcab9ad0b8f5ccec200; >>> Source unpacked in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. I cannot say if `git checkout -f` is used, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514:2618,Availability,ERROR,ERROR,2618,"-3.8.1 ...; git checkout --quiet refs/tags/3.8-1; GIT NEW branch -->; repository: https://github.com/broadgsa/gatk.git; at the commit: f2ed14489851ff2c00da3dcab9ad0b8f5ccec200; >>> Source unpacked in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. I cannot say if `git checkout -f` is used, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514:2656,Availability,error,errors,2656,"-3.8.1 ...; git checkout --quiet refs/tags/3.8-1; GIT NEW branch -->; repository: https://github.com/broadgsa/gatk.git; at the commit: f2ed14489851ff2c00da3dcab9ad0b8f5ccec200; >>> Source unpacked in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. I cannot say if `git checkout -f` is used, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514:2725,Availability,ERROR,ERROR,2725,"-3.8.1 ...; git checkout --quiet refs/tags/3.8-1; GIT NEW branch -->; repository: https://github.com/broadgsa/gatk.git; at the commit: f2ed14489851ff2c00da3dcab9ad0b8f5ccec200; >>> Source unpacked in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. I cannot say if `git checkout -f` is used, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514:2817,Availability,ERROR,ERROR,2817,"-3.8.1 ...; git checkout --quiet refs/tags/3.8-1; GIT NEW branch -->; repository: https://github.com/broadgsa/gatk.git; at the commit: f2ed14489851ff2c00da3dcab9ad0b8f5ccec200; >>> Source unpacked in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. I cannot say if `git checkout -f` is used, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514:2597,Testability,log,logging,2597,"-3.8.1 ...; git checkout --quiet refs/tags/3.8-1; GIT NEW branch -->; repository: https://github.com/broadgsa/gatk.git; at the commit: f2ed14489851ff2c00da3dcab9ad0b8f5ccec200; >>> Source unpacked in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work; >>> Preparing source in /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1 ...; Equivalent maven command; mvn -Dmaven.repo.local=/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/.m2/repository verify '-Ddisable.shadepackage'; [INFO] Scanning for projects...; [ERROR] [ERROR] Some problems were encountered while processing the POMs:; [FATAL] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3; @ ; [ERROR] The build could not read 1 project -> [Help 1]; [ERROR] ; [ERROR] The project org.broadinstitute.gatk:gatk-aggregator:[unknown-version] (/scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/pom.xml) has 1 error; [ERROR] Non-parseable POM /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml: unexpected character in markup < (position: END_TAG seen ...</artifactId>\n<<... @15:3) @ /scratch/var/tmp/portage/sci-biology/gatk-3.8.1/work/gatk-3.8.1/public/gatk-root/pom.xml, line 15, column 3 -> [Help 2]; [ERROR] ; [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.; [ERROR] Re-run Maven using the -X switch to enable full debug logging.; [ERROR] ; [ERROR] For more information about the errors and possible solutions, please read the following articles:; [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException; [ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException; ```. I cannot say if `git checkout -f` is used, though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-383184514
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-390003142:0,Availability,ping,ping,0,ping @vdauwera,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-390003142
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-395597795:18,Availability,ping,ping,18,@vdauwera monthly ping :),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-395597795
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-404325727:10,Availability,Ping,Ping,10,@vdauwera Ping,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-404325727
https://github.com/broadinstitute/gatk/issues/4685#issuecomment-412888688:18,Availability,ping,ping,18,"@vdauwera Monthly ping -- the final GATK 3.x tag is still broken, and you're the only one who can fix it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4685#issuecomment-412888688
https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383190737:11,Testability,log,log,11,Full build.log.txt from 3.8 is here ; [build.log.txt](https://github.com/broadinstitute/gatk/files/1933298/build.log.txt),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383190737
https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383190737:45,Testability,log,log,45,Full build.log.txt from 3.8 is here ; [build.log.txt](https://github.com/broadinstitute/gatk/files/1933298/build.log.txt),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383190737
https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383190737:113,Testability,log,log,113,Full build.log.txt from 3.8 is here ; [build.log.txt](https://github.com/broadinstitute/gatk/files/1933298/build.log.txt),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383190737
https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383194874:235,Integrability,message,message,235,`mvn clean` did not help although I do see its output logged properly. Let's ignore the `external-example` build issue as it is not present in 3.8-1 (as I managed to build it now). I care therefore only about the `rm: missing operand` message.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383194874
https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383194874:54,Testability,log,logged,54,`mvn clean` did not help although I do see its output logged properly. Let's ignore the `external-example` build issue as it is not present in 3.8-1 (as I managed to build it now). I care therefore only about the `rm: missing operand` message.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383194874
https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383196456:214,Availability,error,error,214,"@mmokrejs That message is harmless -- the build system is just trying to delete some symlinks that may exist if `mvn clean` was not run before the build, but it's not finding anything to delete. It's not an actual error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383196456
https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383196456:15,Integrability,message,message,15,"@mmokrejs That message is harmless -- the build system is just trying to delete some symlinks that may exist if `mvn clean` was not run before the build, but it's not finding anything to delete. It's not an actual error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383196456
https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383197966:66,Testability,log,log,66,"OK, but would you mind using `rm -f` in future? Here is the build.log from 3.8-1, for reference: ; [build.log.txt](https://github.com/broadinstitute/gatk/files/1933394/build.log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383197966
https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383197966:106,Testability,log,log,106,"OK, but would you mind using `rm -f` in future? Here is the build.log from 3.8-1, for reference: ; [build.log.txt](https://github.com/broadinstitute/gatk/files/1933394/build.log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383197966
https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383197966:174,Testability,log,log,174,"OK, but would you mind using `rm -f` in future? Here is the build.log from 3.8-1, for reference: ; [build.log.txt](https://github.com/broadinstitute/gatk/files/1933394/build.log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4686#issuecomment-383197966
https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383216894:29,Availability,error,error,29,"It looks like the underlying error is ""No default remote / No remotes defined"". What does `git remote show origin` output? If it doesn't show the URL of this repo under ""Fetch URL"" and ""Push URL"", then your git clone was not created correctly. In this case, you should re-clone the repo using a command like `git clone https://github.com/broadinstitute/gatk.git gatk`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383216894
https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183:401,Availability,error,errors,401,"```; $ git remote show origin; fatal: 'origin' does not appear to be a git repository; fatal: Could not read from remote repository. Please make sure you have the correct access rights; and the repository exists.; $ cat .git/config ; [core]; 	repositoryformatversion = 0; 	filemode = true; 	bare = false; 	logallrefupdates = true; $; ```. Hmm, here is the full log, actually I see some shared library errors at the top. Grr, I have `ncurses-6` library only. Why doesn't the build system die immediately upon an error? Anyway, this is exactly why Gentoo does not like executing zillions of evil jar files and other executables. As I said in the past, your step away from Apache ant build system was a very bad decision. You can see in the log the git tag too. I am not sure if the build system used `master` instead of `gatk` branch. Is that a problem?. [build.log.txt](https://github.com/broadinstitute/gatk/files/1933626/build.log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183
https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183:511,Availability,error,error,511,"```; $ git remote show origin; fatal: 'origin' does not appear to be a git repository; fatal: Could not read from remote repository. Please make sure you have the correct access rights; and the repository exists.; $ cat .git/config ; [core]; 	repositoryformatversion = 0; 	filemode = true; 	bare = false; 	logallrefupdates = true; $; ```. Hmm, here is the full log, actually I see some shared library errors at the top. Grr, I have `ncurses-6` library only. Why doesn't the build system die immediately upon an error? Anyway, this is exactly why Gentoo does not like executing zillions of evil jar files and other executables. As I said in the past, your step away from Apache ant build system was a very bad decision. You can see in the log the git tag too. I am not sure if the build system used `master` instead of `gatk` branch. Is that a problem?. [build.log.txt](https://github.com/broadinstitute/gatk/files/1933626/build.log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183
https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183:225,Modifiability,config,config,225,"```; $ git remote show origin; fatal: 'origin' does not appear to be a git repository; fatal: Could not read from remote repository. Please make sure you have the correct access rights; and the repository exists.; $ cat .git/config ; [core]; 	repositoryformatversion = 0; 	filemode = true; 	bare = false; 	logallrefupdates = true; $; ```. Hmm, here is the full log, actually I see some shared library errors at the top. Grr, I have `ncurses-6` library only. Why doesn't the build system die immediately upon an error? Anyway, this is exactly why Gentoo does not like executing zillions of evil jar files and other executables. As I said in the past, your step away from Apache ant build system was a very bad decision. You can see in the log the git tag too. I am not sure if the build system used `master` instead of `gatk` branch. Is that a problem?. [build.log.txt](https://github.com/broadinstitute/gatk/files/1933626/build.log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183
https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183:171,Security,access,access,171,"```; $ git remote show origin; fatal: 'origin' does not appear to be a git repository; fatal: Could not read from remote repository. Please make sure you have the correct access rights; and the repository exists.; $ cat .git/config ; [core]; 	repositoryformatversion = 0; 	filemode = true; 	bare = false; 	logallrefupdates = true; $; ```. Hmm, here is the full log, actually I see some shared library errors at the top. Grr, I have `ncurses-6` library only. Why doesn't the build system die immediately upon an error? Anyway, this is exactly why Gentoo does not like executing zillions of evil jar files and other executables. As I said in the past, your step away from Apache ant build system was a very bad decision. You can see in the log the git tag too. I am not sure if the build system used `master` instead of `gatk` branch. Is that a problem?. [build.log.txt](https://github.com/broadinstitute/gatk/files/1933626/build.log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183
https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183:306,Testability,log,logallrefupdates,306,"```; $ git remote show origin; fatal: 'origin' does not appear to be a git repository; fatal: Could not read from remote repository. Please make sure you have the correct access rights; and the repository exists.; $ cat .git/config ; [core]; 	repositoryformatversion = 0; 	filemode = true; 	bare = false; 	logallrefupdates = true; $; ```. Hmm, here is the full log, actually I see some shared library errors at the top. Grr, I have `ncurses-6` library only. Why doesn't the build system die immediately upon an error? Anyway, this is exactly why Gentoo does not like executing zillions of evil jar files and other executables. As I said in the past, your step away from Apache ant build system was a very bad decision. You can see in the log the git tag too. I am not sure if the build system used `master` instead of `gatk` branch. Is that a problem?. [build.log.txt](https://github.com/broadinstitute/gatk/files/1933626/build.log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183
https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183:361,Testability,log,log,361,"```; $ git remote show origin; fatal: 'origin' does not appear to be a git repository; fatal: Could not read from remote repository. Please make sure you have the correct access rights; and the repository exists.; $ cat .git/config ; [core]; 	repositoryformatversion = 0; 	filemode = true; 	bare = false; 	logallrefupdates = true; $; ```. Hmm, here is the full log, actually I see some shared library errors at the top. Grr, I have `ncurses-6` library only. Why doesn't the build system die immediately upon an error? Anyway, this is exactly why Gentoo does not like executing zillions of evil jar files and other executables. As I said in the past, your step away from Apache ant build system was a very bad decision. You can see in the log the git tag too. I am not sure if the build system used `master` instead of `gatk` branch. Is that a problem?. [build.log.txt](https://github.com/broadinstitute/gatk/files/1933626/build.log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183
https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183:738,Testability,log,log,738,"```; $ git remote show origin; fatal: 'origin' does not appear to be a git repository; fatal: Could not read from remote repository. Please make sure you have the correct access rights; and the repository exists.; $ cat .git/config ; [core]; 	repositoryformatversion = 0; 	filemode = true; 	bare = false; 	logallrefupdates = true; $; ```. Hmm, here is the full log, actually I see some shared library errors at the top. Grr, I have `ncurses-6` library only. Why doesn't the build system die immediately upon an error? Anyway, this is exactly why Gentoo does not like executing zillions of evil jar files and other executables. As I said in the past, your step away from Apache ant build system was a very bad decision. You can see in the log the git tag too. I am not sure if the build system used `master` instead of `gatk` branch. Is that a problem?. [build.log.txt](https://github.com/broadinstitute/gatk/files/1933626/build.log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183
https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183:860,Testability,log,log,860,"```; $ git remote show origin; fatal: 'origin' does not appear to be a git repository; fatal: Could not read from remote repository. Please make sure you have the correct access rights; and the repository exists.; $ cat .git/config ; [core]; 	repositoryformatversion = 0; 	filemode = true; 	bare = false; 	logallrefupdates = true; $; ```. Hmm, here is the full log, actually I see some shared library errors at the top. Grr, I have `ncurses-6` library only. Why doesn't the build system die immediately upon an error? Anyway, this is exactly why Gentoo does not like executing zillions of evil jar files and other executables. As I said in the past, your step away from Apache ant build system was a very bad decision. You can see in the log the git tag too. I am not sure if the build system used `master` instead of `gatk` branch. Is that a problem?. [build.log.txt](https://github.com/broadinstitute/gatk/files/1933626/build.log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183
https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183:928,Testability,log,log,928,"```; $ git remote show origin; fatal: 'origin' does not appear to be a git repository; fatal: Could not read from remote repository. Please make sure you have the correct access rights; and the repository exists.; $ cat .git/config ; [core]; 	repositoryformatversion = 0; 	filemode = true; 	bare = false; 	logallrefupdates = true; $; ```. Hmm, here is the full log, actually I see some shared library errors at the top. Grr, I have `ncurses-6` library only. Why doesn't the build system die immediately upon an error? Anyway, this is exactly why Gentoo does not like executing zillions of evil jar files and other executables. As I said in the past, your step away from Apache ant build system was a very bad decision. You can see in the log the git tag too. I am not sure if the build system used `master` instead of `gatk` branch. Is that a problem?. [build.log.txt](https://github.com/broadinstitute/gatk/files/1933626/build.log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687#issuecomment-383221183
https://github.com/broadinstitute/gatk/pull/4688#issuecomment-383229944:130,Deployability,pipeline,pipeline,130,"Not ready for merge yet! Needs tests + some additional fixes for `AnalyzeCovariates`. The basic `BaseRecalibrator` -> `ApplyBQSR` pipeline with a custom covariate seems to be working, however. @takutosato please try this branch out with your new homopolymer covariate and let me know whether it works! Feel free to grab some time on my calendar next week for help working through any issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4688#issuecomment-383229944
https://github.com/broadinstitute/gatk/pull/4688#issuecomment-383229944:31,Testability,test,tests,31,"Not ready for merge yet! Needs tests + some additional fixes for `AnalyzeCovariates`. The basic `BaseRecalibrator` -> `ApplyBQSR` pipeline with a custom covariate seems to be working, however. @takutosato please try this branch out with your new homopolymer covariate and let me know whether it works! Feel free to grab some time on my calendar next week for help working through any issues.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4688#issuecomment-383229944
https://github.com/broadinstitute/gatk/pull/4690#issuecomment-383357595:25,Availability,failure,failure,25,@takutosato The one test failure occurs because when there is low normal coverage germline risk might get triggered too often. I might need to adjust the default value but it won't affect the code otherwise.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4690#issuecomment-383357595
https://github.com/broadinstitute/gatk/pull/4690#issuecomment-383357595:91,Safety,risk,risk,91,@takutosato The one test failure occurs because when there is low normal coverage germline risk might get triggered too often. I might need to adjust the default value but it won't affect the code otherwise.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4690#issuecomment-383357595
https://github.com/broadinstitute/gatk/pull/4690#issuecomment-383357595:20,Testability,test,test,20,@takutosato The one test failure occurs because when there is low normal coverage germline risk might get triggered too often. I might need to adjust the default value but it won't affect the code otherwise.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4690#issuecomment-383357595
https://github.com/broadinstitute/gatk/issues/4691#issuecomment-383763740:433,Availability,error,error,433,"In this case, allele-specific annotations are outputted to GVCFs files by 'HaplotypeCaller -ERC GVCF -G AS_StandardAnnotation' command. Although some warning message of 'HaplotypeCaller -ERC GVCF -G AS_StandardAnnotation' command.; ```; WARN:StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; ```; This warning message does not happen in all of my cases, but the FORMAT/AD error happen in all of my cases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4691#issuecomment-383763740
https://github.com/broadinstitute/gatk/issues/4691#issuecomment-383763740:158,Integrability,message,message,158,"In this case, allele-specific annotations are outputted to GVCFs files by 'HaplotypeCaller -ERC GVCF -G AS_StandardAnnotation' command. Although some warning message of 'HaplotypeCaller -ERC GVCF -G AS_StandardAnnotation' command.; ```; WARN:StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; ```; This warning message does not happen in all of my cases, but the FORMAT/AD error happen in all of my cases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4691#issuecomment-383763740
https://github.com/broadinstitute/gatk/issues/4691#issuecomment-383763740:371,Integrability,message,message,371,"In this case, allele-specific annotations are outputted to GVCFs files by 'HaplotypeCaller -ERC GVCF -G AS_StandardAnnotation' command. Although some warning message of 'HaplotypeCaller -ERC GVCF -G AS_StandardAnnotation' command.; ```; WARN:StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; ```; This warning message does not happen in all of my cases, but the FORMAT/AD error happen in all of my cases.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4691#issuecomment-383763740
https://github.com/broadinstitute/gatk/issues/4691#issuecomment-727523636:458,Availability,error,error,458,"> In this case, allele-specific annotations are outputted to GVCFs files by 'HaplotypeCaller -ERC GVCF -G AS_StandardAnnotation' command.; > ; > Although some warning message of 'HaplotypeCaller -ERC GVCF -G AS_StandardAnnotation' command.; > ; > ```; > WARN:StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; > ```; > ; > This warning message does not happen in all of my cases, but the FORMAT/AD error happen in all of my cases. hello , i met the warning message:; `DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null`; `WARN:StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null`. could you help me ? i just am a fresh . So i don't know what should i do next step and why it reported some warning messages. ; Thank you very much !",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4691#issuecomment-727523636
https://github.com/broadinstitute/gatk/issues/4691#issuecomment-727523636:167,Integrability,message,message,167,"> In this case, allele-specific annotations are outputted to GVCFs files by 'HaplotypeCaller -ERC GVCF -G AS_StandardAnnotation' command.; > ; > Although some warning message of 'HaplotypeCaller -ERC GVCF -G AS_StandardAnnotation' command.; > ; > ```; > WARN:StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; > ```; > ; > This warning message does not happen in all of my cases, but the FORMAT/AD error happen in all of my cases. hello , i met the warning message:; `DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null`; `WARN:StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null`. could you help me ? i just am a fresh . So i don't know what should i do next step and why it reported some warning messages. ; Thank you very much !",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4691#issuecomment-727523636
https://github.com/broadinstitute/gatk/issues/4691#issuecomment-727523636:396,Integrability,message,message,396,"> In this case, allele-specific annotations are outputted to GVCFs files by 'HaplotypeCaller -ERC GVCF -G AS_StandardAnnotation' command.; > ; > Although some warning message of 'HaplotypeCaller -ERC GVCF -G AS_StandardAnnotation' command.; > ; > ```; > WARN:StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; > ```; > ; > This warning message does not happen in all of my cases, but the FORMAT/AD error happen in all of my cases. hello , i met the warning message:; `DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null`; `WARN:StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null`. could you help me ? i just am a fresh . So i don't know what should i do next step and why it reported some warning messages. ; Thank you very much !",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4691#issuecomment-727523636
https://github.com/broadinstitute/gatk/issues/4691#issuecomment-727523636:517,Integrability,message,message,517,"> In this case, allele-specific annotations are outputted to GVCFs files by 'HaplotypeCaller -ERC GVCF -G AS_StandardAnnotation' command.; > ; > Although some warning message of 'HaplotypeCaller -ERC GVCF -G AS_StandardAnnotation' command.; > ; > ```; > WARN:StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; > ```; > ; > This warning message does not happen in all of my cases, but the FORMAT/AD error happen in all of my cases. hello , i met the warning message:; `DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null`; `WARN:StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null`. could you help me ? i just am a fresh . So i don't know what should i do next step and why it reported some warning messages. ; Thank you very much !",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4691#issuecomment-727523636
https://github.com/broadinstitute/gatk/issues/4691#issuecomment-727523636:872,Integrability,message,messages,872,"> In this case, allele-specific annotations are outputted to GVCFs files by 'HaplotypeCaller -ERC GVCF -G AS_StandardAnnotation' command.; > ; > Although some warning message of 'HaplotypeCaller -ERC GVCF -G AS_StandardAnnotation' command.; > ; > ```; > WARN:StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; > ```; > ; > This warning message does not happen in all of my cases, but the FORMAT/AD error happen in all of my cases. hello , i met the warning message:; `DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null`; `WARN:StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null`. could you help me ? i just am a fresh . So i don't know what should i do next step and why it reported some warning messages. ; Thank you very much !",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4691#issuecomment-727523636
https://github.com/broadinstitute/gatk/issues/4693#issuecomment-383709501:254,Modifiability,refactor,refactoring,254,This needs to happen for MAF too. . We should add a method `OutputRenderer::sanitizeField` that we can plug into the annotation process that will automatically sanitize each field for illegal characters as they are added to the output. This will require refactoring the `OutputRenderer::write` method to be concrete with a call to another write method and this sanitizeField method to get the benefits automatically for all OutputRenderers.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4693#issuecomment-383709501
https://github.com/broadinstitute/gatk/issues/4693#issuecomment-383709501:76,Security,sanitiz,sanitizeField,76,This needs to happen for MAF too. . We should add a method `OutputRenderer::sanitizeField` that we can plug into the annotation process that will automatically sanitize each field for illegal characters as they are added to the output. This will require refactoring the `OutputRenderer::write` method to be concrete with a call to another write method and this sanitizeField method to get the benefits automatically for all OutputRenderers.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4693#issuecomment-383709501
https://github.com/broadinstitute/gatk/issues/4693#issuecomment-383709501:160,Security,sanitiz,sanitize,160,This needs to happen for MAF too. . We should add a method `OutputRenderer::sanitizeField` that we can plug into the annotation process that will automatically sanitize each field for illegal characters as they are added to the output. This will require refactoring the `OutputRenderer::write` method to be concrete with a call to another write method and this sanitizeField method to get the benefits automatically for all OutputRenderers.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4693#issuecomment-383709501
https://github.com/broadinstitute/gatk/issues/4693#issuecomment-383709501:361,Security,sanitiz,sanitizeField,361,This needs to happen for MAF too. . We should add a method `OutputRenderer::sanitizeField` that we can plug into the annotation process that will automatically sanitize each field for illegal characters as they are added to the output. This will require refactoring the `OutputRenderer::write` method to be concrete with a call to another write method and this sanitizeField method to get the benefits automatically for all OutputRenderers.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4693#issuecomment-383709501
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383712768:233,Deployability,configurat,configuration,233,"Hello @SZLux,. This looks suspiciously like #3050. I suspect this isn't a PathSeq issue, but to be sure can you please try to run another GATK Spark tool such as CountReadsSpark? If that does not work, it's likely an issue with your configuration or Spark/Java versions being incompatible. . What kind of environment are you running in? My suspicion is you are running on a cluster and have the correct Spark/Java version on the driver (master node) but perhaps not on the workers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383712768
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383712768:233,Modifiability,config,configuration,233,"Hello @SZLux,. This looks suspiciously like #3050. I suspect this isn't a PathSeq issue, but to be sure can you please try to run another GATK Spark tool such as CountReadsSpark? If that does not work, it's likely an issue with your configuration or Spark/Java versions being incompatible. . What kind of environment are you running in? My suspicion is you are running on a cluster and have the correct Spark/Java version on the driver (master node) but perhaps not on the workers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383712768
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494:508,Availability,error,error,508,"Hi mwalker174. I tryed with CountReadsSpark, same problem indeed. I do not think it is a java version problem, as without the option --spark-master the software runs smoothly (I guess it uses an included spark and libraries set). So this command runs:; CountReadsSpark --input test_sample.bam --output output.readcount.txt --verbosity DEBUG. While this does not:; CountReadsSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --output output.readcount.txt --verbosity DEBUG ; And I get the error:; ```; 18/04/24 14:34:27 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job; 18/04/24 14:34:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool; 18/04/24 14:34:27 INFO TaskSchedulerImpl: Cancelling stage 0; 18/04/24 14:34:27 INFO DAGScheduler: ResultStage 0 (first at ReadsSparkSource.java:221) failed in 4.532 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494:539,Availability,ERROR,ERROR,539,"Hi mwalker174. I tryed with CountReadsSpark, same problem indeed. I do not think it is a java version problem, as without the option --spark-master the software runs smoothly (I guess it uses an included spark and libraries set). So this command runs:; CountReadsSpark --input test_sample.bam --output output.readcount.txt --verbosity DEBUG. While this does not:; CountReadsSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --output output.readcount.txt --verbosity DEBUG ; And I get the error:; ```; 18/04/24 14:34:27 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job; 18/04/24 14:34:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool; 18/04/24 14:34:27 INFO TaskSchedulerImpl: Cancelling stage 0; 18/04/24 14:34:27 INFO DAGScheduler: ResultStage 0 (first at ReadsSparkSource.java:221) failed in 4.532 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494:917,Availability,failure,failure,917,"Hi mwalker174. I tryed with CountReadsSpark, same problem indeed. I do not think it is a java version problem, as without the option --spark-master the software runs smoothly (I guess it uses an included spark and libraries set). So this command runs:; CountReadsSpark --input test_sample.bam --output output.readcount.txt --verbosity DEBUG. While this does not:; CountReadsSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --output output.readcount.txt --verbosity DEBUG ; And I get the error:; ```; 18/04/24 14:34:27 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job; 18/04/24 14:34:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool; 18/04/24 14:34:27 INFO TaskSchedulerImpl: Cancelling stage 0; 18/04/24 14:34:27 INFO DAGScheduler: ResultStage 0 (first at ReadsSparkSource.java:221) failed in 4.532 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494:974,Availability,failure,failure,974,"Hi mwalker174. I tryed with CountReadsSpark, same problem indeed. I do not think it is a java version problem, as without the option --spark-master the software runs smoothly (I guess it uses an included spark and libraries set). So this command runs:; CountReadsSpark --input test_sample.bam --output output.readcount.txt --verbosity DEBUG. While this does not:; CountReadsSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --output output.readcount.txt --verbosity DEBUG ; And I get the error:; ```; 18/04/24 14:34:27 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job; 18/04/24 14:34:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool; 18/04/24 14:34:27 INFO TaskSchedulerImpl: Cancelling stage 0; 18/04/24 14:34:27 INFO DAGScheduler: ResultStage 0 (first at ReadsSparkSource.java:221) failed in 4.532 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494:2812,Availability,error,error,2812," 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/24 14:34:27 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 4.816635 s; ```; Our system is an HPC, where all the nodes share the same file system. I run my SPARK on only one node to test the software. I red elesewhere that this might be aproblem of missing jars, so I tried to inlcude these libraries in the SPARK jar folder and added the option:; `; --conf [--jars=""~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-common-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-hadoop2-compat-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hive-hbase-handler-1.2.1.spark2.jar"" ]`. But I still get the error. Is GATK using hbase? If yes shall some jars be included to a local SPARK system to enable it to run GATK tools? Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494:1907,Performance,concurren,concurrent,1907,"tage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/24 14:34:27 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 4.816635 s; ```; Our system is an HPC, where all the nodes share the same file system. I run my SPARK on only one node to test the software. I red elesewhere that this might be aproblem of missing jars, so I tried to inlcude these libraries in the SPARK jar folder and added the option:; `; --conf [--jars=""~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-common-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-hadoop2-compat-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hive-hbase-handler-1.2.1.spark2.jar"" ]`. But I still get the error. Is GATK using hbase? If yes shall some jars be included to a local SPARK system to enable it t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494:1991,Performance,concurren,concurrent,1991," 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/24 14:34:27 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 4.816635 s; ```; Our system is an HPC, where all the nodes share the same file system. I run my SPARK on only one node to test the software. I red elesewhere that this might be aproblem of missing jars, so I tried to inlcude these libraries in the SPARK jar folder and added the option:; `; --conf [--jars=""~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-common-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-hadoop2-compat-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hive-hbase-handler-1.2.1.spark2.jar"" ]`. But I still get the error. Is GATK using hbase? If yes shall some jars be included to a local SPARK system to enable it to run GATK tools? Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494:597,Safety,abort,aborting,597,"Hi mwalker174. I tryed with CountReadsSpark, same problem indeed. I do not think it is a java version problem, as without the option --spark-master the software runs smoothly (I guess it uses an included spark and libraries set). So this command runs:; CountReadsSpark --input test_sample.bam --output output.readcount.txt --verbosity DEBUG. While this does not:; CountReadsSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --output output.readcount.txt --verbosity DEBUG ; And I get the error:; ```; 18/04/24 14:34:27 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job; 18/04/24 14:34:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool; 18/04/24 14:34:27 INFO TaskSchedulerImpl: Cancelling stage 0; 18/04/24 14:34:27 INFO DAGScheduler: ResultStage 0 (first at ReadsSparkSource.java:221) failed in 4.532 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494:896,Safety,abort,aborted,896,"Hi mwalker174. I tryed with CountReadsSpark, same problem indeed. I do not think it is a java version problem, as without the option --spark-master the software runs smoothly (I guess it uses an included spark and libraries set). So this command runs:; CountReadsSpark --input test_sample.bam --output output.readcount.txt --verbosity DEBUG. While this does not:; CountReadsSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --output output.readcount.txt --verbosity DEBUG ; And I get the error:; ```; 18/04/24 14:34:27 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job; 18/04/24 14:34:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool; 18/04/24 14:34:27 INFO TaskSchedulerImpl: Cancelling stage 0; 18/04/24 14:34:27 INFO DAGScheduler: ResultStage 0 (first at ReadsSparkSource.java:221) failed in 4.532 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494:2338,Testability,test,test,2338," 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, xx.xx.xx.xx, executor 0): java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 18/04/24 14:34:27 INFO DAGScheduler: Job 0 failed: first at ReadsSparkSource.java:221, took 4.816635 s; ```; Our system is an HPC, where all the nodes share the same file system. I run my SPARK on only one node to test the software. I red elesewhere that this might be aproblem of missing jars, so I tried to inlcude these libraries in the SPARK jar folder and added the option:; `; --conf [--jars=""~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-client-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-common-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hbase-hadoop2-compat-1.4.3.jar, ~/bin/spark-2.2.0-bin-hadoop2.7/jars/hive-hbase-handler-1.2.1.spark2.jar"" ]`. But I still get the error. Is GATK using hbase? If yes shall some jars be included to a local SPARK system to enable it to run GATK tools? Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383916494
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383973549:271,Deployability,Update,Update,271,"Hi @SZLux,. GATK does not use hbase, so I'm not sure those JARs will help. Can you please try the following command:. CountReadsSpark --input test_sample.bam --output output.readcount.txt --verbosity DEBUG -- --spark-runner SPARK --spark-master spark://xx.xx.xx.xx:7077. Update: To launch a Spark tool on a remote machine, your command must end with the arguments:. -- --spark-runner SPARK --spark-master spark://xx.xx.xx.xx. This will attempt to initialize Spark in cluster mode. Currently there is no way to run a job in local mode on a remote machine using the gatk launcher. To do that, you could ssh into the remote machine and launch it locally i.e. with:. -- --spark-runner LOCAL --spark-master local[*]",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383973549
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:121,Availability,error,error,121,"Hi mwalker174,; I tried both command lines. As lbergelson predicted, the one with --spark-runner LOCAL produces the same error as before (see below, could you explain me why?), while the one with --spark-runner SPARK runs smoothly. . Is this option ok with running a master-workers system? Can I use this option safely with . I have now a different issue with PathSeqPipelineSpark? As I tried, the first error solved, but I have another issue (I think it's better to open a new thred for that, since it is about an input file not found). Thank you! . ```; -bash-4.1$ ../../../gatk PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt --spark-runner SPARK; Using GATK jar /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar; Running:; /home/int/eva/username/bin/spark-2.2.0-bin-hadoop2.7//bin/spark-submit --master spark://xx.xx.xx.xx:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:404,Availability,error,error,404,"Hi mwalker174,; I tried both command lines. As lbergelson predicted, the one with --spark-runner LOCAL produces the same error as before (see below, could you explain me why?), while the one with --spark-runner SPARK runs smoothly. . Is this option ok with running a master-workers system? Can I use this option safely with . I have now a different issue with PathSeqPipelineSpark? As I tried, the first error solved, but I have another issue (I think it's better to open a new thred for that, since it is about an input file not found). Thank you! . ```; -bash-4.1$ ../../../gatk PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt --spark-runner SPARK; Using GATK jar /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar; Running:; /home/int/eva/username/bin/spark-2.2.0-bin-hadoop2.7//bin/spark-submit --master spark://xx.xx.xx.xx:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:24479,Availability,Error,Error,24479,"end$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (xx.xx.xx.24:44439) with ID 4; 18/04/24 17:40:27 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.24:49115 with 366.3 MB RAM, BlockManagerId(4, xx.xx.xx.24, 49115, None); 18/04/24 17:40:50 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.25:39218 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:40:50 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.23:55590 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:40:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.25:39218 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:40:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.23:55590 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:40:52 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 4, xx.xx.xx.23, executor 5): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeS",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:26628,Availability,Error,Error,26628,"ask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 18/04/24 17:40:52 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 01:33 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:40:52 INFO TaskSetManager: Starting task 0.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 4, partition 0, PROCESS_LOCAL, 6010 bytes); 01:33 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:40:52 INFO TaskSetManager: Starting task 1.1 in stage 2.0 (TID 6, xx.xx.xx.24, executor 1, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:49115 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:41:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.24:49115 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:41:31 WARN TaskSetManager: Lost task 0.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 4): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:27666,Availability,Error,Error,27666,"i.hss (No such file or directory)) [duplicate 1]; 01:33 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:40:52 INFO TaskSetManager: Starting task 0.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 4, partition 0, PROCESS_LOCAL, 6010 bytes); 01:33 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:40:52 INFO TaskSetManager: Starting task 1.1 in stage 2.0 (TID 6, xx.xx.xx.24, executor 1, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:49115 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:41:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.24:49115 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:41:31 WARN TaskSetManager: Lost task 0.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 4): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeS",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:30365,Availability,Error,Error,30365,"utStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:12 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:31 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.24, executor 1, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:44322 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:41:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.24:44322 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:41:53 WARN TaskSetManager: Lost task 0.2 in stage 2.0 (TID 7, xx.xx.xx.24, executor 1): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeS",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:32807,Availability,Error,Error,32807,"r$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:34 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:53 INFO TaskSetManager: Lost task 1.1 in stage 2.0 (TID 6) on xx.xx.xx.24, executor 1: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 02:34 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.25, executor 6, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.xx:45142 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:42:02 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 8) on xx.xx.xx.xx, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 18/04/24 17:42:02 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:42:02 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.j",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:33424,Availability,Error,Error,33424,"_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:53 INFO TaskSetManager: Lost task 1.1 in stage 2.0 (TID 6) on xx.xx.xx.24, executor 1: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 02:34 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.25, executor 6, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.xx:45142 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:42:02 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 8) on xx.xx.xx.xx, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 18/04/24 17:42:02 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:42:02 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 117.782 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.Contai",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:33539,Availability,ERROR,ERROR,33539,"ge 2.0 (TID 8, xx.xx.xx.xx, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:53 INFO TaskSetManager: Lost task 1.1 in stage 2.0 (TID 6) on xx.xx.xx.24, executor 1: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 02:34 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.25, executor 6, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.xx:45142 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:42:02 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 8) on xx.xx.xx.xx, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 18/04/24 17:42:02 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:42:02 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 117.782 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.to",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:33878,Availability,failure,failure,33878,"e or directory)) [duplicate 1]; 02:34 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.25, executor 6, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.xx:45142 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:42:02 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 8) on xx.xx.xx.xx, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 18/04/24 17:42:02 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:42:02 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 117.782 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:33935,Availability,failure,failure,33935," WrappedArray(null); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.25, executor 6, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.xx:45142 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:42:02 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 8) on xx.xx.xx.xx, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 18/04/24 17:42:02 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:42:02 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 117.782 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$an",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:34103,Availability,Error,Error,34103," INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.xx:45142 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:42:02 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 8) on xx.xx.xx.xx, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 18/04/24 17:42:02 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:42:02 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 117.782 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeS",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:36323,Availability,down,down,36323,"Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:42:02 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 117.869179 s; 18/04/24 17:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/24 17:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:42:02 ERROR TransportRequestHandler: Error sending result StreamResponse{streamId=/jars/gatk-package-4.0.3.0-spark.jar, byteCount=138618122, body=FileSegmentManagedBuffer{file=/scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar, offset=0, length=138618122}} to /xx.xx.xx.25:57139; closing connection; java.io.IOException: Connection reset by peer; at sun.nio.ch.FileChannelImpl.transferTo0(Native Method); at sun.nio.ch.FileChannelImpl.transferToDirectlyInternal(FileChannelImpl.java:428); at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:493); at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:608); at io.netty.channel.DefaultFileRegion.transferTo(DefaultFileRegion.java:139); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:121); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:287); at io.netty.channel.n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:36441,Availability,down,down,36441,"Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:42:02 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 117.869179 s; 18/04/24 17:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/24 17:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:42:02 ERROR TransportRequestHandler: Error sending result StreamResponse{streamId=/jars/gatk-package-4.0.3.0-spark.jar, byteCount=138618122, body=FileSegmentManagedBuffer{file=/scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar, offset=0, length=138618122}} to /xx.xx.xx.25:57139; closing connection; java.io.IOException: Connection reset by peer; at sun.nio.ch.FileChannelImpl.transferTo0(Native Method); at sun.nio.ch.FileChannelImpl.transferToDirectlyInternal(FileChannelImpl.java:428); at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:493); at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:608); at io.netty.channel.DefaultFileRegion.transferTo(DefaultFileRegion.java:139); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:121); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:287); at io.netty.channel.n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:36465,Availability,ERROR,ERROR,36465,"Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:42:02 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 117.869179 s; 18/04/24 17:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/24 17:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:42:02 ERROR TransportRequestHandler: Error sending result StreamResponse{streamId=/jars/gatk-package-4.0.3.0-spark.jar, byteCount=138618122, body=FileSegmentManagedBuffer{file=/scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar, offset=0, length=138618122}} to /xx.xx.xx.25:57139; closing connection; java.io.IOException: Connection reset by peer; at sun.nio.ch.FileChannelImpl.transferTo0(Native Method); at sun.nio.ch.FileChannelImpl.transferToDirectlyInternal(FileChannelImpl.java:428); at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:493); at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:608); at io.netty.channel.DefaultFileRegion.transferTo(DefaultFileRegion.java:139); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:121); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:287); at io.netty.channel.n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:36496,Availability,Error,Error,36496,"Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:42:02 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 117.869179 s; 18/04/24 17:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/24 17:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:42:02 ERROR TransportRequestHandler: Error sending result StreamResponse{streamId=/jars/gatk-package-4.0.3.0-spark.jar, byteCount=138618122, body=FileSegmentManagedBuffer{file=/scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar, offset=0, length=138618122}} to /xx.xx.xx.25:57139; closing connection; java.io.IOException: Connection reset by peer; at sun.nio.ch.FileChannelImpl.transferTo0(Native Method); at sun.nio.ch.FileChannelImpl.transferToDirectlyInternal(FileChannelImpl.java:428); at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:493); at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:608); at io.netty.channel.DefaultFileRegion.transferTo(DefaultFileRegion.java:139); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:121); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:287); at io.netty.channel.n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:38831,Availability,down,down,38831,"tty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:748); 18/04/24 17:42:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:42:11 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:42:11 INFO BlockManager: BlockManager stopped; 18/04/24 17:42:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:42:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:42:11 INFO SparkContext: Successfully stopped SparkContext; 17:42:11.053 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:42:11 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=866648064; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hel",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:39074,Availability,failure,failure,39074,"o.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:748); 18/04/24 17:42:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:42:11 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:42:11 INFO BlockManager: BlockManager stopped; 18/04/24 17:42:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:42:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:42:11 INFO SparkContext: Successfully stopped SparkContext; 17:42:11.053 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:42:11 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=866648064; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:39131,Availability,failure,failure,39131,"gleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:748); 18/04/24 17:42:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:42:11 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:42:11 INFO BlockManager: BlockManager stopped; 18/04/24 17:42:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:42:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:42:11 INFO SparkContext: Successfully stopped SparkContext; 17:42:11.053 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:42:11 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=866648064; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$an",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:39299,Availability,Error,Error,39299,"8); 18/04/24 17:42:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:42:11 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:42:11 INFO BlockManager: BlockManager stopped; 18/04/24 17:42:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:42:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:42:11 INFO SparkContext: Successfully stopped SparkContext; 17:42:11.053 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:42:11 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=866648064; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeS",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:44700,Availability,Error,Error,44700,98); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeS,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5082,Deployability,Configurat,Configuration,5082,"EATE_MD5 : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - g",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:6631,Deployability,patch,patch,6631,"iver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:39:19 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls di",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:12349,Deployability,update,updated,12349,"23:49023 with 16 cores, 1024.0 MB RAM; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20180424173921-0001/6 on worker-20180424173107-xx.xx.xx.25-33478 (xx.xx.xx.25:33478) with 16 cores; 18/04/24 17:39:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20180424173921-0001/6 on hostPort xx.xx.xx.25:33478 with 16 cores, 1024.0 MB RAM; 18/04/24 17:39:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 18/04/24 17:39:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.xx:42081 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/0 is now RUNNING; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/1 is now RUNNING; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/2 is now RUNNING; 18/04/24 17:39:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/6 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/4 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/3 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/5 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:12468,Deployability,update,updated,12468,"23:49023 with 16 cores, 1024.0 MB RAM; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20180424173921-0001/6 on worker-20180424173107-xx.xx.xx.25-33478 (xx.xx.xx.25:33478) with 16 cores; 18/04/24 17:39:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20180424173921-0001/6 on hostPort xx.xx.xx.25:33478 with 16 cores, 1024.0 MB RAM; 18/04/24 17:39:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 18/04/24 17:39:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.xx:42081 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/0 is now RUNNING; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/1 is now RUNNING; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/2 is now RUNNING; 18/04/24 17:39:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/6 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/4 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/3 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/5 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:12587,Deployability,update,updated,12587,"23:49023 with 16 cores, 1024.0 MB RAM; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20180424173921-0001/6 on worker-20180424173107-xx.xx.xx.25-33478 (xx.xx.xx.25:33478) with 16 cores; 18/04/24 17:39:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20180424173921-0001/6 on hostPort xx.xx.xx.25:33478 with 16 cores, 1024.0 MB RAM; 18/04/24 17:39:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 18/04/24 17:39:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.xx:42081 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/0 is now RUNNING; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/1 is now RUNNING; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/2 is now RUNNING; 18/04/24 17:39:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/6 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/4 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/3 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/5 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:12936,Deployability,update,updated,12936,"gistering block manager xx.xx.xx.xx:42081 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/0 is now RUNNING; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/1 is now RUNNING; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/2 is now RUNNING; 18/04/24 17:39:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/6 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/4 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/3 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/5 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:39:22 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:39:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 276.0 KB, free 366.0 MB); 00:08 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/24 17:39:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.1 KB, free 366.0 MB); 18/04/24 17:39:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.xx:42081 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:39:27 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/24 17:39:27 INFO FileInputFormat: Total input p",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:13055,Deployability,update,updated,13055,"gistering block manager xx.xx.xx.xx:42081 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/0 is now RUNNING; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/1 is now RUNNING; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/2 is now RUNNING; 18/04/24 17:39:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/6 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/4 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/3 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/5 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:39:22 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:39:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 276.0 KB, free 366.0 MB); 00:08 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/24 17:39:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.1 KB, free 366.0 MB); 18/04/24 17:39:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.xx:42081 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:39:27 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/24 17:39:27 INFO FileInputFormat: Total input p",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:13174,Deployability,update,updated,13174,"gistering block manager xx.xx.xx.xx:42081 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/0 is now RUNNING; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/1 is now RUNNING; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/2 is now RUNNING; 18/04/24 17:39:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/6 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/4 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/3 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/5 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:39:22 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:39:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 276.0 KB, free 366.0 MB); 00:08 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/24 17:39:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.1 KB, free 366.0 MB); 18/04/24 17:39:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.xx:42081 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:39:27 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/24 17:39:27 INFO FileInputFormat: Total input p",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:13293,Deployability,update,updated,13293,"gistering block manager xx.xx.xx.xx:42081 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/0 is now RUNNING; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/1 is now RUNNING; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/2 is now RUNNING; 18/04/24 17:39:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/6 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/4 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/3 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/5 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:39:22 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:39:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 276.0 KB, free 366.0 MB); 00:08 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/24 17:39:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.1 KB, free 366.0 MB); 18/04/24 17:39:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.xx:42081 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:39:27 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/24 17:39:27 INFO FileInputFormat: Total input p",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:44222,Deployability,deploy,deploy,44222,ine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadin,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:44259,Deployability,deploy,deploy,44259,Tool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.Contai,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:44331,Deployability,deploy,deploy,44331,arkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15);,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:44407,Deployability,deploy,deploy,44407,nstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:44478,Deployability,deploy,deploy,44478,am.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaR,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:44547,Deployability,deploy,deploy,44547,gram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:13395,Energy Efficiency,Schedul,SchedulerBackend,13395,"gistering block manager xx.xx.xx.xx:42081 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/0 is now RUNNING; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/1 is now RUNNING; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/2 is now RUNNING; 18/04/24 17:39:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/6 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/4 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/3 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/5 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:39:22 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:39:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 276.0 KB, free 366.0 MB); 00:08 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/24 17:39:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.1 KB, free 366.0 MB); 18/04/24 17:39:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.xx:42081 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:39:27 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/24 17:39:27 INFO FileInputFormat: Total input p",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:13425,Energy Efficiency,schedul,scheduling,13425,"gistering block manager xx.xx.xx.xx:42081 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/0 is now RUNNING; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/1 is now RUNNING; 18/04/24 17:39:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/2 is now RUNNING; 18/04/24 17:39:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.xx, 42081, None); 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/6 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/4 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/3 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424173921-0001/5 is now RUNNING; 18/04/24 17:39:22 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:39:22 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:39:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 276.0 KB, free 366.0 MB); 00:08 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/24 17:39:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.1 KB, free 366.0 MB); 18/04/24 17:39:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.xx:42081 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:39:27 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/24 17:39:27 INFO FileInputFormat: Total input p",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:25537,Energy Efficiency,schedul,scheduler,25537,"ile or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 18/04/24 17:40:52 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadins",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:25616,Energy Efficiency,schedul,scheduler,25616,"ile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 18/04/24 17:40:52 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:25695,Energy Efficiency,schedul,scheduler,25695,".PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 18/04/24 17:40:52 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or di",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:28724,Energy Efficiency,schedul,scheduler,28724,"ile or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:12 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:28803,Energy Efficiency,schedul,scheduler,28803,"ile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:12 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:31 INFO TaskSetManager: Start",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:28882,Energy Efficiency,schedul,scheduler,28882,".PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:12 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:31 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.24, executor 1, partition 0, PROCESS",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:31423,Energy Efficiency,schedul,scheduler,31423,"ile or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:34 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:31502,Energy Efficiency,schedul,scheduler,31502,"ile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:34 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:53 INFO TaskSetManager: Start",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:31581,Energy Efficiency,schedul,scheduler,31581,".PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:34 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3, partition 0, PROCESS",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:35161,Energy Efficiency,schedul,scheduler,35161,"ile or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:42:02 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, too",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:35240,Energy Efficiency,schedul,scheduler,35240,"ile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:42:02 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 117.869179 s; 18/04/24 17:42:02 INFO SparkUI: Stopped Spark web UI at http://",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:35319,Energy Efficiency,schedul,scheduler,35319,".PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:42:02 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 117.869179 s; 18/04/24 17:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/24 17:42:02 INFO StandaloneSchedulerBackend: Shutting d",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:40357,Energy Efficiency,schedul,scheduler,40357,ile or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAnd,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:40436,Energy Efficiency,schedul,scheduler,40436,ile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGSc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:40515,Energy Efficiency,schedul,scheduler,40515,.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41288,Energy Efficiency,schedul,scheduler,41288,ite(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41328,Energy Efficiency,schedul,scheduler,41328,park.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41426,Energy Efficiency,schedul,scheduler,41426,park.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41523,Energy Efficiency,schedul,scheduler,41523,k.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41774,Energy Efficiency,schedul,scheduler,41774,or.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41854,Energy Efficiency,schedul,scheduler,41854,otFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41959,Energy Efficiency,schedul,scheduler,41959,d); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD.count(RDD.scala:1158); at org.apache.spark.api.java.JavaRDDLike$cla,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:42107,Energy Efficiency,schedul,scheduler,42107,am.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD.count(RDD.scala:1158); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tool,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:42195,Energy Efficiency,schedul,scheduler,42195,ls.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD.count(RDD.scala:1158); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); at org.broa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:42292,Energy Efficiency,schedul,scheduler,42292,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD.count(RDD.scala:1158); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broa,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:42387,Energy Efficiency,schedul,scheduler,42387,.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD.count(RDD.scala:1158); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:42550,Energy Efficiency,schedul,scheduler,42550,abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD.count(RDD.scala:1158); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:45758,Energy Efficiency,schedul,scheduler,45758,ile or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more; 18/04/24 17:42:11 INFO ShutdownHookManager: Shutdown hook called; 18/04/24 17:42:11 INFO ShutdownHookManager: Dele,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:45837,Energy Efficiency,schedul,scheduler,45837,enFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more; 18/04/24 17:42:11 INFO ShutdownHookManager: Shutdown hook called; 18/04/24 17:42:11 INFO ShutdownHookManager: Deleting directory /tmp/username/spark-99d4cb79-5c44-425b-8f72-9476e7fd884c; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:45916,Energy Efficiency,schedul,scheduler,45916,enFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more; 18/04/24 17:42:11 INFO ShutdownHookManager: Shutdown hook called; 18/04/24 17:42:11 INFO ShutdownHookManager: Deleting directory /tmp/username/spark-99d4cb79-5c44-425b-8f72-9476e7fd884c; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:22658,Integrability,Wrap,WrappedArray,22658,"er: Missing parents: List(ShuffleMapStage 6); 18/04/24 17:40:04 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[19] at mapToPair at PSFilter.java:125), which has no missing parents; 18/04/24 17:40:04 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 14.2 KB, free 366.0 MB); 00:45 DEBUG: [kryo] Write: byte[]; 18/04/24 17:40:04 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.4 KB, free 366.0 MB); 18/04/24 17:40:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.xx:42081 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:40:04 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006; 18/04/24 17:40:04 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[19] at mapToPair at PSFilter.java:125) (first 15 tasks are for partitions Vector(0, 1)); 18/04/24 17:40:04 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks; 00:45 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:40:04 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, xx.xx.xx.25, executor 2, partition 0, PROCESS_LOCAL, 6010 bytes); 00:45 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:40:04 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4, xx.xx.xx.23, executor 5, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:40:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (xx.xx.xx.24:44437) with ID 1; 18/04/24 17:40:21 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.24:44322 with 366.3 MB RAM, BlockManagerId(1, xx.xx.xx.24, 44322, None); 18/04/24 17:40:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (xx.xx.xx.24:44439) with ID 4; 18/04/24 17:40:27 INFO BlockManagerMasterEndp",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:22951,Integrability,Wrap,WrappedArray,22951,"4.2 KB, free 366.0 MB); 00:45 DEBUG: [kryo] Write: byte[]; 18/04/24 17:40:04 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.4 KB, free 366.0 MB); 18/04/24 17:40:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.xx:42081 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:40:04 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006; 18/04/24 17:40:04 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[19] at mapToPair at PSFilter.java:125) (first 15 tasks are for partitions Vector(0, 1)); 18/04/24 17:40:04 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks; 00:45 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:40:04 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, xx.xx.xx.25, executor 2, partition 0, PROCESS_LOCAL, 6010 bytes); 00:45 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:40:04 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4, xx.xx.xx.23, executor 5, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:40:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (xx.xx.xx.24:44437) with ID 1; 18/04/24 17:40:21 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.24:44322 with 366.3 MB RAM, BlockManagerId(1, xx.xx.xx.24, 44322, None); 18/04/24 17:40:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (xx.xx.xx.24:44439) with ID 4; 18/04/24 17:40:27 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.24:49115 with 366.3 MB RAM, BlockManagerId(4, xx.xx.xx.24, 49115, None); 18/04/24 17:40:50 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.25:39218 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:40:50 INFO BlockManagerInfo: Added broadcas",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:26752,Integrability,Wrap,WrappedArray,26752,"08); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 18/04/24 17:40:52 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 01:33 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:40:52 INFO TaskSetManager: Starting task 0.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 4, partition 0, PROCESS_LOCAL, 6010 bytes); 01:33 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:40:52 INFO TaskSetManager: Starting task 1.1 in stage 2.0 (TID 6, xx.xx.xx.24, executor 1, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:49115 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:41:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.24:49115 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:41:31 WARN TaskSetManager: Lost task 0.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 4): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:27045,Integrability,Wrap,WrappedArray,27045,"tFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 18/04/24 17:40:52 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 01:33 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:40:52 INFO TaskSetManager: Starting task 0.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 4, partition 0, PROCESS_LOCAL, 6010 bytes); 01:33 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:40:52 INFO TaskSetManager: Starting task 1.1 in stage 2.0 (TID 6, xx.xx.xx.24, executor 1, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:49115 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:41:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.24:49115 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:41:31 WARN TaskSetManager: Lost task 0.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 4): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilt",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:29642,Integrability,Wrap,WrappedArray,29642,"SortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:12 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:31 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.24, executor 1, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:44322 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:41:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.24:44322 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:41:53 WARN TaskSetManager: Lost task 0.2 in stage 2.0 (TID 7, xx.xx.xx.24, executor 1): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:32341,Integrability,Wrap,WrappedArray,32341,"SortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:34 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:53 INFO TaskSetManager: Lost task 1.1 in stage 2.0 (TID 6) on xx.xx.xx.24, executor 1: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 02:34 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.25, executor 6, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.xx:45142 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:42:02 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 8) on xx.xx.xx.xx, executor 3: org.broadinst",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:32931,Integrability,Wrap,WrappedArray,32931,"dException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:34 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:53 INFO TaskSetManager: Lost task 1.1 in stage 2.0 (TID 6) on xx.xx.xx.24, executor 1: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 02:34 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.25, executor 6, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.xx:45142 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:42:02 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 8) on xx.xx.xx.xx, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 18/04/24 17:42:02 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:42:02 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 117.782 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most rec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:37226,Integrability,protocol,protocol,37226,"b UI at http://xx.xx.xx.xx:4040; 18/04/24 17:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:42:02 ERROR TransportRequestHandler: Error sending result StreamResponse{streamId=/jars/gatk-package-4.0.3.0-spark.jar, byteCount=138618122, body=FileSegmentManagedBuffer{file=/scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar, offset=0, length=138618122}} to /xx.xx.xx.25:57139; closing connection; java.io.IOException: Connection reset by peer; at sun.nio.ch.FileChannelImpl.transferTo0(Native Method); at sun.nio.ch.FileChannelImpl.transferToDirectlyInternal(FileChannelImpl.java:428); at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:493); at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:608); at io.netty.channel.DefaultFileRegion.transferTo(DefaultFileRegion.java:139); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:121); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:287); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:314); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:802); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.forceFlush(AbstractNioChannel.java:319); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:637); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorat",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:37235,Integrability,Message,MessageWithHeader,37235,"/xx.xx.xx.xx:4040; 18/04/24 17:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:42:02 ERROR TransportRequestHandler: Error sending result StreamResponse{streamId=/jars/gatk-package-4.0.3.0-spark.jar, byteCount=138618122, body=FileSegmentManagedBuffer{file=/scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar, offset=0, length=138618122}} to /xx.xx.xx.25:57139; closing connection; java.io.IOException: Connection reset by peer; at sun.nio.ch.FileChannelImpl.transferTo0(Native Method); at sun.nio.ch.FileChannelImpl.transferToDirectlyInternal(FileChannelImpl.java:428); at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:493); at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:608); at io.netty.channel.DefaultFileRegion.transferTo(DefaultFileRegion.java:139); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:121); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:287); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:314); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:802); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.forceFlush(AbstractNioChannel.java:319); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:637); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(Defaul",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:37264,Integrability,Message,MessageWithHeader,37264,"4/24 17:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:42:02 ERROR TransportRequestHandler: Error sending result StreamResponse{streamId=/jars/gatk-package-4.0.3.0-spark.jar, byteCount=138618122, body=FileSegmentManagedBuffer{file=/scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar, offset=0, length=138618122}} to /xx.xx.xx.25:57139; closing connection; java.io.IOException: Connection reset by peer; at sun.nio.ch.FileChannelImpl.transferTo0(Native Method); at sun.nio.ch.FileChannelImpl.transferToDirectlyInternal(FileChannelImpl.java:428); at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:493); at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:608); at io.netty.channel.DefaultFileRegion.transferTo(DefaultFileRegion.java:139); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:121); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:287); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:314); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:802); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.forceFlush(AbstractNioChannel.java:319); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:637); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:2348,Modifiability,variab,variables,2348,tools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; 17:39:18.382 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 17:39:18.825 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 17:39:18.857 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/username/libgkl_compression3681606702485397808.so; 17:39:19.218 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 17:39:19.218 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 17:39:19.218 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:39:19.219 INFO PathSeqPipelineSpark - Executing as username@node016 on Linux v2.6.32-220.4.1.el6.x86_64 amd64; 17:39:19.220 INFO PathSeqPipelineSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11; 17:39:19.220 INFO PathSeqPipelineSpark - ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:2474,Modifiability,config,configured,2474,tools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; 17:39:18.382 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 17:39:18.825 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 17:39:18.857 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/username/libgkl_compression3681606702485397808.so; 17:39:19.218 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 17:39:19.218 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 17:39:19.218 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:39:19.219 INFO PathSeqPipelineSpark - Executing as username@node016 on Linux v2.6.32-220.4.1.el6.x86_64 amd64; 17:39:19.220 INFO PathSeqPipelineSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11; 17:39:19.220 INFO PathSeqPipelineSpark - ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5066,Modifiability,Config,ConfigFactory,5066,"EATE_MD5 : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - g",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5082,Modifiability,Config,Configuration,5082,"EATE_MD5 : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - g",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5129,Modifiability,Config,ConfigFactory,5129,"HTSJDK Defaults.CUSTOM_READER_FACTORY :; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBU",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5184,Modifiability,Config,ConfigFactory,5184,"39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_s",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5249,Modifiability,Config,ConfigFactory,5249,"PPY_COMPRESSOR : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5324,Modifiability,Config,ConfigFactory,5324,"aults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.us",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5391,Modifiability,Config,ConfigFactory,5391,"am/md5/%s; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipeline",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5466,Modifiability,Config,ConfigFactory,5466,"O_BUFFER_SIZE : 131072; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - I",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5535,Modifiability,Config,ConfigFactory,5535,"DK Defaults.REFERENCE_FASTA : null; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5612,Modifiability,Config,ConfigFactory,5612,"HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using googl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5680,Modifiability,Config,ConfigFactory,5680,"athSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from htt",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5750,Modifiability,Config,ConfigFactory,5750,"7:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 IN",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5880,Modifiability,Config,ConfigFactory,5880," - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:39:19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:5941,Modifiability,Config,ConfigFactory,5941,":19.226 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:6007,Modifiability,Config,ConfigFactory,6007,"DOWNLOAD : false; 17:39:19.226 DEBUG ConfigFactory - Configuration file values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:6071,Modifiability,Config,ConfigFactory,6071," values:; 17:39:19.244 DEBUG ConfigFactory - gcsMaxRetries = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop lib",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:6149,Modifiability,Config,ConfigFactory,6149,"ies = 20; 17:39:19.244 DEBUG ConfigFactory - samjdk.compression_level = 2; 17:39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes whe",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:6227,Modifiability,Config,ConfigFactory,6227,"39:19.245 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 17:39:19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:39:19 INFO SparkContext: Submitted application: Pat",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:6305,Modifiability,Config,ConfigFactory,6305,"19.245 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 17:39:19.245 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 17:39:19.245 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 17:39:19.245 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 17:39:19.245 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:39:19.245 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:39:19.245 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:39:19.245 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:39:19.245 DEBUG ConfigFactory - createOutputBamIndex = true; 17:39:19.245 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:39:19 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:2535,Performance,Load,Loading,2535,"onf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt; 17:39:18.382 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 17:39:18.825 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 17:39:18.857 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/username/libgkl_compression3681606702485397808.so; 17:39:19.218 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 17:39:19.218 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.3.0; 17:39:19.218 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:39:19.219 INFO PathSeqPipelineSpark - Executing as username@node016 on Linux v2.6.32-220.4.1.el6.x86_64 amd64; 17:39:19.220 INFO PathSeqPipelineSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_131-b11; 17:39:19.220 INFO PathSeqPipelineSpark - Start Date/Time: April 24, 2018 5:39:18 PM CEST; 17:39:19.220 INFO PathSeqPipelineSpark - --------------------------------------",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:7078,Performance,load,load,7078,nfigFactory - gatk_stacktrace_on_user_exception = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:39:19.245 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:39:19.245 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:39:19.246 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:39:19.246 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:39:19 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(username); groups with view permissions: Set(); users with modify permissions: Set(username); groups with modify permissions: Set(); 18/04/24 17:39:20 INFO Utils: Successfully started service 'sparkDriver' on port 46576.; 18/04/24 17:39:20 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:39:20 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:39:20 INFO BlockManagerMasterEndpo,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:25818,Performance,concurren,concurrent,25818,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 18/04/24 17:40:52 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 01:33 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_00",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:25902,Performance,concurren,concurrent,25902,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 18/04/24 17:40:52 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx.xx.xx.25, executor 2: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 01:33 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:40:52 INFO TaskSetMana",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:29005,Performance,concurren,concurrent,29005,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:12 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:31 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.24, executor 1, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:44322 (size:",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:29089,Performance,concurren,concurrent,29089,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:12 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:31 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 7, xx.xx.xx.24, executor 1, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.24:44322 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:41:49 INFO BlockManagerInfo: Added broadcast_0",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:31704,Performance,concurren,concurrent,31704,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:34 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:53 INFO TaskSetManager: Lost task 1.1 in stage 2.0 (TID 6) on xx.xx.xx.24, executor 1: o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:31788,Performance,concurren,concurrent,31788,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 02:34 DEBUG: [kryo] Write: WrappedArray([NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED, NC_000913.3_127443_127875_0:0:0_0:0:0_a507 UNMAPPED]); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3, partition 0, PROCESS_LOCAL, 6010 bytes); 18/04/24 17:41:53 INFO TaskSetManager: Lost task 1.1 in stage 2.0 (TID 6) on xx.xx.xx.24, executor 1: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn'",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:35442,Performance,concurren,concurrent,35442,"ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:42:02 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 117.869179 s; 18/04/24 17:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/24 17:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:35526,Performance,concurren,concurrent,35526,"park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:42:02 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 117.869179 s; 18/04/24 17:42:02 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.xx:4040; 18/04/24 17:42:02 INFO StandaloneSchedulerBackend: Shutting down all executors; 18/04/24 17:42:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down; 18/04/24 17:42:02 ERROR TransportRequestHandler: Error sending result StreamResponse{",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:38080,Performance,concurren,concurrent,38080,"nsferTo(FileChannelImpl.java:608); at io.netty.channel.DefaultFileRegion.transferTo(DefaultFileRegion.java:139); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:121); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:287); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:314); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:802); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.forceFlush(AbstractNioChannel.java:319); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:637); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:748); 18/04/24 17:42:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:42:11 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:42:11 INFO BlockManager: BlockManager stopped; 18/04/24 17:42:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:42:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:42:11 INFO SparkContext: Successfully stopped SparkContext; 17:42:11.053 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:42:11 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=866648064; org.apache.spark.SparkException: Job aborted due to stage failure: Tas",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:38177,Performance,concurren,concurrent,38177,"gion.java:139); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:121); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:287); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:314); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:802); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.forceFlush(AbstractNioChannel.java:319); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:637); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:748); 18/04/24 17:42:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:42:11 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:42:11 INFO BlockManager: BlockManager stopped; 18/04/24 17:42:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:42:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:42:11 INFO SparkContext: Successfully stopped SparkContext; 17:42:11.053 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:42:11 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=866648064; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:40638,Performance,concurren,concurrent,40638,ter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$c,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:40722,Performance,concurren,concurrent,40722,park.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.forea,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:46039,Performance,concurren,concurrent,46039,enFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more; 18/04/24 17:42:11 INFO ShutdownHookManager: Shutdown hook called; 18/04/24 17:42:11 INFO ShutdownHookManager: Deleting directory /tmp/username/spark-99d4cb79-5c44-425b-8f72-9476e7fd884c; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:46123,Performance,concurren,concurrent,46123,enFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more; 18/04/24 17:42:11 INFO ShutdownHookManager: Shutdown hook called; 18/04/24 17:42:11 INFO ShutdownHookManager: Deleting directory /tmp/username/spark-99d4cb79-5c44-425b-8f72-9476e7fd884c; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:58,Safety,predict,predicted,58,"Hi mwalker174,; I tried both command lines. As lbergelson predicted, the one with --spark-runner LOCAL produces the same error as before (see below, could you explain me why?), while the one with --spark-runner SPARK runs smoothly. . Is this option ok with running a master-workers system? Can I use this option safely with . I have now a different issue with PathSeqPipelineSpark? As I tried, the first error solved, but I have another issue (I think it's better to open a new thred for that, since it is about an input file not found). Thank you! . ```; -bash-4.1$ ../../../gatk PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt --spark-runner SPARK; Using GATK jar /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar; Running:; /home/int/eva/username/bin/spark-2.2.0-bin-hadoop2.7//bin/spark-submit --master spark://xx.xx.xx.xx:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:312,Safety,safe,safely,312,"Hi mwalker174,; I tried both command lines. As lbergelson predicted, the one with --spark-runner LOCAL produces the same error as before (see below, could you explain me why?), while the one with --spark-runner SPARK runs smoothly. . Is this option ok with running a master-workers system? Can I use this option safely with . I have now a different issue with PathSeqPipelineSpark? As I tried, the first error solved, but I have another issue (I think it's better to open a new thred for that, since it is about an input file not found). Thank you! . ```; -bash-4.1$ ../../../gatk PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --verbosity DEBUG --scores-output output.pathseq.txt --spark-runner SPARK; Using GATK jar /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar; Running:; /home/int/eva/username/bin/spark-2.2.0-bin-hadoop2.7//bin/spark-submit --master spark://xx.xx.xx.xx:7077 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /scratch/home/int/eva/username/bin/gatk-4.0.3.0/gatk-package-4.0.3.0-spark.jar PathSeqPipelineSpark --spark-master spark://xx.xx.xx.xx:7077 --input",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:33597,Safety,abort,aborting,33597,"executor 1: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 1]; 02:34 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.25, executor 6, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.xx:45142 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:42:02 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 8) on xx.xx.xx.xx, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 18/04/24 17:42:02 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:42:02 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 117.782 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.ca",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:33857,Safety,abort,aborted,33857,"e or directory)) [duplicate 1]; 02:34 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:41:53 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.25, executor 6, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:41:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx.xx.xx.xx:45142 (size: 6.4 KB, free: 366.3 MB); 18/04/24 17:42:02 INFO TaskSetManager: Lost task 0.3 in stage 2.0 (TID 8) on xx.xx.xx.xx, executor 3: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile (Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory)) [duplicate 2]; 18/04/24 17:42:02 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Cancelling stage 2; 18/04/24 17:42:02 INFO TaskSchedulerImpl: Stage 2 was cancelled; 18/04/24 17:42:02 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at PSFilter.java:125) failed in 117.782 s due to Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:39053,Safety,abort,aborted,39053,"o.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:748); 18/04/24 17:42:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:42:11 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:42:11 INFO BlockManager: BlockManager stopped; 18/04/24 17:42:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:42:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:42:11 INFO SparkContext: Successfully stopped SparkContext; 17:42:11.053 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:42:11 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=866648064; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41458,Safety,abort,abortStage,41458,MapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41555,Safety,abort,abortStage,41555, at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:41797,Safety,abort,abortStage,41797,ead.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:7276,Security,Secur,SecurityManager,7276,FO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:39:19 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(username); groups with view permissions: Set(); users with modify permissions: Set(username); groups with modify permissions: Set(); 18/04/24 17:39:20 INFO Utils: Successfully started service 'sparkDriver' on port 46576.; 18/04/24 17:39:20 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:39:20 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/24 17:39:20 INFO DiskBlockManager: Created local directory at /tmp/username/blockmgr-24b23f43-effa-45a6-8539-b9de234fa346; 18/04/24 17:39:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 18/04/24 17:39:20 INFO SparkEnv: Registering OutputComm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:7349,Security,Secur,SecurityManager,7349,FO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:39:19 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(username); groups with view permissions: Set(); users with modify permissions: Set(username); groups with modify permissions: Set(); 18/04/24 17:39:20 INFO Utils: Successfully started service 'sparkDriver' on port 46576.; 18/04/24 17:39:20 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:39:20 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/24 17:39:20 INFO DiskBlockManager: Created local directory at /tmp/username/blockmgr-24b23f43-effa-45a6-8539-b9de234fa346; 18/04/24 17:39:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 18/04/24 17:39:20 INFO SparkEnv: Registering OutputComm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:7424,Security,Secur,SecurityManager,7424,FO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:39:19 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(username); groups with view permissions: Set(); users with modify permissions: Set(username); groups with modify permissions: Set(); 18/04/24 17:39:20 INFO Utils: Successfully started service 'sparkDriver' on port 46576.; 18/04/24 17:39:20 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:39:20 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/24 17:39:20 INFO DiskBlockManager: Created local directory at /tmp/username/blockmgr-24b23f43-effa-45a6-8539-b9de234fa346; 18/04/24 17:39:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 18/04/24 17:39:20 INFO SparkEnv: Registering OutputComm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:7495,Security,Secur,SecurityManager,7495,FO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:39:19 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(username); groups with view permissions: Set(); users with modify permissions: Set(username); groups with modify permissions: Set(); 18/04/24 17:39:20 INFO Utils: Successfully started service 'sparkDriver' on port 46576.; 18/04/24 17:39:20 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:39:20 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/24 17:39:20 INFO DiskBlockManager: Created local directory at /tmp/username/blockmgr-24b23f43-effa-45a6-8539-b9de234fa346; 18/04/24 17:39:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 18/04/24 17:39:20 INFO SparkEnv: Registering OutputComm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:7568,Security,Secur,SecurityManager,7568,FO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:39:19 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(username); groups with view permissions: Set(); users with modify permissions: Set(username); groups with modify permissions: Set(); 18/04/24 17:39:20 INFO Utils: Successfully started service 'sparkDriver' on port 46576.; 18/04/24 17:39:20 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:39:20 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/24 17:39:20 INFO DiskBlockManager: Created local directory at /tmp/username/blockmgr-24b23f43-effa-45a6-8539-b9de234fa346; 18/04/24 17:39:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 18/04/24 17:39:20 INFO SparkEnv: Registering OutputComm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:7585,Security,Secur,SecurityManager,7585,FO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:39:19 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(username); groups with view permissions: Set(); users with modify permissions: Set(username); groups with modify permissions: Set(); 18/04/24 17:39:20 INFO Utils: Successfully started service 'sparkDriver' on port 46576.; 18/04/24 17:39:20 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:39:20 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/24 17:39:20 INFO DiskBlockManager: Created local directory at /tmp/username/blockmgr-24b23f43-effa-45a6-8539-b9de234fa346; 18/04/24 17:39:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 18/04/24 17:39:20 INFO SparkEnv: Registering OutputComm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:7602,Security,authenticat,authentication,7602,FO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:39:19.246 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:39:19.246 INFO PathSeqPipelineSpark - Initializing engine; 17:39:19.246 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:39:19 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:39:19 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls to: username; 18/04/24 17:39:20 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:39:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(username); groups with view permissions: Set(); users with modify permissions: Set(username); groups with modify permissions: Set(); 18/04/24 17:39:20 INFO Utils: Successfully started service 'sparkDriver' on port 46576.; 18/04/24 17:39:20 INFO SparkEnv: Registering MapOutputTracker; 18/04/24 17:39:20 INFO SparkEnv: Registering BlockManagerMaster; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 18/04/24 17:39:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 18/04/24 17:39:20 INFO DiskBlockManager: Created local directory at /tmp/username/blockmgr-24b23f43-effa-45a6-8539-b9de234fa346; 18/04/24 17:39:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 18/04/24 17:39:20 INFO SparkEnv: Registering OutputComm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616:38456,Usability,clear,cleared,38456,"nel.java:314); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:802); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.forceFlush(AbstractNioChannel.java:319); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:637); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:748); 18/04/24 17:42:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/04/24 17:42:11 INFO MemoryStore: MemoryStore cleared; 18/04/24 17:42:11 INFO BlockManager: BlockManager stopped; 18/04/24 17:42:11 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/04/24 17:42:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/04/24 17:42:11 INFO SparkContext: Successfully stopped SparkContext; 17:42:11.053 INFO PathSeqPipelineSpark - Shutting down engine; [April 24, 2018 5:42:11 PM CEST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 2.87 minutes.; Runtime.totalMemory()=866648064; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 8, xx.xx.xx.xx, executor 3): org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilte",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-383986616
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-384335260:518,Availability,error,error,518,"Hi @SZLux,. The --spark-runner LOCAL option is only valid if you want to launch the GATK from the same machine you want to run on, i.e. on your laptop or on the head node of a cluster you have logged into via ssh. We do not currently support --spark-runner LOCAL if you want to, say, launch from the head node and run on another node. Your second issue is specific to PathSeq. Can you please open another ticket for that? That way if another user has the same issue it'll be easier to find the solution. P.S. Your new error may be because the .hss file could not be found on HDFS. You may need to give it the full HDFS URI i.e. hdfs://path/to/kmers.hss",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-384335260
https://github.com/broadinstitute/gatk/issues/4694#issuecomment-384335260:193,Testability,log,logged,193,"Hi @SZLux,. The --spark-runner LOCAL option is only valid if you want to launch the GATK from the same machine you want to run on, i.e. on your laptop or on the head node of a cluster you have logged into via ssh. We do not currently support --spark-runner LOCAL if you want to, say, launch from the head node and run on another node. Your second issue is specific to PathSeq. Can you please open another ticket for that? That way if another user has the same issue it'll be easier to find the solution. P.S. Your new error may be because the .hss file could not be found on HDFS. You may need to give it the full HDFS URI i.e. hdfs://path/to/kmers.hss",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694#issuecomment-384335260
https://github.com/broadinstitute/gatk/pull/4695#issuecomment-383997874:10,Energy Efficiency,green,green,10,"Travis is green, this is ready for review!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4695#issuecomment-383997874
https://github.com/broadinstitute/gatk/issues/4696#issuecomment-383909813:66,Testability,test,testing,66,"@heliac2000 Thanks for the report - I don't think we've done much testing with gpu-enabled environments yet, but one thing that would be useful is to re-run adding the `--enable-journal` command line argument to GATK. That will produce a file in the current directory with a name of the form ""gatkStreamingProcessJournal-nnnnnnn.txt"". If you post that file here it will help us see a bit more about what's happening.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696#issuecomment-383909813
https://github.com/broadinstitute/gatk/issues/4696#issuecomment-384231912:73,Integrability,message,message,73,"Thanks for your reply. I try `--enable-journal` option and the following message is repeated many times. Received from stderr: [>>> ]. Sending: [vqsr_cnn.score_and_write_batch(args, model, tempFile, fifoFile, 2, 2, ''); ]. I trace the python process that is piped-forked by CNNScoreVariants. write(2, "">>> "", 4) = -1 EPIPE (Broken pipe); --- SIGPIPE {si_signo=SIGPIPE, si_code=SI_USER, si_pid=58415, si_uid=xxxx} ---; read(0, """", 1) = 0; write(2, ""\n"", 1) = -1 EPIPE (Broken pipe); --- SIGPIPE {si_signo=SIGPIPE, si_code=SI_USER, si_pid=58415, si_uid=xxxx} ---; rt_sigaction(SIGINT, {SIG_DFL, [], SA_RESTORER, 0x7f054d1fe5e0}, {0x5006d0, [], SA_RESTORER, 0x7f054d1fe5e0}, 8) = 0; close(3) = 0; write(4, ""952\n1\t121482273\tA\t[G]\t-14.449\n1\t""..., 2018) = 2018; close(4) = 0; :; close(21) = 0; exit_group(0) = ?; +++ exited with 0 +++; ; EPIPE is occured after CNNScoreVariants stops.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696#issuecomment-384231912
https://github.com/broadinstitute/gatk/issues/4696#issuecomment-384300005:262,Availability,robust,robust,262,"@heliac2000 Thanks for the trace. I suspect that once the timeout initially occurs, the GATK process terminates, but the Python process is still trying to write back to it and gets the ""broken pipe"" return code. We already have plan to make the IPC/timeout more robust. In the meantime it would be interesting to see the last 50-100 lines of the journal file if it contains contains anything other than the repeated `Sending: [vqsr_cnn.score_and_write_batch(args, model, tempFile, fifoFile, 2, 2, '')` lines (which are expected).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696#issuecomment-384300005
https://github.com/broadinstitute/gatk/issues/4696#issuecomment-384300005:58,Safety,timeout,timeout,58,"@heliac2000 Thanks for the trace. I suspect that once the timeout initially occurs, the GATK process terminates, but the Python process is still trying to write back to it and gets the ""broken pipe"" return code. We already have plan to make the IPC/timeout more robust. In the meantime it would be interesting to see the last 50-100 lines of the journal file if it contains contains anything other than the repeated `Sending: [vqsr_cnn.score_and_write_batch(args, model, tempFile, fifoFile, 2, 2, '')` lines (which are expected).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696#issuecomment-384300005
https://github.com/broadinstitute/gatk/issues/4696#issuecomment-384300005:249,Safety,timeout,timeout,249,"@heliac2000 Thanks for the trace. I suspect that once the timeout initially occurs, the GATK process terminates, but the Python process is still trying to write back to it and gets the ""broken pipe"" return code. We already have plan to make the IPC/timeout more robust. In the meantime it would be interesting to see the last 50-100 lines of the journal file if it contains contains anything other than the repeated `Sending: [vqsr_cnn.score_and_write_batch(args, model, tempFile, fifoFile, 2, 2, '')` lines (which are expected).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4696#issuecomment-384300005
https://github.com/broadinstitute/gatk/issues/4699#issuecomment-384352326:536,Performance,perform,performances,536,"Thanks to a suggestion of mwalker174, I solved the issue. I had to specify, for each input and output file, the full path from the root. All the HPC nodes share the same hdfs, so it worked. What leaves me a bit perplexed, is that the task took 5 minutes to run on a 16 cores nodes, and exactly the same to run on a master-worker setup with 5 workers and 16 cores each (the log cofirms that the tasks were distributed to the workers IP addreses). Is this something expected? Shall I maybe try with alarger input to see the difference in performances? Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699#issuecomment-384352326
https://github.com/broadinstitute/gatk/issues/4699#issuecomment-384352326:373,Testability,log,log,373,"Thanks to a suggestion of mwalker174, I solved the issue. I had to specify, for each input and output file, the full path from the root. All the HPC nodes share the same hdfs, so it worked. What leaves me a bit perplexed, is that the task took 5 minutes to run on a 16 cores nodes, and exactly the same to run on a master-worker setup with 5 workers and 16 cores each (the log cofirms that the tasks were distributed to the workers IP addreses). Is this something expected? Shall I maybe try with alarger input to see the difference in performances? Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699#issuecomment-384352326
https://github.com/broadinstitute/gatk/issues/4705#issuecomment-384228852:99,Availability,error,error,99,"The `-selectType` argument now is `--select-type`, and that's what is failing. It is true that the error message is not that useful. @cmnbroad - is this related with https://github.com/broadinstitute/barclay/issues/119, no?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4705#issuecomment-384228852
https://github.com/broadinstitute/gatk/issues/4705#issuecomment-384228852:105,Integrability,message,message,105,"The `-selectType` argument now is `--select-type`, and that's what is failing. It is true that the error message is not that useful. @cmnbroad - is this related with https://github.com/broadinstitute/barclay/issues/119, no?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4705#issuecomment-384228852
https://github.com/broadinstitute/gatk/pull/4706#issuecomment-384762571:56,Usability,simpl,simpler,56,closing after discussing with the engine team that it's simpler to roll up one's own comparator; Thanks @magicDGS for looking!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4706#issuecomment-384762571
https://github.com/broadinstitute/gatk/issues/4709#issuecomment-384568889:90,Availability,down,downstream,90,"I prefer to keep out of this user exceptions the arguments from the wrapper script due to downstream projects including tools from GATK (and using the `UserException` classes). Thus, I vote for refere to the `--TMP_DIR`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4709#issuecomment-384568889
https://github.com/broadinstitute/gatk/issues/4709#issuecomment-384568889:68,Integrability,wrap,wrapper,68,"I prefer to keep out of this user exceptions the arguments from the wrapper script due to downstream projects including tools from GATK (and using the `UserException` classes). Thus, I vote for refere to the `--TMP_DIR`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4709#issuecomment-384568889
https://github.com/broadinstitute/gatk/pull/4710#issuecomment-384518443:228,Testability,test,tests,228,"Working on Firecloud after one tweak: `if [ -f ${normal_bam} ]` doesn't work when `normal_bam` is a `String` because nothing is localized, so I replaced it with `if [ ! -z ${normal_bam} ]`. I will merge around 6 am after Travis tests hopefully pass.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4710#issuecomment-384518443
https://github.com/broadinstitute/gatk/pull/4711#issuecomment-384791581:1559,Testability,test,test,1559,==================================; Files 1080 1080 ; Lines 63109 63093 -16 ; Branches 10184 10179 -5 ; ===============================================; - Hits 50475 50460 -15 ; - Misses 8646 8648 +2 ; + Partials 3988 3985 -3; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4711?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...institute/hellbender/exceptions/UserException.java](https://codecov.io/gh/broadinstitute/gatk/pull/4711/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9leGNlcHRpb25zL1VzZXJFeGNlcHRpb24uamF2YQ==) | `70.229% <0%> (-1.089%)` | `3 <0> (ø)` | |; | [.../tools/copynumber/PostprocessGermlineCNVCalls.java](https://codecov.io/gh/broadinstitute/gatk/pull/4711/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL1Bvc3Rwcm9jZXNzR2VybWxpbmVDTlZDYWxscy5qYXZh) | `91.071% <100%> (ø)` | `23 <0> (ø)` | :arrow_down: |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4711/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `63.91% <100%> (-0.535%)` | `33 <1> (ø)` | |; | [...dinstitute/hellbender/utils/R/RScriptExecutor.java](https://codecov.io/gh/broadinstitute/gatk/pull/4711/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9SL1JTY3JpcHRFeGVjdXRvci5qYXZh) | `80.556% <100%> (ø)` | `17 <0> (ø)` | :arrow_down: |; | [...rg/broadinstitute/hellbender/utils/io/IOUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4711/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pby9JT1V0aWxzLmphdmE=) | `62.366% <60%> (-0.011%)` | `45 <2> (-7)` | |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/4711/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4711#issuecomment-384791581
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-384991868:207,Availability,error,error,207,"Hi @chatchawit,. It's hard to tell what's going on from the information you provided. Could you include the exact commandline that you're trying to run. Also, the complete stack trace from both the original error and the new error message would be very helpful.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-384991868
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-384991868:225,Availability,error,error,225,"Hi @chatchawit,. It's hard to tell what's going on from the information you provided. Could you include the exact commandline that you're trying to run. Also, the complete stack trace from both the original error and the new error message would be very helpful.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-384991868
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-384991868:231,Integrability,message,message,231,"Hi @chatchawit,. It's hard to tell what's going on from the information you provided. Could you include the exact commandline that you're trying to run. Also, the complete stack trace from both the original error and the new error message would be very helpful.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-384991868
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:2791,Availability,down,down,2791,"55.067 INFO Funcotator - Picard Version: 2.17.2; 22:56:55.067 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 22:56:55.067 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:56:55.068 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:56:55.068 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:56:55.068 INFO Funcotator - Deflater: IntelDeflater; 22:56:55.068 INFO Funcotator - Inflater: IntelInflater; 22:56:55.068 INFO Funcotator - GCS max retries/reopens: 20; 22:56:55.068 INFO Funcotator - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 22:56:55.068 INFO Funcotator - Initializing engine; 22:56:56.227 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/sm/out/sample21.vcf; 22:56:56.402 INFO Funcotator - Done initializing engine; 22:56:56.425 INFO Funcotator - Shutting down engine; [April 27, 2018 10:56:56 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2276982784; java.lang.NullPointerException; at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:330); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:897); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275). Next, I tried only ""gencode"" in the data-source folder. Using GATK jar /omics/chatchawit/gatk/gatk-package-4.0.0.0-local",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:6806,Availability,down,down,6806,"le-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 23:01:57.345 INFO Funcotator - Initializing engine; 23:01:58.372 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/sm/out/sample21.vcf; 23:01:58.541 INFO Funcotator - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.Funcotator).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:01:58.560 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/test/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; 23:02:05.335 INFO ProgressMeter - Starting traversal; 23:02:05.337 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:02:06.530 INFO Funcotator - Shutting down engine; [April 27, 2018 11:02:06 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=3420979200; java.lang.IllegalArgumentException: Invalid interval. Contig:ENST00000342066.7|ENSG00000187634.11|OTTHUMG00000040719.10|OTTHUMT00000276866.2|SAMD11-202|SAMD11|2551|protein_coding| start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.getCodingSequenceFromTranscriptFasta(GencodeFuncotationFactory.java:418); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSequenceComparison(GencodeFuncotationFactory.java:1177); at org.broadinstitute.hellbender.tools.funcotator.dataSo",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:2407,Deployability,patch,patch,2407,"time: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 22:56:55.066 INFO Funcotator - Start Date/Time: April 27, 2018 10:56:54 PM ICT; 22:56:55.066 INFO Funcotator - ------------------------------------------------------------; 22:56:55.066 INFO Funcotator - ------------------------------------------------------------; 22:56:55.067 INFO Funcotator - HTSJDK Version: 2.13.2; 22:56:55.067 INFO Funcotator - Picard Version: 2.17.2; 22:56:55.067 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 22:56:55.067 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:56:55.068 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:56:55.068 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:56:55.068 INFO Funcotator - Deflater: IntelDeflater; 22:56:55.068 INFO Funcotator - Inflater: IntelInflater; 22:56:55.068 INFO Funcotator - GCS max retries/reopens: 20; 22:56:55.068 INFO Funcotator - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 22:56:55.068 INFO Funcotator - Initializing engine; 22:56:56.227 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/sm/out/sample21.vcf; 22:56:56.402 INFO Funcotator - Done initializing engine; 22:56:56.425 INFO Funcotator - Shutting down engine; [April 27, 2018 10:56:56 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2276982784; java.lang.NullPointerException; at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:330); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:897); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdlin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:5829,Deployability,patch,patch,5829,"time: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 23:01:57.342 INFO Funcotator - Start Date/Time: April 27, 2018 11:01:57 PM ICT; 23:01:57.343 INFO Funcotator - ------------------------------------------------------------; 23:01:57.343 INFO Funcotator - ------------------------------------------------------------; 23:01:57.344 INFO Funcotator - HTSJDK Version: 2.13.2; 23:01:57.344 INFO Funcotator - Picard Version: 2.17.2; 23:01:57.344 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 23:01:57.344 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:01:57.344 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:01:57.344 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:01:57.344 INFO Funcotator - Deflater: IntelDeflater; 23:01:57.344 INFO Funcotator - Inflater: IntelInflater; 23:01:57.345 INFO Funcotator - GCS max retries/reopens: 20; 23:01:57.345 INFO Funcotator - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 23:01:57.345 INFO Funcotator - Initializing engine; 23:01:58.372 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/sm/out/sample21.vcf; 23:01:58.541 INFO Funcotator - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.Funcotator).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:01:58.560 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/test/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; 23:02:05.335 INFO ProgressMeter - Starting traversal; 23:02:05.337 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:02:06.530 INFO Funcotator - Shutting down engine; [April 27, 2018 11:02",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:9110,Integrability,wrap,wrapAndCopyInto,9110,tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotations(GencodeFuncotationFactory.java:487); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotations(GencodeFuncotationFactory.java:243); at org.broadinstitute.hellbender.tools.funcotator.Funcotator.enqueueAndHandleVariant(Funcotator.java:387); at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:316); at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hel,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:859,Performance,Load,Loading,859,"Here is the command line. I'm using funcotator_dataSources.v1.2.20180329.tar.gz.; /omics/chatchawit/gatk/gatk Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38. Using GATK jar /omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38; 22:56:54.836 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:56:55.064 INFO Funcotator - ------------------------------------------------------------; 22:56:55.065 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.0.0; 22:56:55.065 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:56:55.065 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 22:56:55.066 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 22:56:55.066 INFO Funcotator - Start Date/Time: April 27, 2018 10:56:54 PM ICT; 22:56:55.066 INFO Funcotator - ------------------------------------------------------------; 22:56:55.066 INFO Funcotator - ------------------------------------------------------------; 22:56:55.067 INFO Funcotator - HTSJDK Version: 2.13.2; 22:56:55.067 INFO Funcotator - Picard Version: 2.17.2; 22:56:55.067 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 22:56:55.067 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:4281,Performance,Load,Loading,4281,"itute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275). Next, I tried only ""gencode"" in the data-source folder. Using GATK jar /omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --data-sources-path /omics/chatchawit/bundle/test/ --ref-version hg38; 23:01:57.151 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:01:57.341 INFO Funcotator - ------------------------------------------------------------; 23:01:57.341 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.0.0; 23:01:57.341 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:01:57.342 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 23:01:57.342 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 23:01:57.342 INFO Funcotator - Start Date/Time: April 27, 2018 11:01:57 PM ICT; 23:01:57.343 INFO Funcotator - ------------------------------------------------------------; 23:01:57.343 INFO Funcotator - ------------------------------------------------------------; 23:01:57.344 INFO Funcotator - HTSJDK Version: 2.13.2; 23:01:57.344 INFO Funcotator - Picard Version: 2.17.2; 23:01:57",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:7219,Security,validat,validateArg,7219,"broadinstitute.hellbender.tools.funcotator.Funcotator).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:01:58.560 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/test/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; 23:02:05.335 INFO ProgressMeter - Starting traversal; 23:02:05.337 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:02:06.530 INFO Funcotator - Shutting down engine; [April 27, 2018 11:02:06 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=3420979200; java.lang.IllegalArgumentException: Invalid interval. Contig:ENST00000342066.7|ENSG00000187634.11|OTTHUMG00000040719.10|OTTHUMT00000276866.2|SAMD11-202|SAMD11|2551|protein_coding| start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.getCodingSequenceFromTranscriptFasta(GencodeFuncotationFactory.java:418); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSequenceComparison(GencodeFuncotationFactory.java:1177); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createCodingRegionFuncotation(GencodeFuncotationFactory.java:619); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createGencodeFuncotationOnTranscript(GencodeFuncotationFactory.java:575); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotations(GencodeFuncotationFa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:7302,Security,validat,validatePositions,7302,"log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:01:58.560 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/test/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; 23:02:05.335 INFO ProgressMeter - Starting traversal; 23:02:05.337 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:02:06.530 INFO Funcotator - Shutting down engine; [April 27, 2018 11:02:06 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=3420979200; java.lang.IllegalArgumentException: Invalid interval. Contig:ENST00000342066.7|ENSG00000187634.11|OTTHUMG00000040719.10|OTTHUMT00000276866.2|SAMD11-202|SAMD11|2551|protein_coding| start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.getCodingSequenceFromTranscriptFasta(GencodeFuncotationFactory.java:418); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSequenceComparison(GencodeFuncotationFactory.java:1177); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createCodingRegionFuncotation(GencodeFuncotationFactory.java:619); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createGencodeFuncotationOnTranscript(GencodeFuncotationFactory.java:575); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotations(GencodeFuncotationFactory.java:487); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.Genc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:4215,Testability,test,test,4215,"mmandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275). Next, I tried only ""gencode"" in the data-source folder. Using GATK jar /omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --data-sources-path /omics/chatchawit/bundle/test/ --ref-version hg38; 23:01:57.151 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk/gatk-package-4.0.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:01:57.341 INFO Funcotator - ------------------------------------------------------------; 23:01:57.341 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.0.0; 23:01:57.341 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:01:57.342 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 23:01:57.342 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 23:01:57.342 INFO Funcotator - Start Date/Time: April 27, 2018 11:01:57 PM ICT; 23:01:57.343 INFO Funcotator - ------------------------------------------------------------; 23:01:57.343 INFO Funcotator - ------------------------------------------------------------; 23:01:57.344 INFO Funcotator - HTSJDK Version: ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:6216,Testability,log,logger,6216,"FO Funcotator - HTSJDK Version: 2.13.2; 23:01:57.344 INFO Funcotator - Picard Version: 2.17.2; 23:01:57.344 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 23:01:57.344 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:01:57.344 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:01:57.344 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:01:57.344 INFO Funcotator - Deflater: IntelDeflater; 23:01:57.344 INFO Funcotator - Inflater: IntelInflater; 23:01:57.345 INFO Funcotator - GCS max retries/reopens: 20; 23:01:57.345 INFO Funcotator - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 23:01:57.345 INFO Funcotator - Initializing engine; 23:01:58.372 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/sm/out/sample21.vcf; 23:01:58.541 INFO Funcotator - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.Funcotator).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:01:58.560 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/test/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; 23:02:05.335 INFO ProgressMeter - Starting traversal; 23:02:05.337 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:02:06.530 INFO Funcotator - Shutting down engine; [April 27, 2018 11:02:06 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=3420979200; java.lang.IllegalArgumentException: Invalid interval. Contig:ENST00000342066.7|ENSG00000187634.11|OTTHUMG00000040719.10|OTTHUMT00000276866.2|SAMD11-202|SAMD11|2551|protein_coding| start:0 end:0; at ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:6364,Testability,log,logging,6364,"FO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:01:57.344 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:01:57.344 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:01:57.344 INFO Funcotator - Deflater: IntelDeflater; 23:01:57.344 INFO Funcotator - Inflater: IntelInflater; 23:01:57.345 INFO Funcotator - GCS max retries/reopens: 20; 23:01:57.345 INFO Funcotator - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 23:01:57.345 INFO Funcotator - Initializing engine; 23:01:58.372 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/sm/out/sample21.vcf; 23:01:58.541 INFO Funcotator - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.Funcotator).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:01:58.560 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/test/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; 23:02:05.335 INFO ProgressMeter - Starting traversal; 23:02:05.337 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:02:06.530 INFO Funcotator - Shutting down engine; [April 27, 2018 11:02:06 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=3420979200; java.lang.IllegalArgumentException: Invalid interval. Contig:ENST00000342066.7|ENSG00000187634.11|OTTHUMG00000040719.10|OTTHUMT00000276866.2|SAMD11-202|SAMD11|2551|protein_coding| start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:6535,Testability,test,test,6535,"SAMTOOLS : true; 23:01:57.344 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:01:57.344 INFO Funcotator - Deflater: IntelDeflater; 23:01:57.344 INFO Funcotator - Inflater: IntelInflater; 23:01:57.345 INFO Funcotator - GCS max retries/reopens: 20; 23:01:57.345 INFO Funcotator - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 23:01:57.345 INFO Funcotator - Initializing engine; 23:01:58.372 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/sm/out/sample21.vcf; 23:01:58.541 INFO Funcotator - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.Funcotator).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:01:58.560 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/test/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; 23:02:05.335 INFO ProgressMeter - Starting traversal; 23:02:05.337 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:02:06.530 INFO Funcotator - Shutting down engine; [April 27, 2018 11:02:06 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=3420979200; java.lang.IllegalArgumentException: Invalid interval. Contig:ENST00000342066.7|ENSG00000187634.11|OTTHUMG00000040719.10|OTTHUMT00000276866.2|SAMD11-202|SAMD11|2551|protein_coding| start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencod",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:7287,Usability,Simpl,SimpleInterval,7287,"N Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:01:58.560 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/test/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; 23:02:05.335 INFO ProgressMeter - Starting traversal; 23:02:05.337 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:02:06.530 INFO Funcotator - Shutting down engine; [April 27, 2018 11:02:06 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=3420979200; java.lang.IllegalArgumentException: Invalid interval. Contig:ENST00000342066.7|ENSG00000187634.11|OTTHUMG00000040719.10|OTTHUMT00000276866.2|SAMD11-202|SAMD11|2551|protein_coding| start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.getCodingSequenceFromTranscriptFasta(GencodeFuncotationFactory.java:418); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSequenceComparison(GencodeFuncotationFactory.java:1177); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createCodingRegionFuncotation(GencodeFuncotationFactory.java:619); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createGencodeFuncotationOnTranscript(GencodeFuncotationFactory.java:575); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotations(GencodeFuncotationFactory.java:487); at org.broadinstitute.hellbender.tools.funcotator.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:7320,Usability,Simpl,SimpleInterval,7320,"log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:01:58.560 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/test/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; 23:02:05.335 INFO ProgressMeter - Starting traversal; 23:02:05.337 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:02:06.530 INFO Funcotator - Shutting down engine; [April 27, 2018 11:02:06 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=3420979200; java.lang.IllegalArgumentException: Invalid interval. Contig:ENST00000342066.7|ENSG00000187634.11|OTTHUMG00000040719.10|OTTHUMT00000276866.2|SAMD11-202|SAMD11|2551|protein_coding| start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.getCodingSequenceFromTranscriptFasta(GencodeFuncotationFactory.java:418); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSequenceComparison(GencodeFuncotationFactory.java:1177); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createCodingRegionFuncotation(GencodeFuncotationFactory.java:619); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createGencodeFuncotationOnTranscript(GencodeFuncotationFactory.java:575); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotations(GencodeFuncotationFactory.java:487); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.Genc",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:7384,Usability,Simpl,SimpleInterval,7384,"2/faq.html#noconfig for more info.; 23:01:58.560 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/test/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; 23:02:05.335 INFO ProgressMeter - Starting traversal; 23:02:05.337 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:02:06.530 INFO Funcotator - Shutting down engine; [April 27, 2018 11:02:06 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=3420979200; java.lang.IllegalArgumentException: Invalid interval. Contig:ENST00000342066.7|ENSG00000187634.11|OTTHUMG00000040719.10|OTTHUMT00000276866.2|SAMD11-202|SAMD11|2551|protein_coding| start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.getCodingSequenceFromTranscriptFasta(GencodeFuncotationFactory.java:418); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSequenceComparison(GencodeFuncotationFactory.java:1177); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createCodingRegionFuncotation(GencodeFuncotationFactory.java:619); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createGencodeFuncotationOnTranscript(GencodeFuncotationFactory.java:575); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotations(GencodeFuncotationFactory.java:487); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotations(GencodeFuncotationFactory.java:2",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157:7406,Usability,Simpl,SimpleInterval,7406," for more info.; 23:01:58.560 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/test/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; 23:02:05.335 INFO ProgressMeter - Starting traversal; 23:02:05.337 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:02:06.530 INFO Funcotator - Shutting down engine; [April 27, 2018 11:02:06 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.16 minutes.; Runtime.totalMemory()=3420979200; java.lang.IllegalArgumentException: Invalid interval. Contig:ENST00000342066.7|ENSG00000187634.11|OTTHUMG00000040719.10|OTTHUMT00000276866.2|SAMD11-202|SAMD11|2551|protein_coding| start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.getCodingSequenceFromTranscriptFasta(GencodeFuncotationFactory.java:418); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSequenceComparison(GencodeFuncotationFactory.java:1177); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createCodingRegionFuncotation(GencodeFuncotationFactory.java:619); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createGencodeFuncotationOnTranscript(GencodeFuncotationFactory.java:575); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotations(GencodeFuncotationFactory.java:487); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotations(GencodeFuncotationFactory.java:243); at org.broadi",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385021157
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385029450:144,Deployability,update,updates,144,"@chatchawit Thank you for the additional information, thats very helpful. ; It looks like you're using gatk 4.0.0.0, there have a been a lot of updates to funcotator since then. I might try running with the newest release 4.0.4.0 and see if that resolves the problem. @jonn-smith As the funcomaster, you may have some insight?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385029450
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385029450:214,Deployability,release,release,214,"@chatchawit Thank you for the additional information, thats very helpful. ; It looks like you're using gatk 4.0.0.0, there have a been a lot of updates to funcotator since then. I might try running with the newest release 4.0.4.0 and see if that resolves the problem. @jonn-smith As the funcomaster, you may have some insight?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385029450
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385055029:57,Deployability,update,updates,57,"@chatchawit Like Louis said, there have been quite a few updates. The issue you're seeing is a data sources problem relating to HG38. This has been fixed in the latest release of funcotator and the data sources. If you grab the latest software and the latest data sources release this problem should go away. Sorry for the inconvenience.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385055029
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385055029:168,Deployability,release,release,168,"@chatchawit Like Louis said, there have been quite a few updates. The issue you're seeing is a data sources problem relating to HG38. This has been fixed in the latest release of funcotator and the data sources. If you grab the latest software and the latest data sources release this problem should go away. Sorry for the inconvenience.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385055029
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385055029:272,Deployability,release,release,272,"@chatchawit Like Louis said, there have been quite a few updates. The issue you're seeing is a data sources problem relating to HG38. This has been fixed in the latest release of funcotator and the data sources. If you grab the latest software and the latest data sources release this problem should go away. Sorry for the inconvenience.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385055029
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363:38,Availability,error,error,38,"I've tried the latest GATK today. The error message changed. Please help. Thanks. Using GATK jar /omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --output-file-format VCF --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38; 10:24:13.971 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:14.182 INFO Funcotator - ------------------------------------------------------------; 10:24:14.183 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.4.0-0.0.2; 10:24:14.183 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:24:14.183 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 10:24:14.184 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 10:24:14.184 INFO Funcotator - Start Date/Time: April 28, 2018 10:24:13 AM ICT; 10:24:14.184 INFO Funcotator - ------------------------------------------------------------; 10:24:14.184 INFO Funcotator - ------------------------------------------------------------; 10:24:14.185 INFO Funcotator - HTSJDK Version: 2.14.3; 10:24:14.185 INFO Funcotator - Picard Version: 2.18.2; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:24:14.186 INFO Fu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363:3918,Availability,down,down,3918,"Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 10:24:15.728 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/bundle/dsrc/dbsnp/hg38/hg38_All_20170710.vcf.gz; 10:24:15.884 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///omics/chatchawit/bundle/dsrc/oreganno/hg38/oreganno.tsv; 10:24:15.945 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/dsrc/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; WARNING 2018-04-28 10:24:17 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 10:24:29.369 INFO ProgressMeter - Starting traversal; 10:24:29.370 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 10:24:36.192 INFO Funcotator - Shutting down engine; [April 28, 2018 10:24:36 AM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.37 minutes.; Runtime.totalMemory()=5195694080; java.lang.StringIndexOutOfBoundsException: String index out of range: -2; at java.lang.String.substring(String.java:1967); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createUtrFuncotation(GencodeFuncotationFactory.java:1088); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createGencodeFuncotationOnTranscript(GencodeFuncotationFactory.java:601); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsHelper(GencodeFuncotationFactory.java:529); at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createFuncotationsOnVariant(GencodeFuncotationFactory.java:276); at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotatio",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363:2208,Deployability,patch,patch,2208,"time: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 10:24:14.184 INFO Funcotator - Start Date/Time: April 28, 2018 10:24:13 AM ICT; 10:24:14.184 INFO Funcotator - ------------------------------------------------------------; 10:24:14.184 INFO Funcotator - ------------------------------------------------------------; 10:24:14.185 INFO Funcotator - HTSJDK Version: 2.14.3; 10:24:14.185 INFO Funcotator - Picard Version: 2.18.2; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:24:14.186 INFO Funcotator - Deflater: IntelDeflater; 10:24:14.186 INFO Funcotator - Inflater: IntelInflater; 10:24:14.186 INFO Funcotator - GCS max retries/reopens: 20; 10:24:14.186 INFO Funcotator - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 10:24:14.187 WARN Funcotator -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: Funcotator is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 10:24:14.187 INFO Funcotator - Initializing engine; 10:24:15.574 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/sm/out/sample21.vcf; 10:24:15.701 INFO Funcotator - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 10:24:15.728 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/bundle/dsrc/dbsnp/hg38/hg38_All_20170710.vcf.gz; 10:24:15.884 INFO ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363:44,Integrability,message,message,44,"I've tried the latest GATK today. The error message changed. Please help. Thanks. Using GATK jar /omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --output-file-format VCF --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38; 10:24:13.971 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:14.182 INFO Funcotator - ------------------------------------------------------------; 10:24:14.183 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.4.0-0.0.2; 10:24:14.183 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:24:14.183 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 10:24:14.184 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 10:24:14.184 INFO Funcotator - Start Date/Time: April 28, 2018 10:24:13 AM ICT; 10:24:14.184 INFO Funcotator - ------------------------------------------------------------; 10:24:14.184 INFO Funcotator - ------------------------------------------------------------; 10:24:14.185 INFO Funcotator - HTSJDK Version: 2.14.3; 10:24:14.185 INFO Funcotator - Picard Version: 2.18.2; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:24:14.186 INFO Fu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363:5847,Integrability,wrap,wrapAndCopyInto,5847,g.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:137); at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:113); at org.broadinstitute.hellbender.tools.funcotator.Funcotator.enqueueAndHandleVariant(Funcotator.java:496); at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:387); at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:892); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hel,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363:653,Performance,Load,Loading,653,"I've tried the latest GATK today. The error message changed. Please help. Thanks. Using GATK jar /omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/sample21.vcf -O /omics/chatchawit/sm/anno/sample21.vcf --output-file-format VCF --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38; 10:24:13.971 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk2/gatk-package-4.0.4.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:14.182 INFO Funcotator - ------------------------------------------------------------; 10:24:14.183 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.4.0-0.0.2; 10:24:14.183 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:24:14.183 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 10:24:14.184 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 10:24:14.184 INFO Funcotator - Start Date/Time: April 28, 2018 10:24:13 AM ICT; 10:24:14.184 INFO Funcotator - ------------------------------------------------------------; 10:24:14.184 INFO Funcotator - ------------------------------------------------------------; 10:24:14.185 INFO Funcotator - HTSJDK Version: 2.14.3; 10:24:14.185 INFO Funcotator - Picard Version: 2.18.2; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:24:14.186 INFO Fu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363:2830,Testability,log,logger,2830,AMTOOLS : false; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:24:14.186 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:24:14.186 INFO Funcotator - Deflater: IntelDeflater; 10:24:14.186 INFO Funcotator - Inflater: IntelInflater; 10:24:14.186 INFO Funcotator - GCS max retries/reopens: 20; 10:24:14.186 INFO Funcotator - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 10:24:14.187 WARN Funcotator -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: Funcotator is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 10:24:14.187 INFO Funcotator - Initializing engine; 10:24:15.574 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/sm/out/sample21.vcf; 10:24:15.701 INFO Funcotator - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 10:24:15.728 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/bundle/dsrc/dbsnp/hg38/hg38_All_20170710.vcf.gz; 10:24:15.884 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///omics/chatchawit/bundle/dsrc/oreganno/hg38/oreganno.tsv; 10:24:15.945 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/dsrc/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; WARNING 2018-04-28 10:24:17 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 10:24:29.369 INFO ProgressMeter - Starting traversal; 10:24:29.370 ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363:2995,Testability,log,logging,2995,"14.186 INFO Funcotator - Deflater: IntelDeflater; 10:24:14.186 INFO Funcotator - Inflater: IntelInflater; 10:24:14.186 INFO Funcotator - GCS max retries/reopens: 20; 10:24:14.186 INFO Funcotator - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 10:24:14.187 WARN Funcotator -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: Funcotator is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 10:24:14.187 INFO Funcotator - Initializing engine; 10:24:15.574 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/sm/out/sample21.vcf; 10:24:15.701 INFO Funcotator - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 10:24:15.728 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/bundle/dsrc/dbsnp/hg38/hg38_All_20170710.vcf.gz; 10:24:15.884 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///omics/chatchawit/bundle/dsrc/oreganno/hg38/oreganno.tsv; 10:24:15.945 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/dsrc/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; WARNING 2018-04-28 10:24:17 AsciiLineReader Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 10:24:29.369 INFO ProgressMeter - Starting traversal; 10:24:29.370 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 10:24:36.192 INFO Funcotator - Shutting down engine; [April 28, 2018 10:24:36 AM ICT] org.broadinstitute.hell",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-385137363
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-386193333:267,Safety,safe,safely,267,I've checked the README file (see below). I think it's the latest one because I cannot see the newer version in the ftp site. Funcotator was able to produce some output. So I tried Funcotator with the same VCF input but with only first 5 loci. The program terminated safely. I hypothesized that the bug happened with only some records in the VCF file which was produced from Mutect2. Please help. +---------------------------------------------+; | Data Source Version Information |; +---------------------------------------------+. Version: 1.2.20180329; Source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.2.20180; 329.tar.gz; Alternate Source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.2.20180329.tar.gz,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-386193333
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-386195260:102,Usability,intuit,intuition,102,@chatchawit You're correct - you have the latest version. I just wanted to double-check. I think your intuition is correct about it only happening for certain variants. Are you able to share your file somehow? I can see where it's happening but having some variants that cause the issue would be helpful to debug.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-386195260
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-386198553:37,Availability,error,error,37,"I gave the two lines that caused the error. By removing these lines in VCF input, the program was able to proceed to the next. There was the next line that caused the error. Unfortunately, I did not know how many lines. Note that VCF input was produced from Mutect2. Complete VCF: https://drive.google.com/open?id=1UOUTtnNwVpBzTlJ9SPukon9aSHGOqJfW. chr1 959248 . A AAGAT . . DP=111;ECNT=4;POP_AF=0.016;P_GERMLINE=-8.838e-03;TLOD=3.19 GT:AD:AF:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:SA_MAP_AF:SA_POST_PROB 0/1:107,2:0.027:31,2:76,0:16:186,156:40:1:0.00,0.020,0.018:0.073,1.660e-03,0.925. chr1 1273803 . G GCGC . . DP=12;ECNT=1;IN_PON;POP_AF=0.016;P_GERMLINE=-1.931e-05;RPA=5,6;RU=CGC;STR;TLOD=6.39 GT:AD:AF:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:SA_MAP_AF:SA_POST_PROB 0/1:7,2:0.291:4,2:3,0:37:188,211:42:44:0.192,0.192,0.222:0.023,0.021,0.956",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-386198553
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-386198553:167,Availability,error,error,167,"I gave the two lines that caused the error. By removing these lines in VCF input, the program was able to proceed to the next. There was the next line that caused the error. Unfortunately, I did not know how many lines. Note that VCF input was produced from Mutect2. Complete VCF: https://drive.google.com/open?id=1UOUTtnNwVpBzTlJ9SPukon9aSHGOqJfW. chr1 959248 . A AAGAT . . DP=111;ECNT=4;POP_AF=0.016;P_GERMLINE=-8.838e-03;TLOD=3.19 GT:AD:AF:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:SA_MAP_AF:SA_POST_PROB 0/1:107,2:0.027:31,2:76,0:16:186,156:40:1:0.00,0.020,0.018:0.073,1.660e-03,0.925. chr1 1273803 . G GCGC . . DP=12;ECNT=1;IN_PON;POP_AF=0.016;P_GERMLINE=-1.931e-05;RPA=5,6;RU=CGC;STR;TLOD=6.39 GT:AD:AF:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:SA_MAP_AF:SA_POST_PROB 0/1:7,2:0.291:4,2:3,0:37:188,211:42:44:0.192,0.192,0.222:0.023,0.021,0.956",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-386198553
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-388403388:62,Availability,error,error,62,Should I wait for a bug fix ? Can I do something to avoid the error? Thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-388403388
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-388403388:52,Safety,avoid,avoid,52,Should I wait for a bug fix ? Can I do something to avoid the error? Thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-388403388
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-388905709:6,Deployability,update,update,6,Quick update - this bug occurs when handling alternate alleles (I believe of size > 1 base) in 5' UTRs. I'm working on a fix now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-388905709
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-389590814:194,Availability,down,download,194,![image](https://user-images.githubusercontent.com/38786115/40131493-50819d2a-5964-11e8-9e95-3c834521f7b7.png). I guess you've done something. How can I know that the bug has been fixed? How to download and install the new version? Please help. Thank you.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-389590814
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-389590814:207,Deployability,install,install,207,![image](https://user-images.githubusercontent.com/38786115/40131493-50819d2a-5964-11e8-9e95-3c834521f7b7.png). I guess you've done something. How can I know that the bug has been fixed? How to download and install the new version? Please help. Thank you.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-389590814
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-390326875:132,Deployability,update,update,132,"@chatchawit I just merged the code into master, so it should be live now. If you build the GATK from source you will get the latest update - do you need help doing this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-390326875
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391371859:268,Availability,down,downloaded,268,"@jonn-smith I have sucessfully built GATK by these commands. git clone https://github.com/broadinstitute/gatk; gradlew bundle. I've got this zip file in folder ""build"".; gatk-4.0.4.0-34-g2cc7abd-SNAPSHOT.zip. So I unzipped and used this to replace GATK-4.0.4.0 that I downloaded from https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip. I still found errors. 21:09:33.811 INFO ProgressMeter - Starting traversal; 21:09:33.811 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:09:46.267 INFO ProgressMeter - chr1:24929636 0.2 3000 14453.2; 21:09:59.072 INFO ProgressMeter - chr1:64681324 0.4 6000 14251.2; 21:10:09.456 INFO ProgressMeter - chr1:156245393 0.6 9000 15149.8; 21:10:21.510 INFO ProgressMeter - chr1:206965947 0.8 12000 15094.7; 21:10:26.132 INFO Funcotator - Shutting down engine; [May 23, 2018 9:10:26 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 1.36 minutes.; Runtime.totalMemory()=11500781568; java.lang.IllegalArgumentException: Genomic positions must be > 0.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:722). What should I do? Can you send me one that is ready-to-use? Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391371859
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391371859:332,Availability,down,download,332,"@jonn-smith I have sucessfully built GATK by these commands. git clone https://github.com/broadinstitute/gatk; gradlew bundle. I've got this zip file in folder ""build"".; gatk-4.0.4.0-34-g2cc7abd-SNAPSHOT.zip. So I unzipped and used this to replace GATK-4.0.4.0 that I downloaded from https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip. I still found errors. 21:09:33.811 INFO ProgressMeter - Starting traversal; 21:09:33.811 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:09:46.267 INFO ProgressMeter - chr1:24929636 0.2 3000 14453.2; 21:09:59.072 INFO ProgressMeter - chr1:64681324 0.4 6000 14251.2; 21:10:09.456 INFO ProgressMeter - chr1:156245393 0.6 9000 15149.8; 21:10:21.510 INFO ProgressMeter - chr1:206965947 0.8 12000 15094.7; 21:10:26.132 INFO Funcotator - Shutting down engine; [May 23, 2018 9:10:26 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 1.36 minutes.; Runtime.totalMemory()=11500781568; java.lang.IllegalArgumentException: Genomic positions must be > 0.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:722). What should I do? Can you send me one that is ready-to-use? Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391371859
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391371859:381,Availability,error,errors,381,"@jonn-smith I have sucessfully built GATK by these commands. git clone https://github.com/broadinstitute/gatk; gradlew bundle. I've got this zip file in folder ""build"".; gatk-4.0.4.0-34-g2cc7abd-SNAPSHOT.zip. So I unzipped and used this to replace GATK-4.0.4.0 that I downloaded from https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip. I still found errors. 21:09:33.811 INFO ProgressMeter - Starting traversal; 21:09:33.811 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:09:46.267 INFO ProgressMeter - chr1:24929636 0.2 3000 14453.2; 21:09:59.072 INFO ProgressMeter - chr1:64681324 0.4 6000 14251.2; 21:10:09.456 INFO ProgressMeter - chr1:156245393 0.6 9000 15149.8; 21:10:21.510 INFO ProgressMeter - chr1:206965947 0.8 12000 15094.7; 21:10:26.132 INFO Funcotator - Shutting down engine; [May 23, 2018 9:10:26 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 1.36 minutes.; Runtime.totalMemory()=11500781568; java.lang.IllegalArgumentException: Genomic positions must be > 0.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:722). What should I do? Can you send me one that is ready-to-use? Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391371859
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391371859:850,Availability,down,down,850,"@jonn-smith I have sucessfully built GATK by these commands. git clone https://github.com/broadinstitute/gatk; gradlew bundle. I've got this zip file in folder ""build"".; gatk-4.0.4.0-34-g2cc7abd-SNAPSHOT.zip. So I unzipped and used this to replace GATK-4.0.4.0 that I downloaded from https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip. I still found errors. 21:09:33.811 INFO ProgressMeter - Starting traversal; 21:09:33.811 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:09:46.267 INFO ProgressMeter - chr1:24929636 0.2 3000 14453.2; 21:09:59.072 INFO ProgressMeter - chr1:64681324 0.4 6000 14251.2; 21:10:09.456 INFO ProgressMeter - chr1:156245393 0.6 9000 15149.8; 21:10:21.510 INFO ProgressMeter - chr1:206965947 0.8 12000 15094.7; 21:10:26.132 INFO Funcotator - Shutting down engine; [May 23, 2018 9:10:26 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 1.36 minutes.; Runtime.totalMemory()=11500781568; java.lang.IllegalArgumentException: Genomic positions must be > 0.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:722). What should I do? Can you send me one that is ready-to-use? Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391371859
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391371859:323,Deployability,release,releases,323,"@jonn-smith I have sucessfully built GATK by these commands. git clone https://github.com/broadinstitute/gatk; gradlew bundle. I've got this zip file in folder ""build"".; gatk-4.0.4.0-34-g2cc7abd-SNAPSHOT.zip. So I unzipped and used this to replace GATK-4.0.4.0 that I downloaded from https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip. I still found errors. 21:09:33.811 INFO ProgressMeter - Starting traversal; 21:09:33.811 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:09:46.267 INFO ProgressMeter - chr1:24929636 0.2 3000 14453.2; 21:09:59.072 INFO ProgressMeter - chr1:64681324 0.4 6000 14251.2; 21:10:09.456 INFO ProgressMeter - chr1:156245393 0.6 9000 15149.8; 21:10:21.510 INFO ProgressMeter - chr1:206965947 0.8 12000 15094.7; 21:10:26.132 INFO Funcotator - Shutting down engine; [May 23, 2018 9:10:26 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 1.36 minutes.; Runtime.totalMemory()=11500781568; java.lang.IllegalArgumentException: Genomic positions must be > 0.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:722). What should I do? Can you send me one that is ready-to-use? Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391371859
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391371859:1134,Security,validat,validateArg,1134,"@jonn-smith I have sucessfully built GATK by these commands. git clone https://github.com/broadinstitute/gatk; gradlew bundle. I've got this zip file in folder ""build"".; gatk-4.0.4.0-34-g2cc7abd-SNAPSHOT.zip. So I unzipped and used this to replace GATK-4.0.4.0 that I downloaded from https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip. I still found errors. 21:09:33.811 INFO ProgressMeter - Starting traversal; 21:09:33.811 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 21:09:46.267 INFO ProgressMeter - chr1:24929636 0.2 3000 14453.2; 21:09:59.072 INFO ProgressMeter - chr1:64681324 0.4 6000 14251.2; 21:10:09.456 INFO ProgressMeter - chr1:156245393 0.6 9000 15149.8; 21:10:21.510 INFO ProgressMeter - chr1:206965947 0.8 12000 15094.7; 21:10:26.132 INFO Funcotator - Shutting down engine; [May 23, 2018 9:10:26 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 1.36 minutes.; Runtime.totalMemory()=11500781568; java.lang.IllegalArgumentException: Genomic positions must be > 0.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:722). What should I do? Can you send me one that is ready-to-use? Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391371859
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032:4228,Availability,down,down,4228,"vcf.gz; 23:24:51.451 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///omics/chatchawit/bundle/dsrc/oreganno/hg38/oreganno.tsv; 23:24:51.535 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/dsrc/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; WARNING	2018-05-23 23:24:53	AsciiLineReader	Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 23:25:09.380 INFO ProgressMeter - Starting traversal; 23:25:09.381 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:25:20.674 INFO ProgressMeter - chr1:24929636 0.2 3000 15941.9; 23:25:42.601 INFO ProgressMeter - chr1:64681324 0.6 6000 10837.2; 23:25:54.659 INFO ProgressMeter - chr1:156245393 0.8 9000 11926.3; 23:26:06.846 INFO ProgressMeter - chr1:206965947 1.0 12000 12529.6; 23:26:12.318 INFO Funcotator - Shutting down engine; [May 23, 2018 11:26:12 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 1.38 minutes.; Runtime.totalMemory()=10974920704; java.lang.IllegalArgumentException: Genomic positions must be > 0.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:722); 	at org.broadinstitute.hellbender.utils.param.ParamUtils.isPositive(ParamUtils.java:153); 	at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.getAlignedPosition(FuncotatorUtils.java:336); 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSequenceComparison(GencodeFuncotationFactory.java:1392); 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createCodingRegionFuncotationForProteinCodingFeature(GencodeFuncotationFactory.java:751); 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createExonFuncotation(GencodeFuncotationFactory",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032:2254,Deployability,patch,patch,2254,"untime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 23:24:49.935 INFO Funcotator - Start Date/Time: May 23, 2018 11:24:49 PM ICT; 23:24:49.935 INFO Funcotator - ------------------------------------------------------------; 23:24:49.935 INFO Funcotator - ------------------------------------------------------------; 23:24:49.936 INFO Funcotator - HTSJDK Version: 2.15.0; 23:24:49.936 INFO Funcotator - Picard Version: 2.18.2; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:24:49.937 INFO Funcotator - Deflater: IntelDeflater; 23:24:49.937 INFO Funcotator - Inflater: IntelInflater; 23:24:49.937 INFO Funcotator - GCS max retries/reopens: 20; 23:24:49.937 INFO Funcotator - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 23:24:49.937 WARN Funcotator - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: Funcotator is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 23:24:49.937 INFO Funcotator - Initializing engine; 23:24:51.025 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/sm/out/test.vcf; 23:24:51.169 INFO Funcotator - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:24:51.204 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/bundle/dsrc/dbsnp/hg38/hg38_All_20170710.vcf.gz; 23:24:51.451 INFO Fea",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032:6741,Integrability,wrap,wrapAndCopyInto,6741,titute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:137); 	at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:113); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.enqueueAndHandleVariant(Funcotator.java:496); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:387); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:110); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:108); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:918); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032:659,Performance,Load,Loading,659,"Please find the stack trace below. Hopefully helpful. Using GATK jar /omics/chatchawit/gatk3/gatk-package-4.0.4.0-34-g2cc7abd-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/chatchawit/gatk3/gatk-package-4.0.4.0-34-g2cc7abd-SNAPSHOT-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/test.vcf -O /omics/chatchawit/sm/anno/test.vcf --output-file-format VCF --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38; 23:24:49.725 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk3/gatk-package-4.0.4.0-34-g2cc7abd-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:24:49.934 INFO Funcotator - ------------------------------------------------------------; 23:24:49.934 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.4.0-34-g2cc7abd-SNAPSHOT-0.0.3; 23:24:49.934 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:24:49.935 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 23:24:49.935 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 23:24:49.935 INFO Funcotator - Start Date/Time: May 23, 2018 11:24:49 PM ICT; 23:24:49.935 INFO Funcotator - ------------------------------------------------------------; 23:24:49.935 INFO Funcotator - ------------------------------------------------------------; 23:24:49.936 INFO Funcotator - HTSJDK Version: 2.15.0; 23:24:49.936 INFO Funcotator - Picard Version: 2.18.2; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_W",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032:4514,Security,validat,validateArg,4514,"de.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; WARNING	2018-05-23 23:24:53	AsciiLineReader	Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 23:25:09.380 INFO ProgressMeter - Starting traversal; 23:25:09.381 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:25:20.674 INFO ProgressMeter - chr1:24929636 0.2 3000 15941.9; 23:25:42.601 INFO ProgressMeter - chr1:64681324 0.6 6000 10837.2; 23:25:54.659 INFO ProgressMeter - chr1:156245393 0.8 9000 11926.3; 23:26:06.846 INFO ProgressMeter - chr1:206965947 1.0 12000 12529.6; 23:26:12.318 INFO Funcotator - Shutting down engine; [May 23, 2018 11:26:12 PM ICT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 1.38 minutes.; Runtime.totalMemory()=10974920704; java.lang.IllegalArgumentException: Genomic positions must be > 0.; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:722); 	at org.broadinstitute.hellbender.utils.param.ParamUtils.isPositive(ParamUtils.java:153); 	at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.getAlignedPosition(FuncotatorUtils.java:336); 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createSequenceComparison(GencodeFuncotationFactory.java:1392); 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createCodingRegionFuncotationForProteinCodingFeature(GencodeFuncotationFactory.java:751); 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createExonFuncotation(GencodeFuncotationFactory.java:649); 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.createGencodeFuncotationOnTranscript(GencodeFuncotationFactory.java:609); 	at org.broadinstitute.hellbender.tools.funcotator.dataSources.gencode.GencodeFuncotationFactory.create",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032:476,Testability,test,test,476,"Please find the stack trace below. Hopefully helpful. Using GATK jar /omics/chatchawit/gatk3/gatk-package-4.0.4.0-34-g2cc7abd-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/chatchawit/gatk3/gatk-package-4.0.4.0-34-g2cc7abd-SNAPSHOT-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/test.vcf -O /omics/chatchawit/sm/anno/test.vcf --output-file-format VCF --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38; 23:24:49.725 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk3/gatk-package-4.0.4.0-34-g2cc7abd-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:24:49.934 INFO Funcotator - ------------------------------------------------------------; 23:24:49.934 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.4.0-34-g2cc7abd-SNAPSHOT-0.0.3; 23:24:49.934 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:24:49.935 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 23:24:49.935 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 23:24:49.935 INFO Funcotator - Start Date/Time: May 23, 2018 11:24:49 PM ICT; 23:24:49.935 INFO Funcotator - ------------------------------------------------------------; 23:24:49.935 INFO Funcotator - ------------------------------------------------------------; 23:24:49.936 INFO Funcotator - HTSJDK Version: 2.15.0; 23:24:49.936 INFO Funcotator - Picard Version: 2.18.2; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_W",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032:514,Testability,test,test,514,"Please find the stack trace below. Hopefully helpful. Using GATK jar /omics/chatchawit/gatk3/gatk-package-4.0.4.0-34-g2cc7abd-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /omics/chatchawit/gatk3/gatk-package-4.0.4.0-34-g2cc7abd-SNAPSHOT-local.jar Funcotator -R /omics/chatchawit/bundle/hsa38.fasta -V /omics/chatchawit/sm/out/test.vcf -O /omics/chatchawit/sm/anno/test.vcf --output-file-format VCF --data-sources-path /omics/chatchawit/bundle/dsrc/ --ref-version hg38; 23:24:49.725 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/omics/chatchawit/gatk3/gatk-package-4.0.4.0-34-g2cc7abd-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:24:49.934 INFO Funcotator - ------------------------------------------------------------; 23:24:49.934 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.0.4.0-34-g2cc7abd-SNAPSHOT-0.0.3; 23:24:49.934 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:24:49.935 INFO Funcotator - Executing as chatchawit@omics on Linux v3.13.0-133-generic amd64; 23:24:49.935 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 23:24:49.935 INFO Funcotator - Start Date/Time: May 23, 2018 11:24:49 PM ICT; 23:24:49.935 INFO Funcotator - ------------------------------------------------------------; 23:24:49.935 INFO Funcotator - ------------------------------------------------------------; 23:24:49.936 INFO Funcotator - HTSJDK Version: 2.15.0; 23:24:49.936 INFO Funcotator - Picard Version: 2.18.2; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_W",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032:2763,Testability,test,test,2763,tator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:24:49.937 INFO Funcotator - Deflater: IntelDeflater; 23:24:49.937 INFO Funcotator - Inflater: IntelInflater; 23:24:49.937 INFO Funcotator - GCS max retries/reopens: 20; 23:24:49.937 INFO Funcotator - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 23:24:49.937 WARN Funcotator - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: Funcotator is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 23:24:49.937 INFO Funcotator - Initializing engine; 23:24:51.025 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/sm/out/test.vcf; 23:24:51.169 INFO Funcotator - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:24:51.204 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/bundle/dsrc/dbsnp/hg38/hg38_All_20170710.vcf.gz; 23:24:51.451 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///omics/chatchawit/bundle/dsrc/oreganno/hg38/oreganno.tsv; 23:24:51.535 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/dsrc/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; WARNING	2018-05-23 23:24:53	AsciiLineReader	Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBu,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032:2873,Testability,log,logger,2873,R_SAMTOOLS : false; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:24:49.936 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:24:49.937 INFO Funcotator - Deflater: IntelDeflater; 23:24:49.937 INFO Funcotator - Inflater: IntelInflater; 23:24:49.937 INFO Funcotator - GCS max retries/reopens: 20; 23:24:49.937 INFO Funcotator - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 23:24:49.937 WARN Funcotator - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: Funcotator is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 23:24:49.937 INFO Funcotator - Initializing engine; 23:24:51.025 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/sm/out/test.vcf; 23:24:51.169 INFO Funcotator - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:24:51.204 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/bundle/dsrc/dbsnp/hg38/hg38_All_20170710.vcf.gz; 23:24:51.451 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///omics/chatchawit/bundle/dsrc/oreganno/hg38/oreganno.tsv; 23:24:51.535 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/dsrc/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; WARNING	2018-05-23 23:24:53	AsciiLineReader	Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 23:25:09.380 INFO ProgressMeter - Starting traversal; 23:25:09.381 ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032:3038,Testability,log,logging,3038,24:49.937 INFO Funcotator - Deflater: IntelDeflater; 23:24:49.937 INFO Funcotator - Inflater: IntelInflater; 23:24:49.937 INFO Funcotator - GCS max retries/reopens: 20; 23:24:49.937 INFO Funcotator - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 23:24:49.937 WARN Funcotator - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: Funcotator is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 23:24:49.937 INFO Funcotator - Initializing engine; 23:24:51.025 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/sm/out/test.vcf; 23:24:51.169 INFO Funcotator - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.dataSources.DataSourceUtils).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:24:51.204 INFO FeatureManager - Using codec VCFCodec to read file file:///omics/chatchawit/bundle/dsrc/dbsnp/hg38/hg38_All_20170710.vcf.gz; 23:24:51.451 INFO FeatureManager - Using codec XsvLocatableTableCodec to read file file:///omics/chatchawit/bundle/dsrc/oreganno/hg38/oreganno.tsv; 23:24:51.535 INFO FeatureManager - Using codec GencodeGtfCodec to read file file:///omics/chatchawit/bundle/dsrc/gencode/hg38/gencode.v27.chr_patch_hapl_scaff.annotation.REORDERED.gtf; WARNING	2018-05-23 23:24:53	AsciiLineReader	Creating an indexable source for an AsciiFeatureCodec using a stream that is neither a PositionalBufferedStream nor a BlockCompressedInputStream; 23:25:09.380 INFO ProgressMeter - Starting traversal; 23:25:09.381 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:25:20.674 INFO ProgressMeter - chr1:24929636 0.2 3000 15941.9; 23:25:42.601 INFO ProgressMeter - chr1:6468,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391421032
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391468128:241,Modifiability,refactor,refactoring,241,"@chatchawit Thanks. From the stack trace it looks like another issue that I'm actually currently working on. I'm not sure when it will be done, but I'm going through it as part of fixing #4739 (it's an aggravating bug that will require some refactoring).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-391468128
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-396631902:62,Deployability,release,release,62,"@chatchawit If you haven't already, try again with the latest release. It should help. There are also updated Data Sources in the usual place.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-396631902
https://github.com/broadinstitute/gatk/issues/4712#issuecomment-396631902:102,Deployability,update,updated,102,"@chatchawit If you haven't already, try again with the latest release. It should help. There are also updated Data Sources in the usual place.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4712#issuecomment-396631902
https://github.com/broadinstitute/gatk/issues/4713#issuecomment-385053920:79,Testability,test,tests,79,"Perhaps @tedsharpe would have some thoughts---seems related to hist work on KS tests?. EDIT: Made a curiously appropriate typo, keepin' it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4713#issuecomment-385053920
https://github.com/broadinstitute/gatk/issues/4713#issuecomment-385056876:23,Deployability,pipeline,pipeline,23,"There's code in the SV pipeline that does exactly this: it takes a pass on the BAM to determine fragment length statistics, and then takes another pass to determine bins that have significantly deviant statistics. Seems to work pretty well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4713#issuecomment-385056876
https://github.com/broadinstitute/gatk/pull/4715#issuecomment-385072115:2896,Testability,test,test,2896,uamF2YQ==) | `100% <ø> (ø)` | `1 <0> (ø)` | :arrow_down: |; | [...ellbender/cmdline/StandardArgumentDefinitions.java](https://codecov.io/gh/broadinstitute/gatk/pull/4715/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL1N0YW5kYXJkQXJndW1lbnREZWZpbml0aW9ucy5qYXZh) | `0% <ø> (ø)` | `0 <0> (ø)` | :arrow_down: |; | [...ections/MarkDuplicatesSparkArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4715/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL2FyZ3VtZW50Y29sbGVjdGlvbnMvTWFya0R1cGxpY2F0ZXNTcGFya0FyZ3VtZW50Q29sbGVjdGlvbi5qYXZh) | `100% <100%> (ø)` | `1 <0> (ø)` | :arrow_down: |; | [...ates/AbstractMarkDuplicatesCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/4715/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL0Fic3RyYWN0TWFya0R1cGxpY2F0ZXNDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `93.333% <100%> (ø)` | `24 <0> (ø)` | :arrow_down: |; | [...ls/read/markduplicates/OpticalDuplicateFinder.java](https://codecov.io/gh/broadinstitute/gatk/pull/4715/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL09wdGljYWxEdXBsaWNhdGVGaW5kZXIuamF2YQ==) | `73.034% <50%> (ø)` | `28 <0> (ø)` | :arrow_down: |; | [...ils/test/testers/AbstractMarkDuplicatesTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/4715/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L3Rlc3RlcnMvQWJzdHJhY3RNYXJrRHVwbGljYXRlc1Rlc3Rlci5qYXZh) | `79.487% <66.667%> (ø)` | `17 <0> (ø)` | :arrow_down: |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/4715/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (-30%)` | `1% <0%> (-2%)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4715#issuecomment-385072115
https://github.com/broadinstitute/gatk/pull/4715#issuecomment-385072115:2901,Testability,test,testers,2901,uamF2YQ==) | `100% <ø> (ø)` | `1 <0> (ø)` | :arrow_down: |; | [...ellbender/cmdline/StandardArgumentDefinitions.java](https://codecov.io/gh/broadinstitute/gatk/pull/4715/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL1N0YW5kYXJkQXJndW1lbnREZWZpbml0aW9ucy5qYXZh) | `0% <ø> (ø)` | `0 <0> (ø)` | :arrow_down: |; | [...ections/MarkDuplicatesSparkArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4715/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL2FyZ3VtZW50Y29sbGVjdGlvbnMvTWFya0R1cGxpY2F0ZXNTcGFya0FyZ3VtZW50Q29sbGVjdGlvbi5qYXZh) | `100% <100%> (ø)` | `1 <0> (ø)` | :arrow_down: |; | [...ates/AbstractMarkDuplicatesCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/4715/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL0Fic3RyYWN0TWFya0R1cGxpY2F0ZXNDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `93.333% <100%> (ø)` | `24 <0> (ø)` | :arrow_down: |; | [...ls/read/markduplicates/OpticalDuplicateFinder.java](https://codecov.io/gh/broadinstitute/gatk/pull/4715/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL09wdGljYWxEdXBsaWNhdGVGaW5kZXIuamF2YQ==) | `73.034% <50%> (ø)` | `28 <0> (ø)` | :arrow_down: |; | [...ils/test/testers/AbstractMarkDuplicatesTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/4715/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L3Rlc3RlcnMvQWJzdHJhY3RNYXJrRHVwbGljYXRlc1Rlc3Rlci5qYXZh) | `79.487% <66.667%> (ø)` | `17 <0> (ø)` | :arrow_down: |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/4715/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (-30%)` | `1% <0%> (-2%)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4715#issuecomment-385072115
https://github.com/broadinstitute/gatk/issues/4716#issuecomment-385468513:100,Deployability,release,release,100,"@cwhelan @ldgauthier @kgururaj informs me that he's already fixed this in GenomicsDB, and will do a release soon. Once it's out, there will be a PR later this week to update GATK to the latest version.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4716#issuecomment-385468513
https://github.com/broadinstitute/gatk/issues/4716#issuecomment-385468513:167,Deployability,update,update,167,"@cwhelan @ldgauthier @kgururaj informs me that he's already fixed this in GenomicsDB, and will do a release soon. Once it's out, there will be a PR later this week to update GATK to the latest version.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4716#issuecomment-385468513
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385654418:432,Availability,reliab,reliable,432,"Standard disclaimer up front that I'm still a bit new and can't speak to; the needs of the GATK outside the small bits I've looked at. The data I've wanted so far had all come from the UCSC genome browser,; which has a reasonable format and good documentation. My personal feeling; is there's little reason to reformat the data or choose a different format; for data we may create. For me the real problem is establishing fast and; reliable look up of that data. I've so far just thrown a couple light; weight tracts into the resources on my git branch and loaded them into an; interval tree. That storage strategy will not scale for all of the UCSC; data, and in any case might antagonize them. On Mon, Apr 30, 2018, 11:59 PM samuelklee <notifications@github.com> wrote:. > I tend to agree. VCF is perhaps a special case of the format we actually; > need, so no point in shoehorning output that doesn’t fit just for the sake; > of standardization. @sooheelee <https://github.com/sooheelee> may feel; > more strongly otherwise.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385593864>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGKhCLeOf8aOnroZA0wirz8zeBoeodNsks5tt92xgaJpZM4TtIPq>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385654418
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385654418:557,Performance,load,loaded,557,"Standard disclaimer up front that I'm still a bit new and can't speak to; the needs of the GATK outside the small bits I've looked at. The data I've wanted so far had all come from the UCSC genome browser,; which has a reasonable format and good documentation. My personal feeling; is there's little reason to reformat the data or choose a different format; for data we may create. For me the real problem is establishing fast and; reliable look up of that data. I've so far just thrown a couple light; weight tracts into the resources on my git branch and loaded them into an; interval tree. That storage strategy will not scale for all of the UCSC; data, and in any case might antagonize them. On Mon, Apr 30, 2018, 11:59 PM samuelklee <notifications@github.com> wrote:. > I tend to agree. VCF is perhaps a special case of the format we actually; > need, so no point in shoehorning output that doesn’t fit just for the sake; > of standardization. @sooheelee <https://github.com/sooheelee> may feel; > more strongly otherwise.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385593864>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGKhCLeOf8aOnroZA0wirz8zeBoeodNsks5tt92xgaJpZM4TtIPq>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385654418
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385656468:1078,Availability,down,downside,1078,"@TedBrookings which formats are you using, in particular? If there is a format out there that nicely fits our needs, we should adopt it. In the CNV package, I've taken pains to unify how tabular data are represented in Java, depending on whether each record is Locatable or whether the collection of records can be associated with a sample name or sequence dictionary. This allows us to represent records that extend Locatable with *multidimensional numerical or non-numerical annotations* along with some metadata (sample name and sequence dictionary) with a minimum of boilerplate. There are also base methods for producing interval trees, etc. This pretty much satisfies all of the CNV team's needs (and is, in my opinion, a necessary improvement over the horrowshow of read/write utility methods for each file format that we had previously...). However, this unification effort was a quick push I made before release, so some polishing or redesigning may be warranted. We may also want to add more forms of metadata, etc. if other teams would require more features. Another downside is that this code lacks the indexing, NIO support, etc. that some of the other standardized/Tribble formats enjoy. For CNV data, this isn't a huge issue, but I think it would be nice to unify how we represent such data GATK-wide. As I said above, I don't think VCF is the correct answer, but certainly it could fit into whatever framework we adopt or come up with.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385656468
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385656468:913,Deployability,release,release,913,"@TedBrookings which formats are you using, in particular? If there is a format out there that nicely fits our needs, we should adopt it. In the CNV package, I've taken pains to unify how tabular data are represented in Java, depending on whether each record is Locatable or whether the collection of records can be associated with a sample name or sequence dictionary. This allows us to represent records that extend Locatable with *multidimensional numerical or non-numerical annotations* along with some metadata (sample name and sequence dictionary) with a minimum of boilerplate. There are also base methods for producing interval trees, etc. This pretty much satisfies all of the CNV team's needs (and is, in my opinion, a necessary improvement over the horrowshow of read/write utility methods for each file format that we had previously...). However, this unification effort was a quick push I made before release, so some polishing or redesigning may be warranted. We may also want to add more forms of metadata, etc. if other teams would require more features. Another downside is that this code lacks the indexing, NIO support, etc. that some of the other standardized/Tribble formats enjoy. For CNV data, this isn't a huge issue, but I think it would be nice to unify how we represent such data GATK-wide. As I said above, I don't think VCF is the correct answer, but certainly it could fit into whatever framework we adopt or come up with.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385656468
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385656468:225,Integrability,depend,depending,225,"@TedBrookings which formats are you using, in particular? If there is a format out there that nicely fits our needs, we should adopt it. In the CNV package, I've taken pains to unify how tabular data are represented in Java, depending on whether each record is Locatable or whether the collection of records can be associated with a sample name or sequence dictionary. This allows us to represent records that extend Locatable with *multidimensional numerical or non-numerical annotations* along with some metadata (sample name and sequence dictionary) with a minimum of boilerplate. There are also base methods for producing interval trees, etc. This pretty much satisfies all of the CNV team's needs (and is, in my opinion, a necessary improvement over the horrowshow of read/write utility methods for each file format that we had previously...). However, this unification effort was a quick push I made before release, so some polishing or redesigning may be warranted. We may also want to add more forms of metadata, etc. if other teams would require more features. Another downside is that this code lacks the indexing, NIO support, etc. that some of the other standardized/Tribble formats enjoy. For CNV data, this isn't a huge issue, but I think it would be nice to unify how we represent such data GATK-wide. As I said above, I don't think VCF is the correct answer, but certainly it could fit into whatever framework we adopt or come up with.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385656468
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385656468:410,Modifiability,extend,extend,410,"@TedBrookings which formats are you using, in particular? If there is a format out there that nicely fits our needs, we should adopt it. In the CNV package, I've taken pains to unify how tabular data are represented in Java, depending on whether each record is Locatable or whether the collection of records can be associated with a sample name or sequence dictionary. This allows us to represent records that extend Locatable with *multidimensional numerical or non-numerical annotations* along with some metadata (sample name and sequence dictionary) with a minimum of boilerplate. There are also base methods for producing interval trees, etc. This pretty much satisfies all of the CNV team's needs (and is, in my opinion, a necessary improvement over the horrowshow of read/write utility methods for each file format that we had previously...). However, this unification effort was a quick push I made before release, so some polishing or redesigning may be warranted. We may also want to add more forms of metadata, etc. if other teams would require more features. Another downside is that this code lacks the indexing, NIO support, etc. that some of the other standardized/Tribble formats enjoy. For CNV data, this isn't a huge issue, but I think it would be nice to unify how we represent such data GATK-wide. As I said above, I don't think VCF is the correct answer, but certainly it could fit into whatever framework we adopt or come up with.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385656468
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385683551:2201,Availability,down,downside,2201,"97968 347968 contig; >; > chr1 535988 585988 contig; >; > chr1 2702781 2746290 scaffold; >; >; For what it's worth, your description of your approach sounds like a; sensible one to me.; I am concerned about the size of the data and how we'd access it. I've; chosen the tracts I have because they are small enough to jam into; resources. On Tue, May 1, 2018 at 8:06 AM samuelklee <notifications@github.com> wrote:. > @TedBrookings <https://github.com/TedBrookings> which formats are you; > using, in particular?; >; > In the CNV package, I've taken pains to unify how tabular data are; > represented in Java, depending on whether each record is Locatable or; > whether the collection of records can be associated with a sample name or; > sequence dictionary. This allows us to represent records that extend; > Locatable with multidimensional numerical or non-numerical annotations; > along with some metadata (sample name and sequence dictionary) with a; > minimum of boilerplate. There are also base methods for producing interval; > trees, etc.; >; > However, this unification effort was a quick push I made before release,; > so some polishing or redesigning may be warranted. We may also want to add; > more forms of metadata, etc. if other teams would require more features.; > Another downside is that this code lacks the indexing, NIO support, etc.; > that some of the other standardized/Tribble formats enjoy. For CNV data,; > this isn't a huge issue, but I think it would be nice to unify how we; > represent such data GATK-wide. As I said above, I don't think VCF is the; > correct answer, but certainly it could fit into whatever framework we come; > up with.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385656468>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGKhCDUca-ZFaXWUoM6bn-LrGlgzx8jDks5tuE_QgaJpZM4TtIPq>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385683551
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385683551:2027,Deployability,release,release,2027,"97968 347968 contig; >; > chr1 535988 585988 contig; >; > chr1 2702781 2746290 scaffold; >; >; For what it's worth, your description of your approach sounds like a; sensible one to me.; I am concerned about the size of the data and how we'd access it. I've; chosen the tracts I have because they are small enough to jam into; resources. On Tue, May 1, 2018 at 8:06 AM samuelklee <notifications@github.com> wrote:. > @TedBrookings <https://github.com/TedBrookings> which formats are you; > using, in particular?; >; > In the CNV package, I've taken pains to unify how tabular data are; > represented in Java, depending on whether each record is Locatable or; > whether the collection of records can be associated with a sample name or; > sequence dictionary. This allows us to represent records that extend; > Locatable with multidimensional numerical or non-numerical annotations; > along with some metadata (sample name and sequence dictionary) with a; > minimum of boilerplate. There are also base methods for producing interval; > trees, etc.; >; > However, this unification effort was a quick push I made before release,; > so some polishing or redesigning may be warranted. We may also want to add; > more forms of metadata, etc. if other teams would require more features.; > Another downside is that this code lacks the indexing, NIO support, etc.; > that some of the other standardized/Tribble formats enjoy. For CNV data,; > this isn't a huge issue, but I think it would be nice to unify how we; > represent such data GATK-wide. As I said above, I don't think VCF is the; > correct answer, but certainly it could fit into whatever framework we come; > up with.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385656468>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGKhCDUca-ZFaXWUoM6bn-LrGlgzx8jDks5tuE_QgaJpZM4TtIPq>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385683551
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385683551:1519,Integrability,depend,depending,1519,"ption of the columns (each being of form; tract_name.column_name). There's nothing at all sophisticated about this; format, but it's pretty generalizable and easy to parse (and create). An; example; >; > # hgIntegrator: database=hg38 region=genome Wed Apr 18 11:15:34 2018; >; > #gap.chrom gap.chromStart gap.chromEnd gap.type; >; > chr1 0 10000 telomere; >; > chr1 207666 257666 contig; >; > chr1 297968 347968 contig; >; > chr1 535988 585988 contig; >; > chr1 2702781 2746290 scaffold; >; >; For what it's worth, your description of your approach sounds like a; sensible one to me.; I am concerned about the size of the data and how we'd access it. I've; chosen the tracts I have because they are small enough to jam into; resources. On Tue, May 1, 2018 at 8:06 AM samuelklee <notifications@github.com> wrote:. > @TedBrookings <https://github.com/TedBrookings> which formats are you; > using, in particular?; >; > In the CNV package, I've taken pains to unify how tabular data are; > represented in Java, depending on whether each record is Locatable or; > whether the collection of records can be associated with a sample name or; > sequence dictionary. This allows us to represent records that extend; > Locatable with multidimensional numerical or non-numerical annotations; > along with some metadata (sample name and sequence dictionary) with a; > minimum of boilerplate. There are also base methods for producing interval; > trees, etc.; >; > However, this unification effort was a quick push I made before release,; > so some polishing or redesigning may be warranted. We may also want to add; > more forms of metadata, etc. if other teams would require more features.; > Another downside is that this code lacks the indexing, NIO support, etc.; > that some of the other standardized/Tribble formats enjoy. For CNV data,; > this isn't a huge issue, but I think it would be nice to unify how we; > represent such data GATK-wide. As I said above, I don't think VCF is the; > correct answer, but",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385683551
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385683551:1710,Modifiability,extend,extend,1710," 2018; >; > #gap.chrom gap.chromStart gap.chromEnd gap.type; >; > chr1 0 10000 telomere; >; > chr1 207666 257666 contig; >; > chr1 297968 347968 contig; >; > chr1 535988 585988 contig; >; > chr1 2702781 2746290 scaffold; >; >; For what it's worth, your description of your approach sounds like a; sensible one to me.; I am concerned about the size of the data and how we'd access it. I've; chosen the tracts I have because they are small enough to jam into; resources. On Tue, May 1, 2018 at 8:06 AM samuelklee <notifications@github.com> wrote:. > @TedBrookings <https://github.com/TedBrookings> which formats are you; > using, in particular?; >; > In the CNV package, I've taken pains to unify how tabular data are; > represented in Java, depending on whether each record is Locatable or; > whether the collection of records can be associated with a sample name or; > sequence dictionary. This allows us to represent records that extend; > Locatable with multidimensional numerical or non-numerical annotations; > along with some metadata (sample name and sequence dictionary) with a; > minimum of boilerplate. There are also base methods for producing interval; > trees, etc.; >; > However, this unification effort was a quick push I made before release,; > so some polishing or redesigning may be warranted. We may also want to add; > more forms of metadata, etc. if other teams would require more features.; > Another downside is that this code lacks the indexing, NIO support, etc.; > that some of the other standardized/Tribble formats enjoy. For CNV data,; > this isn't a huge issue, but I think it would be nice to unify how we; > represent such data GATK-wide. As I said above, I don't think VCF is the; > correct answer, but certainly it could fit into whatever framework we come; > up with.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385656468>,; > or",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385683551
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385683551:121,Performance,perform,performance,121,"I'm still not very familiar with the way people have used extensions of; Locatable, or how indexing can be used to boost performance. The stuff I've; worked on is a bit of a corner case, and I didn't write much of the; infrastructure, I've been tacking on features. For now I've just been keeping the gzipped text files from the UCSC; browser. They're tab delimited with two header lines, the first basically; giving info about context of the data (it's genome data for the , hg38) and; the second being a description of the columns (each being of form; tract_name.column_name). There's nothing at all sophisticated about this; format, but it's pretty generalizable and easy to parse (and create). An; example; >; > # hgIntegrator: database=hg38 region=genome Wed Apr 18 11:15:34 2018; >; > #gap.chrom gap.chromStart gap.chromEnd gap.type; >; > chr1 0 10000 telomere; >; > chr1 207666 257666 contig; >; > chr1 297968 347968 contig; >; > chr1 535988 585988 contig; >; > chr1 2702781 2746290 scaffold; >; >; For what it's worth, your description of your approach sounds like a; sensible one to me.; I am concerned about the size of the data and how we'd access it. I've; chosen the tracts I have because they are small enough to jam into; resources. On Tue, May 1, 2018 at 8:06 AM samuelklee <notifications@github.com> wrote:. > @TedBrookings <https://github.com/TedBrookings> which formats are you; > using, in particular?; >; > In the CNV package, I've taken pains to unify how tabular data are; > represented in Java, depending on whether each record is Locatable or; > whether the collection of records can be associated with a sample name or; > sequence dictionary. This allows us to represent records that extend; > Locatable with multidimensional numerical or non-numerical annotations; > along with some metadata (sample name and sequence dictionary) with a; > minimum of boilerplate. There are also base methods for producing interval; > trees, etc.; >; > However, this unification effort was a",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385683551
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385683551:1152,Security,access,access,1152,"ance. The stuff I've; worked on is a bit of a corner case, and I didn't write much of the; infrastructure, I've been tacking on features. For now I've just been keeping the gzipped text files from the UCSC; browser. They're tab delimited with two header lines, the first basically; giving info about context of the data (it's genome data for the , hg38) and; the second being a description of the columns (each being of form; tract_name.column_name). There's nothing at all sophisticated about this; format, but it's pretty generalizable and easy to parse (and create). An; example; >; > # hgIntegrator: database=hg38 region=genome Wed Apr 18 11:15:34 2018; >; > #gap.chrom gap.chromStart gap.chromEnd gap.type; >; > chr1 0 10000 telomere; >; > chr1 207666 257666 contig; >; > chr1 297968 347968 contig; >; > chr1 535988 585988 contig; >; > chr1 2702781 2746290 scaffold; >; >; For what it's worth, your description of your approach sounds like a; sensible one to me.; I am concerned about the size of the data and how we'd access it. I've; chosen the tracts I have because they are small enough to jam into; resources. On Tue, May 1, 2018 at 8:06 AM samuelklee <notifications@github.com> wrote:. > @TedBrookings <https://github.com/TedBrookings> which formats are you; > using, in particular?; >; > In the CNV package, I've taken pains to unify how tabular data are; > represented in Java, depending on whether each record is Locatable or; > whether the collection of records can be associated with a sample name or; > sequence dictionary. This allows us to represent records that extend; > Locatable with multidimensional numerical or non-numerical annotations; > along with some metadata (sample name and sequence dictionary) with a; > minimum of boilerplate. There are also base methods for producing interval; > trees, etc.; >; > However, this unification effort was a quick push I made before release,; > so some polishing or redesigning may be warranted. We may also want to add; > more forms o",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385683551
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385748457:476,Modifiability,variab,variables,476,"I'm more familiar with working with interval data files so I can't speak much to variant call formats except that VCF tends to be a bit wonky with interval data (such as SV calls) because, as @samuelklee mentioned, most of the fields are likely to be irrelevant and thus waste space. I feel strongly that BED-like formats are the way to go for interval data, i.e. #contig start end x1 x2 x3 x4 ...; chr1 2938 3949 3.9 0 + cat ...; ... where x1, x2, x3, x4 ... are columns for variables of different types (which could be specified by additional header lines like in VCF).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-385748457
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386121869:730,Security,sanitiz,sanitized,730,"Annotations in VCF are a nightmare due to format requirements. I'd recommend against using VCF to store annotations unless it's absolutely necessary. The mechanism to do so is too unwieldy - either you add annotations by name per allele as real annotations (i.e. accounted for in the header and then added to the INFO field as applicable with per-allele annotations separated by commas), or you add them to the INFO field as a pipe-delimited single annotation field, with commas separating this long annotation for each allele (this is currently what Funcotator does). Both are kind of gross, with the former taking a LOT of extra space and the latter being basically unreadable by eye. You also need to make sure annotations are sanitized for illegal characters (such as commas). Funcotator has an open issue for this. A tabular format for annotations makes more sense, and, as much as it pains me to suggest it, MAF may be a quick answer here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386121869
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386386120:315,Availability,avail,available,315,"First to clarify, GetPileupSummaries and CollectAllelicCounts tabulate ALT vs. REF counts at SNP sites. These tools do not consider anything larger than 1bp at a time and are basically akin to pileup callers. File size is a sacrifice that I would trade for easy parsing of the data, which the plethora of VCF tools available enable. . - If the unconventionality is the issue, then consider Heng's use of the SAM format for the ALT reference file. He unconventionally uses the format to store CIGAR strings of ALT/HLA/DECOY contigs mapped back to the primary assembly. This file is easy to parse with samtools. @mwalker174's comment gives me an idea. If we need a format that stores intervals, or _blocks_, then we already have one--the GVCF format. This format is already compatible with IGV. Here's an example record:; ```; chr1 58999897 . T <NON_REF> . . END=58999902 GT:DP:GQ:MIN_DP:PL0/0:31:90:31:0,90,1350; ```; The `END` value is interpreted by IGV as the end of the block and the GVCF format is recognized as a valid VCF format, one that uses a symbolic allele demarcated with `<...>`. Symbolic allele representations are described as early as [VCF v4.1 specs](https://samtools.github.io/hts-specs/) and expanded upon in v4.3 specs. I think @jonn-smith you refer to MAF-format functional annotations. Yes? Here we are discussing much less complex variant call level annotations like genotype that would have a few associated numerical values, e.g. 10th, 50th, and 90th confidence interval allele fractions AND copy ratio value for each CN segment. . One last thing of interest towards visualization. I've been reminded recently of some open-source software that is a standard in visualizing somatic data for the biologist in the Cancer Genetics field--[CBioPortal](http://www.cbioportal.org/index.do?cancer_study_id=blca_mskcc_solit_2014&Z_SCORE_THRESHOLD=2.0&RPPA_SCORE_THRESHOLD=2.0&data_priority=0&case_set_id=blca_mskcc_solit_2014_cnaseq&gene_list=RB1%2520RBL1%2520RBL2%2520CCNA1%2520CCNB1%",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386386120
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386386120:2653,Integrability,bridg,bridging,2653,"ations are described as early as [VCF v4.1 specs](https://samtools.github.io/hts-specs/) and expanded upon in v4.3 specs. I think @jonn-smith you refer to MAF-format functional annotations. Yes? Here we are discussing much less complex variant call level annotations like genotype that would have a few associated numerical values, e.g. 10th, 50th, and 90th confidence interval allele fractions AND copy ratio value for each CN segment. . One last thing of interest towards visualization. I've been reminded recently of some open-source software that is a standard in visualizing somatic data for the biologist in the Cancer Genetics field--[CBioPortal](http://www.cbioportal.org/index.do?cancer_study_id=blca_mskcc_solit_2014&Z_SCORE_THRESHOLD=2.0&RPPA_SCORE_THRESHOLD=2.0&data_priority=0&case_set_id=blca_mskcc_solit_2014_cnaseq&gene_list=RB1%2520RBL1%2520RBL2%2520CCNA1%2520CCNB1%2520CDK1%2520CCNE1%2520CDK2%2520CDC25A%2520CCND1%2520CDK4%2520CDK6%2520CCND2%2520CDKN2A%2520CDKN2B%2520MYC%2520CDKN1A%2520CDKN1B%2520E2F1%2520E2F2%2520E2F3%2520E2F4%2520E2F5%2520E2F6%2520E2F7%2520E2F8%2520SRC%2520JAK1%2520JAK2%2520STAT1%2520STAT2%2520STAT3%2520STAT5A%2520STAT5B&geneset_list=+&tab_index=tab_visualize&Action=Submit&genetic_profile_ids_PROFILE_MUTATION_EXTENDED=blca_mskcc_solit_2014_mutations&genetic_profile_ids_PROFILE_COPY_NUMBER_ALTERATION=blca_mskcc_solit_2014_cna) (I plugged in some data--try clicking on the different tabs). They are Dockerized and their repo is at https://github.com/cBioPortal/cbioportal. They do a good job bridging the gap between genomics data and what the biologist is interested in. Their CNA visualization currently takes GISTIC-type data. For segmented CNs, they use a JavaScript version of IGV. It's worth checking out and perhaps enabling GATK users to visualize their somatic data with CBioPortal. . I'd like for us to save all our time for the interesting questions and leverage the hard work of others towards questions such as standarization and visualization.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386386120
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386578762:492,Energy Efficiency,adapt,adapters,492,"I think it's best not to co-opt existing formats for storing *variant calls* and *mutations* if we want to store generic annotations. Furthermore, many of the drawbacks of VCF (e.g, wasted space from repeated tags/unused fields) are really not worth dealing with if our data is strictly tabular and well structured. I think if we can settle on a format internally that satisfies all of our needs, then it'd probably a *very small* amount of effort on the part of external developers to write adapters to consume it. After all, we are only talking about metadata (hopefully in a standardized but suitably flexible format, e.g. SAM/VCF-style header) + tabular data. It may also be that there is a format out there that already fits the bill, in which case we just need to do some more research and discussion. I think this would be better than causing confusion and setting a bad example by co-opting unsuitable formats, even if this would require no additional effort for external developers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386578762
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386578762:492,Integrability,adapter,adapters,492,"I think it's best not to co-opt existing formats for storing *variant calls* and *mutations* if we want to store generic annotations. Furthermore, many of the drawbacks of VCF (e.g, wasted space from repeated tags/unused fields) are really not worth dealing with if our data is strictly tabular and well structured. I think if we can settle on a format internally that satisfies all of our needs, then it'd probably a *very small* amount of effort on the part of external developers to write adapters to consume it. After all, we are only talking about metadata (hopefully in a standardized but suitably flexible format, e.g. SAM/VCF-style header) + tabular data. It may also be that there is a format out there that already fits the bill, in which case we just need to do some more research and discussion. I think this would be better than causing confusion and setting a bad example by co-opting unsuitable formats, even if this would require no additional effort for external developers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386578762
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386578762:492,Modifiability,adapt,adapters,492,"I think it's best not to co-opt existing formats for storing *variant calls* and *mutations* if we want to store generic annotations. Furthermore, many of the drawbacks of VCF (e.g, wasted space from repeated tags/unused fields) are really not worth dealing with if our data is strictly tabular and well structured. I think if we can settle on a format internally that satisfies all of our needs, then it'd probably a *very small* amount of effort on the part of external developers to write adapters to consume it. After all, we are only talking about metadata (hopefully in a standardized but suitably flexible format, e.g. SAM/VCF-style header) + tabular data. It may also be that there is a format out there that already fits the bill, in which case we just need to do some more research and discussion. I think this would be better than causing confusion and setting a bad example by co-opting unsuitable formats, even if this would require no additional effort for external developers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386578762
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386578762:604,Modifiability,flexible,flexible,604,"I think it's best not to co-opt existing formats for storing *variant calls* and *mutations* if we want to store generic annotations. Furthermore, many of the drawbacks of VCF (e.g, wasted space from repeated tags/unused fields) are really not worth dealing with if our data is strictly tabular and well structured. I think if we can settle on a format internally that satisfies all of our needs, then it'd probably a *very small* amount of effort on the part of external developers to write adapters to consume it. After all, we are only talking about metadata (hopefully in a standardized but suitably flexible format, e.g. SAM/VCF-style header) + tabular data. It may also be that there is a format out there that already fits the bill, in which case we just need to do some more research and discussion. I think this would be better than causing confusion and setting a bad example by co-opting unsuitable formats, even if this would require no additional effort for external developers.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386578762
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926:303,Availability,down,downstream,303,"That is a separate matter altogether from both 1) unifying the allele-count collection tools, and 2) standardizing the format of tabular data. The most appropriate place for integration of Mutect2 SNV calls would be as input to the tumor-heterogeneity tool (along with the ModelSegments output) further downstream. This is because it is unlikely that including the SNVs as input to ModelSegments would significantly improve either segmentation or modeling there. If the allele-count collection tools are unified, I think that the only redundant work done across both pipelines would be the calling of hets from the pileups, which is extremely cheap. However, we should certainly also unify the code to do this (which I've spoken to @davidbenjamin about as well).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926:535,Availability,redundant,redundant,535,"That is a separate matter altogether from both 1) unifying the allele-count collection tools, and 2) standardizing the format of tabular data. The most appropriate place for integration of Mutect2 SNV calls would be as input to the tumor-heterogeneity tool (along with the ModelSegments output) further downstream. This is because it is unlikely that including the SNVs as input to ModelSegments would significantly improve either segmentation or modeling there. If the allele-count collection tools are unified, I think that the only redundant work done across both pipelines would be the calling of hets from the pileups, which is extremely cheap. However, we should certainly also unify the code to do this (which I've spoken to @davidbenjamin about as well).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926:174,Deployability,integrat,integration,174,"That is a separate matter altogether from both 1) unifying the allele-count collection tools, and 2) standardizing the format of tabular data. The most appropriate place for integration of Mutect2 SNV calls would be as input to the tumor-heterogeneity tool (along with the ModelSegments output) further downstream. This is because it is unlikely that including the SNVs as input to ModelSegments would significantly improve either segmentation or modeling there. If the allele-count collection tools are unified, I think that the only redundant work done across both pipelines would be the calling of hets from the pileups, which is extremely cheap. However, we should certainly also unify the code to do this (which I've spoken to @davidbenjamin about as well).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926:567,Deployability,pipeline,pipelines,567,"That is a separate matter altogether from both 1) unifying the allele-count collection tools, and 2) standardizing the format of tabular data. The most appropriate place for integration of Mutect2 SNV calls would be as input to the tumor-heterogeneity tool (along with the ModelSegments output) further downstream. This is because it is unlikely that including the SNVs as input to ModelSegments would significantly improve either segmentation or modeling there. If the allele-count collection tools are unified, I think that the only redundant work done across both pipelines would be the calling of hets from the pileups, which is extremely cheap. However, we should certainly also unify the code to do this (which I've spoken to @davidbenjamin about as well).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926:174,Integrability,integrat,integration,174,"That is a separate matter altogether from both 1) unifying the allele-count collection tools, and 2) standardizing the format of tabular data. The most appropriate place for integration of Mutect2 SNV calls would be as input to the tumor-heterogeneity tool (along with the ModelSegments output) further downstream. This is because it is unlikely that including the SNVs as input to ModelSegments would significantly improve either segmentation or modeling there. If the allele-count collection tools are unified, I think that the only redundant work done across both pipelines would be the calling of hets from the pileups, which is extremely cheap. However, we should certainly also unify the code to do this (which I've spoken to @davidbenjamin about as well).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926:535,Safety,redund,redundant,535,"That is a separate matter altogether from both 1) unifying the allele-count collection tools, and 2) standardizing the format of tabular data. The most appropriate place for integration of Mutect2 SNV calls would be as input to the tumor-heterogeneity tool (along with the ModelSegments output) further downstream. This is because it is unlikely that including the SNVs as input to ModelSegments would significantly improve either segmentation or modeling there. If the allele-count collection tools are unified, I think that the only redundant work done across both pipelines would be the calling of hets from the pileups, which is extremely cheap. However, we should certainly also unify the code to do this (which I've spoken to @davidbenjamin about as well).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-386734926
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-387100399:413,Modifiability,extend,extend,413,"Agree with those above arguing that VCF isn't appropriate for this purpose, and would be a very bad fit. I certainly support the goal of adopting a single unified, standard format for tabular data throughout the GATK, however, and would ideally favor a solution at the HTSJDK/tribble level to get all the benefits that that provides, such as support for NIO and indexing (even if it means that engine team has to extend tribble to support records that are not `Locatable`). . We're happy to help with any efforts in the direction of unification, and would be eager to participate in any methods-wide discussions on this topic.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-387100399
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329:1375,Availability,down,downstream,1375,"dGraph only allows a single annotation/track. I'm not sure if the track definition line is intended to hold any metadata other than display parameters, either? https://genome.ucsc.edu/goldenPath/help/bedgraph.html. As for the unmarked column header line, the reason I decided this would be useful in the CNV TSV formats is that it's very easy to throw the table into a pandas or R dataframe for quick analysis, where you can then use the column names to manipulate the table. Typically, pandas/R TSV loading methods let you specify the `@` comment character to strip the SAM header (although we recently ran into some trouble with this in https://github.com/broadinstitute/gatk/pull/581). Note that we *require* a single unmarked column header, which is easy enough to skip (in the case you don't want to use it) if you know it's there. On the other hand, one could argue that if we store the type of each column in the metadata, then any analysis code should technically use that to parse the table (rather than letting pandas/R automatically infer the type of each column). So a marked column header line would make quick analyses a bit more difficult (as users would need to write parsing code), but could encourage more careful downstream code practices. @SHuang-Broad Just to be clear, the way I originally used ""annotation"" refers to any quantity that could be represented by a single type in a column (not in the sense of variant annotation). If string types are allowed, this is indeed pretty flexible! All I care about extracting is the common functionality related to the fact that we have locatable columns. I think the concerns you raise about e.g. SV representation in VCF are a separate matter, but happy to discuss further. I think once we decide what the header needs to be able to represent and what it should look like, this problem is mostly solved. There may be some things to decide about e.g. representation of doubles, NaNs, etc. but I don't think we need to be too rigid here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329:1644,Modifiability,flexible,flexible,1644,"dGraph only allows a single annotation/track. I'm not sure if the track definition line is intended to hold any metadata other than display parameters, either? https://genome.ucsc.edu/goldenPath/help/bedgraph.html. As for the unmarked column header line, the reason I decided this would be useful in the CNV TSV formats is that it's very easy to throw the table into a pandas or R dataframe for quick analysis, where you can then use the column names to manipulate the table. Typically, pandas/R TSV loading methods let you specify the `@` comment character to strip the SAM header (although we recently ran into some trouble with this in https://github.com/broadinstitute/gatk/pull/581). Note that we *require* a single unmarked column header, which is easy enough to skip (in the case you don't want to use it) if you know it's there. On the other hand, one could argue that if we store the type of each column in the metadata, then any analysis code should technically use that to parse the table (rather than letting pandas/R automatically infer the type of each column). So a marked column header line would make quick analyses a bit more difficult (as users would need to write parsing code), but could encourage more careful downstream code practices. @SHuang-Broad Just to be clear, the way I originally used ""annotation"" refers to any quantity that could be represented by a single type in a column (not in the sense of variant annotation). If string types are allowed, this is indeed pretty flexible! All I care about extracting is the common functionality related to the fact that we have locatable columns. I think the concerns you raise about e.g. SV representation in VCF are a separate matter, but happy to discuss further. I think once we decide what the header needs to be able to represent and what it should look like, this problem is mostly solved. There may be some things to decide about e.g. representation of doubles, NaNs, etc. but I don't think we need to be too rigid here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329:643,Performance,load,loading,643,"Thanks for chiming in!. @davidbenjamin can you give a little more detail on the kind of merging operations you'd need?. @tedsharpe I believe bedGraph only allows a single annotation/track. I'm not sure if the track definition line is intended to hold any metadata other than display parameters, either? https://genome.ucsc.edu/goldenPath/help/bedgraph.html. As for the unmarked column header line, the reason I decided this would be useful in the CNV TSV formats is that it's very easy to throw the table into a pandas or R dataframe for quick analysis, where you can then use the column names to manipulate the table. Typically, pandas/R TSV loading methods let you specify the `@` comment character to strip the SAM header (although we recently ran into some trouble with this in https://github.com/broadinstitute/gatk/pull/581). Note that we *require* a single unmarked column header, which is easy enough to skip (in the case you don't want to use it) if you know it's there. On the other hand, one could argue that if we store the type of each column in the metadata, then any analysis code should technically use that to parse the table (rather than letting pandas/R automatically infer the type of each column). So a marked column header line would make quick analyses a bit more difficult (as users would need to write parsing code), but could encourage more careful downstream code practices. @SHuang-Broad Just to be clear, the way I originally used ""annotation"" refers to any quantity that could be represented by a single type in a column (not in the sense of variant annotation). If string types are allowed, this is indeed pretty flexible! All I care about extracting is the common functionality related to the fact that we have locatable columns. I think the concerns you raise about e.g. SV representation in VCF are a separate matter, but happy to discuss further. I think once we decide what the header needs to be able to represent and what it should look like, this problem is most",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329:1427,Usability,clear,clear,1427,"dGraph only allows a single annotation/track. I'm not sure if the track definition line is intended to hold any metadata other than display parameters, either? https://genome.ucsc.edu/goldenPath/help/bedgraph.html. As for the unmarked column header line, the reason I decided this would be useful in the CNV TSV formats is that it's very easy to throw the table into a pandas or R dataframe for quick analysis, where you can then use the column names to manipulate the table. Typically, pandas/R TSV loading methods let you specify the `@` comment character to strip the SAM header (although we recently ran into some trouble with this in https://github.com/broadinstitute/gatk/pull/581). Note that we *require* a single unmarked column header, which is easy enough to skip (in the case you don't want to use it) if you know it's there. On the other hand, one could argue that if we store the type of each column in the metadata, then any analysis code should technically use that to parse the table (rather than letting pandas/R automatically infer the type of each column). So a marked column header line would make quick analyses a bit more difficult (as users would need to write parsing code), but could encourage more careful downstream code practices. @SHuang-Broad Just to be clear, the way I originally used ""annotation"" refers to any quantity that could be represented by a single type in a column (not in the sense of variant annotation). If string types are allowed, this is indeed pretty flexible! All I care about extracting is the common functionality related to the fact that we have locatable columns. I think the concerns you raise about e.g. SV representation in VCF are a separate matter, but happy to discuss further. I think once we decide what the header needs to be able to represent and what it should look like, this problem is mostly solved. There may be some things to decide about e.g. representation of doubles, NaNs, etc. but I don't think we need to be too rigid here.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480917329
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480956381:339,Safety,risk,risk,339,"Thanks for helping me to understand why you didn't mark the metadata. This may seem like quibbling, but I'd suggest that we mark the metadata with a comment character, and let the pandas/R users remove it. They'll notice if they forget to do that, because the columns won't be named as expected, and they'll have to fix it up. Whereas the risk for automated programs is that they'll simply delete the first row, which might be real data if the file has been reordered for some reason, or if the tool implementing the standard is non-compliant. The resulting bugs will be subtle, and might easily go undetected. Building in behavior to delete lines starting with ""CHROM\t"" seems odd and fraught with peril in a way that building in behavior to strip comments, doesn't. That's my take, anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480956381
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480956381:383,Usability,simpl,simply,383,"Thanks for helping me to understand why you didn't mark the metadata. This may seem like quibbling, but I'd suggest that we mark the metadata with a comment character, and let the pandas/R users remove it. They'll notice if they forget to do that, because the columns won't be named as expected, and they'll have to fix it up. Whereas the risk for automated programs is that they'll simply delete the first row, which might be real data if the file has been reordered for some reason, or if the tool implementing the standard is non-compliant. The resulting bugs will be subtle, and might easily go undetected. Building in behavior to delete lines starting with ""CHROM\t"" seems odd and fraught with peril in a way that building in behavior to strip comments, doesn't. That's my take, anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480956381
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480968991:481,Integrability,interface,interfaces,481,"@davidbenjamin OK, we have a utility method in `AbstractLocatableCollection` that essentially enables this for our particular use case (sorting and concatenating sharded tables, which are themselves non-overlapping and sorted, and returning the sort order; no reason why we couldn't just return the resulting concatenated table, either). These sorts of methods can be easily made generic if we have good base classes for individual locatable records that implement the appropriate interfaces.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480968991
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480975105:78,Availability,error,error-prone,78,"Including the schema in a header in text format is broken, and makes sharding error-prone and inefficient. I'd suggest defining schema using [Apache Avro](https://avro.apache.org/), and storing the data in [Apache Parquet](https://parquet.apache.org/) format on disk.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-480975105
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481322968:193,Usability,simpl,simply,193,"@samuelklee I agree with your comments, but if **arbitrary** type of annotations are allowed, especially when multiple values (lists) are allowed, then it is going to look very like VCF. I was simply pre-warning.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481322968
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481408414:501,Availability,robust,robust,501,"@heuermh Thanks for your input, we'll look into those options. I think the use case most of us are imagining is for narrow tables with ~1M-1B records or fewer (sometimes far fewer). For the CNV pipeline, Tribble support is a much higher priority than Hadoop integration, efficient compression, etc., and it's very unlikely we'll need to do out-of-core computations that won't be enabled by Tribble. However, that might not be true of more general use cases, so it's certainly worth investigating more robust formats. @SHuang-Broad For all current CNV use cases, we use separate columns for each annotation. Not sure how much VCF code actually needs to be shared if all we care about is extracting parsing and Tribble indexing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481408414
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481408414:194,Deployability,pipeline,pipeline,194,"@heuermh Thanks for your input, we'll look into those options. I think the use case most of us are imagining is for narrow tables with ~1M-1B records or fewer (sometimes far fewer). For the CNV pipeline, Tribble support is a much higher priority than Hadoop integration, efficient compression, etc., and it's very unlikely we'll need to do out-of-core computations that won't be enabled by Tribble. However, that might not be true of more general use cases, so it's certainly worth investigating more robust formats. @SHuang-Broad For all current CNV use cases, we use separate columns for each annotation. Not sure how much VCF code actually needs to be shared if all we care about is extracting parsing and Tribble indexing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481408414
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481408414:258,Deployability,integrat,integration,258,"@heuermh Thanks for your input, we'll look into those options. I think the use case most of us are imagining is for narrow tables with ~1M-1B records or fewer (sometimes far fewer). For the CNV pipeline, Tribble support is a much higher priority than Hadoop integration, efficient compression, etc., and it's very unlikely we'll need to do out-of-core computations that won't be enabled by Tribble. However, that might not be true of more general use cases, so it's certainly worth investigating more robust formats. @SHuang-Broad For all current CNV use cases, we use separate columns for each annotation. Not sure how much VCF code actually needs to be shared if all we care about is extracting parsing and Tribble indexing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481408414
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481408414:271,Energy Efficiency,efficient,efficient,271,"@heuermh Thanks for your input, we'll look into those options. I think the use case most of us are imagining is for narrow tables with ~1M-1B records or fewer (sometimes far fewer). For the CNV pipeline, Tribble support is a much higher priority than Hadoop integration, efficient compression, etc., and it's very unlikely we'll need to do out-of-core computations that won't be enabled by Tribble. However, that might not be true of more general use cases, so it's certainly worth investigating more robust formats. @SHuang-Broad For all current CNV use cases, we use separate columns for each annotation. Not sure how much VCF code actually needs to be shared if all we care about is extracting parsing and Tribble indexing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481408414
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481408414:258,Integrability,integrat,integration,258,"@heuermh Thanks for your input, we'll look into those options. I think the use case most of us are imagining is for narrow tables with ~1M-1B records or fewer (sometimes far fewer). For the CNV pipeline, Tribble support is a much higher priority than Hadoop integration, efficient compression, etc., and it's very unlikely we'll need to do out-of-core computations that won't be enabled by Tribble. However, that might not be true of more general use cases, so it's certainly worth investigating more robust formats. @SHuang-Broad For all current CNV use cases, we use separate columns for each annotation. Not sure how much VCF code actually needs to be shared if all we care about is extracting parsing and Tribble indexing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481408414
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481753416:870,Modifiability,flexible,flexible,870,"@samuelklee ; My understanding: the code that **can** (and I think should) be borrowed from VCF is `CHROM`, `POS`, `ID`, `INFO`, with `END` from `INFO` extracted to be its own column. ; Then; * `FILTER` can be optional.; * `QUAL` can be optional but it is a nice-to-have feature as a quick-glance confidence measure, if that applies.; * `FORMAT` is going to be hard, because I understand the complaint that they can be wasting space, but I have seen VCF files that have rows with different numbers of fields in `FORMAT`, and that is spec-compliant. If this flexibility is allowed, i.e. allowing sample specific information to be missing on several rows, then the `FORMAT` column can be shared. Recap: only `REF`, `ALT` are missing, which is not much code I believe. I think VCF just happens to have a name that starts with V. Stripping out the `REF`, `ALT`, it is quite flexible for describing any annotated interval (OK, 0-length is up for debate) on a piecewise linear coordinate. And I just made myself sound like a VCF-lover. I simply think much of it can be reused.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481753416
https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481753416:1032,Usability,simpl,simply,1032,"@samuelklee ; My understanding: the code that **can** (and I think should) be borrowed from VCF is `CHROM`, `POS`, `ID`, `INFO`, with `END` from `INFO` extracted to be its own column. ; Then; * `FILTER` can be optional.; * `QUAL` can be optional but it is a nice-to-have feature as a quick-glance confidence measure, if that applies.; * `FORMAT` is going to be hard, because I understand the complaint that they can be wasting space, but I have seen VCF files that have rows with different numbers of fields in `FORMAT`, and that is spec-compliant. If this flexibility is allowed, i.e. allowing sample specific information to be missing on several rows, then the `FORMAT` column can be shared. Recap: only `REF`, `ALT` are missing, which is not much code I believe. I think VCF just happens to have a name that starts with V. Stripping out the `REF`, `ALT`, it is quite flexible for describing any annotated interval (OK, 0-length is up for debate) on a piecewise linear coordinate. And I just made myself sound like a VCF-lover. I simply think much of it can be reused.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4717#issuecomment-481753416
https://github.com/broadinstitute/gatk/issues/4718#issuecomment-385800585:100,Security,encrypt,encryption,100,"It's a feature, we're so committed to open source that we don't allow it to be made private through encryption.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4718#issuecomment-385800585
https://github.com/broadinstitute/gatk/issues/4718#issuecomment-386327818:126,Availability,error,errors,126,"Minor mystery solved. I somehow overlooked the corresponding .vcf.idx files, which were over or at the 144 byte limit, so the errors are less surprising. PR made.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4718#issuecomment-386327818
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289:609,Availability,down,downside,609,"Thanks for bringing this up! I actually think that I prefer option 1, although not ideal (since, as you say, it places more burden on the user). The whole point of having generically parameterized models is that we can apply them to many data types. To single out a few with hardcoded sets of defaults seems like a slippery slope to me. (Of course, we should definitely provide defaults for typical data types in *documentation*.) And in the end, I think it is beneficial for users that wish to tweak knobs to do some work to understand what those knobs actually do (even if just at a basic level). The other downside of option 2 is that it might not be immediately obvious from the command line what parameters are being used. For example, if a user chooses a set of defaults but then overrides some of them, we should make it so they don't have to go digging through the logs to see what parameters are actually used in the end. Nor should they have to go back and check what the defaults were for whatever version of the jar they were using at the time. Option 2 might also make it easier to inadvertently override parameters, etc. via command-line typos or copy-and-paste errors---it's much more straightforward to require and check that every parameter is specified once and fallback to a default if not, as we do now. Not to say that we couldn't get around any of these issues in Barclay, but I think it'll require some thought and careful design. Would be interested to hear Engine team's opinions. Finally, one point that I think will become more relevant as our tools and pipelines become more flexible and parameterized: I think we should start thinking of ""Best Practices Recommendations"" less as ""here is the best set of parameters to use with your data"" and more as ""here is *how to find* the best set of parameters to use with your data (for a given truth set, sensitivity requirement, etc.)"". After all, if we are putting together pipelines to do hyperparameter optimization, there is n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289:1176,Availability,error,errors---it,1176,"m to many data types. To single out a few with hardcoded sets of defaults seems like a slippery slope to me. (Of course, we should definitely provide defaults for typical data types in *documentation*.) And in the end, I think it is beneficial for users that wish to tweak knobs to do some work to understand what those knobs actually do (even if just at a basic level). The other downside of option 2 is that it might not be immediately obvious from the command line what parameters are being used. For example, if a user chooses a set of defaults but then overrides some of them, we should make it so they don't have to go digging through the logs to see what parameters are actually used in the end. Nor should they have to go back and check what the defaults were for whatever version of the jar they were using at the time. Option 2 might also make it easier to inadvertently override parameters, etc. via command-line typos or copy-and-paste errors---it's much more straightforward to require and check that every parameter is specified once and fallback to a default if not, as we do now. Not to say that we couldn't get around any of these issues in Barclay, but I think it'll require some thought and careful design. Would be interested to hear Engine team's opinions. Finally, one point that I think will become more relevant as our tools and pipelines become more flexible and parameterized: I think we should start thinking of ""Best Practices Recommendations"" less as ""here is the best set of parameters to use with your data"" and more as ""here is *how to find* the best set of parameters to use with your data (for a given truth set, sensitivity requirement, etc.)"". After all, if we are putting together pipelines to do hyperparameter optimization, there is no reason not to share them with the community. This would also relax the requirement that the defaults in the WDL (which have to be kept in sync with those in the GATK jar) represent some sort of Best Practices Recommendation, w",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289:1581,Deployability,pipeline,pipelines,1581," should definitely provide defaults for typical data types in *documentation*.) And in the end, I think it is beneficial for users that wish to tweak knobs to do some work to understand what those knobs actually do (even if just at a basic level). The other downside of option 2 is that it might not be immediately obvious from the command line what parameters are being used. For example, if a user chooses a set of defaults but then overrides some of them, we should make it so they don't have to go digging through the logs to see what parameters are actually used in the end. Nor should they have to go back and check what the defaults were for whatever version of the jar they were using at the time. Option 2 might also make it easier to inadvertently override parameters, etc. via command-line typos or copy-and-paste errors---it's much more straightforward to require and check that every parameter is specified once and fallback to a default if not, as we do now. Not to say that we couldn't get around any of these issues in Barclay, but I think it'll require some thought and careful design. Would be interested to hear Engine team's opinions. Finally, one point that I think will become more relevant as our tools and pipelines become more flexible and parameterized: I think we should start thinking of ""Best Practices Recommendations"" less as ""here is the best set of parameters to use with your data"" and more as ""here is *how to find* the best set of parameters to use with your data (for a given truth set, sensitivity requirement, etc.)"". After all, if we are putting together pipelines to do hyperparameter optimization, there is no reason not to share them with the community. This would also relax the requirement that the defaults in the WDL (which have to be kept in sync with those in the GATK jar) represent some sort of Best Practices Recommendation, which is awkward in exactly scenarios like the one you highlight. @vdauwera @LeeTL1220 @sooheelee might have some thoughts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289:1946,Deployability,pipeline,pipelines,1946," should definitely provide defaults for typical data types in *documentation*.) And in the end, I think it is beneficial for users that wish to tweak knobs to do some work to understand what those knobs actually do (even if just at a basic level). The other downside of option 2 is that it might not be immediately obvious from the command line what parameters are being used. For example, if a user chooses a set of defaults but then overrides some of them, we should make it so they don't have to go digging through the logs to see what parameters are actually used in the end. Nor should they have to go back and check what the defaults were for whatever version of the jar they were using at the time. Option 2 might also make it easier to inadvertently override parameters, etc. via command-line typos or copy-and-paste errors---it's much more straightforward to require and check that every parameter is specified once and fallback to a default if not, as we do now. Not to say that we couldn't get around any of these issues in Barclay, but I think it'll require some thought and careful design. Would be interested to hear Engine team's opinions. Finally, one point that I think will become more relevant as our tools and pipelines become more flexible and parameterized: I think we should start thinking of ""Best Practices Recommendations"" less as ""here is the best set of parameters to use with your data"" and more as ""here is *how to find* the best set of parameters to use with your data (for a given truth set, sensitivity requirement, etc.)"". After all, if we are putting together pipelines to do hyperparameter optimization, there is no reason not to share them with the community. This would also relax the requirement that the defaults in the WDL (which have to be kept in sync with those in the GATK jar) represent some sort of Best Practices Recommendation, which is awkward in exactly scenarios like the one you highlight. @vdauwera @LeeTL1220 @sooheelee might have some thoughts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289:183,Modifiability,parameteriz,parameterized,183,"Thanks for bringing this up! I actually think that I prefer option 1, although not ideal (since, as you say, it places more burden on the user). The whole point of having generically parameterized models is that we can apply them to many data types. To single out a few with hardcoded sets of defaults seems like a slippery slope to me. (Of course, we should definitely provide defaults for typical data types in *documentation*.) And in the end, I think it is beneficial for users that wish to tweak knobs to do some work to understand what those knobs actually do (even if just at a basic level). The other downside of option 2 is that it might not be immediately obvious from the command line what parameters are being used. For example, if a user chooses a set of defaults but then overrides some of them, we should make it so they don't have to go digging through the logs to see what parameters are actually used in the end. Nor should they have to go back and check what the defaults were for whatever version of the jar they were using at the time. Option 2 might also make it easier to inadvertently override parameters, etc. via command-line typos or copy-and-paste errors---it's much more straightforward to require and check that every parameter is specified once and fallback to a default if not, as we do now. Not to say that we couldn't get around any of these issues in Barclay, but I think it'll require some thought and careful design. Would be interested to hear Engine team's opinions. Finally, one point that I think will become more relevant as our tools and pipelines become more flexible and parameterized: I think we should start thinking of ""Best Practices Recommendations"" less as ""here is the best set of parameters to use with your data"" and more as ""here is *how to find* the best set of parameters to use with your data (for a given truth set, sensitivity requirement, etc.)"". After all, if we are putting together pipelines to do hyperparameter optimization, there is n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289:1603,Modifiability,flexible,flexible,1603," should definitely provide defaults for typical data types in *documentation*.) And in the end, I think it is beneficial for users that wish to tweak knobs to do some work to understand what those knobs actually do (even if just at a basic level). The other downside of option 2 is that it might not be immediately obvious from the command line what parameters are being used. For example, if a user chooses a set of defaults but then overrides some of them, we should make it so they don't have to go digging through the logs to see what parameters are actually used in the end. Nor should they have to go back and check what the defaults were for whatever version of the jar they were using at the time. Option 2 might also make it easier to inadvertently override parameters, etc. via command-line typos or copy-and-paste errors---it's much more straightforward to require and check that every parameter is specified once and fallback to a default if not, as we do now. Not to say that we couldn't get around any of these issues in Barclay, but I think it'll require some thought and careful design. Would be interested to hear Engine team's opinions. Finally, one point that I think will become more relevant as our tools and pipelines become more flexible and parameterized: I think we should start thinking of ""Best Practices Recommendations"" less as ""here is the best set of parameters to use with your data"" and more as ""here is *how to find* the best set of parameters to use with your data (for a given truth set, sensitivity requirement, etc.)"". After all, if we are putting together pipelines to do hyperparameter optimization, there is no reason not to share them with the community. This would also relax the requirement that the defaults in the WDL (which have to be kept in sync with those in the GATK jar) represent some sort of Best Practices Recommendation, which is awkward in exactly scenarios like the one you highlight. @vdauwera @LeeTL1220 @sooheelee might have some thoughts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289:1616,Modifiability,parameteriz,parameterized,1616," should definitely provide defaults for typical data types in *documentation*.) And in the end, I think it is beneficial for users that wish to tweak knobs to do some work to understand what those knobs actually do (even if just at a basic level). The other downside of option 2 is that it might not be immediately obvious from the command line what parameters are being used. For example, if a user chooses a set of defaults but then overrides some of them, we should make it so they don't have to go digging through the logs to see what parameters are actually used in the end. Nor should they have to go back and check what the defaults were for whatever version of the jar they were using at the time. Option 2 might also make it easier to inadvertently override parameters, etc. via command-line typos or copy-and-paste errors---it's much more straightforward to require and check that every parameter is specified once and fallback to a default if not, as we do now. Not to say that we couldn't get around any of these issues in Barclay, but I think it'll require some thought and careful design. Would be interested to hear Engine team's opinions. Finally, one point that I think will become more relevant as our tools and pipelines become more flexible and parameterized: I think we should start thinking of ""Best Practices Recommendations"" less as ""here is the best set of parameters to use with your data"" and more as ""here is *how to find* the best set of parameters to use with your data (for a given truth set, sensitivity requirement, etc.)"". After all, if we are putting together pipelines to do hyperparameter optimization, there is no reason not to share them with the community. This would also relax the requirement that the defaults in the WDL (which have to be kept in sync with those in the GATK jar) represent some sort of Best Practices Recommendation, which is awkward in exactly scenarios like the one you highlight. @vdauwera @LeeTL1220 @sooheelee might have some thoughts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289:1977,Performance,optimiz,optimization,1977," should definitely provide defaults for typical data types in *documentation*.) And in the end, I think it is beneficial for users that wish to tweak knobs to do some work to understand what those knobs actually do (even if just at a basic level). The other downside of option 2 is that it might not be immediately obvious from the command line what parameters are being used. For example, if a user chooses a set of defaults but then overrides some of them, we should make it so they don't have to go digging through the logs to see what parameters are actually used in the end. Nor should they have to go back and check what the defaults were for whatever version of the jar they were using at the time. Option 2 might also make it easier to inadvertently override parameters, etc. via command-line typos or copy-and-paste errors---it's much more straightforward to require and check that every parameter is specified once and fallback to a default if not, as we do now. Not to say that we couldn't get around any of these issues in Barclay, but I think it'll require some thought and careful design. Would be interested to hear Engine team's opinions. Finally, one point that I think will become more relevant as our tools and pipelines become more flexible and parameterized: I think we should start thinking of ""Best Practices Recommendations"" less as ""here is the best set of parameters to use with your data"" and more as ""here is *how to find* the best set of parameters to use with your data (for a given truth set, sensitivity requirement, etc.)"". After all, if we are putting together pipelines to do hyperparameter optimization, there is no reason not to share them with the community. This would also relax the requirement that the defaults in the WDL (which have to be kept in sync with those in the GATK jar) represent some sort of Best Practices Recommendation, which is awkward in exactly scenarios like the one you highlight. @vdauwera @LeeTL1220 @sooheelee might have some thoughts.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289:873,Testability,log,logs,873,"Thanks for bringing this up! I actually think that I prefer option 1, although not ideal (since, as you say, it places more burden on the user). The whole point of having generically parameterized models is that we can apply them to many data types. To single out a few with hardcoded sets of defaults seems like a slippery slope to me. (Of course, we should definitely provide defaults for typical data types in *documentation*.) And in the end, I think it is beneficial for users that wish to tweak knobs to do some work to understand what those knobs actually do (even if just at a basic level). The other downside of option 2 is that it might not be immediately obvious from the command line what parameters are being used. For example, if a user chooses a set of defaults but then overrides some of them, we should make it so they don't have to go digging through the logs to see what parameters are actually used in the end. Nor should they have to go back and check what the defaults were for whatever version of the jar they were using at the time. Option 2 might also make it easier to inadvertently override parameters, etc. via command-line typos or copy-and-paste errors---it's much more straightforward to require and check that every parameter is specified once and fallback to a default if not, as we do now. Not to say that we couldn't get around any of these issues in Barclay, but I think it'll require some thought and careful design. Would be interested to hear Engine team's opinions. Finally, one point that I think will become more relevant as our tools and pipelines become more flexible and parameterized: I think we should start thinking of ""Best Practices Recommendations"" less as ""here is the best set of parameters to use with your data"" and more as ""here is *how to find* the best set of parameters to use with your data (for a given truth set, sensitivity requirement, etc.)"". After all, if we are putting together pipelines to do hyperparameter optimization, there is n",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379:932,Availability,down,downside,932,"I like option 2. If one of the other options is specified, use the specified value. For example, the following would use 0.1 for interval-psi-scale:. --set-defaults-for-data-type WES. --interval-psi-scale 0.1. On Mon, Apr 30, 2018 at 10:27 PM, samuelklee <notifications@github.com>; wrote:. > Thanks for bringing this up! I actually think that I prefer option 1,; > although not ideal (since, as you say, it places more burden on the user).; > The whole point of having generically parameterized models is that we can; > apply them to many data types. To single out a few with hardcoded sets of; > defaults seems like a slippery slope to me. (Of course, we should; > definitely provide defaults for typical data types in *documentation*.); > And in the end, I think it is beneficial for users that wish to tweak knobs; > to do some work to understand what those knobs actually do (even if just at; > a basic level).; >; > The other downside of option 2 is that it might not be immediately obvious; > from the command line what parameters are being used. For example, if a; > user chooses a set of defaults but then overrides some of them, we should; > make it so they don't have to go digging through the logs to see what; > parameters are actually used in the end. Nor should they have to go back; > and check what the defaults were for whatever version of the jar they were; > using at the time. Option 2 might also make it easier to inadvertently; > override parameters, etc. via command-line typos or copy-and-paste; > errors---it's much more straightforward to require and check that every; > parameter is specified once and fallback to a default if not, as we do now.; > Not to say that we couldn't get around any of these issues in Barclay, but; > I think it'll require some thought and careful design. Would be interested; > to hear Engine team's opinions.; >; > Finally, one point that I think will become more relevant as our tools and; > pipeline become more flexible and parameterized: I t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379:1523,Availability,error,errors---it,1523,"h hardcoded sets of; > defaults seems like a slippery slope to me. (Of course, we should; > definitely provide defaults for typical data types in *documentation*.); > And in the end, I think it is beneficial for users that wish to tweak knobs; > to do some work to understand what those knobs actually do (even if just at; > a basic level).; >; > The other downside of option 2 is that it might not be immediately obvious; > from the command line what parameters are being used. For example, if a; > user chooses a set of defaults but then overrides some of them, we should; > make it so they don't have to go digging through the logs to see what; > parameters are actually used in the end. Nor should they have to go back; > and check what the defaults were for whatever version of the jar they were; > using at the time. Option 2 might also make it easier to inadvertently; > override parameters, etc. via command-line typos or copy-and-paste; > errors---it's much more straightforward to require and check that every; > parameter is specified once and fallback to a default if not, as we do now.; > Not to say that we couldn't get around any of these issues in Barclay, but; > I think it'll require some thought and careful design. Would be interested; > to hear Engine team's opinions.; >; > Finally, one point that I think will become more relevant as our tools and; > pipeline become more flexible and parameterized: I think we should start; > thinking of ""Best Practices Recommendations"" less as ""here is the best set; > of parameters to use with your data"" and more as ""here is *how to find*; > the best set of parameters to use with your data (for a given truth set,; > sensitivity requirement, etc.)"". After all, if we are putting together; > pipelines to do hyperparameter optimization, there is no reason not to; > share them with the community.; >; > This would also relax the requirement that the defaults in the WDL (which; > have to be kept in sync with those in the GATK jar) represen",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379:1949,Deployability,pipeline,pipeline,1949,"> user chooses a set of defaults but then overrides some of them, we should; > make it so they don't have to go digging through the logs to see what; > parameters are actually used in the end. Nor should they have to go back; > and check what the defaults were for whatever version of the jar they were; > using at the time. Option 2 might also make it easier to inadvertently; > override parameters, etc. via command-line typos or copy-and-paste; > errors---it's much more straightforward to require and check that every; > parameter is specified once and fallback to a default if not, as we do now.; > Not to say that we couldn't get around any of these issues in Barclay, but; > I think it'll require some thought and careful design. Would be interested; > to hear Engine team's opinions.; >; > Finally, one point that I think will become more relevant as our tools and; > pipeline become more flexible and parameterized: I think we should start; > thinking of ""Best Practices Recommendations"" less as ""here is the best set; > of parameters to use with your data"" and more as ""here is *how to find*; > the best set of parameters to use with your data (for a given truth set,; > sensitivity requirement, etc.)"". After all, if we are putting together; > pipelines to do hyperparameter optimization, there is no reason not to; > share them with the community.; >; > This would also relax the requirement that the defaults in the WDL (which; > have to be kept in sync with those in the GATK jar) represent some sort of; > Best Practices Recommendation, which is awkward in exactly scenarios like; > the one you highlight.; >; > @vdauwera <https://github.com/vdauwera> @LeeTL1220; > <https://github.com/LeeTL1220> @sooheelee <https://github.com/sooheelee>; > might have some thoughts.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289>,; > or mute the thread; > ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379:2328,Deployability,pipeline,pipelines,2328," go back; > and check what the defaults were for whatever version of the jar they were; > using at the time. Option 2 might also make it easier to inadvertently; > override parameters, etc. via command-line typos or copy-and-paste; > errors---it's much more straightforward to require and check that every; > parameter is specified once and fallback to a default if not, as we do now.; > Not to say that we couldn't get around any of these issues in Barclay, but; > I think it'll require some thought and careful design. Would be interested; > to hear Engine team's opinions.; >; > Finally, one point that I think will become more relevant as our tools and; > pipeline become more flexible and parameterized: I think we should start; > thinking of ""Best Practices Recommendations"" less as ""here is the best set; > of parameters to use with your data"" and more as ""here is *how to find*; > the best set of parameters to use with your data (for a given truth set,; > sensitivity requirement, etc.)"". After all, if we are putting together; > pipelines to do hyperparameter optimization, there is no reason not to; > share them with the community.; >; > This would also relax the requirement that the defaults in the WDL (which; > have to be kept in sync with those in the GATK jar) represent some sort of; > Best Practices Recommendation, which is awkward in exactly scenarios like; > the one you highlight.; >; > @vdauwera <https://github.com/vdauwera> @LeeTL1220; > <https://github.com/LeeTL1220> @sooheelee <https://github.com/sooheelee>; > might have some thoughts.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2h5MhZ7nXrNgo6MrFpMD-TGiAE8ks5tt8gjgaJpZM4TtVZZ>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379:482,Modifiability,parameteriz,parameterized,482,"I like option 2. If one of the other options is specified, use the specified value. For example, the following would use 0.1 for interval-psi-scale:. --set-defaults-for-data-type WES. --interval-psi-scale 0.1. On Mon, Apr 30, 2018 at 10:27 PM, samuelklee <notifications@github.com>; wrote:. > Thanks for bringing this up! I actually think that I prefer option 1,; > although not ideal (since, as you say, it places more burden on the user).; > The whole point of having generically parameterized models is that we can; > apply them to many data types. To single out a few with hardcoded sets of; > defaults seems like a slippery slope to me. (Of course, we should; > definitely provide defaults for typical data types in *documentation*.); > And in the end, I think it is beneficial for users that wish to tweak knobs; > to do some work to understand what those knobs actually do (even if just at; > a basic level).; >; > The other downside of option 2 is that it might not be immediately obvious; > from the command line what parameters are being used. For example, if a; > user chooses a set of defaults but then overrides some of them, we should; > make it so they don't have to go digging through the logs to see what; > parameters are actually used in the end. Nor should they have to go back; > and check what the defaults were for whatever version of the jar they were; > using at the time. Option 2 might also make it easier to inadvertently; > override parameters, etc. via command-line typos or copy-and-paste; > errors---it's much more straightforward to require and check that every; > parameter is specified once and fallback to a default if not, as we do now.; > Not to say that we couldn't get around any of these issues in Barclay, but; > I think it'll require some thought and careful design. Would be interested; > to hear Engine team's opinions.; >; > Finally, one point that I think will become more relevant as our tools and; > pipeline become more flexible and parameterized: I t",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379:1970,Modifiability,flexible,flexible,1970,"> user chooses a set of defaults but then overrides some of them, we should; > make it so they don't have to go digging through the logs to see what; > parameters are actually used in the end. Nor should they have to go back; > and check what the defaults were for whatever version of the jar they were; > using at the time. Option 2 might also make it easier to inadvertently; > override parameters, etc. via command-line typos or copy-and-paste; > errors---it's much more straightforward to require and check that every; > parameter is specified once and fallback to a default if not, as we do now.; > Not to say that we couldn't get around any of these issues in Barclay, but; > I think it'll require some thought and careful design. Would be interested; > to hear Engine team's opinions.; >; > Finally, one point that I think will become more relevant as our tools and; > pipeline become more flexible and parameterized: I think we should start; > thinking of ""Best Practices Recommendations"" less as ""here is the best set; > of parameters to use with your data"" and more as ""here is *how to find*; > the best set of parameters to use with your data (for a given truth set,; > sensitivity requirement, etc.)"". After all, if we are putting together; > pipelines to do hyperparameter optimization, there is no reason not to; > share them with the community.; >; > This would also relax the requirement that the defaults in the WDL (which; > have to be kept in sync with those in the GATK jar) represent some sort of; > Best Practices Recommendation, which is awkward in exactly scenarios like; > the one you highlight.; >; > @vdauwera <https://github.com/vdauwera> @LeeTL1220; > <https://github.com/LeeTL1220> @sooheelee <https://github.com/sooheelee>; > might have some thoughts.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289>,; > or mute the thread; > ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379:1983,Modifiability,parameteriz,parameterized,1983,"> user chooses a set of defaults but then overrides some of them, we should; > make it so they don't have to go digging through the logs to see what; > parameters are actually used in the end. Nor should they have to go back; > and check what the defaults were for whatever version of the jar they were; > using at the time. Option 2 might also make it easier to inadvertently; > override parameters, etc. via command-line typos or copy-and-paste; > errors---it's much more straightforward to require and check that every; > parameter is specified once and fallback to a default if not, as we do now.; > Not to say that we couldn't get around any of these issues in Barclay, but; > I think it'll require some thought and careful design. Would be interested; > to hear Engine team's opinions.; >; > Finally, one point that I think will become more relevant as our tools and; > pipeline become more flexible and parameterized: I think we should start; > thinking of ""Best Practices Recommendations"" less as ""here is the best set; > of parameters to use with your data"" and more as ""here is *how to find*; > the best set of parameters to use with your data (for a given truth set,; > sensitivity requirement, etc.)"". After all, if we are putting together; > pipelines to do hyperparameter optimization, there is no reason not to; > share them with the community.; >; > This would also relax the requirement that the defaults in the WDL (which; > have to be kept in sync with those in the GATK jar) represent some sort of; > Best Practices Recommendation, which is awkward in exactly scenarios like; > the one you highlight.; >; > @vdauwera <https://github.com/vdauwera> @LeeTL1220; > <https://github.com/LeeTL1220> @sooheelee <https://github.com/sooheelee>; > might have some thoughts.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289>,; > or mute the thread; > ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379:2359,Performance,optimiz,optimization,2359," go back; > and check what the defaults were for whatever version of the jar they were; > using at the time. Option 2 might also make it easier to inadvertently; > override parameters, etc. via command-line typos or copy-and-paste; > errors---it's much more straightforward to require and check that every; > parameter is specified once and fallback to a default if not, as we do now.; > Not to say that we couldn't get around any of these issues in Barclay, but; > I think it'll require some thought and careful design. Would be interested; > to hear Engine team's opinions.; >; > Finally, one point that I think will become more relevant as our tools and; > pipeline become more flexible and parameterized: I think we should start; > thinking of ""Best Practices Recommendations"" less as ""here is the best set; > of parameters to use with your data"" and more as ""here is *how to find*; > the best set of parameters to use with your data (for a given truth set,; > sensitivity requirement, etc.)"". After all, if we are putting together; > pipelines to do hyperparameter optimization, there is no reason not to; > share them with the community.; >; > This would also relax the requirement that the defaults in the WDL (which; > have to be kept in sync with those in the GATK jar) represent some sort of; > Best Practices Recommendation, which is awkward in exactly scenarios like; > the one you highlight.; >; > @vdauwera <https://github.com/vdauwera> @LeeTL1220; > <https://github.com/LeeTL1220> @sooheelee <https://github.com/sooheelee>; > might have some thoughts.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385584289>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2h5MhZ7nXrNgo6MrFpMD-TGiAE8ks5tt8gjgaJpZM4TtVZZ>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379:1205,Testability,log,logs,1205,"aults-for-data-type WES. --interval-psi-scale 0.1. On Mon, Apr 30, 2018 at 10:27 PM, samuelklee <notifications@github.com>; wrote:. > Thanks for bringing this up! I actually think that I prefer option 1,; > although not ideal (since, as you say, it places more burden on the user).; > The whole point of having generically parameterized models is that we can; > apply them to many data types. To single out a few with hardcoded sets of; > defaults seems like a slippery slope to me. (Of course, we should; > definitely provide defaults for typical data types in *documentation*.); > And in the end, I think it is beneficial for users that wish to tweak knobs; > to do some work to understand what those knobs actually do (even if just at; > a basic level).; >; > The other downside of option 2 is that it might not be immediately obvious; > from the command line what parameters are being used. For example, if a; > user chooses a set of defaults but then overrides some of them, we should; > make it so they don't have to go digging through the logs to see what; > parameters are actually used in the end. Nor should they have to go back; > and check what the defaults were for whatever version of the jar they were; > using at the time. Option 2 might also make it easier to inadvertently; > override parameters, etc. via command-line typos or copy-and-paste; > errors---it's much more straightforward to require and check that every; > parameter is specified once and fallback to a default if not, as we do now.; > Not to say that we couldn't get around any of these issues in Barclay, but; > I think it'll require some thought and careful design. Would be interested; > to hear Engine team's opinions.; >; > Finally, one point that I think will become more relevant as our tools and; > pipeline become more flexible and parameterized: I think we should start; > thinking of ""Best Practices Recommendations"" less as ""here is the best set; > of parameters to use with your data"" and more as ""here i",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385677379
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385681493:144,Deployability,integrat,integrated,144,"If the pattern is common, we could certainly explore adding some kind of ""argset"" support to Barclay. It would have the advantage of getting it integrated into the help and doc, though that could get complicated. Another possibility is a hybrid approach, where Barclay supports declarative argset definitions, but uses them for help interrogation only, with a command line argument that takes an argset name and generates output or an executable command line with recommended defaults for that argset:. `gatk GermlineCNVCaller --argsetfor WGS`. would generate:. `gatk GermlineCNVCaller --std-log-mean-bias 1.0 --interval-psi-scale 0.0001 ...`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385681493
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385681493:144,Integrability,integrat,integrated,144,"If the pattern is common, we could certainly explore adding some kind of ""argset"" support to Barclay. It would have the advantage of getting it integrated into the help and doc, though that could get complicated. Another possibility is a hybrid approach, where Barclay supports declarative argset definitions, but uses them for help interrogation only, with a command line argument that takes an argset name and generates output or an executable command line with recommended defaults for that argset:. `gatk GermlineCNVCaller --argsetfor WGS`. would generate:. `gatk GermlineCNVCaller --std-log-mean-bias 1.0 --interval-psi-scale 0.0001 ...`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385681493
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385681493:592,Testability,log,log-mean-bias,592,"If the pattern is common, we could certainly explore adding some kind of ""argset"" support to Barclay. It would have the advantage of getting it integrated into the help and doc, though that could get complicated. Another possibility is a hybrid approach, where Barclay supports declarative argset definitions, but uses them for help interrogation only, with a command line argument that takes an argset name and generates output or an executable command line with recommended defaults for that argset:. `gatk GermlineCNVCaller --argsetfor WGS`. would generate:. `gatk GermlineCNVCaller --std-log-mean-bias 1.0 --interval-psi-scale 0.0001 ...`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385681493
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385682838:53,Safety,avoid,avoids,53,"I think that’s a great idea, @cmnbroad. I think this avoids most if not all of the pitfalls I mentioned above. Moreover, if each tool also has an argset containing default values for all optional parameters, it probably also solves the problem of having to sync defaults in the WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385682838
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385879758:64,Modifiability,plugin,plugins,64,"My 2 cents: actually the argset strategy would be nice also for plugins. For example, in ReadFilters it might allow to specify ""recommended"" filters but not necessary; and in the annotations to convert the groups to a argument set. +1 to the argset for many use-cases and not only this one!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-385879758
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-386411104:342,Modifiability,parameteriz,parameterization,342,"I attended a journal club some months ago where a paper stated most researchers use default settings of tools. The paper benchmarked tool with default settings and with tweaked parameters. I can dig up the paper if anyone is interested. So there is some expectation from the user community that the default parameters reflect some sweet spot parameterization for running the tool. . I highly support @cmnbroad's suggestion for making argument sets callable by one flag. For exomes, please do not label flag as `WES`. We want to refer instead to _targeted exomes_, so `EXOMES` or variation is preferable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-386411104
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-386411104:121,Testability,benchmark,benchmarked,121,"I attended a journal club some months ago where a paper stated most researchers use default settings of tools. The paper benchmarked tool with default settings and with tweaked parameters. I can dig up the paper if anyone is interested. So there is some expectation from the user community that the default parameters reflect some sweet spot parameterization for running the tool. . I highly support @cmnbroad's suggestion for making argument sets callable by one flag. For exomes, please do not label flag as `WES`. We want to refer instead to _targeted exomes_, so `EXOMES` or variation is preferable.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-386411104
https://github.com/broadinstitute/gatk/issues/4719#issuecomment-391014975:221,Availability,error,error,221,"Just to clarify, I think argsets without the ability to override individual parameters would be fine. Although it’s probably not difficult to implement, I think allowing overrides introduces unnecessary possibilities for error and confusion. If you are a user that wants to just go with one of the provided argsets, fine; if you’re a user that wants to set all parameters individually, also fine. Will leave it up to engine team and Comms if they want to support overrides, though. In any case, when these defaults represent sweet spots for *Broad-generated data*, let’s communicate that in documentation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4719#issuecomment-391014975
https://github.com/broadinstitute/gatk/pull/4720#issuecomment-386454276:364,Availability,error,error,364,"I tested this on a collection of 32 tandem duplications (~10kbp-50kbp events, 5kbp padding, 100bp bins) from an HGSV proband case. Using the latest release docker it nailed almost all of them (hooray!) but it hardly called any dups when I tried to do the calling/postprocessing locally using this branch (log reports successful convergence). This could just be an error on my end but we should check before merging.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720#issuecomment-386454276
https://github.com/broadinstitute/gatk/pull/4720#issuecomment-386454276:148,Deployability,release,release,148,"I tested this on a collection of 32 tandem duplications (~10kbp-50kbp events, 5kbp padding, 100bp bins) from an HGSV proband case. Using the latest release docker it nailed almost all of them (hooray!) but it hardly called any dups when I tried to do the calling/postprocessing locally using this branch (log reports successful convergence). This could just be an error on my end but we should check before merging.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720#issuecomment-386454276
https://github.com/broadinstitute/gatk/pull/4720#issuecomment-386454276:2,Testability,test,tested,2,"I tested this on a collection of 32 tandem duplications (~10kbp-50kbp events, 5kbp padding, 100bp bins) from an HGSV proband case. Using the latest release docker it nailed almost all of them (hooray!) but it hardly called any dups when I tried to do the calling/postprocessing locally using this branch (log reports successful convergence). This could just be an error on my end but we should check before merging.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720#issuecomment-386454276
https://github.com/broadinstitute/gatk/pull/4720#issuecomment-386454276:305,Testability,log,log,305,"I tested this on a collection of 32 tandem duplications (~10kbp-50kbp events, 5kbp padding, 100bp bins) from an HGSV proband case. Using the latest release docker it nailed almost all of them (hooray!) but it hardly called any dups when I tried to do the calling/postprocessing locally using this branch (log reports successful convergence). This could just be an error on my end but we should check before merging.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720#issuecomment-386454276
https://github.com/broadinstitute/gatk/pull/4721#issuecomment-387826797:0,Availability,ping,ping,0,"ping? It's been a little while, a review would be nice. If @droazen is busy, perhaps @lbergelson can have a look?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4721#issuecomment-387826797
https://github.com/broadinstitute/gatk/pull/4723#issuecomment-385578759:169,Availability,failure,failure,169,"I am also running this on a VCF w/ ~500k variants and Funcotator seems to be producing annotations. Definitely, seems to have made progress beyond the original point of failure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4723#issuecomment-385578759
https://github.com/broadinstitute/gatk/pull/4723#issuecomment-386162511:141,Testability,test,test,141,I have no OSX laptop. Can I solicit someone in GATK Engine team? @droazen @lbergelson @jonn-smith I can provide everything you need to run a test on the jar file...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4723#issuecomment-386162511
https://github.com/broadinstitute/gatk/pull/4723#issuecomment-386299184:34,Availability,error,error,34,"@lbergelson Here is how I got the error... GATK: release 4.0.4.0 jar file; Download and untar the datasources: `ftp://ftp.broadinstitute.org/bundle/funcotator/`; (You must get the latest, largest datasources); Make sure you have the ref: `Homo_sapiens_assembly19.fasta`; The input NA12878 VCF is here (Broad internal): `/seq/tng/jcarey/for-lee/0816201804HC0_R01C01.vcf.gz`. Alter the command below to suit you...; ```; ${GATK_CMD} Funcotator --output-file-format VCF \; --ref-version hg19 --data-sources-path /path/to/untared/datasources ; -R /path/to/Homo_sapiens_assembly19.fasta -V /path/to/0816201804HC0_R01C01.vcf.gz \; -O 0816201804HC0_R01C01.funcotated.vcf \; -L 1:1-10000000; ```. You should not see any real progress before getting a GATK failure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4723#issuecomment-386299184
https://github.com/broadinstitute/gatk/pull/4723#issuecomment-386299184:75,Availability,Down,Download,75,"@lbergelson Here is how I got the error... GATK: release 4.0.4.0 jar file; Download and untar the datasources: `ftp://ftp.broadinstitute.org/bundle/funcotator/`; (You must get the latest, largest datasources); Make sure you have the ref: `Homo_sapiens_assembly19.fasta`; The input NA12878 VCF is here (Broad internal): `/seq/tng/jcarey/for-lee/0816201804HC0_R01C01.vcf.gz`. Alter the command below to suit you...; ```; ${GATK_CMD} Funcotator --output-file-format VCF \; --ref-version hg19 --data-sources-path /path/to/untared/datasources ; -R /path/to/Homo_sapiens_assembly19.fasta -V /path/to/0816201804HC0_R01C01.vcf.gz \; -O 0816201804HC0_R01C01.funcotated.vcf \; -L 1:1-10000000; ```. You should not see any real progress before getting a GATK failure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4723#issuecomment-386299184
https://github.com/broadinstitute/gatk/pull/4723#issuecomment-386299184:748,Availability,failure,failure,748,"@lbergelson Here is how I got the error... GATK: release 4.0.4.0 jar file; Download and untar the datasources: `ftp://ftp.broadinstitute.org/bundle/funcotator/`; (You must get the latest, largest datasources); Make sure you have the ref: `Homo_sapiens_assembly19.fasta`; The input NA12878 VCF is here (Broad internal): `/seq/tng/jcarey/for-lee/0816201804HC0_R01C01.vcf.gz`. Alter the command below to suit you...; ```; ${GATK_CMD} Funcotator --output-file-format VCF \; --ref-version hg19 --data-sources-path /path/to/untared/datasources ; -R /path/to/Homo_sapiens_assembly19.fasta -V /path/to/0816201804HC0_R01C01.vcf.gz \; -O 0816201804HC0_R01C01.funcotated.vcf \; -L 1:1-10000000; ```. You should not see any real progress before getting a GATK failure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4723#issuecomment-386299184
https://github.com/broadinstitute/gatk/pull/4723#issuecomment-386299184:49,Deployability,release,release,49,"@lbergelson Here is how I got the error... GATK: release 4.0.4.0 jar file; Download and untar the datasources: `ftp://ftp.broadinstitute.org/bundle/funcotator/`; (You must get the latest, largest datasources); Make sure you have the ref: `Homo_sapiens_assembly19.fasta`; The input NA12878 VCF is here (Broad internal): `/seq/tng/jcarey/for-lee/0816201804HC0_R01C01.vcf.gz`. Alter the command below to suit you...; ```; ${GATK_CMD} Funcotator --output-file-format VCF \; --ref-version hg19 --data-sources-path /path/to/untared/datasources ; -R /path/to/Homo_sapiens_assembly19.fasta -V /path/to/0816201804HC0_R01C01.vcf.gz \; -O 0816201804HC0_R01C01.funcotated.vcf \; -L 1:1-10000000; ```. You should not see any real progress before getting a GATK failure.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4723#issuecomment-386299184
https://github.com/broadinstitute/gatk/issues/4724#issuecomment-385782919:32,Availability,error,error,32,"Dear @shulik7, according to the error message, gcnvkernel is expecting ploidy calls at the following path: `/scratch/users/shulik7/test_GATK_CNV/Postprocess/../DetermineGermlineContigPloidy/model/test_run-calls/`. Could you please assert that the above path is indeed the ploidy *calls* paths, and if so, whether it contains `test_sample_0` under one of `SAMPLE_x` subdirs? if the path is valid, please try running `PostprocessGermlineCNVCalls` with an absolute path and report back.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4724#issuecomment-385782919
https://github.com/broadinstitute/gatk/issues/4724#issuecomment-385782919:38,Integrability,message,message,38,"Dear @shulik7, according to the error message, gcnvkernel is expecting ploidy calls at the following path: `/scratch/users/shulik7/test_GATK_CNV/Postprocess/../DetermineGermlineContigPloidy/model/test_run-calls/`. Could you please assert that the above path is indeed the ploidy *calls* paths, and if so, whether it contains `test_sample_0` under one of `SAMPLE_x` subdirs? if the path is valid, please try running `PostprocessGermlineCNVCalls` with an absolute path and report back.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4724#issuecomment-385782919
https://github.com/broadinstitute/gatk/issues/4724#issuecomment-385782919:231,Testability,assert,assert,231,"Dear @shulik7, according to the error message, gcnvkernel is expecting ploidy calls at the following path: `/scratch/users/shulik7/test_GATK_CNV/Postprocess/../DetermineGermlineContigPloidy/model/test_run-calls/`. Could you please assert that the above path is indeed the ploidy *calls* paths, and if so, whether it contains `test_sample_0` under one of `SAMPLE_x` subdirs? if the path is valid, please try running `PostprocessGermlineCNVCalls` with an absolute path and report back.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4724#issuecomment-385782919
https://github.com/broadinstitute/gatk/issues/4725#issuecomment-398506155:53,Availability,heartbeat,heartbeat,53,"Hello @abdohlman, apologies for the late response. A heartbeat timeout usually means one of the executors is crashing. If you are able to inspect the error logs from each worker node, you may find which one it was and why. It is likely that one or more are running out of memory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725#issuecomment-398506155
https://github.com/broadinstitute/gatk/issues/4725#issuecomment-398506155:150,Availability,error,error,150,"Hello @abdohlman, apologies for the late response. A heartbeat timeout usually means one of the executors is crashing. If you are able to inspect the error logs from each worker node, you may find which one it was and why. It is likely that one or more are running out of memory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725#issuecomment-398506155
https://github.com/broadinstitute/gatk/issues/4725#issuecomment-398506155:63,Safety,timeout,timeout,63,"Hello @abdohlman, apologies for the late response. A heartbeat timeout usually means one of the executors is crashing. If you are able to inspect the error logs from each worker node, you may find which one it was and why. It is likely that one or more are running out of memory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725#issuecomment-398506155
https://github.com/broadinstitute/gatk/issues/4725#issuecomment-398506155:156,Testability,log,logs,156,"Hello @abdohlman, apologies for the late response. A heartbeat timeout usually means one of the executors is crashing. If you are able to inspect the error logs from each worker node, you may find which one it was and why. It is likely that one or more are running out of memory.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725#issuecomment-398506155
https://github.com/broadinstitute/gatk/issues/4727#issuecomment-385945590:314,Deployability,install,installing,314,"I see the VCF is large, but if you are able to share it we would be happy to take a look. My first guess is it is some kind of unicode encoding mix up. While the CNN tool is written to be Python2/3 agnostic, we are moving towards a pure Python3 implementation. So you may want to try with Python3, for example, by installing the gatk conda env with:; `conda env create -n gatk -f ./scripts/gatkcondaenv.yml`; and then; `source activate gatk`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-385945590
https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368:25,Availability,error,error,25,"Hi. I encounter the same error with GATK4.0.4.0 and the python environment created by gatkcondaenv.yml. ```; 14:49:35.361 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:/tmp/--------/1d_cnn_mix_train_full_bn.4552731615279398677.json and weights:/tmp/--------/1d_cnn_mix_train_full_bn.2635334538442041575.hd5; 14:49:36.747 INFO ProgressMeter - Starting traversal; 14:49:36.747 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 14:49:36.768 INFO ProgressMeter - unmapped 0.0 1 3157.9; 14:49:36.769 INFO ProgressMeter - Traversal complete. Processed 1 total variants in 0.0 minutes.; 14:49:36.772 INFO CNNScoreVariants - Shutting down engine; [May 9, 2018 2:49:36 PM CST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=41160278016; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: Traceback detected: Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 51, in score_and_write_batch; reference_batch.append(reference_string_to_tensor(fifo_data[4])); File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 107, in reference_string_to_tensor; raise ValueError('Error! Unknown code:', b); ValueError: ('Error! Unknown code:', '\x00'); >>>; 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:214); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalSuccess(CNNScoreVariants.java:390); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:894); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368
https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368:679,Availability,down,down,679,"Hi. I encounter the same error with GATK4.0.4.0 and the python environment created by gatkcondaenv.yml. ```; 14:49:35.361 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:/tmp/--------/1d_cnn_mix_train_full_bn.4552731615279398677.json and weights:/tmp/--------/1d_cnn_mix_train_full_bn.2635334538442041575.hd5; 14:49:36.747 INFO ProgressMeter - Starting traversal; 14:49:36.747 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 14:49:36.768 INFO ProgressMeter - unmapped 0.0 1 3157.9; 14:49:36.769 INFO ProgressMeter - Traversal complete. Processed 1 total variants in 0.0 minutes.; 14:49:36.772 INFO CNNScoreVariants - Shutting down engine; [May 9, 2018 2:49:36 PM CST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=41160278016; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: Traceback detected: Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 51, in score_and_write_batch; reference_batch.append(reference_string_to_tensor(fifo_data[4])); File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 107, in reference_string_to_tensor; raise ValueError('Error! Unknown code:', b); ValueError: ('Error! Unknown code:', '\x00'); >>>; 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:214); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalSuccess(CNNScoreVariants.java:390); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:894); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368
https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368:1388,Availability,Error,Error,1388,"gressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 14:49:36.768 INFO ProgressMeter - unmapped 0.0 1 3157.9; 14:49:36.769 INFO ProgressMeter - Traversal complete. Processed 1 total variants in 0.0 minutes.; 14:49:36.772 INFO CNNScoreVariants - Shutting down engine; [May 9, 2018 2:49:36 PM CST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=41160278016; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: Traceback detected: Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 51, in score_and_write_batch; reference_batch.append(reference_string_to_tensor(fifo_data[4])); File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 107, in reference_string_to_tensor; raise ValueError('Error! Unknown code:', b); ValueError: ('Error! Unknown code:', '\x00'); >>>; 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:214); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalSuccess(CNNScoreVariants.java:390); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:894); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. These are the vcf files I used.; http://w.csie.org/~xxxx/vcf/ok.vcf; http",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368
https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368:1429,Availability,Error,Error,1429,"gressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 14:49:36.768 INFO ProgressMeter - unmapped 0.0 1 3157.9; 14:49:36.769 INFO ProgressMeter - Traversal complete. Processed 1 total variants in 0.0 minutes.; 14:49:36.772 INFO CNNScoreVariants - Shutting down engine; [May 9, 2018 2:49:36 PM CST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=41160278016; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: Traceback detected: Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 51, in score_and_write_batch; reference_batch.append(reference_string_to_tensor(fifo_data[4])); File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 107, in reference_string_to_tensor; raise ValueError('Error! Unknown code:', b); ValueError: ('Error! Unknown code:', '\x00'); >>>; 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:214); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalSuccess(CNNScoreVariants.java:390); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:894); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. These are the vcf files I used.; http://w.csie.org/~xxxx/vcf/ok.vcf; http",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368
https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368:2423,Availability,error,error,2423,"eter - Traversal complete. Processed 1 total variants in 0.0 minutes.; 14:49:36.772 INFO CNNScoreVariants - Shutting down engine; [May 9, 2018 2:49:36 PM CST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=41160278016; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: Traceback detected: Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 51, in score_and_write_batch; reference_batch.append(reference_string_to_tensor(fifo_data[4])); File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 107, in reference_string_to_tensor; raise ValueError('Error! Unknown code:', b); ValueError: ('Error! Unknown code:', '\x00'); >>>; 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:214); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalSuccess(CNNScoreVariants.java:390); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:894); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. These are the vcf files I used.; http://w.csie.org/~xxxx/vcf/ok.vcf; http://w.csie.org/~xxxx/vcf/error.vcf. ok.vcf is able to be processed successfully, while error.vcf is not.; The only different between these two file is the position.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368
https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368:2485,Availability,error,error,2485,"eter - Traversal complete. Processed 1 total variants in 0.0 minutes.; 14:49:36.772 INFO CNNScoreVariants - Shutting down engine; [May 9, 2018 2:49:36 PM CST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=41160278016; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: Traceback detected: Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 51, in score_and_write_batch; reference_batch.append(reference_string_to_tensor(fifo_data[4])); File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 107, in reference_string_to_tensor; raise ValueError('Error! Unknown code:', b); ValueError: ('Error! Unknown code:', '\x00'); >>>; 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:214); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalSuccess(CNNScoreVariants.java:390); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:894); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. These are the vcf files I used.; http://w.csie.org/~xxxx/vcf/ok.vcf; http://w.csie.org/~xxxx/vcf/error.vcf. ok.vcf is able to be processed successfully, while error.vcf is not.; The only different between these two file is the position.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368
https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368:941,Safety,detect,detected,941,"Hi. I encounter the same error with GATK4.0.4.0 and the python environment created by gatkcondaenv.yml. ```; 14:49:35.361 INFO CNNScoreVariants - Using key:CNN_1D for CNN architecture:/tmp/--------/1d_cnn_mix_train_full_bn.4552731615279398677.json and weights:/tmp/--------/1d_cnn_mix_train_full_bn.2635334538442041575.hd5; 14:49:36.747 INFO ProgressMeter - Starting traversal; 14:49:36.747 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 14:49:36.768 INFO ProgressMeter - unmapped 0.0 1 3157.9; 14:49:36.769 INFO ProgressMeter - Traversal complete. Processed 1 total variants in 0.0 minutes.; 14:49:36.772 INFO CNNScoreVariants - Shutting down engine; [May 9, 2018 2:49:36 PM CST] org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=41160278016; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: Traceback detected: Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 51, in score_and_write_batch; reference_batch.append(reference_string_to_tensor(fifo_data[4])); File ""/home/--------/anaconda3/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 107, in reference_string_to_tensor; raise ValueError('Error! Unknown code:', b); ValueError: ('Error! Unknown code:', '\x00'); >>>; 	at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.getAccumulatedOutput(StreamingPythonScriptExecutor.java:214); 	at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalSuccess(CNNScoreVariants.java:390); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:894); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgr",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387639368
https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387737569:175,Security,validat,validated,175,"I see that this is occurring in the mitochondrial chromosome. The model was trained mostly with the autosomes and so scores on the mitochondrial DNA have not been extensively validated. That said, this looks like a bug and we hope to have a fix in soon.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-387737569
https://github.com/broadinstitute/gatk/issues/4727#issuecomment-388777837:1308,Availability,Error,Error,1308,"Thanks zzxzxzzxz. . I insert a debug print to `score_and_write_batch()` function in `inference.py` and run CNNScoreVariants with `err.vcf`.; The following data is received by python process via fifo:. 'chrM\t16521\tC\t[T]\tGGGCCCATAACACTTGGGGGTAGCTAAAGTGAACTGTATCCGACATCTGGTTCCTACTTCAGGGCCATAAAGCCTAAATAGCCCACACGTTCCCCTTAAATAAGACATCACGATG\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\tAC=2;MQRankSum=-0.712;MQ=57.64;AF=1.00;MLEAC=2;BaseQRankSum=5.566;ExcessHet=3.0103;MLEAF=1.00;DP=4492;ReadPosRankSum=-1.183;AN=2;FS=0.000;QD=30.32;SOR=0.761;ClippingRankSum=0.053;\tSNP\n'. There are 13 NULL data in reference bases. This data are created by `transferToPythonViaFifo()` method in `CNNScoreVariants.java`. private void transferToPythonViaFifo(final VariantContext variant, final ReferenceContext referenceContext) {; try {; final String outDat = String.format(""%s\t%s\t%s\t%s\n"",; getVariantDataString(variant),; ---> new String(Arrays.copyOfRange(referenceContext.getBases(), 0, windowSize), ""UTF-8""),; getVariantInfoString(variant),; variant.isSNP() ? ""SNP"" : variant.isIndel() ? ""INDEL"" : ""OTHER"");. (windowSize == 128) / 2 - (chrM length(16571) - variant position(16521) + 1) = 13. I add the `fifo_data[4] = fifo_data[4].strip('\x00')` in `score_and_write_batch()` function and rerun:. ValueError: Error when checking : expected reference to have shape (128, 4) but got array with shape (115, 4). What should I do ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4727#issuecomment-388777837
https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386268150:933,Deployability,pipeline,pipeline,933,"@LeeTL1220 do you have any opinions on making the somatic CNV workflow scatter by contig? This could allow WGS to complete basically ~20x faster and could allow us to avoid issues such as #4734. A few issues:. 1) Do we want a single WDL that can optionally scatter, depending on WES vs. WGS? It would be nicer to have just one workflow, but I haven't thought about how an optional scatter might look in WDL. 2) For segmentation and modeling, scattering should have little impact on the final result (although there are a few global-level quantities in the models that would be reduced to contig-level quantities, which might slightly affect the quality of their inference). However, we'd want to concatenate all per-contig results for both plotting and segment calling. It'd be relatively easy to either have a separate tool to concatenate AbstractLocatableCollections (there is already a method to do this that is used for the gCNV pipeline), or to make the plotting and calling tools take in parallel inputs and combine them. I'd tend towards the former, just so we don't have to deliver per-contig files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386268150
https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386268150:577,Energy Efficiency,reduce,reduced,577,"@LeeTL1220 do you have any opinions on making the somatic CNV workflow scatter by contig? This could allow WGS to complete basically ~20x faster and could allow us to avoid issues such as #4734. A few issues:. 1) Do we want a single WDL that can optionally scatter, depending on WES vs. WGS? It would be nicer to have just one workflow, but I haven't thought about how an optional scatter might look in WDL. 2) For segmentation and modeling, scattering should have little impact on the final result (although there are a few global-level quantities in the models that would be reduced to contig-level quantities, which might slightly affect the quality of their inference). However, we'd want to concatenate all per-contig results for both plotting and segment calling. It'd be relatively easy to either have a separate tool to concatenate AbstractLocatableCollections (there is already a method to do this that is used for the gCNV pipeline), or to make the plotting and calling tools take in parallel inputs and combine them. I'd tend towards the former, just so we don't have to deliver per-contig files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386268150
https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386268150:266,Integrability,depend,depending,266,"@LeeTL1220 do you have any opinions on making the somatic CNV workflow scatter by contig? This could allow WGS to complete basically ~20x faster and could allow us to avoid issues such as #4734. A few issues:. 1) Do we want a single WDL that can optionally scatter, depending on WES vs. WGS? It would be nicer to have just one workflow, but I haven't thought about how an optional scatter might look in WDL. 2) For segmentation and modeling, scattering should have little impact on the final result (although there are a few global-level quantities in the models that would be reduced to contig-level quantities, which might slightly affect the quality of their inference). However, we'd want to concatenate all per-contig results for both plotting and segment calling. It'd be relatively easy to either have a separate tool to concatenate AbstractLocatableCollections (there is already a method to do this that is used for the gCNV pipeline), or to make the plotting and calling tools take in parallel inputs and combine them. I'd tend towards the former, just so we don't have to deliver per-contig files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386268150
https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386268150:167,Safety,avoid,avoid,167,"@LeeTL1220 do you have any opinions on making the somatic CNV workflow scatter by contig? This could allow WGS to complete basically ~20x faster and could allow us to avoid issues such as #4734. A few issues:. 1) Do we want a single WDL that can optionally scatter, depending on WES vs. WGS? It would be nicer to have just one workflow, but I haven't thought about how an optional scatter might look in WDL. 2) For segmentation and modeling, scattering should have little impact on the final result (although there are a few global-level quantities in the models that would be reduced to contig-level quantities, which might slightly affect the quality of their inference). However, we'd want to concatenate all per-contig results for both plotting and segment calling. It'd be relatively easy to either have a separate tool to concatenate AbstractLocatableCollections (there is already a method to do this that is used for the gCNV pipeline), or to make the plotting and calling tools take in parallel inputs and combine them. I'd tend towards the former, just so we don't have to deliver per-contig files.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386268150
https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386269562:180,Deployability,pipeline,pipeline,180,"To add, just in case it wasn't clear, note that this is almost certainly overkill for most somatic applications. However, if this is going to double as a more lightweight germline pipeline (as it is for the time being, as we are using some of the results to prototype SV integration), it might be worthwhile.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386269562
https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386269562:271,Deployability,integrat,integration,271,"To add, just in case it wasn't clear, note that this is almost certainly overkill for most somatic applications. However, if this is going to double as a more lightweight germline pipeline (as it is for the time being, as we are using some of the results to prototype SV integration), it might be worthwhile.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386269562
https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386269562:271,Integrability,integrat,integration,271,"To add, just in case it wasn't clear, note that this is almost certainly overkill for most somatic applications. However, if this is going to double as a more lightweight germline pipeline (as it is for the time being, as we are using some of the results to prototype SV integration), it might be worthwhile.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386269562
https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386269562:31,Usability,clear,clear,31,"To add, just in case it wasn't clear, note that this is almost certainly overkill for most somatic applications. However, if this is going to double as a more lightweight germline pipeline (as it is for the time being, as we are using some of the results to prototype SV integration), it might be worthwhile.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386269562
https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386341401:136,Usability,simpl,simple,136,"That's correct. We could easily add an optional sequence-dictionary input for plotting in the WDL, if desired, but I decided to keep it simple for now.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4728#issuecomment-386341401
https://github.com/broadinstitute/gatk/pull/4732#issuecomment-387510242:82,Testability,test,test,82,"Compile warning:. ```; :compileTestJava/home/travis/build/broadinstitute/gatk/src/test/java/org/broadinstitute/hellbender/engine/spark/datasources/ReadsSparkSourceUnitTest.java:339: warning: [static] static method should be qualified by type name, ReadsSparkSource, instead of by an expression; readsSparkSource.putPairsInSamePartition(header, problemReads, ctx);; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4732#issuecomment-387510242
https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386247582:23,Integrability,message,message,23,"When you run a tool, a message with the bundled versions should show in the log.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386247582
https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386247582:76,Testability,log,log,76,"When you run a tool, a message with the bundled versions should show in the log.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386247582
https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386416863:219,Testability,log,log,219,"You can also find the Picard version for a given version of GATK by inspecting `build.gradle`, if you have the source checked out. But the easiest way is just to run any GATK tool, and check for ""Picard version"" in the log output. Closing as resolved",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386416863
https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386976106:1412,Availability,avail,available,1412,7_PDS/MCF7_PDS.bam --ALIGNMENT_STATUS=Aligned --PF_STATUS PF --HEADER_ONLY true ; Using GATK jar /resources/tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /resources/; tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar ViewSam -I /data/MCF7_PDS.bam --ALIGNMENT_STATUS=Aligned --PF_STATUS PF --HEADER_ONLY; true; 08:51:42.543 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/resources/tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar!/com/intel/gkl/native/libgkl_compr; ession.so; [Mon May 07 08:51:42 CEST 2018] ViewSam --INPUT /data/MCF7_PDS.bam --ALIGNMENT_STATUS Aligned --PF_STATUS PF --HEADER_ONLY true --REC; ORDS_ONLY false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH; _CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Mon May 07 08:51:42 CEST 2018] Executing as [...] on Linux 4.4.0-87-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12; Deflater; : Intel; Inflater: Intel; Provider GCS is available; **Picard version: Version:4.0.4.0**; ```; And if I use `--version true`:; ```; $ gatk ViewSam -I /data/MCF7_PDS.bam --ALIGNMENT_STATUS=Aligned --PF_STATUS PF --HEADER_ONLY true --version true; Using GATK jar /resources/tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /resources/tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar ViewSam -I /data/MCF7_PDS.bam --ALIGNMENT_STATUS=Aligned --PF_STATUS PF --HEADER_ONLY true --version true; Version:4.0.4.0; Tool returned:; 1; ```,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386976106
https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386976106:605,Performance,Load,Loading,605,Sorry but it doesn't seem to be resolved.; ```; $ gatk ViewSam -I /data/MCF7_PDS/MCF7_PDS.bam --ALIGNMENT_STATUS=Aligned --PF_STATUS PF --HEADER_ONLY true ; Using GATK jar /resources/tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /resources/; tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar ViewSam -I /data/MCF7_PDS.bam --ALIGNMENT_STATUS=Aligned --PF_STATUS PF --HEADER_ONLY; true; 08:51:42.543 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/resources/tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar!/com/intel/gkl/native/libgkl_compr; ession.so; [Mon May 07 08:51:42 CEST 2018] ViewSam --INPUT /data/MCF7_PDS.bam --ALIGNMENT_STATUS Aligned --PF_STATUS PF --HEADER_ONLY true --REC; ORDS_ONLY false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH; _CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Mon May 07 08:51:42 CEST 2018] Executing as [...] on Linux 4.4.0-87-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12; Deflater; : Intel; Inflater: Intel; Provider GCS is available; **Picard version: Version:4.0.4.0**; ```; And if I use `--version true`:; ```; $ gatk ViewSam -I /data/MCF7_PDS.bam --ALIGNMENT_STATUS=Aligned --PF_STATUS PF --HEADER_ONLY true --version true; Using GATK jar /resources/tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /resources/tools/gatk-4.0.4.0/gatk-package-4.0.4.0-local.jar ViewSam -I /data/MCF7_PDS.bam --ALIGNMENT_STATUS=Aligned --PF_STATUS PF ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386976106
https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386994409:398,Availability,down,downstream,398,"I think that this is because in Picard tools the version is populated from the manifest `Version` attribute, and thus it prints the GATK version. @droazen and @cmnbroad - this shows that https://github.com/broadinstitute/gatk/issues/4101 in combination with a common CLP barclay class is really needed to set versions properly to combine toolkits in one (GATK/Picard, integrate common tools into a downstream project, etc.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386994409
https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386994409:368,Deployability,integrat,integrate,368,"I think that this is because in Picard tools the version is populated from the manifest `Version` attribute, and thus it prints the GATK version. @droazen and @cmnbroad - this shows that https://github.com/broadinstitute/gatk/issues/4101 in combination with a common CLP barclay class is really needed to set versions properly to combine toolkits in one (GATK/Picard, integrate common tools into a downstream project, etc.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386994409
https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386994409:368,Integrability,integrat,integrate,368,"I think that this is because in Picard tools the version is populated from the manifest `Version` attribute, and thus it prints the GATK version. @droazen and @cmnbroad - this shows that https://github.com/broadinstitute/gatk/issues/4101 in combination with a common CLP barclay class is really needed to set versions properly to combine toolkits in one (GATK/Picard, integrate common tools into a downstream project, etc.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4733#issuecomment-386994409
https://github.com/broadinstitute/gatk/issues/4733#issuecomment-387087349:81,Testability,test,test,81,"@cmnbroad ok, I see the difference, coincidentally I used only Picard tools as a test.; Running PrintReads as you suggested definitely displays the right version; `Picard Version: 2.18.2`; Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4733#issuecomment-387087349
https://github.com/broadinstitute/gatk/issues/4733#issuecomment-387349805:597,Availability,down,downstream,597,"The problem is in https://github.com/broadinstitute/picard/blob/aa911f9440e70f59a32dddadd90e5a07c7764d87/src/main/java/picard/cmdline/CommandLineProgram.java#L267-L274, which is using barclays `CommandLineParser.getVersion`, which implementation uses the `this.callerOptions.getClass().getPackage().getImplementationVersion()` (see https://github.com/broadinstitute/barclay/blob/master/src/main/java/org/broadinstitute/barclay/argparser/CommandLineArgumentParser.java#L268). I think that in general is a good idea to include the versions in the `MANIFEST`, but it should be somehow extensible for downstream projects - and I guess that it should be done at the barclay level to do not populate directly the `Version` attribute but the `ToolkitName-Version` attribute...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4733#issuecomment-387349805
https://github.com/broadinstitute/gatk/pull/4735#issuecomment-386333319:2327,Testability,test,test,2327,roadinstitute/gatk/pull/4735/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL0Fzc2VtYmx5Q29udGlnQWxpZ25tZW50U2lnbmF0dXJlQ2xhc3NpZmllci5qYXZh) | `58.602% <0%> (-25.269%)` | `30% <0%> (-10%)` | |; | [...ery/inference/SimpleNovelAdjacencyInterpreter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4735/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1NpbXBsZU5vdmVsQWRqYWNlbmN5SW50ZXJwcmV0ZXIuamF2YQ==) | `80.435% <0%> (-19.565%)` | `16% <0%> (+9%)` | |; | [.../DiscoverVariantsFromContigAlignmentsSAMSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4735/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvRGlzY292ZXJWYXJpYW50c0Zyb21Db250aWdBbGlnbm1lbnRzU0FNU3BhcmsuamF2YQ==) | `88.889% <0%> (-4.659%)` | `45% <0%> (+23%)` | |; | [...ellbender/utils/test/CommandLineProgramTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/4735/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0NvbW1hbmRMaW5lUHJvZ3JhbVRlc3Rlci5qYXZh) | `91.667% <0%> (-3.571%)` | `11% <0%> (+2%)` | |; | [...r/arguments/CopyNumberArgumentValidationUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4735/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2FyZ3VtZW50cy9Db3B5TnVtYmVyQXJndW1lbnRWYWxpZGF0aW9uVXRpbHMuamF2YQ==) | `65.333% <0%> (-1.831%)` | `19% <0%> (ø)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4735/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `62.887% <0%> (-1.023%)` | `53% <0%> (+20%)` | |; | [...itute/hellbender/tools/walkers/bqsr/ApplyBQSR.java](https://codecov.io/gh/broadinstitute/gatk/pull/4735/diff,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4735#issuecomment-386333319
https://github.com/broadinstitute/gatk/pull/4735#issuecomment-386333319:2977,Testability,test,test,2977,verVariantsFromContigAlignmentsSAMSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4735/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvRGlzY292ZXJWYXJpYW50c0Zyb21Db250aWdBbGlnbm1lbnRzU0FNU3BhcmsuamF2YQ==) | `88.889% <0%> (-4.659%)` | `45% <0%> (+23%)` | |; | [...ellbender/utils/test/CommandLineProgramTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/4735/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0NvbW1hbmRMaW5lUHJvZ3JhbVRlc3Rlci5qYXZh) | `91.667% <0%> (-3.571%)` | `11% <0%> (+2%)` | |; | [...r/arguments/CopyNumberArgumentValidationUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4735/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2FyZ3VtZW50cy9Db3B5TnVtYmVyQXJndW1lbnRWYWxpZGF0aW9uVXRpbHMuamF2YQ==) | `65.333% <0%> (-1.831%)` | `19% <0%> (ø)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4735/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `62.887% <0%> (-1.023%)` | `53% <0%> (+20%)` | |; | [...itute/hellbender/tools/walkers/bqsr/ApplyBQSR.java](https://codecov.io/gh/broadinstitute/gatk/pull/4735/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQXBwbHlCUVNSLmphdmE=) | `90.909% <0%> (-0.758%)` | `11% <0%> (+5%)` | |; | [...iscoverFromLocalAssemblyContigAlignmentsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4735/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvU3ZEaXNjb3ZlckZyb21Mb2NhbEFzc2VtYmx5Q29udGlnQWxpZ25tZW50c1NwYXJrLmphdmE=) | `76.048% <0%> (-0.474%)` | `6% <0%> (+1%)` | |; | [...ute/hellbender/tools/copynumber/ModelSegments.java](https://codecov.io/gh/broadinstitute/gatk/pull/,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4735#issuecomment-386333319
https://github.com/broadinstitute/gatk/pull/4735#issuecomment-386333319:1631,Usability,Simpl,SimpleNovelAdjacencyInterpreter,1631,nches 10179 11987 +1808 ; ===============================================; + Hits 50460 56366 +5906 ; - Misses 8648 9368 +720 ; - Partials 3985 4276 +291; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4735?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ellbender/tools/walkers/vqsr/CNNScoreVariants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4735/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3Zxc3IvQ05OU2NvcmVWYXJpYW50cy5qYXZh) | `80.612% <26.667%> (+5.726%)` | `55 <0> (+14)` | :arrow_up: |; | [...ce/AssemblyContigAlignmentSignatureClassifier.java](https://codecov.io/gh/broadinstitute/gatk/pull/4735/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL0Fzc2VtYmx5Q29udGlnQWxpZ25tZW50U2lnbmF0dXJlQ2xhc3NpZmllci5qYXZh) | `58.602% <0%> (-25.269%)` | `30% <0%> (-10%)` | |; | [...ery/inference/SimpleNovelAdjacencyInterpreter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4735/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1NpbXBsZU5vdmVsQWRqYWNlbmN5SW50ZXJwcmV0ZXIuamF2YQ==) | `80.435% <0%> (-19.565%)` | `16% <0%> (+9%)` | |; | [.../DiscoverVariantsFromContigAlignmentsSAMSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4735/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvRGlzY292ZXJWYXJpYW50c0Zyb21Db250aWdBbGlnbm1lbnRzU0FNU3BhcmsuamF2YQ==) | `88.889% <0%> (-4.659%)` | `45% <0%> (+23%)` | |; | [...ellbender/utils/test/CommandLineProgramTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/4735/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0NvbW1hbmRMaW5lUHJvZ3JhbVRlc3Rlci5qYXZh) | `91.667% <0%> (-3.571%)` | `11% <0%> (+2%)` | |; | [...r/arguments/CopyNumberArgumen,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4735#issuecomment-386333319
https://github.com/broadinstitute/gatk/pull/4735#issuecomment-387099626:35,Deployability,update,updates,35,Back to @erniebrau. Thanks for the updates - I requested a few changes which I'm happy to help with if you want.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4735#issuecomment-387099626
https://github.com/broadinstitute/gatk/pull/4735#issuecomment-387223099:165,Deployability,update,update,165,"@erniebrau I'm making the gradle changes to generate the yml files from a template in another branch, since I had to do so anyway for unrelated reasons. Will put an update here when thats ready.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4735#issuecomment-387223099
https://github.com/broadinstitute/gatk/issues/4737#issuecomment-386594579:230,Energy Efficiency,allocate,allocate,230,"What do you mean by more automated? It looks like you're allocating space based on the input file sizes and some padding, which is already more automated than the user adjusting disk size by hand. Do you mean that Cromwell should allocate space appropriately given the inputs? The issue is that you also need space for the outputs, which is harder to predict unless you have a sense of what the task is doing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4737#issuecomment-386594579
https://github.com/broadinstitute/gatk/issues/4737#issuecomment-386594579:351,Safety,predict,predict,351,"What do you mean by more automated? It looks like you're allocating space based on the input file sizes and some padding, which is already more automated than the user adjusting disk size by hand. Do you mean that Cromwell should allocate space appropriately given the inputs? The issue is that you also need space for the outputs, which is harder to predict unless you have a sense of what the task is doing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4737#issuecomment-386594579
https://github.com/broadinstitute/gatk/issues/4738#issuecomment-387828741:65,Security,validat,validating,65,"@ldgauthier Concordance with XHMM would definitely be useful for validating calls on clusters for which we do not have WGS data. ; We need to modify code for having a fixed common CNV regions, but that should be straightforward.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4738#issuecomment-387828741
https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387300313:166,Availability,error,error,166,"This is an issue with the Gencode files chosen for the datasources. The Fasta file we use currently is a subset of the total genes in Gencode, so this is an expected error (though it shouldn't be a user error). We have 2 options:; 1) changing the code to throw a warning and ignore variants in transcripts not in the Fasta.; 2) Download a larger set of sequences from Gencode and make sure all transcripts are represented in the Fasta file. My choice is 2 because it is a relatively seamless update (though the data sources will need to be updated, and this will add about 1.6 GB to the data sources). @LeeTL1220 Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387300313
https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387300313:203,Availability,error,error,203,"This is an issue with the Gencode files chosen for the datasources. The Fasta file we use currently is a subset of the total genes in Gencode, so this is an expected error (though it shouldn't be a user error). We have 2 options:; 1) changing the code to throw a warning and ignore variants in transcripts not in the Fasta.; 2) Download a larger set of sequences from Gencode and make sure all transcripts are represented in the Fasta file. My choice is 2 because it is a relatively seamless update (though the data sources will need to be updated, and this will add about 1.6 GB to the data sources). @LeeTL1220 Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387300313
https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387300313:328,Availability,Down,Download,328,"This is an issue with the Gencode files chosen for the datasources. The Fasta file we use currently is a subset of the total genes in Gencode, so this is an expected error (though it shouldn't be a user error). We have 2 options:; 1) changing the code to throw a warning and ignore variants in transcripts not in the Fasta.; 2) Download a larger set of sequences from Gencode and make sure all transcripts are represented in the Fasta file. My choice is 2 because it is a relatively seamless update (though the data sources will need to be updated, and this will add about 1.6 GB to the data sources). @LeeTL1220 Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387300313
https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387300313:492,Deployability,update,update,492,"This is an issue with the Gencode files chosen for the datasources. The Fasta file we use currently is a subset of the total genes in Gencode, so this is an expected error (though it shouldn't be a user error). We have 2 options:; 1) changing the code to throw a warning and ignore variants in transcripts not in the Fasta.; 2) Download a larger set of sequences from Gencode and make sure all transcripts are represented in the Fasta file. My choice is 2 because it is a relatively seamless update (though the data sources will need to be updated, and this will add about 1.6 GB to the data sources). @LeeTL1220 Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387300313
https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387300313:540,Deployability,update,updated,540,"This is an issue with the Gencode files chosen for the datasources. The Fasta file we use currently is a subset of the total genes in Gencode, so this is an expected error (though it shouldn't be a user error). We have 2 options:; 1) changing the code to throw a warning and ignore variants in transcripts not in the Fasta.; 2) Download a larger set of sequences from Gencode and make sure all transcripts are represented in the Fasta file. My choice is 2 because it is a relatively seamless update (though the data sources will need to be updated, and this will add about 1.6 GB to the data sources). @LeeTL1220 Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387300313
https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387377523:267,Availability,error,error,267,"Option 2, for sure. On Tue, May 8, 2018, 02:38 Jonn Smith <notifications@github.com> wrote:. > This is an issue with the Gencode files chosen for the datasources. The; > Fasta file we use currently is a subset of the total genes in Gencode, so; > this is an expected error (though it shouldn't be a user error).; >; > We have 2 options:; >; > 1. changing the code to throw a warning and ignore variants in; > transcripts not in the Fasta.; > 2. Download a larger set of sequences from Gencode and make sure all; > transcripts are represented in the Fasta file.; >; > My choice is 2 because it is a relatively seamless update (though the data; > sources will need to be updated, and this will add about 1.6 GB to the data; > sources).; >; > @LeeTL1220 <https://github.com/LeeTL1220> Thoughts?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387300313>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_4_-aePFLbkcvBhmHeQhZRgWY0Sks5twT1igaJpZM4T1InN>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387377523
https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387377523:304,Availability,error,error,304,"Option 2, for sure. On Tue, May 8, 2018, 02:38 Jonn Smith <notifications@github.com> wrote:. > This is an issue with the Gencode files chosen for the datasources. The; > Fasta file we use currently is a subset of the total genes in Gencode, so; > this is an expected error (though it shouldn't be a user error).; >; > We have 2 options:; >; > 1. changing the code to throw a warning and ignore variants in; > transcripts not in the Fasta.; > 2. Download a larger set of sequences from Gencode and make sure all; > transcripts are represented in the Fasta file.; >; > My choice is 2 because it is a relatively seamless update (though the data; > sources will need to be updated, and this will add about 1.6 GB to the data; > sources).; >; > @LeeTL1220 <https://github.com/LeeTL1220> Thoughts?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387300313>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_4_-aePFLbkcvBhmHeQhZRgWY0Sks5twT1igaJpZM4T1InN>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387377523
https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387377523:445,Availability,Down,Download,445,"Option 2, for sure. On Tue, May 8, 2018, 02:38 Jonn Smith <notifications@github.com> wrote:. > This is an issue with the Gencode files chosen for the datasources. The; > Fasta file we use currently is a subset of the total genes in Gencode, so; > this is an expected error (though it shouldn't be a user error).; >; > We have 2 options:; >; > 1. changing the code to throw a warning and ignore variants in; > transcripts not in the Fasta.; > 2. Download a larger set of sequences from Gencode and make sure all; > transcripts are represented in the Fasta file.; >; > My choice is 2 because it is a relatively seamless update (though the data; > sources will need to be updated, and this will add about 1.6 GB to the data; > sources).; >; > @LeeTL1220 <https://github.com/LeeTL1220> Thoughts?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387300313>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_4_-aePFLbkcvBhmHeQhZRgWY0Sks5twT1igaJpZM4T1InN>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387377523
https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387377523:618,Deployability,update,update,618,"Option 2, for sure. On Tue, May 8, 2018, 02:38 Jonn Smith <notifications@github.com> wrote:. > This is an issue with the Gencode files chosen for the datasources. The; > Fasta file we use currently is a subset of the total genes in Gencode, so; > this is an expected error (though it shouldn't be a user error).; >; > We have 2 options:; >; > 1. changing the code to throw a warning and ignore variants in; > transcripts not in the Fasta.; > 2. Download a larger set of sequences from Gencode and make sure all; > transcripts are represented in the Fasta file.; >; > My choice is 2 because it is a relatively seamless update (though the data; > sources will need to be updated, and this will add about 1.6 GB to the data; > sources).; >; > @LeeTL1220 <https://github.com/LeeTL1220> Thoughts?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387300313>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_4_-aePFLbkcvBhmHeQhZRgWY0Sks5twT1igaJpZM4T1InN>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387377523
https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387377523:669,Deployability,update,updated,669,"Option 2, for sure. On Tue, May 8, 2018, 02:38 Jonn Smith <notifications@github.com> wrote:. > This is an issue with the Gencode files chosen for the datasources. The; > Fasta file we use currently is a subset of the total genes in Gencode, so; > this is an expected error (though it shouldn't be a user error).; >; > We have 2 options:; >; > 1. changing the code to throw a warning and ignore variants in; > transcripts not in the Fasta.; > 2. Download a larger set of sequences from Gencode and make sure all; > transcripts are represented in the Fasta file.; >; > My choice is 2 because it is a relatively seamless update (though the data; > sources will need to be updated, and this will add about 1.6 GB to the data; > sources).; >; > @LeeTL1220 <https://github.com/LeeTL1220> Thoughts?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387300313>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_4_-aePFLbkcvBhmHeQhZRgWY0Sks5twT1igaJpZM4T1InN>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4739#issuecomment-387377523
https://github.com/broadinstitute/gatk/pull/4740#issuecomment-387475240:41,Performance,perform,performance,41,"@lbergelson Can you add a summary of the performance improvements (eg., runtime % speedup) introduced in this branch as a comment to the master Funcotator performance ticket (https://github.com/broadinstitute/gatk/issues/4586)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4740#issuecomment-387475240
https://github.com/broadinstitute/gatk/pull/4740#issuecomment-387475240:155,Performance,perform,performance,155,"@lbergelson Can you add a summary of the performance improvements (eg., runtime % speedup) introduced in this branch as a comment to the master Funcotator performance ticket (https://github.com/broadinstitute/gatk/issues/4586)?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4740#issuecomment-387475240
https://github.com/broadinstitute/gatk/pull/4740#issuecomment-387635917:34,Deployability,update,update,34,@lbergelson - might make sense to update the Funcotator version from `0.0.2` to `0.0.3` since this is a big improvement.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4740#issuecomment-387635917
https://github.com/broadinstitute/gatk/issues/4741#issuecomment-387155861:11,Availability,error,error,11,"@markw The error message above looks like the one I'd expect you'd get if you haven't built the python archive (`./gradlew createPythonPackageAchive`), as discussed in the Slack thread. I assume you get a different error message once you've done that (which was in turn resolved by your moving the .yml file). If thats right, can you update this ticket with that message/output, and the conda version you're using.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741#issuecomment-387155861
https://github.com/broadinstitute/gatk/issues/4741#issuecomment-387155861:215,Availability,error,error,215,"@markw The error message above looks like the one I'd expect you'd get if you haven't built the python archive (`./gradlew createPythonPackageAchive`), as discussed in the Slack thread. I assume you get a different error message once you've done that (which was in turn resolved by your moving the .yml file). If thats right, can you update this ticket with that message/output, and the conda version you're using.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741#issuecomment-387155861
https://github.com/broadinstitute/gatk/issues/4741#issuecomment-387155861:334,Deployability,update,update,334,"@markw The error message above looks like the one I'd expect you'd get if you haven't built the python archive (`./gradlew createPythonPackageAchive`), as discussed in the Slack thread. I assume you get a different error message once you've done that (which was in turn resolved by your moving the .yml file). If thats right, can you update this ticket with that message/output, and the conda version you're using.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741#issuecomment-387155861
https://github.com/broadinstitute/gatk/issues/4741#issuecomment-387155861:17,Integrability,message,message,17,"@markw The error message above looks like the one I'd expect you'd get if you haven't built the python archive (`./gradlew createPythonPackageAchive`), as discussed in the Slack thread. I assume you get a different error message once you've done that (which was in turn resolved by your moving the .yml file). If thats right, can you update this ticket with that message/output, and the conda version you're using.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741#issuecomment-387155861
https://github.com/broadinstitute/gatk/issues/4741#issuecomment-387155861:221,Integrability,message,message,221,"@markw The error message above looks like the one I'd expect you'd get if you haven't built the python archive (`./gradlew createPythonPackageAchive`), as discussed in the Slack thread. I assume you get a different error message once you've done that (which was in turn resolved by your moving the .yml file). If thats right, can you update this ticket with that message/output, and the conda version you're using.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741#issuecomment-387155861
https://github.com/broadinstitute/gatk/issues/4741#issuecomment-387155861:363,Integrability,message,message,363,"@markw The error message above looks like the one I'd expect you'd get if you haven't built the python archive (`./gradlew createPythonPackageAchive`), as discussed in the Slack thread. I assume you get a different error message once you've done that (which was in turn resolved by your moving the .yml file). If thats right, can you update this ticket with that message/output, and the conda version you're using.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741#issuecomment-387155861
https://github.com/broadinstitute/gatk/issues/4741#issuecomment-387166077:124,Availability,error,error,124,"@cmnbroad Apologies that I didn't specify that I did run `./gradlew createPythonPackageArchive` first. You can see from the error that it's looking for the archive in `gatk/scripts/build/` (which doesn't exist) instead of `gatk/build`. It seems to be searching for `./build/gatkPythonPackageArchive.zip` relative to the scripts sub-directory rather than the root (also didn't mention I ran conda from the gatk root dir). Running conda v4.5.2, python v3.6.4. I run everything from a conda env created with: `conda create -n py36 python=3.6 anaconda`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741#issuecomment-387166077
https://github.com/broadinstitute/gatk/issues/4741#issuecomment-387168692:22,Deployability,upgrade,upgrade,22,"Oh yeah, I see. I did upgrade to latests minconda and now I see the same issue. So something changed in conda.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741#issuecomment-387168692
https://github.com/broadinstitute/gatk/issues/4741#issuecomment-387439532:994,Deployability,update,updates,994,"It does look like there are some references to similar issues(see comments in https://stackoverflow.com/questions/35245401/combining-conda-environment-yml-with-pip-requirements-txt) in some stack overflow threads, and some conda tickets that may be related. Given this issue, and the unrelated but evolving requirement that we have more than a single conda yml (we need a separate one with the intel tensorflow version that supports acceleration on intel hardware), I'm going to make the following changes:. Add a single conda template and a gradle task that generates the two (nearly but not quite identical) yml files as part of the build. These will be stored in the build directory with the python package archive, so the yml file no longer has to specify a path to the zip (which we previously had to strip when we included the file in the zip/tar distribution). The zip and yml files will be copied into the tar/zip distributions as was previously done.; Add a gradle target that creates/updates the local conda dev. This is for local use by devs during iterative development. The yml files will still exist in the build directory, and can be used manually like they previously were, but from the build directory instead of in the root.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4741#issuecomment-387439532
https://github.com/broadinstitute/gatk/issues/4742#issuecomment-387139875:53,Integrability,depend,dependencies,53,"What is the timeline for removing pysam, PyVCF, etc. dependencies (#4465)? I think we agreed to abide by certain rules when we started python development, but I don't know who is responsible for enforcing them...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-387139875
https://github.com/broadinstitute/gatk/issues/4742#issuecomment-387157643:26,Integrability,depend,dependent,26,"I think removing pysam is dependent on writing Java front ends for the CNN tools that are currently just pass-through to python (https://github.com/broadinstitute/gatk/issues/4533, https://github.com/broadinstitute/gatk/issues/4534 and https://github.com/broadinstitute/gatk/issues/4535). @lucidtronix Do you have a timeframe for that work ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-387157643
https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193:386,Deployability,install,installed,386,"Sorry, just saw this now. We still don't have a simple solution for training models without pysam. We can probably do something similar to what we do with inference, but I think the current priority is to improve inference throughput so it will probably be a little while before we get to re-writing the training code. If people feel we should re-prioritize please let me know.; I have installed the conda environment on the same OSX version, without seeing this issue.; Which gcc version are you using @mwalker174 ? ; My `gcc -v` output is:; ```; Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1; Apple LLVM version 8.0.0 (clang-800.0.42.1); Target: x86_64-apple-darwin15.6.0; Thread model: posix; InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193
https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193:769,Deployability,Install,InstalledDir,769,"Sorry, just saw this now. We still don't have a simple solution for training models without pysam. We can probably do something similar to what we do with inference, but I think the current priority is to improve inference throughput so it will probably be a little while before we get to re-writing the training code. If people feel we should re-prioritize please let me know.; I have installed the conda environment on the same OSX version, without seeing this issue.; Which gcc version are you using @mwalker174 ? ; My `gcc -v` output is:; ```; Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1; Apple LLVM version 8.0.0 (clang-800.0.42.1); Target: x86_64-apple-darwin15.6.0; Thread model: posix; InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193
https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193:548,Modifiability,Config,Configured,548,"Sorry, just saw this now. We still don't have a simple solution for training models without pysam. We can probably do something similar to what we do with inference, but I think the current priority is to improve inference throughput so it will probably be a little while before we get to re-writing the training code. If people feel we should re-prioritize please let me know.; I have installed the conda environment on the same OSX version, without seeing this issue.; Which gcc version are you using @mwalker174 ? ; My `gcc -v` output is:; ```; Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1; Apple LLVM version 8.0.0 (clang-800.0.42.1); Target: x86_64-apple-darwin15.6.0; Thread model: posix; InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193
https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193:223,Performance,throughput,throughput,223,"Sorry, just saw this now. We still don't have a simple solution for training models without pysam. We can probably do something similar to what we do with inference, but I think the current priority is to improve inference throughput so it will probably be a little while before we get to re-writing the training code. If people feel we should re-prioritize please let me know.; I have installed the conda environment on the same OSX version, without seeing this issue.; Which gcc version are you using @mwalker174 ? ; My `gcc -v` output is:; ```; Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1; Apple LLVM version 8.0.0 (clang-800.0.42.1); Target: x86_64-apple-darwin15.6.0; Thread model: posix; InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193
https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193:48,Usability,simpl,simple,48,"Sorry, just saw this now. We still don't have a simple solution for training models without pysam. We can probably do something similar to what we do with inference, but I think the current priority is to improve inference throughput so it will probably be a little while before we get to re-writing the training code. If people feel we should re-prioritize please let me know.; I have installed the conda environment on the same OSX version, without seeing this issue.; Which gcc version are you using @mwalker174 ? ; My `gcc -v` output is:; ```; Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1; Apple LLVM version 8.0.0 (clang-800.0.42.1); Target: x86_64-apple-darwin15.6.0; Thread model: posix; InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391014193
https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177:45,Deployability,configurat,configuration,45,@lucidtronix Probably something's up with my configuration...; `; $ gcc -v; Using built-in specs.; COLLECT_GCC=gcc; COLLECT_LTO_WRAPPER=/Users/markw/anaconda/envs/py27/bin/../libexec/gcc/x86_64-apple-darwin11.4.2/4.8.5/lto-wrapper; Target: x86_64-apple-darwin11.4.2; Configured with: ./configure --prefix=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-gxx-include-dir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/gcc/include/c++ --bindir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/bin --datarootdir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/share --libdir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/lib --with-gmp=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-mpfr=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_147764901285,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177
https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177:3700,Deployability,release,release,3700,"ehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-mpfr=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-mpc=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-isl=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-cloog=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-boot-ldflags='-Wl,-headerpad_max_install_names -Wl,-L/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/lib -Wl,-L/usr/lib' --with-stage1-ldflags='-Wl,-headerpad_max_install_names -Wl,-L/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/lib -Wl,-L/usr/lib' --enable-checking=release --with-tune=generic --disable-multilib; Thread model: posix; gcc version 4.8.5 (GCC); `; Perhaps using the Xcode version would fix things?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177
https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177:223,Integrability,wrap,wrapper,223,@lucidtronix Probably something's up with my configuration...; `; $ gcc -v; Using built-in specs.; COLLECT_GCC=gcc; COLLECT_LTO_WRAPPER=/Users/markw/anaconda/envs/py27/bin/../libexec/gcc/x86_64-apple-darwin11.4.2/4.8.5/lto-wrapper; Target: x86_64-apple-darwin11.4.2; Configured with: ./configure --prefix=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-gxx-include-dir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/gcc/include/c++ --bindir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/bin --datarootdir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/share --libdir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/lib --with-gmp=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-mpfr=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_147764901285,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177
https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177:45,Modifiability,config,configuration,45,@lucidtronix Probably something's up with my configuration...; `; $ gcc -v; Using built-in specs.; COLLECT_GCC=gcc; COLLECT_LTO_WRAPPER=/Users/markw/anaconda/envs/py27/bin/../libexec/gcc/x86_64-apple-darwin11.4.2/4.8.5/lto-wrapper; Target: x86_64-apple-darwin11.4.2; Configured with: ./configure --prefix=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-gxx-include-dir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/gcc/include/c++ --bindir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/bin --datarootdir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/share --libdir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/lib --with-gmp=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-mpfr=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_147764901285,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177
https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177:267,Modifiability,Config,Configured,267,@lucidtronix Probably something's up with my configuration...; `; $ gcc -v; Using built-in specs.; COLLECT_GCC=gcc; COLLECT_LTO_WRAPPER=/Users/markw/anaconda/envs/py27/bin/../libexec/gcc/x86_64-apple-darwin11.4.2/4.8.5/lto-wrapper; Target: x86_64-apple-darwin11.4.2; Configured with: ./configure --prefix=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-gxx-include-dir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/gcc/include/c++ --bindir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/bin --datarootdir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/share --libdir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/lib --with-gmp=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-mpfr=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_147764901285,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177
https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177:286,Modifiability,config,configure,286,@lucidtronix Probably something's up with my configuration...; `; $ gcc -v; Using built-in specs.; COLLECT_GCC=gcc; COLLECT_LTO_WRAPPER=/Users/markw/anaconda/envs/py27/bin/../libexec/gcc/x86_64-apple-darwin11.4.2/4.8.5/lto-wrapper; Target: x86_64-apple-darwin11.4.2; Configured with: ./configure --prefix=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-gxx-include-dir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/gcc/include/c++ --bindir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/bin --datarootdir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/share --libdir=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/lib --with-gmp=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-mpfr=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_147764901285,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177
https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177:3715,Performance,tune,tune,3715,"ehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-mpfr=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-mpc=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-isl=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-cloog=/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --with-boot-ldflags='-Wl,-headerpad_max_install_names -Wl,-L/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/lib -Wl,-L/usr/lib' --with-stage1-ldflags='-Wl,-headerpad_max_install_names -Wl,-L/Users/ray/mc-x64-3.5/conda-bld/gcc-4.8_1477649012852/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/lib -Wl,-L/usr/lib' --enable-checking=release --with-tune=generic --disable-multilib; Thread model: posix; gcc version 4.8.5 (GCC); `; Perhaps using the Xcode version would fix things?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742#issuecomment-391065177
https://github.com/broadinstitute/gatk/pull/4744#issuecomment-387807462:2236,Testability,test,test,2236, [...titute/hellbender/engine/TwoPassVariantWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/4744/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvVHdvUGFzc1ZhcmlhbnRXYWxrZXIuamF2YQ==) | `95% <95%> (ø)` | `4 <4> (?)` | |; | [...ery/inference/SimpleNovelAdjacencyInterpreter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4744/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1NpbXBsZU5vdmVsQWRqYWNlbmN5SW50ZXJwcmV0ZXIuamF2YQ==) | `80.435% <0%> (-19.565%)` | `15% <0%> (+8%)` | |; | [.../DiscoverVariantsFromContigAlignmentsSAMSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4744/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvRGlzY292ZXJWYXJpYW50c0Zyb21Db250aWdBbGlnbm1lbnRzU0FNU3BhcmsuamF2YQ==) | `88.889% <0%> (-4.659%)` | `49% <0%> (+27%)` | |; | [...ellbender/utils/test/CommandLineProgramTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/4744/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0NvbW1hbmRMaW5lUHJvZ3JhbVRlc3Rlci5qYXZh) | `91.667% <0%> (-3.571%)` | `11% <0%> (+2%)` | |; | [...r/utils/read/markduplicates/sparkrecords/Pair.java](https://codecov.io/gh/broadinstitute/gatk/pull/4744/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9QYWlyLmphdmE=) | `93.893% <0%> (-1.612%)` | `44% <0%> (+22%)` | |; | [...iscoverFromLocalAssemblyContigAlignmentsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4744/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvU3ZEaXNjb3ZlckZyb21Mb2NhbEFzc2VtYmx5Q29udGlnQWxpZ25tZW50c1NwYXJrLmphdmE=) | `76.048% <0%> (-0.474%)` | `5% <0%> (ø)` | |; | [...lbender/tools/spark/sv/discovery/SimpleSVType.java](https://c,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4744#issuecomment-387807462
https://github.com/broadinstitute/gatk/pull/4744#issuecomment-387807462:1540,Usability,Simpl,SimpleNovelAdjacencyInterpreter,1540,=====================================; Files 1081 1086 +5 ; Lines 63118 66825 +3707 ; Branches 10195 11063 +868 ; ===============================================; + Hits 50485 53594 +3109 ; - Misses 8648 9073 +425 ; - Partials 3985 4158 +173; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4744?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...er/tools/examples/ExampleTwoPassVariantWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/4744/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leGFtcGxlcy9FeGFtcGxlVHdvUGFzc1ZhcmlhbnRXYWxrZXIuamF2YQ==) | `89.286% <89.286%> (ø)` | `8 <8> (?)` | |; | [...titute/hellbender/engine/TwoPassVariantWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/4744/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvVHdvUGFzc1ZhcmlhbnRXYWxrZXIuamF2YQ==) | `95% <95%> (ø)` | `4 <4> (?)` | |; | [...ery/inference/SimpleNovelAdjacencyInterpreter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4744/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1NpbXBsZU5vdmVsQWRqYWNlbmN5SW50ZXJwcmV0ZXIuamF2YQ==) | `80.435% <0%> (-19.565%)` | `15% <0%> (+8%)` | |; | [.../DiscoverVariantsFromContigAlignmentsSAMSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4744/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvRGlzY292ZXJWYXJpYW50c0Zyb21Db250aWdBbGlnbm1lbnRzU0FNU3BhcmsuamF2YQ==) | `88.889% <0%> (-4.659%)` | `49% <0%> (+27%)` | |; | [...ellbender/utils/test/CommandLineProgramTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/4744/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0NvbW1hbmRMaW5lUHJvZ3JhbVRlc3Rlci5qYXZh) | `91.667% <0%> (-3.571%)` | `11% <0%> (+2%)` | |; | [...r/utils/read/markduplicates/s,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4744#issuecomment-387807462
https://github.com/broadinstitute/gatk/pull/4744#issuecomment-387807462:3215,Usability,Simpl,SimpleSVType,3215,%> (+27%)` | |; | [...ellbender/utils/test/CommandLineProgramTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/4744/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0NvbW1hbmRMaW5lUHJvZ3JhbVRlc3Rlci5qYXZh) | `91.667% <0%> (-3.571%)` | `11% <0%> (+2%)` | |; | [...r/utils/read/markduplicates/sparkrecords/Pair.java](https://codecov.io/gh/broadinstitute/gatk/pull/4744/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9QYWlyLmphdmE=) | `93.893% <0%> (-1.612%)` | `44% <0%> (+22%)` | |; | [...iscoverFromLocalAssemblyContigAlignmentsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4744/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvU3ZEaXNjb3ZlckZyb21Mb2NhbEFzc2VtYmx5Q29udGlnQWxpZ25tZW50c1NwYXJrLmphdmE=) | `76.048% <0%> (-0.474%)` | `5% <0%> (ø)` | |; | [...lbender/tools/spark/sv/discovery/SimpleSVType.java](https://codecov.io/gh/broadinstitute/gatk/pull/4744/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvU2ltcGxlU1ZUeXBlLmphdmE=) | `86% <0%> (-0.441%)` | `3% <0%> (+1%)` | |; | [.../discovery/inference/ImpreciseVariantDetector.java](https://codecov.io/gh/broadinstitute/gatk/pull/4744/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL0ltcHJlY2lzZVZhcmlhbnREZXRlY3Rvci5qYXZh) | `80.952% <0%> (-0.298%)` | `6% <0%> (ø)` | |; | [...itute/hellbender/engine/spark/GATKRegistrator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4744/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1JlZ2lzdHJhdG9yLmphdmE=) | `100% <0%> (ø)` | `6% <0%> (+3%)` | :arrow_up: |; | ... and [57 more](https://codecov.io/gh/broadinstitute/gatk/pull/4744/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4744#issuecomment-387807462
https://github.com/broadinstitute/gatk/pull/4744#issuecomment-389992421:72,Testability,test,tests,72,I pushed the final requested changes myself just now -- will merge once tests pass.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4744#issuecomment-389992421
https://github.com/broadinstitute/gatk/issues/4745#issuecomment-387218068:61,Performance,tune,tune,61,Are we interested in writing some definitive guide on how to tune the `af-of-alleles-not-in-resource` parameter for different contexts?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4745#issuecomment-387218068
https://github.com/broadinstitute/gatk/issues/4745#issuecomment-387218068:45,Usability,guid,guide,45,Are we interested in writing some definitive guide on how to tune the `af-of-alleles-not-in-resource` parameter for different contexts?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4745#issuecomment-387218068
https://github.com/broadinstitute/gatk/issues/4745#issuecomment-387262066:25,Deployability,release,release,25,"@sooheelee In the latest release the tool automatically chooses different defaults for tumor-only versus tumor-normal, if no value is given. The best practice is now to specify nothing unless the user really knows what she is doing.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4745#issuecomment-387262066
https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387445952:151,Testability,test,test,151,One potential caveat I just noticed: it looks like the native libraries might introduce more multithreading which might not always be desired. In some test runs I'm seeing CPU usage spike very high (eg up to 3200% in `top`) even though I've specified `--spark-master local[8]`.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387445952
https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906:132,Modifiability,extend,extended,132,"This looks promising, at a minimum we should try setting up the docker with hadoop native libraries so the performance gains can be extended to most use cases. This might also include adding some magic to the gatk launch script inside the docker to detect and run with the correct version of the hadoop libraries.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906
https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906:107,Performance,perform,performance,107,"This looks promising, at a minimum we should try setting up the docker with hadoop native libraries so the performance gains can be extended to most use cases. This might also include adding some magic to the gatk launch script inside the docker to detect and run with the correct version of the hadoop libraries.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906
https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906:249,Safety,detect,detect,249,"This looks promising, at a minimum we should try setting up the docker with hadoop native libraries so the performance gains can be extended to most use cases. This might also include adding some magic to the gatk launch script inside the docker to detect and run with the correct version of the hadoop libraries.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746#issuecomment-387519906
https://github.com/broadinstitute/gatk/pull/4749#issuecomment-387489170:20,Testability,test,test,20,@mwalker174 Can you test this branch out?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4749#issuecomment-387489170
https://github.com/broadinstitute/gatk/pull/4749#issuecomment-390822655:3854,Testability,test,test,3854,yContigAlignmentsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4749/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvU3ZEaXNjb3ZlckZyb21Mb2NhbEFzc2VtYmx5Q29udGlnQWxpZ25tZW50c1NwYXJrLmphdmE=) | `77.019% <0%> (-0.963%)` | `3% <0%> (+1%)` | |; | [...kers/variantutils/CalculateGenotypePosteriors.java](https://codecov.io/gh/broadinstitute/gatk/pull/4749/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9DYWxjdWxhdGVHZW5vdHlwZVBvc3RlcmlvcnMuamF2YQ==) | `92.308% <0%> (-0.549%)` | `14% <0%> (-3%)` | |; | [...tools/walkers/genotyper/AlleleSubsettingUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4749/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9BbGxlbGVTdWJzZXR0aW5nVXRpbHMuamF2YQ==) | `91.228% <0%> (-0.507%)` | `45% <0%> (ø)` | |; | [...der/tools/walkers/mutect/M2ArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4749/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NMkFyZ3VtZW50Q29sbGVjdGlvbi5qYXZh) | `93.939% <0%> (-0.505%)` | `5% <0%> (+2%)` | |; | [...kers/haplotypecaller/ReferenceConfidenceModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/4749/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9SZWZlcmVuY2VDb25maWRlbmNlTW9kZWwuamF2YQ==) | `93.194% <0%> (-0.339%)` | `61% <0%> (-7%)` | |; | [...hellbender/utils/test/VariantContextTestUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4749/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1ZhcmlhbnRDb250ZXh0VGVzdFV0aWxzLmphdmE=) | `83.262% <0%> (-0.283%)` | `64% <0%> (-2%)` | |; | ... and [49 more](https://codecov.io/gh/broadinstitute/gatk/pull/4749/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4749#issuecomment-390822655
https://github.com/broadinstitute/gatk/pull/4750#issuecomment-387898984:3937,Deployability,pipeline,pipelines,3937,o/gh/broadinstitute/gatk/pull/4750/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL21hcmtkdXBsaWNhdGVzL0VzdGltYXRlTGlicmFyeUNvbXBsZXhpdHlHQVRLLmphdmE=) | `0% <0%> (ø)` | `0 <0> (ø)` | :arrow_down: |; | [...hellbender/utils/read/markduplicates/ReadEnds.java](https://codecov.io/gh/broadinstitute/gatk/pull/4750/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL1JlYWRFbmRzLmphdmE=) | `92% <100%> (ø)` | `15 <5> (ø)` | :arrow_down: |; | [...r/utils/read/markduplicates/sparkrecords/Pair.java](https://codecov.io/gh/broadinstitute/gatk/pull/4750/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9QYWlyLmphdmE=) | `93.333% <100%> (+1.199%)` | `23 <7> (+1)` | :arrow_up: |; | [...forms/markduplicates/MarkDuplicatesSparkUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4750/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay90cmFuc2Zvcm1zL21hcmtkdXBsaWNhdGVzL01hcmtEdXBsaWNhdGVzU3BhcmtVdGlscy5qYXZh) | `90% <100%> (+0.36%)` | `69 <8> (+4)` | :arrow_up: |; | [...ates/AbstractMarkDuplicatesCommandLineProgram.java](https://codecov.io/gh/broadinstitute/gatk/pull/4750/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL0Fic3RyYWN0TWFya0R1cGxpY2F0ZXNDb21tYW5kTGluZVByb2dyYW0uamF2YQ==) | `93.333% <100%> (ø)` | `24 <4> (ø)` | :arrow_down: |; | [...k/pipelines/BwaAndMarkDuplicatesPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4750/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvQndhQW5kTWFya0R1cGxpY2F0ZXNQaXBlbGluZVNwYXJrLmphdmE=) | `77.778% <100%> (ø)` | `4 <0> (ø)` | :arrow_down: |; | ... and [13 more](https://codecov.io/gh/broadinstitute/gatk/pull/4750/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4750#issuecomment-387898984
https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388422469:25,Availability,failure,failures,25,@jamesemery We have test failures again https://storage.googleapis.com/hellbender-test-logs/build_reports/master_19079.2/tests/test/index.html,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388422469
https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388422469:20,Testability,test,test,20,@jamesemery We have test failures again https://storage.googleapis.com/hellbender-test-logs/build_reports/master_19079.2/tests/test/index.html,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388422469
https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388422469:82,Testability,test,test-logs,82,@jamesemery We have test failures again https://storage.googleapis.com/hellbender-test-logs/build_reports/master_19079.2/tests/test/index.html,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388422469
https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388422469:121,Testability,test,tests,121,@jamesemery We have test failures again https://storage.googleapis.com/hellbender-test-logs/build_reports/master_19079.2/tests/test/index.html,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388422469
https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388422469:127,Testability,test,test,127,@jamesemery We have test failures again https://storage.googleapis.com/hellbender-test-logs/build_reports/master_19079.2/tests/test/index.html,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388422469
https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388442661:25,Testability,test,test,25,"@lbergelson Yeah, so the test was broken because it was assuming that the pairedEnds y value was getting truncated. Now that test is running on two reads that are ACTUALLY close enough for comparison.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388442661
https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388442661:125,Testability,test,test,125,"@lbergelson Yeah, so the test was broken because it was assuming that the pairedEnds y value was getting truncated. Now that test is running on two reads that are ACTUALLY close enough for comparison.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388442661
https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388942243:42,Testability,test,test-logs,42,https://storage.googleapis.com/hellbender-test-logs/build_reports/master_19134.2/tests/test/index.html,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388942243
https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388942243:81,Testability,test,tests,81,https://storage.googleapis.com/hellbender-test-logs/build_reports/master_19134.2/tests/test/index.html,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388942243
https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388942243:87,Testability,test,test,87,https://storage.googleapis.com/hellbender-test-logs/build_reports/master_19134.2/tests/test/index.html,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4750#issuecomment-388942243
https://github.com/broadinstitute/gatk/pull/4751#issuecomment-387798252:2023,Usability,Simpl,SimpleSVType,2023,institute/gatk/pull/4751/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1NpbXBsZU5vdmVsQWRqYWNlbmN5QW5kQ2hpbWVyaWNBbGlnbm1lbnRFdmlkZW5jZS5qYXZh) | `62.963% <0%> (-24.537%)` | `14% <0%> (+4%)` | |; | [...overy/inference/NovelAdjacencyAndAltHaplotype.java](https://codecov.io/gh/broadinstitute/gatk/pull/4751/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL05vdmVsQWRqYWNlbmN5QW5kQWx0SGFwbG90eXBlLmphdmE=) | `75.556% <0%> (-3.81%)` | `46% <0%> (+17%)` | |; | [...iscoverFromLocalAssemblyContigAlignmentsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4751/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvU3ZEaXNjb3ZlckZyb21Mb2NhbEFzc2VtYmx5Q29udGlnQWxpZ25tZW50c1NwYXJrLmphdmE=) | `76.048% <0%> (-0.794%)` | `5% <0%> (+3%)` | |; | [...lbender/tools/spark/sv/discovery/SimpleSVType.java](https://codecov.io/gh/broadinstitute/gatk/pull/4751/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvU2ltcGxlU1ZUeXBlLmphdmE=) | `86% <0%> (-0.667%)` | `3% <0%> (ø)` | |; | [.../discovery/inference/ImpreciseVariantDetector.java](https://codecov.io/gh/broadinstitute/gatk/pull/4751/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL0ltcHJlY2lzZVZhcmlhbnREZXRlY3Rvci5qYXZh) | `80.952% <0%> (-0.298%)` | `6% <0%> (ø)` | |; | [...ols/spark/sv/discovery/alignment/StrandSwitch.java](https://codecov.io/gh/broadinstitute/gatk/pull/4751/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvYWxpZ25tZW50L1N0cmFuZFN3aXRjaC5qYXZh) | `100% <0%> (ø)` | `2% <0%> (+1%)` | :arrow_up: |; | [.../tools/spark/sv/discovery/BreakEndVariantType.java](https://codecov.io/gh/broadi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4751#issuecomment-387798252
https://github.com/broadinstitute/gatk/pull/4752#issuecomment-388967022:3212,Testability,test,test,3212,667% <100%> (+3.333%)` | `29 <12> (+6)` | :arrow_up: |; | [...ead/markduplicates/sparkrecords/EmptyFragment.java](https://codecov.io/gh/broadinstitute/gatk/pull/4752/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9FbXB0eUZyYWdtZW50LmphdmE=) | `88.889% <100%> (+0.654%)` | `9 <2> (+2)` | :arrow_up: |; | [...forms/markduplicates/MarkDuplicatesSparkUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4752/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay90cmFuc2Zvcm1zL21hcmtkdXBsaWNhdGVzL01hcmtEdXBsaWNhdGVzU3BhcmtVdGlscy5qYXZh) | `89.27% <75%> (-0.73%)` | `69 <2> (ø)` | |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/4752/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (-30%)` | `1% <0%> (-2%)` | |; | [...ellbender/utils/test/CommandLineProgramTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/4752/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0NvbW1hbmRMaW5lUHJvZ3JhbVRlc3Rlci5qYXZh) | `91.667% <0%> (-3.571%)` | `11% <0%> (+2%)` | |; | [...itute/hellbender/engine/spark/GATKRegistrator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4752/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1JlZ2lzdHJhdG9yLmphdmE=) | `100% <0%> (ø)` | `6% <0%> (+3%)` | :arrow_up: |; | [...titute/hellbender/engine/TwoPassVariantWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/4752/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvVHdvUGFzc1ZhcmlhbnRXYWxrZXIuamF2YQ==) | `95% <0%> (ø)` | `4% <0%> (?)` | |; | ... and [5 more](https://codecov.io/gh/broadinstitute/gatk/pull/4752/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4752#issuecomment-388967022
https://github.com/broadinstitute/gatk/issues/4753#issuecomment-387935793:47,Availability,error,error,47,"Yep, the new jar fetches and prints the TileDB error message",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4753#issuecomment-387935793
https://github.com/broadinstitute/gatk/issues/4753#issuecomment-387935793:53,Integrability,message,message,53,"Yep, the new jar fetches and prints the TileDB error message",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4753#issuecomment-387935793
https://github.com/broadinstitute/gatk/issues/4753#issuecomment-437188003:32,Modifiability,variab,variable,32,"FYI, if you set the environment variable TILEDB_DISABLE_FILE_LOCKING=1 before running any GenomicsDB tool, it doesn't try to lock files on POSIX filesystems (Lustre, NFS, xfs, ext4 etc)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4753#issuecomment-437188003
https://github.com/broadinstitute/gatk/issues/4753#issuecomment-439982616:85,Availability,error,error,85,@kgururaj Is it possible to include information about the lock disabling in the lock error messages?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4753#issuecomment-439982616
https://github.com/broadinstitute/gatk/issues/4753#issuecomment-439982616:91,Integrability,message,messages,91,@kgururaj Is it possible to include information about the lock disabling in the lock error messages?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4753#issuecomment-439982616
https://github.com/broadinstitute/gatk/issues/4754#issuecomment-388919109:217,Availability,down,down,217,"For GKL, it looks like we're cleaning up properly (see: [code here](https://github.com/Intel-HLS/GKL/blob/master/src/main/java/com/intel/gkl/NativeLibraryLoader.java#L65)). Might this be an issue of improper shutting down of GATK?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4754#issuecomment-388919109
https://github.com/broadinstitute/gatk/issues/4755#issuecomment-388377762:48,Availability,down,down,48,"@kgururaj Yeah, that's fair. If it doesn't shut down cleanly there's not much that can be done. Thanks for looking into it. One thing we could consider is deleting the generated query.json file when we close the reader the created it, as well as marking it for deletion on exit.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4755#issuecomment-388377762
https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388043128:3444,Deployability,pipeline,pipelines,3444,Rlci5qYXZh) | `83.673% <60%> (ø)` | `11 <1> (?)` | |; | [...nder/utils/runtime/StreamingProcessController.java](https://codecov.io/gh/broadinstitute/gatk/pull/4757/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9ydW50aW1lL1N0cmVhbWluZ1Byb2Nlc3NDb250cm9sbGVyLmphdmE=) | `71.292% <73.333%> (+0.922%)` | `35 <14> (-15)` | :arrow_down: |; | [...er/utils/python/StreamingPythonScriptExecutor.java](https://codecov.io/gh/broadinstitute/gatk/pull/4757/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9weXRob24vU3RyZWFtaW5nUHl0aG9uU2NyaXB0RXhlY3V0b3IuamF2YQ==) | `85.577% <83.333%> (+1.577%)` | `20 <11> (+5)` | :arrow_up: |; | [...nder/engine/datasources/ReferenceHadoopSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4757/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZGF0YXNvdXJjZXMvUmVmZXJlbmNlSGFkb29wU291cmNlLmphdmE=) | `0% <0%> (-100%)` | `0% <0%> (-3%)` | |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/4757/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (-30%)` | `1% <0%> (-2%)` | |; | [...hellbender/tools/spark/pipelines/SortSamSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4757/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvU29ydFNhbVNwYXJrLmphdmE=) | `70.588% <0%> (-29.412%)` | `4% <0%> (-2%)` | |; | [...adinstitute/hellbender/utils/spark/SparkUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4757/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zcGFyay9TcGFya1V0aWxzLmphdmE=) | `72.727% <0%> (-11.797%)` | `12% <0%> (-9%)` | |; | ... and [137 more](https://codecov.io/gh/broadinstitute/gatk/pull/4757/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388043128
https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388099208:245,Availability,reliab,reliably,245,"@lucidtronix Could you try this branch out by running your CNN tool and confirming that it runs to completion, produces correct output, and doesn't time out? It would be helpful if you could run it in a configuration that prior to this PR would reliably time out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388099208
https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388099208:203,Deployability,configurat,configuration,203,"@lucidtronix Could you try this branch out by running your CNN tool and confirming that it runs to completion, produces correct output, and doesn't time out? It would be helpful if you could run it in a configuration that prior to this PR would reliably time out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388099208
https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388099208:203,Modifiability,config,configuration,203,"@lucidtronix Could you try this branch out by running your CNN tool and confirming that it runs to completion, produces correct output, and doesn't time out? It would be helpful if you could run it in a configuration that prior to this PR would reliably time out.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388099208
https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388100436:114,Safety,timeout,timeouts,114,"@lucidtronix Specifically it would be useful to try this with various transfer/inference batch sizes now that the timeouts are gone. I tried with the original defaults (256/128), and got no timeouts, but the larger values seemed to result in longer runtimes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388100436
https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388100436:190,Safety,timeout,timeouts,190,"@lucidtronix Specifically it would be useful to try this with various transfer/inference batch sizes now that the timeouts are gone. I tried with the original defaults (256/128), and got no timeouts, but the larger values seemed to result in longer runtimes.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388100436
https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388113766:105,Safety,timeout,timeouts,105,"Note to self: we should get rid of the MAX_READ_BATCH check as part of this, since that was motivated by timeouts IIRC.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4757#issuecomment-388113766
https://github.com/broadinstitute/gatk/issues/4761#issuecomment-388416287:154,Availability,down,down,154,"@EvanTheB Thanks for moving over to the gatk tracker, I think it make more sense to start here and you'll get more eyes on it. We can always push it back down to htsjdk if we determine that's definitely where the problem is. . I downloaded that vcf, I noticed that the actual download link for it is ; ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/1000G_omni2.5.b37.vcf **.gz** . In zipped form it's a gzip file, but not a bgzip file which is what's required for tribble indexing to work. If you downloaded and unzipped it to just plain old vcf it should be fine though. . I haven't been able to reproduce the issue yet. It looks like to me like line 104 is the first non-header line of the vcf, and I do see that string it's reporting on line 242. Is it possible there's a mix up between files?; ```; 882033 rs2272756 G A . PASS CR=99.79803;GentrainScore=0.7203;HW=4.306476E-6; ```. It sounds a lot like some sort of indexing bug. I don't think it should be #4224 because you're using a vcf instead of a vcf.gz. Hopefully there isn't a different index bug lurking out there... One weird thing I did notice, is that if I regenerate the .idx file, I get a different file from what comes with the vcf from the ftp site. Indexes aren't necessarily unique to a file, but I wonder if the one on the website is corrupted in some way. Could you try running `gatk IndexFeatureFile -F 1000G_omni2.5.b37.vcf` and then re-running your command with the newly generated index. I'm hoping that will fix it. If not we'll have to dig in more.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4761#issuecomment-388416287
https://github.com/broadinstitute/gatk/issues/4761#issuecomment-388416287:229,Availability,down,downloaded,229,"@EvanTheB Thanks for moving over to the gatk tracker, I think it make more sense to start here and you'll get more eyes on it. We can always push it back down to htsjdk if we determine that's definitely where the problem is. . I downloaded that vcf, I noticed that the actual download link for it is ; ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/1000G_omni2.5.b37.vcf **.gz** . In zipped form it's a gzip file, but not a bgzip file which is what's required for tribble indexing to work. If you downloaded and unzipped it to just plain old vcf it should be fine though. . I haven't been able to reproduce the issue yet. It looks like to me like line 104 is the first non-header line of the vcf, and I do see that string it's reporting on line 242. Is it possible there's a mix up between files?; ```; 882033 rs2272756 G A . PASS CR=99.79803;GentrainScore=0.7203;HW=4.306476E-6; ```. It sounds a lot like some sort of indexing bug. I don't think it should be #4224 because you're using a vcf instead of a vcf.gz. Hopefully there isn't a different index bug lurking out there... One weird thing I did notice, is that if I regenerate the .idx file, I get a different file from what comes with the vcf from the ftp site. Indexes aren't necessarily unique to a file, but I wonder if the one on the website is corrupted in some way. Could you try running `gatk IndexFeatureFile -F 1000G_omni2.5.b37.vcf` and then re-running your command with the newly generated index. I'm hoping that will fix it. If not we'll have to dig in more.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4761#issuecomment-388416287
https://github.com/broadinstitute/gatk/issues/4761#issuecomment-388416287:276,Availability,down,download,276,"@EvanTheB Thanks for moving over to the gatk tracker, I think it make more sense to start here and you'll get more eyes on it. We can always push it back down to htsjdk if we determine that's definitely where the problem is. . I downloaded that vcf, I noticed that the actual download link for it is ; ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/1000G_omni2.5.b37.vcf **.gz** . In zipped form it's a gzip file, but not a bgzip file which is what's required for tribble indexing to work. If you downloaded and unzipped it to just plain old vcf it should be fine though. . I haven't been able to reproduce the issue yet. It looks like to me like line 104 is the first non-header line of the vcf, and I do see that string it's reporting on line 242. Is it possible there's a mix up between files?; ```; 882033 rs2272756 G A . PASS CR=99.79803;GentrainScore=0.7203;HW=4.306476E-6; ```. It sounds a lot like some sort of indexing bug. I don't think it should be #4224 because you're using a vcf instead of a vcf.gz. Hopefully there isn't a different index bug lurking out there... One weird thing I did notice, is that if I regenerate the .idx file, I get a different file from what comes with the vcf from the ftp site. Indexes aren't necessarily unique to a file, but I wonder if the one on the website is corrupted in some way. Could you try running `gatk IndexFeatureFile -F 1000G_omni2.5.b37.vcf` and then re-running your command with the newly generated index. I'm hoping that will fix it. If not we'll have to dig in more.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4761#issuecomment-388416287
https://github.com/broadinstitute/gatk/issues/4761#issuecomment-388416287:510,Availability,down,downloaded,510,"@EvanTheB Thanks for moving over to the gatk tracker, I think it make more sense to start here and you'll get more eyes on it. We can always push it back down to htsjdk if we determine that's definitely where the problem is. . I downloaded that vcf, I noticed that the actual download link for it is ; ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/1000G_omni2.5.b37.vcf **.gz** . In zipped form it's a gzip file, but not a bgzip file which is what's required for tribble indexing to work. If you downloaded and unzipped it to just plain old vcf it should be fine though. . I haven't been able to reproduce the issue yet. It looks like to me like line 104 is the first non-header line of the vcf, and I do see that string it's reporting on line 242. Is it possible there's a mix up between files?; ```; 882033 rs2272756 G A . PASS CR=99.79803;GentrainScore=0.7203;HW=4.306476E-6; ```. It sounds a lot like some sort of indexing bug. I don't think it should be #4224 because you're using a vcf instead of a vcf.gz. Hopefully there isn't a different index bug lurking out there... One weird thing I did notice, is that if I regenerate the .idx file, I get a different file from what comes with the vcf from the ftp site. Indexes aren't necessarily unique to a file, but I wonder if the one on the website is corrupted in some way. Could you try running `gatk IndexFeatureFile -F 1000G_omni2.5.b37.vcf` and then re-running your command with the newly generated index. I'm hoping that will fix it. If not we'll have to dig in more.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4761#issuecomment-388416287
https://github.com/broadinstitute/gatk/issues/4763#issuecomment-388453987:22,Deployability,integrat,integration,22,"We'd also need a good integration test that reads in a VCF containing genotypes using `SelectVariants` with `--sites-only`, writes it out, reads it back in using `VariantContextTestUtils.readEntireVCFIntoMemory()`, and asserts that the genotypes are gone. The test should probably live someplace like `GATKToolIntegrationTest` or `FeatureSupportIntegrationTest` -- don't put it in `SelectVariantsIntegrationTest`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4763#issuecomment-388453987
https://github.com/broadinstitute/gatk/issues/4763#issuecomment-388453987:22,Integrability,integrat,integration,22,"We'd also need a good integration test that reads in a VCF containing genotypes using `SelectVariants` with `--sites-only`, writes it out, reads it back in using `VariantContextTestUtils.readEntireVCFIntoMemory()`, and asserts that the genotypes are gone. The test should probably live someplace like `GATKToolIntegrationTest` or `FeatureSupportIntegrationTest` -- don't put it in `SelectVariantsIntegrationTest`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4763#issuecomment-388453987
https://github.com/broadinstitute/gatk/issues/4763#issuecomment-388453987:34,Testability,test,test,34,"We'd also need a good integration test that reads in a VCF containing genotypes using `SelectVariants` with `--sites-only`, writes it out, reads it back in using `VariantContextTestUtils.readEntireVCFIntoMemory()`, and asserts that the genotypes are gone. The test should probably live someplace like `GATKToolIntegrationTest` or `FeatureSupportIntegrationTest` -- don't put it in `SelectVariantsIntegrationTest`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4763#issuecomment-388453987
https://github.com/broadinstitute/gatk/issues/4763#issuecomment-388453987:219,Testability,assert,asserts,219,"We'd also need a good integration test that reads in a VCF containing genotypes using `SelectVariants` with `--sites-only`, writes it out, reads it back in using `VariantContextTestUtils.readEntireVCFIntoMemory()`, and asserts that the genotypes are gone. The test should probably live someplace like `GATKToolIntegrationTest` or `FeatureSupportIntegrationTest` -- don't put it in `SelectVariantsIntegrationTest`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4763#issuecomment-388453987
https://github.com/broadinstitute/gatk/issues/4763#issuecomment-388453987:260,Testability,test,test,260,"We'd also need a good integration test that reads in a VCF containing genotypes using `SelectVariants` with `--sites-only`, writes it out, reads it back in using `VariantContextTestUtils.readEntireVCFIntoMemory()`, and asserts that the genotypes are gone. The test should probably live someplace like `GATKToolIntegrationTest` or `FeatureSupportIntegrationTest` -- don't put it in `SelectVariantsIntegrationTest`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4763#issuecomment-388453987
https://github.com/broadinstitute/gatk/pull/4764#issuecomment-388488961:1238,Testability,test,test,1238,40cb2f902d15b01d84f20b36ae9e8a?src=pr&el=desc) will **increase** coverage by `0.01%`.; > The diff coverage is `88.889%`. ```diff; @@ Coverage Diff @@; ## master #4764 +/- ##; ============================================; + Coverage 80.07% 80.08% +0.01% ; - Complexity 17420 17571 +151 ; ============================================; Files 1080 1081 +1 ; Lines 63131 63815 +684 ; Branches 10200 10400 +200 ; ============================================; + Hits 50549 51103 +554 ; - Misses 8590 8702 +112 ; - Partials 3992 4010 +18; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4764?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ellbender/cmdline/StandardArgumentDefinitions.java](https://codecov.io/gh/broadinstitute/gatk/pull/4764/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL1N0YW5kYXJkQXJndW1lbnREZWZpbml0aW9ucy5qYXZh) | `0% <ø> (ø)` | `0 <0> (ø)` | :arrow_down: |; | [...ellbender/utils/test/CommandLineProgramTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/4764/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0NvbW1hbmRMaW5lUHJvZ3JhbVRlc3Rlci5qYXZh) | `90.909% <0%> (-4.329%)` | `9 <0> (ø)` | |; | [...walkers/haplotypecaller/HaplotypeCallerEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/4764/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJFbmdpbmUuamF2YQ==) | `78.967% <100%> (+0.315%)` | `67 <0> (+1)` | :arrow_up: |; | [...org/broadinstitute/hellbender/engine/GATKTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/4764/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvR0FUS1Rvb2wuamF2YQ==) | `91.429% <100%> (+0.124%)` | `93 <0> (ø)` | :arrow_down: |; | [...tools/walkers/haplotypecaller/HaplotypeCaller.java](https://codecov.io/gh/broadinstitute/gatk/pull/4764/dif,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4764#issuecomment-388488961
https://github.com/broadinstitute/gatk/pull/4765#issuecomment-390023475:1587,Deployability,pipeline,pipelines,1587,1080 1082 +2 ; Lines 63078 63275 +197 ; Branches 10176 10208 +32 ; ===============================================; + Hits 50522 50719 +197 ; + Misses 8570 8568 -2 ; - Partials 3986 3988 +2; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4765?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...stitute/hellbender/tools/HaplotypeCallerSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4765/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9IYXBsb3R5cGVDYWxsZXJTcGFyay5qYXZh) | `83.158% <100%> (ø)` | `25 <0> (ø)` | :arrow_down: |; | [...transforms/markduplicates/MarkDuplicatesSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4765/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay90cmFuc2Zvcm1zL21hcmtkdXBsaWNhdGVzL01hcmtEdXBsaWNhdGVzU3BhcmsuamF2YQ==) | `97.297% <100%> (+1.379%)` | `31 <0> (+15)` | :arrow_up: |; | [...nder/tools/spark/pipelines/ReadsPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4765/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `89.13% <100%> (ø)` | `12 <0> (ø)` | :arrow_down: |; | [...ender/engine/spark/datasources/ReadsSparkSink.java](https://codecov.io/gh/broadinstitute/gatk/pull/4765/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NpbmsuamF2YQ==) | `77.027% <70%> (-0.671%)` | `33 <6> (+4)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4765/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `79.545% <83.333%> (-2.506%)` | `31 <2> (-13)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4765#issuecomment-390023475
https://github.com/broadinstitute/gatk/pull/4765#issuecomment-390023475:2531,Testability,test,test,2531,1.379%)` | `31 <0> (+15)` | :arrow_up: |; | [...nder/tools/spark/pipelines/ReadsPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4765/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `89.13% <100%> (ø)` | `12 <0> (ø)` | :arrow_down: |; | [...ender/engine/spark/datasources/ReadsSparkSink.java](https://codecov.io/gh/broadinstitute/gatk/pull/4765/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NpbmsuamF2YQ==) | `77.027% <70%> (-0.671%)` | `33 <6> (+4)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4765/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `79.545% <83.333%> (-2.506%)` | `31 <2> (-13)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4765/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `65.972% <90.909%> (+2.062%)` | `37 <4> (+4)` | :arrow_up: |; | [...adinstitute/hellbender/utils/spark/SparkUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4765/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zcGFyay9TcGFya1V0aWxzLmphdmE=) | `84.524% <92.5%> (+11.797%)` | `21 <15> (+9)` | :arrow_up: |; | [...ead/markduplicates/sparkrecords/EmptyFragment.java](https://codecov.io/gh/broadinstitute/gatk/pull/4765/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9FbXB0eUZyYWdtZW50LmphdmE=) | `86.207% <0%> (-2.028%)` | `16% <0%> (+9%)` | |; | [...ils/read/markduplicates/sparkrecords/Fragment.java](https://codecov.io/gh/broadinstitute/gatk/pull/4765/diff?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4765#issuecomment-390023475
https://github.com/broadinstitute/gatk/pull/4767#issuecomment-388966167:0,Testability,test,tested,0,tested working. merge when ready.; Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4767#issuecomment-388966167
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:346,Availability,error,error,346,"Since this touches a lot of files, I'll categorize changes here:; 1. Convenience Script Changes; - bug fixes for running on Linux; scripts/sv/manage_sv_pipeline.sh; - detect number of preemptible workers to choose NUM_EXECUTORS correctly; scripts/sv/run_whole_pipeline.sh; - allow sanity_checks.sh to run from outside GATK_DIR, exit correctly on error; scripts/sv/sanity_checks.sh. 2. Minor changes to existing utils to support new filter; - allow construction of IntHistogram.CDF from known cdfFractions and nCounts; src/main/java/org/broadinstitute/hellbender/tools/spark/utils/IntHistogram.java; - in constructor, coverage is passed as a float; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/ReadMetadata.java; - add xgboost maven repository for gradle; build.gradle. 3. Significant changes to existing code to support/invoke new filter; - add arguments for XGBoostEvidenceFilter, changes for scaling density filter by coverage; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/StructuralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hel",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:1696,Deployability,update,updates,1696,"r gradle; build.gradle. 3. Significant changes to existing code to support/invoke new filter; - add arguments for XGBoostEvidenceFilter, changes for scaling density filter by coverage; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/StructuralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:2024,Deployability,integrat,integration,2024,"ralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:2144,Deployability,integrat,integration,2144,"FilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/reso",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:1741,Integrability,interface,interfaces,1741,"r gradle; build.gradle. 3. Significant changes to existing code to support/invoke new filter; - add arguments for XGBoostEvidenceFilter, changes for scaling density filter by coverage; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/StructuralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:2024,Integrability,integrat,integration,2024,"ralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:2144,Integrability,integrat,integration,2144,"FilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/reso",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:3093,Performance,perform,performance,3093,"/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/resources/sv_classifier_test_data.json; src/test/resources/sv_features_test_data.json",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:167,Safety,detect,detect,167,"Since this touches a lot of files, I'll categorize changes here:; 1. Convenience Script Changes; - bug fixes for running on Linux; scripts/sv/manage_sv_pipeline.sh; - detect number of preemptible workers to choose NUM_EXECUTORS correctly; scripts/sv/run_whole_pipeline.sh; - allow sanity_checks.sh to run from outside GATK_DIR, exit correctly on error; scripts/sv/sanity_checks.sh. 2. Minor changes to existing utils to support new filter; - allow construction of IntHistogram.CDF from known cdfFractions and nCounts; src/main/java/org/broadinstitute/hellbender/tools/spark/utils/IntHistogram.java; - in constructor, coverage is passed as a float; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/ReadMetadata.java; - add xgboost maven repository for gradle; build.gradle. 3. Significant changes to existing code to support/invoke new filter; - add arguments for XGBoostEvidenceFilter, changes for scaling density filter by coverage; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/StructuralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hel",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:3079,Security,validat,validation,3079,"/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/resources/sv_classifier_test_data.json; src/test/resources/sv_features_test_data.json",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:1592,Testability,test,testing,1592,"tor, coverage is passed as a float; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/ReadMetadata.java; - add xgboost maven repository for gradle; build.gradle. 3. Significant changes to existing code to support/invoke new filter; - add arguments for XGBoostEvidenceFilter, changes for scaling density filter by coverage; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/StructuralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbend",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:1707,Testability,test,tests,1707,"r gradle; build.gradle. 3. Significant changes to existing code to support/invoke new filter; - add arguments for XGBoostEvidenceFilter, changes for scaling density filter by coverage; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/StructuralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:1757,Testability,test,test,1757,"r gradle; build.gradle. 3. Significant changes to existing code to support/invoke new filter; - add arguments for XGBoostEvidenceFilter, changes for scaling density filter by coverage; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/StructuralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:1859,Testability,test,test,1859," changes for scaling density filter by coverage; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/StructuralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/re",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:1969,Testability,test,test,1969,"ralVariationDiscoveryArgumentCollection.java; - replace calls to BreakpointDensityFilter with calls to BreakpointFilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:2089,Testability,test,test,2089,"FilterFactory; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/reso",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:2674,Testability,test,test,2674,"/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/resources/sv_classifier_test_data.json; src/test/resources/sv_features_test_data.json",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:2706,Testability,test,test,2706,"/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/resources/sv_classifier_test_data.json; src/test/resources/sv_features_test_data.json",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:3113,Testability,test,tests,3113,"/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/resources/sv_classifier_test_data.json; src/test/resources/sv_features_test_data.json",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:3124,Testability,test,test,3124,"/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/resources/sv_classifier_test_data.json; src/test/resources/sv_features_test_data.json",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:3173,Testability,test,test,3173,"/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/resources/sv_classifier_test_data.json; src/test/resources/sv_features_test_data.json",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477:2364,Usability,simpl,simple,2364,"/FindBreakpointEvidenceSpark.java; - input coverage-scaled thresholds, convert to absolute internally. Allow thresholds to be double instead of int; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilter.java; - getter functions added to calculate properties for XGBoostEvidenceFilter. Also fromStringRep() and helper constructors added for testing; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointEvidence.java; - updates to tests reflecting changes to these interfaces; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointDensityFilterTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/FindBreakpointEvidenceSparkUnitTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/FindBreakpointEvidenceSparkIntegrationTest.java; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/integration/SVIntegrationTestDataProvider.java. 4. Added code; - factory to call appropriate BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/BreakpointFilterFactory.java; - simple helper class to hold feature vectors for classifier; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/EvidenceFeatures.java; - implement classifier-based BreakpointEvidence filter; src/main/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilter.java; - unit test for classifier filter; src/test/java/org/broadinstitute/hellbender/tools/spark/sv/evidence/XGBoostEvidenceFilterUnitTest.java. 5. Added resources; - Genome tracts; src/main/resources/large/hg38_centromeres.txt.gz; src/main/resources/large/hg38_gaps.txt.gz; src/main/resources/large/hg38_umap_s100.txt.gz; - Classifier binary file; src/main/resources/large/sv_evidence_classifier.bin; - Data used for validation of performance in unit tests; src/test/resources/sv_classifier_test_data.json; src/test/resources/sv_features_test_data.json",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389218477
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389898045:828,Usability,simpl,simple,828,"Still working on my review but just wanted to respond to a couple of @davidbenjamin 's points:. - Agree that we should get rid of the hard coded paths (and I think most of those resources should live with the reference bundle rather than in GATK resources anyway). Already discussed this with @TedBrookings in person.; - IMO iterators are fine and are still idiomatic Java. Often we do have very large collections that we want to iterate over online, or at least without having to know how they are represented in the calling code. Spark APIs often return or provide iterators and using them lets you port code easily between working on RDDs and working on in memory collections. Also iterators give you the option to implement `remove` which is often important and useful. Of course, they are a little heavier than streams for simple use cases, so it's a tradeoff on whether you think the extra functionality / flexibility is useful.; - `Tuple2` and the other tuple Scala classes are used in the Java Spark API extensively and therefore is in a lot of our code, it's fine to use.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-389898045
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-395889530:3554,Deployability,integrat,integration,3554,/spark/sv/evidence/BreakpointDensityFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4769/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9CcmVha3BvaW50RGVuc2l0eUZpbHRlci5qYXZh) | `97.183% <100%> (ø)` | `27 <0> (ø)` | :arrow_down: |; | [...ute/hellbender/tools/spark/utils/IntHistogram.java](https://codecov.io/gh/broadinstitute/gatk/pull/4769/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay91dGlscy9JbnRIaXN0b2dyYW0uamF2YQ==) | `91.2% <100%> (+0.291%)` | `19 <0> (ø)` | :arrow_down: |; | [...spark/sv/evidence/BreakpointDensityFilterTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4769/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9CcmVha3BvaW50RGVuc2l0eUZpbHRlclRlc3QuamF2YQ==) | `100% <100%> (ø)` | `17 <0> (ø)` | :arrow_down: |; | [...te/hellbender/tools/spark/sv/utils/SVInterval.java](https://codecov.io/gh/broadinstitute/gatk/pull/4769/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi91dGlscy9TVkludGVydmFsLmphdmE=) | `89.362% <100%> (+6.383%)` | `33 <1> (+1)` | :arrow_up: |; | [.../sv/integration/SVIntegrationTestDataProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/4769/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9pbnRlZ3JhdGlvbi9TVkludGVncmF0aW9uVGVzdERhdGFQcm92aWRlci5qYXZh) | `94.118% <100%> (+1.81%)` | `1 <1> (ø)` | :arrow_down: |; | [...er/tools/spark/sv/evidence/BreakpointEvidence.java](https://codecov.io/gh/broadinstitute/gatk/pull/4769/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9CcmVha3BvaW50RXZpZGVuY2UuamF2YQ==) | `78.86% <57.303%> (-2.985%)` | `20 <4> (+8)` | |; | ... and [40 more](https://codecov.io/gh/broadinstitute/gatk/pull/4769/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-395889530
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-395889530:3554,Integrability,integrat,integration,3554,/spark/sv/evidence/BreakpointDensityFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4769/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9CcmVha3BvaW50RGVuc2l0eUZpbHRlci5qYXZh) | `97.183% <100%> (ø)` | `27 <0> (ø)` | :arrow_down: |; | [...ute/hellbender/tools/spark/utils/IntHistogram.java](https://codecov.io/gh/broadinstitute/gatk/pull/4769/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay91dGlscy9JbnRIaXN0b2dyYW0uamF2YQ==) | `91.2% <100%> (+0.291%)` | `19 <0> (ø)` | :arrow_down: |; | [...spark/sv/evidence/BreakpointDensityFilterTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4769/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9CcmVha3BvaW50RGVuc2l0eUZpbHRlclRlc3QuamF2YQ==) | `100% <100%> (ø)` | `17 <0> (ø)` | :arrow_down: |; | [...te/hellbender/tools/spark/sv/utils/SVInterval.java](https://codecov.io/gh/broadinstitute/gatk/pull/4769/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi91dGlscy9TVkludGVydmFsLmphdmE=) | `89.362% <100%> (+6.383%)` | `33 <1> (+1)` | :arrow_up: |; | [.../sv/integration/SVIntegrationTestDataProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/4769/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9pbnRlZ3JhdGlvbi9TVkludGVncmF0aW9uVGVzdERhdGFQcm92aWRlci5qYXZh) | `94.118% <100%> (+1.81%)` | `1 <1> (ø)` | :arrow_down: |; | [...er/tools/spark/sv/evidence/BreakpointEvidence.java](https://codecov.io/gh/broadinstitute/gatk/pull/4769/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9CcmVha3BvaW50RXZpZGVuY2UuamF2YQ==) | `78.86% <57.303%> (-2.985%)` | `20 <4> (+8)` | |; | ... and [40 more](https://codecov.io/gh/broadinstitute/gatk/pull/4769/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-395889530
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-396709375:49,Deployability,integrat,integration,49,"Needed two commits because I forgot to check the integration test the first go-round and I had changed parameter args. Sorry!; Github is complaining that there are conflicts, but since local git is fine, I'm assuming that just means I need to rebase before any merge and I should ignore that for now?. Back to you @cwhelan @davidbenjamin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-396709375
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-396709375:49,Integrability,integrat,integration,49,"Needed two commits because I forgot to check the integration test the first go-round and I had changed parameter args. Sorry!; Github is complaining that there are conflicts, but since local git is fine, I'm assuming that just means I need to rebase before any merge and I should ignore that for now?. Back to you @cwhelan @davidbenjamin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-396709375
https://github.com/broadinstitute/gatk/pull/4769#issuecomment-396709375:61,Testability,test,test,61,"Needed two commits because I forgot to check the integration test the first go-round and I had changed parameter args. Sorry!; Github is complaining that there are conflicts, but since local git is fine, I'm assuming that just means I need to rebase before any merge and I should ignore that for now?. Back to you @cwhelan @davidbenjamin",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4769#issuecomment-396709375
https://github.com/broadinstitute/gatk/issues/4773#issuecomment-492594963:4,Deployability,update,updates,4,Any updates on this issue? One year old today and still very much needed :),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4773#issuecomment-492594963
https://github.com/broadinstitute/gatk/issues/4773#issuecomment-493079492:157,Testability,test,tested,157,"There is a workaround, which is to add the new samples to a new GenomicsDB and then run CombineGVCFs using the two GenomicsDBs as two different inputs. I've tested this on a small number of samples and it works. Then you run GenotypeGVCFs on the combined GVCF and continue as usual. The caveat is that you'll need more non-heap memory to support two GDB readers. If you're running on a VM, you will want to increase the memory while keeping the java Xmx/Xms the same.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4773#issuecomment-493079492
https://github.com/broadinstitute/gatk/pull/4774#issuecomment-389597896:112,Deployability,pipeline,pipelines,112,"Also, didn't you have an ApplyRecalibration-equivalent tool? There should be some application of filters in the pipelines.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4774#issuecomment-389597896
https://github.com/broadinstitute/gatk/pull/4774#issuecomment-391849356:38,Availability,down,down,38,I think I got most of the WDL changes down. Adding the apply step is waiting on the new tranche filtering PR: https://github.com/broadinstitute/gatk/pull/4800.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4774#issuecomment-391849356
https://github.com/broadinstitute/gatk/pull/4774#issuecomment-404210394:71,Performance,optimiz,optimized,71,These now seem to be working on both firecloud and cromwell with intel-optimized tensorflow and t tranche-filtering. Back to you @ldgauthier.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4774#issuecomment-404210394
https://github.com/broadinstitute/gatk/pull/4774#issuecomment-417343741:32,Testability,test,testing,32,@ldgauthier back to you. Travis testing of both cnn_score_variants.wdl and cram2filtered.wdl seems to be working now. I'm still using the gs://broad-dsde-methods/ bucket for a few files until I get the final word from Comms about where they should live.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4774#issuecomment-417343741
https://github.com/broadinstitute/gatk/pull/4774#issuecomment-418509152:59,Testability,test,test,59,"Re-factored common tasks and battled travis again. A cloud test failed in the pr build, but I think it is fleeting, we didn't touch that code. @ldgauthier back to you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4774#issuecomment-418509152
https://github.com/broadinstitute/gatk/pull/4774#issuecomment-418723520:120,Energy Efficiency,green,green,120,"Elegantly succinct! Looks good to me. Since you don't need any more changes, give those clothes tests a poke so it's at green. Then you can merge and notify Brian.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4774#issuecomment-418723520
https://github.com/broadinstitute/gatk/pull/4774#issuecomment-418723520:96,Testability,test,tests,96,"Elegantly succinct! Looks good to me. Since you don't need any more changes, give those clothes tests a poke so it's at green. Then you can merge and notify Brian.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4774#issuecomment-418723520
https://github.com/broadinstitute/gatk/pull/4778#issuecomment-389904639:39,Testability,test,test,39,"@jamesemery I fixed a problem with the test where I wasn't marking it as ""Cloud"", when things pass I'll merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4778#issuecomment-389904639
https://github.com/broadinstitute/gatk/pull/4779#issuecomment-390350686:3865,Testability,test,test,3865,(ø)` | `24 <5> (ø)` | :arrow_down: |; | [...ls/read/markduplicates/GATKDuplicationMetrics.java](https://codecov.io/gh/broadinstitute/gatk/pull/4779/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL0dBVEtEdXBsaWNhdGlvbk1ldHJpY3MuamF2YQ==) | `100% <100%> (ø)` | `13 <13> (?)` | |; | [...forms/markduplicates/MarkDuplicatesSparkUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4779/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay90cmFuc2Zvcm1zL21hcmtkdXBsaWNhdGVzL01hcmtEdXBsaWNhdGVzU3BhcmtVdGlscy5qYXZh) | `89.498% <100%> (-0.29%)` | `62 <4> (-8)` | |; | [...roadinstitute/hellbender/utils/read/ReadUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4779/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL1JlYWRVdGlscy5qYXZh) | `81.87% <100%> (+2.652%)` | `341 <9> (+152)` | :arrow_up: |; | [.../utils/read/markduplicates/LibraryIdGenerator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4779/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL0xpYnJhcnlJZEdlbmVyYXRvci5qYXZh) | `89.474% <100%> (-4.971%)` | `14 <3> (ø)` | |; | [...ols/walkers/markduplicates/MarkDuplicatesGATK.java](https://codecov.io/gh/broadinstitute/gatk/pull/4779/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL21hcmtkdXBsaWNhdGVzL01hcmtEdXBsaWNhdGVzR0FUSy5qYXZh) | `87.917% <69.231%> (-2.003%)` | `69 <0> (-7)` | |; | [...e/hellbender/utils/test/testers/SamFileTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/4779/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L3Rlc3RlcnMvU2FtRmlsZVRlc3Rlci5qYXZh) | `85.271% <79.167%> (-5.45%)` | `35 <8> (+3)` | |; | ... and [21 more](https://codecov.io/gh/broadinstitute/gatk/pull/4779/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4779#issuecomment-390350686
https://github.com/broadinstitute/gatk/pull/4779#issuecomment-390350686:3870,Testability,test,testers,3870,(ø)` | `24 <5> (ø)` | :arrow_down: |; | [...ls/read/markduplicates/GATKDuplicationMetrics.java](https://codecov.io/gh/broadinstitute/gatk/pull/4779/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL0dBVEtEdXBsaWNhdGlvbk1ldHJpY3MuamF2YQ==) | `100% <100%> (ø)` | `13 <13> (?)` | |; | [...forms/markduplicates/MarkDuplicatesSparkUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4779/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay90cmFuc2Zvcm1zL21hcmtkdXBsaWNhdGVzL01hcmtEdXBsaWNhdGVzU3BhcmtVdGlscy5qYXZh) | `89.498% <100%> (-0.29%)` | `62 <4> (-8)` | |; | [...roadinstitute/hellbender/utils/read/ReadUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4779/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL1JlYWRVdGlscy5qYXZh) | `81.87% <100%> (+2.652%)` | `341 <9> (+152)` | :arrow_up: |; | [.../utils/read/markduplicates/LibraryIdGenerator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4779/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL0xpYnJhcnlJZEdlbmVyYXRvci5qYXZh) | `89.474% <100%> (-4.971%)` | `14 <3> (ø)` | |; | [...ols/walkers/markduplicates/MarkDuplicatesGATK.java](https://codecov.io/gh/broadinstitute/gatk/pull/4779/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL21hcmtkdXBsaWNhdGVzL01hcmtEdXBsaWNhdGVzR0FUSy5qYXZh) | `87.917% <69.231%> (-2.003%)` | `69 <0> (-7)` | |; | [...e/hellbender/utils/test/testers/SamFileTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/4779/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L3Rlc3RlcnMvU2FtRmlsZVRlc3Rlci5qYXZh) | `85.271% <79.167%> (-5.45%)` | `35 <8> (+3)` | |; | ... and [21 more](https://codecov.io/gh/broadinstitute/gatk/pull/4779/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4779#issuecomment-390350686
https://github.com/broadinstitute/gatk/pull/4780#issuecomment-389762587:17,Modifiability,extend,extended,17,"It can easely be extended to multiple sequences... it just happened that I didn't need it personally. . I took a quick pick to your branch... do you really care about the contig descriptions?.... I would say that this ""aligner"" class should not be responsible of compose the multi sequence fasta file but rather accept one as a constructor argument and the construction of the fasta is delegated back to the invoker code; in this new more general aligner the current single contig could be implemented as a public static method call.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4780#issuecomment-389762587
https://github.com/broadinstitute/gatk/pull/4780#issuecomment-389990976:385,Usability,clear,clear,385,"In addition just to clarify that the ""contig"" word reference to the sequences in the given reference not the number of contigs that you might align to it, so single-contig-reference-aligner means an aligner that will align several sequences (read, contigs, random-stuff) vs a single sequence/contig reference. . I guess I shall change contig for chromosome or sequence to make it more clear.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4780#issuecomment-389990976
https://github.com/broadinstitute/gatk/pull/4780#issuecomment-390809079:69,Integrability,interface,interface,69,"@SHuang-Broad ; I have addressed comments and changed the SSRAligner interface to make it more general in terms of the type of inputs (base sequence providers) and the type of outputs that it will generate. . Also I added and example as to how to address your SVFastqUtil.FastqRead to SAMRecord realigner may look like but is untested. perhaps you can take on that commit and add the test, . does not need to be part of this PR push though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4780#issuecomment-390809079
https://github.com/broadinstitute/gatk/pull/4780#issuecomment-390809079:384,Testability,test,test,384,"@SHuang-Broad ; I have addressed comments and changed the SSRAligner interface to make it more general in terms of the type of inputs (base sequence providers) and the type of outputs that it will generate. . Also I added and example as to how to address your SVFastqUtil.FastqRead to SAMRecord realigner may look like but is untested. perhaps you can take on that commit and add the test, . does not need to be part of this PR push though.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4780#issuecomment-390809079
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432:558,Availability,error,error,558,"Thanks for bringing this to our attention, @Tintest. I think that we may be able to address this by setting `base_compiledir` via `os.environ[""THEANO_FLAGS""]` appropriately (see http://deeplearning.net/software/theano/library/config.html). @mbabadi @cmnbroad any thoughts? . In any case, thanks for trying out the GermlineCNVCaller pipeline. You may have to tune some parameters, depending on your data type. You may find the following discussions helpful:. https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. https://github.com/broadinstitute/gatk/issues/4719. Note that we're still in beta, but our preliminary evaluations have demonstrated improved performance over other callers in both WES and WGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432:332,Deployability,pipeline,pipeline,332,"Thanks for bringing this to our attention, @Tintest. I think that we may be able to address this by setting `base_compiledir` via `os.environ[""THEANO_FLAGS""]` appropriately (see http://deeplearning.net/software/theano/library/config.html). @mbabadi @cmnbroad any thoughts? . In any case, thanks for trying out the GermlineCNVCaller pipeline. You may have to tune some parameters, depending on your data type. You may find the following discussions helpful:. https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. https://github.com/broadinstitute/gatk/issues/4719. Note that we're still in beta, but our preliminary evaluations have demonstrated improved performance over other callers in both WES and WGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432:380,Integrability,depend,depending,380,"Thanks for bringing this to our attention, @Tintest. I think that we may be able to address this by setting `base_compiledir` via `os.environ[""THEANO_FLAGS""]` appropriately (see http://deeplearning.net/software/theano/library/config.html). @mbabadi @cmnbroad any thoughts? . In any case, thanks for trying out the GermlineCNVCaller pipeline. You may have to tune some parameters, depending on your data type. You may find the following discussions helpful:. https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. https://github.com/broadinstitute/gatk/issues/4719. Note that we're still in beta, but our preliminary evaluations have demonstrated improved performance over other callers in both WES and WGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432:226,Modifiability,config,config,226,"Thanks for bringing this to our attention, @Tintest. I think that we may be able to address this by setting `base_compiledir` via `os.environ[""THEANO_FLAGS""]` appropriately (see http://deeplearning.net/software/theano/library/config.html). @mbabadi @cmnbroad any thoughts? . In any case, thanks for trying out the GermlineCNVCaller pipeline. You may have to tune some parameters, depending on your data type. You may find the following discussions helpful:. https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. https://github.com/broadinstitute/gatk/issues/4719. Note that we're still in beta, but our preliminary evaluations have demonstrated improved performance over other callers in both WES and WGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432:358,Performance,tune,tune,358,"Thanks for bringing this to our attention, @Tintest. I think that we may be able to address this by setting `base_compiledir` via `os.environ[""THEANO_FLAGS""]` appropriately (see http://deeplearning.net/software/theano/library/config.html). @mbabadi @cmnbroad any thoughts? . In any case, thanks for trying out the GermlineCNVCaller pipeline. You may have to tune some parameters, depending on your data type. You may find the following discussions helpful:. https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. https://github.com/broadinstitute/gatk/issues/4719. Note that we're still in beta, but our preliminary evaluations have demonstrated improved performance over other callers in both WES and WGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432:707,Performance,perform,performance,707,"Thanks for bringing this to our attention, @Tintest. I think that we may be able to address this by setting `base_compiledir` via `os.environ[""THEANO_FLAGS""]` appropriately (see http://deeplearning.net/software/theano/library/config.html). @mbabadi @cmnbroad any thoughts? . In any case, thanks for trying out the GermlineCNVCaller pipeline. You may have to tune some parameters, depending on your data type. You may find the following discussions helpful:. https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error. https://github.com/broadinstitute/gatk/issues/4719. Note that we're still in beta, but our preliminary evaluations have demonstrated improved performance over other callers in both WES and WGS.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390303432
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390312412:160,Testability,test,test,160,"Thank you for your links :). I hoped you will be able to add the change in the next GATK version :). If I can run this tool with singularity, I will be able to test a ton of different parameter !",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390312412
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390648848:85,Modifiability,config,config,85,"@samuelklee os.environ would probably work, though it might be easier to set `theano.config.base_compiledir` directly so you don't have to worry about clobbering any other flags.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-390648848
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433441078:2,Availability,down,downloaded,2,I downloaded the most recently gatk docker file yesterday and I met this problem today. I am wondering if you have solved this request? Thank you so much. /gatk/gatk-package-4.0.11.0-local.jar; singularity 2.5.0-dist,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433441078
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433477833:441,Availability,error,error-rate,441,"@sooheelee . Here is my command:. ```; singularity exec gatk.simg test.pbs; ```; Here is test.pbs:. ```; gatk DetermineGermlineContigPloidy \; -L filtered.interval_list \; --input A1.count.hdf5 --input A2.count.hdf5 \; --contig-ploidy-priors contig_ploidy_priors_homo_sapiens.tsv \; --interval-merging-rule OVERLAPPING_ONLY \; --output out \; --output-prefix exomeseq \; --verbosity DEBUG \; --mean-bias-standard-deviation 0.01 \; --mapping-error-rate 0.01 \; --global-psi-scale 0.001 \; --sample-psi-scale 0.0001; ```. Here is the error message and I think the python environment has been activated by Singularity. The task failed when it tried to create directory of '/root/.theano'. ```; 17:03:28.891 INFO DetermineGermlineContigPloidy - Initializing engine; 17:03:28.896 DEBUG ScriptExecutor - Executing:; 17:03:28.896 DEBUG ScriptExecutor - python; 17:03:28.896 DEBUG ScriptExecutor - -c; 17:03:28.896 DEBUG ScriptExecutor - import gcnvkernel. Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433477833
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433477833:532,Availability,error,error,532,"@sooheelee . Here is my command:. ```; singularity exec gatk.simg test.pbs; ```; Here is test.pbs:. ```; gatk DetermineGermlineContigPloidy \; -L filtered.interval_list \; --input A1.count.hdf5 --input A2.count.hdf5 \; --contig-ploidy-priors contig_ploidy_priors_homo_sapiens.tsv \; --interval-merging-rule OVERLAPPING_ONLY \; --output out \; --output-prefix exomeseq \; --verbosity DEBUG \; --mean-bias-standard-deviation 0.01 \; --mapping-error-rate 0.01 \; --global-psi-scale 0.001 \; --sample-psi-scale 0.0001; ```. Here is the error message and I think the python environment has been activated by Singularity. The task failed when it tried to create directory of '/root/.theano'. ```; 17:03:28.891 INFO DetermineGermlineContigPloidy - Initializing engine; 17:03:28.896 DEBUG ScriptExecutor - Executing:; 17:03:28.896 DEBUG ScriptExecutor - python; 17:03:28.896 DEBUG ScriptExecutor - -c; 17:03:28.896 DEBUG ScriptExecutor - import gcnvkernel. Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433477833
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433477833:538,Integrability,message,message,538,"@sooheelee . Here is my command:. ```; singularity exec gatk.simg test.pbs; ```; Here is test.pbs:. ```; gatk DetermineGermlineContigPloidy \; -L filtered.interval_list \; --input A1.count.hdf5 --input A2.count.hdf5 \; --contig-ploidy-priors contig_ploidy_priors_homo_sapiens.tsv \; --interval-merging-rule OVERLAPPING_ONLY \; --output out \; --output-prefix exomeseq \; --verbosity DEBUG \; --mean-bias-standard-deviation 0.01 \; --mapping-error-rate 0.01 \; --global-psi-scale 0.001 \; --sample-psi-scale 0.0001; ```. Here is the error message and I think the python environment has been activated by Singularity. The task failed when it tried to create directory of '/root/.theano'. ```; 17:03:28.891 INFO DetermineGermlineContigPloidy - Initializing engine; 17:03:28.896 DEBUG ScriptExecutor - Executing:; 17:03:28.896 DEBUG ScriptExecutor - python; 17:03:28.896 DEBUG ScriptExecutor - -c; 17:03:28.896 DEBUG ScriptExecutor - import gcnvkernel. Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433477833
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433477833:1051,Modifiability,config,configdefaults,1051,"@sooheelee . Here is my command:. ```; singularity exec gatk.simg test.pbs; ```; Here is test.pbs:. ```; gatk DetermineGermlineContigPloidy \; -L filtered.interval_list \; --input A1.count.hdf5 --input A2.count.hdf5 \; --contig-ploidy-priors contig_ploidy_priors_homo_sapiens.tsv \; --interval-merging-rule OVERLAPPING_ONLY \; --output out \; --output-prefix exomeseq \; --verbosity DEBUG \; --mean-bias-standard-deviation 0.01 \; --mapping-error-rate 0.01 \; --global-psi-scale 0.001 \; --sample-psi-scale 0.0001; ```. Here is the error message and I think the python environment has been activated by Singularity. The task failed when it tried to create directory of '/root/.theano'. ```; 17:03:28.891 INFO DetermineGermlineContigPloidy - Initializing engine; 17:03:28.896 DEBUG ScriptExecutor - Executing:; 17:03:28.896 DEBUG ScriptExecutor - python; 17:03:28.896 DEBUG ScriptExecutor - -c; 17:03:28.896 DEBUG ScriptExecutor - import gcnvkernel. Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433477833
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433477833:66,Testability,test,test,66,"@sooheelee . Here is my command:. ```; singularity exec gatk.simg test.pbs; ```; Here is test.pbs:. ```; gatk DetermineGermlineContigPloidy \; -L filtered.interval_list \; --input A1.count.hdf5 --input A2.count.hdf5 \; --contig-ploidy-priors contig_ploidy_priors_homo_sapiens.tsv \; --interval-merging-rule OVERLAPPING_ONLY \; --output out \; --output-prefix exomeseq \; --verbosity DEBUG \; --mean-bias-standard-deviation 0.01 \; --mapping-error-rate 0.01 \; --global-psi-scale 0.001 \; --sample-psi-scale 0.0001; ```. Here is the error message and I think the python environment has been activated by Singularity. The task failed when it tried to create directory of '/root/.theano'. ```; 17:03:28.891 INFO DetermineGermlineContigPloidy - Initializing engine; 17:03:28.896 DEBUG ScriptExecutor - Executing:; 17:03:28.896 DEBUG ScriptExecutor - python; 17:03:28.896 DEBUG ScriptExecutor - -c; 17:03:28.896 DEBUG ScriptExecutor - import gcnvkernel. Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433477833
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433477833:89,Testability,test,test,89,"@sooheelee . Here is my command:. ```; singularity exec gatk.simg test.pbs; ```; Here is test.pbs:. ```; gatk DetermineGermlineContigPloidy \; -L filtered.interval_list \; --input A1.count.hdf5 --input A2.count.hdf5 \; --contig-ploidy-priors contig_ploidy_priors_homo_sapiens.tsv \; --interval-merging-rule OVERLAPPING_ONLY \; --output out \; --output-prefix exomeseq \; --verbosity DEBUG \; --mean-bias-standard-deviation 0.01 \; --mapping-error-rate 0.01 \; --global-psi-scale 0.001 \; --sample-psi-scale 0.0001; ```. Here is the error message and I think the python environment has been activated by Singularity. The task failed when it tried to create directory of '/root/.theano'. ```; 17:03:28.891 INFO DetermineGermlineContigPloidy - Initializing engine; 17:03:28.896 DEBUG ScriptExecutor - Executing:; 17:03:28.896 DEBUG ScriptExecutor - python; 17:03:28.896 DEBUG ScriptExecutor - -c; 17:03:28.896 DEBUG ScriptExecutor - import gcnvkernel. Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433477833
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433491527:129,Testability,test,testing,129,You can tell if the python environment is active because your bash prompt will be prefaced with `(gatk)` as you see here from my testing: ; ![screenshot 2018-10-26 13 52 19](https://user-images.githubusercontent.com/11543866/47583765-64ec6a00-d926-11e8-8f55-c60d1d5ffd3e.png),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433491527
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433548346:169,Availability,error,error,169,"@sooheelee You are right, the python environment is not activated automatically. However, I manually activated the environment and then run my test.pbs, I still got the error. ```; [shengq2@cqs1 singularity]$ singularity shell gatk.simg; Singularity: Invoking an interactive shell within container...; Singularity gatk.simg:/scratch/cqs/softwares/singularity> source activate gatk; (gatk) Singularity gatk.simg:/scratch/cqs/softwares/singularity> sh test.pbs. ...; 21:27:03.205 INFO DetermineGermlineContigPloidy - Initializing engine; 21:27:03.210 DEBUG ScriptExecutor - Executing:; 21:27:03.210 DEBUG ScriptExecutor - python; 21:27:03.210 DEBUG ScriptExecutor - -c; 21:27:03.210 DEBUG ScriptExecutor - import gcnvkernel. Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433548346
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433548346:825,Modifiability,config,configdefaults,825,"@sooheelee You are right, the python environment is not activated automatically. However, I manually activated the environment and then run my test.pbs, I still got the error. ```; [shengq2@cqs1 singularity]$ singularity shell gatk.simg; Singularity: Invoking an interactive shell within container...; Singularity gatk.simg:/scratch/cqs/softwares/singularity> source activate gatk; (gatk) Singularity gatk.simg:/scratch/cqs/softwares/singularity> sh test.pbs. ...; 21:27:03.205 INFO DetermineGermlineContigPloidy - Initializing engine; 21:27:03.210 DEBUG ScriptExecutor - Executing:; 21:27:03.210 DEBUG ScriptExecutor - python; 21:27:03.210 DEBUG ScriptExecutor - -c; 21:27:03.210 DEBUG ScriptExecutor - import gcnvkernel. Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433548346
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433548346:143,Testability,test,test,143,"@sooheelee You are right, the python environment is not activated automatically. However, I manually activated the environment and then run my test.pbs, I still got the error. ```; [shengq2@cqs1 singularity]$ singularity shell gatk.simg; Singularity: Invoking an interactive shell within container...; Singularity gatk.simg:/scratch/cqs/softwares/singularity> source activate gatk; (gatk) Singularity gatk.simg:/scratch/cqs/softwares/singularity> sh test.pbs. ...; 21:27:03.205 INFO DetermineGermlineContigPloidy - Initializing engine; 21:27:03.210 DEBUG ScriptExecutor - Executing:; 21:27:03.210 DEBUG ScriptExecutor - python; 21:27:03.210 DEBUG ScriptExecutor - -c; 21:27:03.210 DEBUG ScriptExecutor - import gcnvkernel. Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433548346
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433548346:450,Testability,test,test,450,"@sooheelee You are right, the python environment is not activated automatically. However, I manually activated the environment and then run my test.pbs, I still got the error. ```; [shengq2@cqs1 singularity]$ singularity shell gatk.simg; Singularity: Invoking an interactive shell within container...; Singularity gatk.simg:/scratch/cqs/softwares/singularity> source activate gatk; (gatk) Singularity gatk.simg:/scratch/cqs/softwares/singularity> sh test.pbs. ...; 21:27:03.205 INFO DetermineGermlineContigPloidy - Initializing engine; 21:27:03.210 DEBUG ScriptExecutor - Executing:; 21:27:03.210 DEBUG ScriptExecutor - python; 21:27:03.210 DEBUG ScriptExecutor - -c; 21:27:03.210 DEBUG ScriptExecutor - import gcnvkernel. Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433548346
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433549054:344,Deployability,release,release,344,"@shengqh ; Looking at it, it seems that `test.pbs` is basically a shell script. So at the top of that add:. ```; source activate gatk; ```. Or... whatever it is you need to do to activate the environment. What the `exec` command is going to do is launch whatever is passed in. So... something like:. ```; singularity exec gatk.simg cat /etc/os-release; ```; Would show you the `/etc/os-release` file from within the container. `exec` isn't going to load a shell environment, but launch the application directly. In a `shell` look at: `/.singularity.d/actions/exec`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433549054
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433549054:386,Deployability,release,release,386,"@shengqh ; Looking at it, it seems that `test.pbs` is basically a shell script. So at the top of that add:. ```; source activate gatk; ```. Or... whatever it is you need to do to activate the environment. What the `exec` command is going to do is launch whatever is passed in. So... something like:. ```; singularity exec gatk.simg cat /etc/os-release; ```; Would show you the `/etc/os-release` file from within the container. `exec` isn't going to load a shell environment, but launch the application directly. In a `shell` look at: `/.singularity.d/actions/exec`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433549054
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433549054:449,Performance,load,load,449,"@shengqh ; Looking at it, it seems that `test.pbs` is basically a shell script. So at the top of that add:. ```; source activate gatk; ```. Or... whatever it is you need to do to activate the environment. What the `exec` command is going to do is launch whatever is passed in. So... something like:. ```; singularity exec gatk.simg cat /etc/os-release; ```; Would show you the `/etc/os-release` file from within the container. `exec` isn't going to load a shell environment, but launch the application directly. In a `shell` look at: `/.singularity.d/actions/exec`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433549054
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433549054:41,Testability,test,test,41,"@shengqh ; Looking at it, it seems that `test.pbs` is basically a shell script. So at the top of that add:. ```; source activate gatk; ```. Or... whatever it is you need to do to activate the environment. What the `exec` command is going to do is launch whatever is passed in. So... something like:. ```; singularity exec gatk.simg cat /etc/os-release; ```; Would show you the `/etc/os-release` file from within the container. `exec` isn't going to load a shell environment, but launch the application directly. In a `shell` look at: `/.singularity.d/actions/exec`",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433549054
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433550679:194,Availability,error,error,194,"@jmstover Thank you for your recommendation. I added the following line in the beginning of test.pbs. ```; source activate gatk; ```. However, running the following command still threw the same error. ; ```; singularity exec gatk.simg ./test.pbs; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433550679
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433550679:92,Testability,test,test,92,"@jmstover Thank you for your recommendation. I added the following line in the beginning of test.pbs. ```; source activate gatk; ```. However, running the following command still threw the same error. ; ```; singularity exec gatk.simg ./test.pbs; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433550679
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433550679:237,Testability,test,test,237,"@jmstover Thank you for your recommendation. I added the following line in the beginning of test.pbs. ```; source activate gatk; ```. However, running the following command still threw the same error. ; ```; singularity exec gatk.simg ./test.pbs; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433550679
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433557019:89,Availability,error,error,89,"Are you building this from the docker `broadinstitute/gatk` image?. If so, I've found an error in it (@sooheelee, @samuelklee )... the Docker Manifest has set:; ```; export HOME=""/root""; ````; ... That's going to make some things not work if you're using it as a non-root user. If you are using that, try building from a Singularity definition file that looks like:. ```; Bootstrap: docker; From: broadinstitute/gatk. %environment; export HOME=/path/to/home. %runscript; bash -c ""source activate gatk; /gatk/gatk \""$@\""""; ```; I'm still testing this since I don't know the gatk software at all ... but with setting HOME in `%environment` you'll be overriding what the Docker manifest is setting. Setting `%runscript` you can then execute `gatk` by running:. ```; singularity run gatk.simg DetermineGermlineContigPloidy \; -L filtered.interval_list \; --input A1.count.hdf5 --input A2.count.hdf5 \; --contig-ploidy-priors contig_ploidy_priors_homo_sapiens.tsv \; --interval-merging-rule OVERLAPPING_ONLY \; --output out \; --output-prefix exomeseq \; --verbosity DEBUG \; --mean-bias-standard-deviation 0.01 \; --mapping-error-rate 0.01 \; --global-psi-scale 0.001 \; --sample-psi-scale 0.0001; ```; Or if `gatk.simg` is executable, by just calling that image file if `run-singularity` is in your path. You just need to pass the options to gatk, not call gatk. So that would be:; ```; ./gatk.simg DetermineGermlineContigPloidy [...]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433557019
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433557019:1120,Availability,error,error-rate,1120,"Are you building this from the docker `broadinstitute/gatk` image?. If so, I've found an error in it (@sooheelee, @samuelklee )... the Docker Manifest has set:; ```; export HOME=""/root""; ````; ... That's going to make some things not work if you're using it as a non-root user. If you are using that, try building from a Singularity definition file that looks like:. ```; Bootstrap: docker; From: broadinstitute/gatk. %environment; export HOME=/path/to/home. %runscript; bash -c ""source activate gatk; /gatk/gatk \""$@\""""; ```; I'm still testing this since I don't know the gatk software at all ... but with setting HOME in `%environment` you'll be overriding what the Docker manifest is setting. Setting `%runscript` you can then execute `gatk` by running:. ```; singularity run gatk.simg DetermineGermlineContigPloidy \; -L filtered.interval_list \; --input A1.count.hdf5 --input A2.count.hdf5 \; --contig-ploidy-priors contig_ploidy_priors_homo_sapiens.tsv \; --interval-merging-rule OVERLAPPING_ONLY \; --output out \; --output-prefix exomeseq \; --verbosity DEBUG \; --mean-bias-standard-deviation 0.01 \; --mapping-error-rate 0.01 \; --global-psi-scale 0.001 \; --sample-psi-scale 0.0001; ```; Or if `gatk.simg` is executable, by just calling that image file if `run-singularity` is in your path. You just need to pass the options to gatk, not call gatk. So that would be:; ```; ./gatk.simg DetermineGermlineContigPloidy [...]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433557019
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433557019:537,Testability,test,testing,537,"Are you building this from the docker `broadinstitute/gatk` image?. If so, I've found an error in it (@sooheelee, @samuelklee )... the Docker Manifest has set:; ```; export HOME=""/root""; ````; ... That's going to make some things not work if you're using it as a non-root user. If you are using that, try building from a Singularity definition file that looks like:. ```; Bootstrap: docker; From: broadinstitute/gatk. %environment; export HOME=/path/to/home. %runscript; bash -c ""source activate gatk; /gatk/gatk \""$@\""""; ```; I'm still testing this since I don't know the gatk software at all ... but with setting HOME in `%environment` you'll be overriding what the Docker manifest is setting. Setting `%runscript` you can then execute `gatk` by running:. ```; singularity run gatk.simg DetermineGermlineContigPloidy \; -L filtered.interval_list \; --input A1.count.hdf5 --input A2.count.hdf5 \; --contig-ploidy-priors contig_ploidy_priors_homo_sapiens.tsv \; --interval-merging-rule OVERLAPPING_ONLY \; --output out \; --output-prefix exomeseq \; --verbosity DEBUG \; --mean-bias-standard-deviation 0.01 \; --mapping-error-rate 0.01 \; --global-psi-scale 0.001 \; --sample-psi-scale 0.0001; ```; Or if `gatk.simg` is executable, by just calling that image file if `run-singularity` is in your path. You just need to pass the options to gatk, not call gatk. So that would be:; ```; ./gatk.simg DetermineGermlineContigPloidy [...]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433557019
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433561271:216,Deployability,install,install,216,"@jmstover Yes, I pulled from broadinstiture/gatk. ; ```; singularity build gatk.simg docker://broadinstitute/gatk; ```; I don't have root permission to build Singularity image from definition file now. I may need to install a virtual machine first to test it. However, just like the activation of environment, I added the export command in my test.pbs and it worked. :-). ```; export HOME=/scratch/cqs/shengq2; source activate gatk; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433561271
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433561271:251,Testability,test,test,251,"@jmstover Yes, I pulled from broadinstiture/gatk. ; ```; singularity build gatk.simg docker://broadinstitute/gatk; ```; I don't have root permission to build Singularity image from definition file now. I may need to install a virtual machine first to test it. However, just like the activation of environment, I added the export command in my test.pbs and it worked. :-). ```; export HOME=/scratch/cqs/shengq2; source activate gatk; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433561271
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433561271:343,Testability,test,test,343,"@jmstover Yes, I pulled from broadinstiture/gatk. ; ```; singularity build gatk.simg docker://broadinstitute/gatk; ```; I don't have root permission to build Singularity image from definition file now. I may need to install a virtual machine first to test it. However, just like the activation of environment, I added the export command in my test.pbs and it worked. :-). ```; export HOME=/scratch/cqs/shengq2; source activate gatk; ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-433561271
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-434049374:95,Security,access,access,95,"Although there is a workaround, ideally we'd remove the assumption from our docker that it can access the root user's home dir.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-434049374
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-467113459:135,Integrability,message,message,135,@cmnbroad @droazen will there be any action on this? I'm not sure if #5714 might be related (although I'm not sure why we get a ulimit message there instead of a permission denied)?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-467113459
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-468016036:166,Testability,test,tests,166,"Thanks for looking into it! I guess Cromwell also essentially sets `HOME`, which is probably why we don't typically run into this. Do we have any non-Cromwell Docker tests that could catch a regression?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-468016036
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:2401,Availability,down,down,2401,"---------; 14:39:24.082 INFO DetermineGermlineContigPloidy - HTSJDK Version: 2.18.2; 14:39:24.082 INFO DetermineGermlineContigPloidy - Picard Version: 2.18.25; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:39:24.083 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 14:39:24.083 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 14:39:24.083 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 14:39:24.083 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 14:39:24.083 INFO DetermineGermlineContigPloidy - Initializing engine; 14:39:26.111 INFO DetermineGermlineContigPloidy - Shutting down engine; [May 26, 2019 2:39:26 PM UTC] org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=1511522304; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python -c import gcnvkernel. Stdout:; Stderr: Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/opt/miniconda/e",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:2849,Modifiability,config,configdefaults,2849,"MTOOLS : true; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:39:24.083 INFO DetermineGermlineContigPloidy - Deflater: IntelDeflater; 14:39:24.083 INFO DetermineGermlineContigPloidy - Inflater: IntelInflater; 14:39:24.083 INFO DetermineGermlineContigPloidy - GCS max retries/reopens: 20; 14:39:24.083 INFO DetermineGermlineContigPloidy - Requester pays: disabled; 14:39:24.083 INFO DetermineGermlineContigPloidy - Initializing engine; 14:39:26.111 INFO DetermineGermlineContigPloidy - Shutting down engine; [May 26, 2019 2:39:26 PM UTC] org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=1511522304; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException:; python exited with 1; Command Line: python -c import gcnvkernel. Stdout:; Stderr: Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1738, in filter_compiledir; os.makedirs(path, 0o770) # read-write-execute for user and group; File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 210, in makedirs; makedirs(head, mode, exist_ok); File ""/opt/miniconda/envs/gatk/lib/python3.6/os.py"", line 220, in makedirs; mkdir(name, mode); PermissionError: [Errno 13] Permission denied: '/root/.theano'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/__init__.py"", line 5, in <module>; from .distributions import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/__init__.py"", line 1, in <module>; from . import timeseries; File ""/opt/miniconda/envs/gatk/lib/python3.6",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:4769,Modifiability,config,config,4769,"le>; from . import timeseries; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/timeseries.py"", line 1, in <module>; import theano.tensor as tt; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.u",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:4873,Modifiability,config,configparser,4873,"stributions/timeseries.py"", line 1, in <module>; import theano.tensor as tt; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:5007,Modifiability,config,configparser,5007,"s/theano/__init__.py"", line 66, in <module>; from theano.compile import (; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:192); at org.broadinstitute",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:5142,Modifiability,config,configdefaults,5142,"eano/compile/__init__.py"", line 10, in <module>; from theano.compile.function_module import *; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/function_module.py"", line 21, in <module>; import theano.compile.mode; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/compile/mode.py"", line 10, in <module>; import theano.gof.vm; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/vm.py"", line 662, in <module>; from . import lazylinker_c; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 42, in <module>; location = os.path.join(config.compiledir, 'lazylinker_ext'); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 333, in __get__; self.__set__(cls, val_str); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configparser.py"", line 344, in __set__; self.val = self.filter(val); File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/configdefaults.py"", line 1745, in filter_compiledir; "" '%s'. Check the permissions."" % path); ValueError: Unable to create the compiledir directory '/root/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'. Check the permissions. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:192); at org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy.onStartup(DetermineGermlineContigPloidy.java:269); at org.broadinstitute.hell",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081:387,Performance,Load,Loading,387,"Hello,. I did encounter the same behaviour that I reported one year ago with the official Docker GATK 4.1.0.0 container converted into a singularity image. ## Version of softwares:. Singularity : 2.5.1, GATK : 4.1.0.0. ### Command. Singularity : ; singularity build gatk-4.0.4.0.img docker://broadinstitute/gatk:4.0.4.0. ### Actual behavior; ```; 14:39:21.762 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:39:24.079 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 14:39:24.079 INFO DetermineGermlineContigPloidy - The Genome Analysis Toolkit (GATK) v4.1.0.0; 14:39:24.080 INFO DetermineGermlineContigPloidy - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:39:24.081 INFO DetermineGermlineContigPloidy - Executing as tintest@dahu38 on Linux v4.9.0-8-amd64 amd64; 14:39:24.081 INFO DetermineGermlineContigPloidy - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 14:39:24.081 INFO DetermineGermlineContigPloidy - Start Date/Time: May 26, 2019 2:39:21 PM UTC; 14:39:24.081 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 14:39:24.081 INFO DetermineGermlineContigPloidy - ------------------------------------------------------------; 14:39:24.082 INFO DetermineGermlineContigPloidy - HTSJDK Version: 2.18.2; 14:39:24.082 INFO DetermineGermlineContigPloidy - Picard Version: 2.18.25; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:39:24.083 INFO DetermineGermlineContigPloidy - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:39:24.083 INFO DetermineGermline",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-496007081
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-511885475:298,Testability,test,test,298,"@Tintest I also met this problem. However, I solved it by ""export HOME"" before calling gatk command in my shell file call_gatk.sh. ```; export HOME=/scratch/cqs/shengq2/temp/; source activate gatk; gatk --java-options ""-Xmx40G"" DetermineGermlineContigPloidy \; ...; ```. And, here is my shell file test.sh to call singularity :; ```; export R_LIBS=; export PYTHONPATH=; export JAVA_HOME=. singularity exec gatk.4.1.0.0.simg bash /scratch/cqs/shengq2/temp/call_gatk.sh; ```. Putting the ""export HOME"" in test.sh with ""export JAVA_HOME"" before calling singularity didn't work. You have to put it in call_gatk.sh.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-511885475
https://github.com/broadinstitute/gatk/issues/4782#issuecomment-511885475:503,Testability,test,test,503,"@Tintest I also met this problem. However, I solved it by ""export HOME"" before calling gatk command in my shell file call_gatk.sh. ```; export HOME=/scratch/cqs/shengq2/temp/; source activate gatk; gatk --java-options ""-Xmx40G"" DetermineGermlineContigPloidy \; ...; ```. And, here is my shell file test.sh to call singularity :; ```; export R_LIBS=; export PYTHONPATH=; export JAVA_HOME=. singularity exec gatk.4.1.0.0.simg bash /scratch/cqs/shengq2/temp/call_gatk.sh; ```. Putting the ""export HOME"" in test.sh with ""export JAVA_HOME"" before calling singularity didn't work. You have to put it in call_gatk.sh.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4782#issuecomment-511885475
https://github.com/broadinstitute/gatk/pull/4783#issuecomment-391032669:108,Testability,test,tests,108,"@lbergelson @jonn-smith As per offline discussion, I will targz the references and untar as needed. And fix tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4783#issuecomment-391032669
https://github.com/broadinstitute/gatk/pull/4785#issuecomment-390343864:3178,Testability,test,test,3178,%)` | `33% <0%> (+4%)` | |; | [...ils/read/markduplicates/sparkrecords/Fragment.java](https://codecov.io/gh/broadinstitute/gatk/pull/4785/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9GcmFnbWVudC5qYXZh) | `100% <0%> (ø)` | `20% <0%> (+9%)` | :arrow_up: |; | [...itute/hellbender/engine/spark/GATKRegistrator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4785/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1JlZ2lzdHJhdG9yLmphdmE=) | `100% <0%> (ø)` | `5% <0%> (+2%)` | :arrow_up: |; | [...walkers/haplotypecaller/HaplotypeCallerEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/4785/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJFbmdpbmUuamF2YQ==) | `79.62% <0%> (+0.653%)` | `83% <0%> (+16%)` | :arrow_up: |; | [...ellbender/utils/test/CommandLineProgramTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/4785/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0NvbW1hbmRMaW5lUHJvZ3JhbVRlc3Rlci5qYXZh) | `91.667% <0%> (+0.758%)` | `11% <0%> (+2%)` | :arrow_up: |; | [...org/broadinstitute/hellbender/engine/GATKTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/4785/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvR0FUS1Rvb2wuamF2YQ==) | `92.33% <0%> (+0.902%)` | `164% <0%> (+71%)` | :arrow_up: |; | [...tools/walkers/haplotypecaller/HaplotypeCaller.java](https://codecov.io/gh/broadinstitute/gatk/pull/4785/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXIuamF2YQ==) | `95.918% <0%> (+2.37%)` | `26% <0%> (+10%)` | :arrow_up: |; | ... and [2 more](https://codecov.io/gh/broadinstitute/gatk/pull/4785/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4785#issuecomment-390343864
https://github.com/broadinstitute/gatk/issues/4788#issuecomment-590928031:19,Integrability,message,message,19,"@davidbenjamin The message ""Flush-to-zero (FTZ) is enabled when running PairHMM"" is completely normal and not a sign of any problem. The ticket does not have enough detail for us to determine why the output file was empty.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4788#issuecomment-590928031
https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:6140,Integrability,interface,interface,6140,"gic, I prefer not to do it this way, because we can add more logics in the future, and capturing/resolving these BND's that are not suitable for _THIS PARTICULAR_ logic. I guess in general my personal preference is to put less algorithm-related information in VCF for analysts (less reading for them), and produce add on files for tool developers. What's your thoughts?. > Does this even have to be a spark tool? It looks like you are just reading the variants into a parallel spark context, filtering, and then collecting them to actually process them. Why not just make this a non-spark tool and process it all in memory on one node?. Answer: Agree. It doesn't have to be, at least in theory, and it probably is going to be faster as we don't need to incur the Spark overhead for such a typically small job. But (I'm saying too many buts....) up to this point all SV tools are under the package `hellbender.tools.spark.sv`, so I'm following suit here. Note the two classes's main interface methods mentions nothing about RDDs (that's on purpose). ; On the other hand, this is an engineering question I believe, and it depends on whether we want to put as much of discovery code as possible into `StructuralVariationDiscoveryPipelineSpark` (the last commit actually hooks the two classes into it, so a single invocation of the tool produces more variants), or we go wdl in pipelining the whole process. -------. All in all, I think the comments and critics are generally about the ""filtering""/""classifying"" part, and the most serious concern about it is false negatives. Am I understanding correctly? If so, given that the filtering step is only picking the BND's that are suitable to the linking logic, I can imagine the false negative problem be solved in the future by other logics (e.g. more relaxed requirement on pair matching, or not even requiring matching INV55/INV33 pairs, etc.) In fact, that's what I'm planning on.; Another part of the problem is how much I can accomplish in this single",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929
https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:6278,Integrability,depend,depends,6278," and capturing/resolving these BND's that are not suitable for _THIS PARTICULAR_ logic. I guess in general my personal preference is to put less algorithm-related information in VCF for analysts (less reading for them), and produce add on files for tool developers. What's your thoughts?. > Does this even have to be a spark tool? It looks like you are just reading the variants into a parallel spark context, filtering, and then collecting them to actually process them. Why not just make this a non-spark tool and process it all in memory on one node?. Answer: Agree. It doesn't have to be, at least in theory, and it probably is going to be faster as we don't need to incur the Spark overhead for such a typically small job. But (I'm saying too many buts....) up to this point all SV tools are under the package `hellbender.tools.spark.sv`, so I'm following suit here. Note the two classes's main interface methods mentions nothing about RDDs (that's on purpose). ; On the other hand, this is an engineering question I believe, and it depends on whether we want to put as much of discovery code as possible into `StructuralVariationDiscoveryPipelineSpark` (the last commit actually hooks the two classes into it, so a single invocation of the tool produces more variants), or we go wdl in pipelining the whole process. -------. All in all, I think the comments and critics are generally about the ""filtering""/""classifying"" part, and the most serious concern about it is false negatives. Am I understanding correctly? If so, given that the filtering step is only picking the BND's that are suitable to the linking logic, I can imagine the false negative problem be solved in the future by other logics (e.g. more relaxed requirement on pair matching, or not even requiring matching INV55/INV33 pairs, etc.) In fact, that's what I'm planning on.; Another part of the problem is how much I can accomplish in this single PR, and how large it should be&mdash;the forever existing problem for new tools.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929
https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:1123,Testability,log,logic,1123,"oposed algorithm description:. Answer: thanks for looking! That's what I intended as I'm not sure if the algorithm itself would face strong critics. If that's the case, time spent on coding is not worth it IMO. In fact ideally I'd like to write a design doc or something similar, and only code after the design is agreed on. --------. > I would of course prefer not to have to have a hard filter on length. This would mean we would never call a large inversion even if it exists. . Answer: Totally agree. Now looking back, it get clearer to me that this proposal contains two parts: the filtering part, and the breakpoint linking part, separated into two major classes `InversionBreakendPreFilter` and `LinkedInversionBreakpointsInference`. That being said, it doesn't make much sense to separate them into two PRs because _currently_ the filtering part is designed around the linking part, i.e. it is trying to check which BND's are suitable to the logic implemented in the linking part, and if the logic isn't applicable to an BND, the BND simply slips through without generating any new interpretations. So `InversionBreakendPreFilter` is a filter and a classifier at the same time, it function is really diverting different BND's to be handled by different logics, and it definitely should be improved.; If you buy this argument, I am also fully aware of the code design issue that it is preferable to NOT divert&mdash;gather&mdash;send through different handlers like it currently is for calling variants from the assembly contigs, instead it should be a single stream pass through all the BND's. I'll try to follow the preferred design. > What about some other filters more specifically aimed at the artifacts that cause these false large calls? I think it's a good idea to check annotations -- ie. do the mates lie at two regions that are segmental duplications of each other, or one side of the mate looks like a transposable element insertion? I guess it's ok to put in a tool with this limit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929
https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:1173,Testability,log,logic,1173,"oposed algorithm description:. Answer: thanks for looking! That's what I intended as I'm not sure if the algorithm itself would face strong critics. If that's the case, time spent on coding is not worth it IMO. In fact ideally I'd like to write a design doc or something similar, and only code after the design is agreed on. --------. > I would of course prefer not to have to have a hard filter on length. This would mean we would never call a large inversion even if it exists. . Answer: Totally agree. Now looking back, it get clearer to me that this proposal contains two parts: the filtering part, and the breakpoint linking part, separated into two major classes `InversionBreakendPreFilter` and `LinkedInversionBreakpointsInference`. That being said, it doesn't make much sense to separate them into two PRs because _currently_ the filtering part is designed around the linking part, i.e. it is trying to check which BND's are suitable to the logic implemented in the linking part, and if the logic isn't applicable to an BND, the BND simply slips through without generating any new interpretations. So `InversionBreakendPreFilter` is a filter and a classifier at the same time, it function is really diverting different BND's to be handled by different logics, and it definitely should be improved.; If you buy this argument, I am also fully aware of the code design issue that it is preferable to NOT divert&mdash;gather&mdash;send through different handlers like it currently is for calling variants from the assembly contigs, instead it should be a single stream pass through all the BND's. I'll try to follow the preferred design. > What about some other filters more specifically aimed at the artifacts that cause these false large calls? I think it's a good idea to check annotations -- ie. do the mates lie at two regions that are segmental duplications of each other, or one side of the mate looks like a transposable element insertion? I guess it's ok to put in a tool with this limit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929
https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:1434,Testability,log,logics,1434,"IMO. In fact ideally I'd like to write a design doc or something similar, and only code after the design is agreed on. --------. > I would of course prefer not to have to have a hard filter on length. This would mean we would never call a large inversion even if it exists. . Answer: Totally agree. Now looking back, it get clearer to me that this proposal contains two parts: the filtering part, and the breakpoint linking part, separated into two major classes `InversionBreakendPreFilter` and `LinkedInversionBreakpointsInference`. That being said, it doesn't make much sense to separate them into two PRs because _currently_ the filtering part is designed around the linking part, i.e. it is trying to check which BND's are suitable to the logic implemented in the linking part, and if the logic isn't applicable to an BND, the BND simply slips through without generating any new interpretations. So `InversionBreakendPreFilter` is a filter and a classifier at the same time, it function is really diverting different BND's to be handled by different logics, and it definitely should be improved.; If you buy this argument, I am also fully aware of the code design issue that it is preferable to NOT divert&mdash;gather&mdash;send through different handlers like it currently is for calling variants from the assembly contigs, instead it should be a single stream pass through all the BND's. I'll try to follow the preferred design. > What about some other filters more specifically aimed at the artifacts that cause these false large calls? I think it's a good idea to check annotations -- ie. do the mates lie at two regions that are segmental duplications of each other, or one side of the mate looks like a transposable element insertion? I guess it's ok to put in a tool with this limit temporarily, though. Answer: Agree. And I think these kind of checking are not only good, but a mandate for a good caller, i.e. to take advantage of prior knowledge. I am also thinking about improving it ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929
https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:2795,Testability,log,logic,2795,"d be a single stream pass through all the BND's. I'll try to follow the preferred design. > What about some other filters more specifically aimed at the artifacts that cause these false large calls? I think it's a good idea to check annotations -- ie. do the mates lie at two regions that are segmental duplications of each other, or one side of the mate looks like a transposable element insertion? I guess it's ok to put in a tool with this limit temporarily, though. Answer: Agree. And I think these kind of checking are not only good, but a mandate for a good caller, i.e. to take advantage of prior knowledge. I am also thinking about improving it by checking if there are short read evidence supporting the BND's (sometimes there aren't any, which is strange for the alignments of the assembly to point to such novel adjacencies in the first place).; However, I might have to push back a little on this particular request at this point because such features can be later added on like you said, and probably now is not the best time to do it, because the linking logic is unlikely to be affected by presence of these reference annotations.; One note though, and I think you don't mean it literally either, the large ones are not necessarily artifacts. Like what we discussed during the group meetings, they are ""filtered"" partly because they are more likely to overlap with multiple other BND mates, and are more likely to be artifacts or mobile element insertions. In other words, they are not sent for linking because they are less likely to be suited for the linking logic in `LinkedInversionBreakpointsInference`. But if they do, the linking is totally agnostic about size. > Building in an ability to check the copy number of the region in which an inversion breakpoint lies (by checking against a CNV call for example) would probably be really helpful. Answer: Agree. And it shouldn't be too hard to do that. But again, may be in a different PR?. > When you say 'overlaps with CPX' it mig",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929
https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:3302,Testability,log,logic,3302,"ndate for a good caller, i.e. to take advantage of prior knowledge. I am also thinking about improving it by checking if there are short read evidence supporting the BND's (sometimes there aren't any, which is strange for the alignments of the assembly to point to such novel adjacencies in the first place).; However, I might have to push back a little on this particular request at this point because such features can be later added on like you said, and probably now is not the best time to do it, because the linking logic is unlikely to be affected by presence of these reference annotations.; One note though, and I think you don't mean it literally either, the large ones are not necessarily artifacts. Like what we discussed during the group meetings, they are ""filtered"" partly because they are more likely to overlap with multiple other BND mates, and are more likely to be artifacts or mobile element insertions. In other words, they are not sent for linking because they are less likely to be suited for the linking logic in `LinkedInversionBreakpointsInference`. But if they do, the linking is totally agnostic about size. > Building in an ability to check the copy number of the region in which an inversion breakpoint lies (by checking against a CNV call for example) would probably be really helpful. Answer: Agree. And it shouldn't be too hard to do that. But again, may be in a different PR?. > When you say 'overlaps with CPX' it might be helpful have a more particular set of criteria.. A larger inversion event might span over a smaller complex event. Answer: Note taken. I'll improve on this in the next polished implementation that is ready for line-by-line review. > I'd check to see if there are additional filters implemented in SV-Pipe that you could apply here. Answer: Yes. But I'm not sure if by SV-Pipe you mean RDTest repo? Sorry I wasn't around when I was added to that repo so I don't know the context. In addition, I believe this could be another future improvemen",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929
https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:5156,Testability,log,logic,5156,"the context. In addition, I believe this could be another future improvements?. > I don't think it's that necessary to provide reports on things that didn't become breakpoints. If they still exist as BNDs in the original perhaps there are some codes we could add as INFO field annotations to indicate that they were considered for inversion calling but filtered, and the reason for doing so. Answer: Agree. And I think the final format can definitely change, as long as the resolved variants, be it deletions, inverted dispersed duplications, and/or real inversions are registered in a VCF. The BED file that is currently produced is for development use, i.e. for me to check why a certain BND is not sent for the analysis so that I can develop ideas on how to catch them in the future (you know, BED are IGV-friendly), and it can certainly be dropped in the final output. ; Regarding adding another INFO field saying they are not used for _THIS PARTICULAR_ logic, I prefer not to do it this way, because we can add more logics in the future, and capturing/resolving these BND's that are not suitable for _THIS PARTICULAR_ logic. I guess in general my personal preference is to put less algorithm-related information in VCF for analysts (less reading for them), and produce add on files for tool developers. What's your thoughts?. > Does this even have to be a spark tool? It looks like you are just reading the variants into a parallel spark context, filtering, and then collecting them to actually process them. Why not just make this a non-spark tool and process it all in memory on one node?. Answer: Agree. It doesn't have to be, at least in theory, and it probably is going to be faster as we don't need to incur the Spark overhead for such a typically small job. But (I'm saying too many buts....) up to this point all SV tools are under the package `hellbender.tools.spark.sv`, so I'm following suit here. Note the two classes's main interface methods mentions nothing about RDDs (that's on pu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929
https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:5219,Testability,log,logics,5219,"the context. In addition, I believe this could be another future improvements?. > I don't think it's that necessary to provide reports on things that didn't become breakpoints. If they still exist as BNDs in the original perhaps there are some codes we could add as INFO field annotations to indicate that they were considered for inversion calling but filtered, and the reason for doing so. Answer: Agree. And I think the final format can definitely change, as long as the resolved variants, be it deletions, inverted dispersed duplications, and/or real inversions are registered in a VCF. The BED file that is currently produced is for development use, i.e. for me to check why a certain BND is not sent for the analysis so that I can develop ideas on how to catch them in the future (you know, BED are IGV-friendly), and it can certainly be dropped in the final output. ; Regarding adding another INFO field saying they are not used for _THIS PARTICULAR_ logic, I prefer not to do it this way, because we can add more logics in the future, and capturing/resolving these BND's that are not suitable for _THIS PARTICULAR_ logic. I guess in general my personal preference is to put less algorithm-related information in VCF for analysts (less reading for them), and produce add on files for tool developers. What's your thoughts?. > Does this even have to be a spark tool? It looks like you are just reading the variants into a parallel spark context, filtering, and then collecting them to actually process them. Why not just make this a non-spark tool and process it all in memory on one node?. Answer: Agree. It doesn't have to be, at least in theory, and it probably is going to be faster as we don't need to incur the Spark overhead for such a typically small job. But (I'm saying too many buts....) up to this point all SV tools are under the package `hellbender.tools.spark.sv`, so I'm following suit here. Note the two classes's main interface methods mentions nothing about RDDs (that's on pu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929
https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:5321,Testability,log,logic,5321,"the context. In addition, I believe this could be another future improvements?. > I don't think it's that necessary to provide reports on things that didn't become breakpoints. If they still exist as BNDs in the original perhaps there are some codes we could add as INFO field annotations to indicate that they were considered for inversion calling but filtered, and the reason for doing so. Answer: Agree. And I think the final format can definitely change, as long as the resolved variants, be it deletions, inverted dispersed duplications, and/or real inversions are registered in a VCF. The BED file that is currently produced is for development use, i.e. for me to check why a certain BND is not sent for the analysis so that I can develop ideas on how to catch them in the future (you know, BED are IGV-friendly), and it can certainly be dropped in the final output. ; Regarding adding another INFO field saying they are not used for _THIS PARTICULAR_ logic, I prefer not to do it this way, because we can add more logics in the future, and capturing/resolving these BND's that are not suitable for _THIS PARTICULAR_ logic. I guess in general my personal preference is to put less algorithm-related information in VCF for analysts (less reading for them), and produce add on files for tool developers. What's your thoughts?. > Does this even have to be a spark tool? It looks like you are just reading the variants into a parallel spark context, filtering, and then collecting them to actually process them. Why not just make this a non-spark tool and process it all in memory on one node?. Answer: Agree. It doesn't have to be, at least in theory, and it probably is going to be faster as we don't need to incur the Spark overhead for such a typically small job. But (I'm saying too many buts....) up to this point all SV tools are under the package `hellbender.tools.spark.sv`, so I'm following suit here. Note the two classes's main interface methods mentions nothing about RDDs (that's on pu",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929
https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:6856,Testability,log,logic,6856," and capturing/resolving these BND's that are not suitable for _THIS PARTICULAR_ logic. I guess in general my personal preference is to put less algorithm-related information in VCF for analysts (less reading for them), and produce add on files for tool developers. What's your thoughts?. > Does this even have to be a spark tool? It looks like you are just reading the variants into a parallel spark context, filtering, and then collecting them to actually process them. Why not just make this a non-spark tool and process it all in memory on one node?. Answer: Agree. It doesn't have to be, at least in theory, and it probably is going to be faster as we don't need to incur the Spark overhead for such a typically small job. But (I'm saying too many buts....) up to this point all SV tools are under the package `hellbender.tools.spark.sv`, so I'm following suit here. Note the two classes's main interface methods mentions nothing about RDDs (that's on purpose). ; On the other hand, this is an engineering question I believe, and it depends on whether we want to put as much of discovery code as possible into `StructuralVariationDiscoveryPipelineSpark` (the last commit actually hooks the two classes into it, so a single invocation of the tool produces more variants), or we go wdl in pipelining the whole process. -------. All in all, I think the comments and critics are generally about the ""filtering""/""classifying"" part, and the most serious concern about it is false negatives. Am I understanding correctly? If so, given that the filtering step is only picking the BND's that are suitable to the linking logic, I can imagine the false negative problem be solved in the future by other logics (e.g. more relaxed requirement on pair matching, or not even requiring matching INV55/INV33 pairs, etc.) In fact, that's what I'm planning on.; Another part of the problem is how much I can accomplish in this single PR, and how large it should be&mdash;the forever existing problem for new tools.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929
https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:6937,Testability,log,logics,6937," and capturing/resolving these BND's that are not suitable for _THIS PARTICULAR_ logic. I guess in general my personal preference is to put less algorithm-related information in VCF for analysts (less reading for them), and produce add on files for tool developers. What's your thoughts?. > Does this even have to be a spark tool? It looks like you are just reading the variants into a parallel spark context, filtering, and then collecting them to actually process them. Why not just make this a non-spark tool and process it all in memory on one node?. Answer: Agree. It doesn't have to be, at least in theory, and it probably is going to be faster as we don't need to incur the Spark overhead for such a typically small job. But (I'm saying too many buts....) up to this point all SV tools are under the package `hellbender.tools.spark.sv`, so I'm following suit here. Note the two classes's main interface methods mentions nothing about RDDs (that's on purpose). ; On the other hand, this is an engineering question I believe, and it depends on whether we want to put as much of discovery code as possible into `StructuralVariationDiscoveryPipelineSpark` (the last commit actually hooks the two classes into it, so a single invocation of the tool produces more variants), or we go wdl in pipelining the whole process. -------. All in all, I think the comments and critics are generally about the ""filtering""/""classifying"" part, and the most serious concern about it is false negatives. Am I understanding correctly? If so, given that the filtering step is only picking the BND's that are suitable to the linking logic, I can imagine the false negative problem be solved in the future by other logics (e.g. more relaxed requirement on pair matching, or not even requiring matching INV55/INV33 pairs, etc.) In fact, that's what I'm planning on.; Another part of the problem is how much I can accomplish in this single PR, and how large it should be&mdash;the forever existing problem for new tools.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929
https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:703,Usability,clear,clearer,703,"> I've looked through at a high level although I'd prefer not to do a line-by-line code review until you think this is ready to be merged). Here are some comments on your proposed algorithm description:. Answer: thanks for looking! That's what I intended as I'm not sure if the algorithm itself would face strong critics. If that's the case, time spent on coding is not worth it IMO. In fact ideally I'd like to write a design doc or something similar, and only code after the design is agreed on. --------. > I would of course prefer not to have to have a hard filter on length. This would mean we would never call a large inversion even if it exists. . Answer: Totally agree. Now looking back, it get clearer to me that this proposal contains two parts: the filtering part, and the breakpoint linking part, separated into two major classes `InversionBreakendPreFilter` and `LinkedInversionBreakpointsInference`. That being said, it doesn't make much sense to separate them into two PRs because _currently_ the filtering part is designed around the linking part, i.e. it is trying to check which BND's are suitable to the logic implemented in the linking part, and if the logic isn't applicable to an BND, the BND simply slips through without generating any new interpretations. So `InversionBreakendPreFilter` is a filter and a classifier at the same time, it function is really diverting different BND's to be handled by different logics, and it definitely should be improved.; If you buy this argument, I am also fully aware of the code design issue that it is preferable to NOT divert&mdash;gather&mdash;send through different handlers like it currently is for calling variants from the assembly contigs, instead it should be a single stream pass through all the BND's. I'll try to follow the preferred design. > What about some other filters more specifically aimed at the artifacts that cause these false large calls? I think it's a good idea to check annotations -- ie. do the mates lie at two",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929
https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929:1215,Usability,simpl,simply,1215,"oposed algorithm description:. Answer: thanks for looking! That's what I intended as I'm not sure if the algorithm itself would face strong critics. If that's the case, time spent on coding is not worth it IMO. In fact ideally I'd like to write a design doc or something similar, and only code after the design is agreed on. --------. > I would of course prefer not to have to have a hard filter on length. This would mean we would never call a large inversion even if it exists. . Answer: Totally agree. Now looking back, it get clearer to me that this proposal contains two parts: the filtering part, and the breakpoint linking part, separated into two major classes `InversionBreakendPreFilter` and `LinkedInversionBreakpointsInference`. That being said, it doesn't make much sense to separate them into two PRs because _currently_ the filtering part is designed around the linking part, i.e. it is trying to check which BND's are suitable to the logic implemented in the linking part, and if the logic isn't applicable to an BND, the BND simply slips through without generating any new interpretations. So `InversionBreakendPreFilter` is a filter and a classifier at the same time, it function is really diverting different BND's to be handled by different logics, and it definitely should be improved.; If you buy this argument, I am also fully aware of the code design issue that it is preferable to NOT divert&mdash;gather&mdash;send through different handlers like it currently is for calling variants from the assembly contigs, instead it should be a single stream pass through all the BND's. I'll try to follow the preferred design. > What about some other filters more specifically aimed at the artifacts that cause these false large calls? I think it's a good idea to check annotations -- ie. do the mates lie at two regions that are segmental duplications of each other, or one side of the mate looks like a transposable element insertion? I guess it's ok to put in a tool with this limit",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789#issuecomment-406483929
https://github.com/broadinstitute/gatk/issues/4790#issuecomment-394487888:122,Usability,simpl,simply,122,"Hi @wujh2017,. You should filter your variants using the various quality scores described in the VCF header. We find that simply filtering on QS is typically a good strategy. Also, you might find it more helpful to post this sort of question in the GATK forums---other users might benefit from seeing the answer there.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4790#issuecomment-394487888
https://github.com/broadinstitute/gatk/issues/4792#issuecomment-400756954:45,Energy Efficiency,green,green,45,"This is causing an issue in our project with green team, due to ExAC. I will design a fix. @jonn-smith",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4792#issuecomment-400756954
https://github.com/broadinstitute/gatk/issues/4792#issuecomment-400763404:91,Usability,clear,clear,91,"@LeeTL1220 sounds good. Your ideal output format seems like what we should be doing. To be clear, though, `ANNOTATION1=4;ANNOTATION2=Foo;ANNOTATION3=Car` would be funcotations in the Funcotation field?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4792#issuecomment-400763404
https://github.com/broadinstitute/gatk/pull/4793#issuecomment-391479003:2245,Testability,test,test,2245,| [...tools/walkers/haplotypecaller/HaplotypeCaller.java](https://codecov.io/gh/broadinstitute/gatk/pull/4793/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXIuamF2YQ==) | `93.939% <ø> (ø)` | `18 <0> (ø)` | :arrow_down: |; | [...lbender/utils/variant/GATKVariantContextUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4793/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWYXJpYW50Q29udGV4dFV0aWxzLmphdmE=) | `84.32% <ø> (ø)` | `216 <0> (ø)` | :arrow_down: |; | [...plotypecaller/HaplotypeCallerGenotypingEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/4793/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJHZW5vdHlwaW5nRW5naW5lLmphdmE=) | `78.014% <100%> (+0.316%)` | `32 <0> (ø)` | :arrow_down: |; | [...hellbender/utils/test/VariantContextTestUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4793/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1ZhcmlhbnRDb250ZXh0VGVzdFV0aWxzLmphdmE=) | `83.544% <100%> (+0.283%)` | `66 <2> (+2)` | :arrow_up: |; | [...notyper/GenotypeCalculationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4793/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwZUNhbGN1bGF0aW9uQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `100% <100%> (ø)` | `2 <0> (ø)` | :arrow_down: |; | [...kers/haplotypecaller/ReferenceConfidenceModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/4793/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9SZWZlcmVuY2VDb25maWRlbmNlTW9kZWwuamF2YQ==) | `93.532% <100%> (+0.339%)` | `68 <6> (+7)` | :arrow_up: |; | [...tools/walkers/genotyper/AlleleSubsetti,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4793#issuecomment-391479003
https://github.com/broadinstitute/gatk/issues/4794#issuecomment-391385377:437,Safety,avoid,avoid,437,"We are rewriting that tool in java to prevent issues like this. You could pull that branch from here: https://github.com/broadinstitute/gatk/pull/4800.; Otherwise, I think pysam handles gzipped indexed vcfs better than plain text ones. So if you gzip your resource files with ; `; bgzip -c hapmap_3.3.hg19.sites.vcf > hapmap_3.3.hg19.sites.vcf.gz; `; then index them with tabix:; `; tabix -p vcf hapmap_3.3.hg19.sites.vcf.gz; `; you may avoid this crash.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4794#issuecomment-391385377
https://github.com/broadinstitute/gatk/pull/4800#issuecomment-393337753:64,Deployability,integrat,integration,64,"Responded to most of the review, but still need to add a proper integration test and fix up the pipeline test.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4800#issuecomment-393337753
https://github.com/broadinstitute/gatk/pull/4800#issuecomment-393337753:96,Deployability,pipeline,pipeline,96,"Responded to most of the review, but still need to add a proper integration test and fix up the pipeline test.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4800#issuecomment-393337753
https://github.com/broadinstitute/gatk/pull/4800#issuecomment-393337753:64,Integrability,integrat,integration,64,"Responded to most of the review, but still need to add a proper integration test and fix up the pipeline test.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4800#issuecomment-393337753
https://github.com/broadinstitute/gatk/pull/4800#issuecomment-393337753:76,Testability,test,test,76,"Responded to most of the review, but still need to add a proper integration test and fix up the pipeline test.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4800#issuecomment-393337753
https://github.com/broadinstitute/gatk/pull/4800#issuecomment-393337753:105,Testability,test,test,105,"Responded to most of the review, but still need to add a proper integration test and fix up the pipeline test.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4800#issuecomment-393337753
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391372959:1,Testability,test,testSpanningDeletionIsNotConsideredVariant,1,"`testSpanningDeletionIsNotConsideredVariant` fails in master. `testPresenceOfUnlikelySpanningDeletionDoesntAffectResults` passes here and in master, as it should. Its purpose is to check that the spanning deletion logic doesn't break anything.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391372959
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391372959:63,Testability,test,testPresenceOfUnlikelySpanningDeletionDoesntAffectResults,63,"`testSpanningDeletionIsNotConsideredVariant` fails in master. `testPresenceOfUnlikelySpanningDeletionDoesntAffectResults` passes here and in master, as it should. Its purpose is to check that the spanning deletion logic doesn't break anything.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391372959
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391372959:214,Testability,log,logic,214,"`testSpanningDeletionIsNotConsideredVariant` fails in master. `testPresenceOfUnlikelySpanningDeletionDoesntAffectResults` passes here and in master, as it should. Its purpose is to check that the spanning deletion logic doesn't break anything.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391372959
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391455182:14,Testability,test,test,14,If the second test passed in master then I don't consider it to be a good test of your fix. Can you make a test where the PLs with and without span del would lead to different QUALs in the old version? Maybe a case with a confident deletion in sample1 and a low quality SNP in sample2 that causes a spanning deletion in sample1. You could probably do this by modifying your first test by adding a sample that has A or B called with low GQ with respect to the reference.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391455182
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391455182:74,Testability,test,test,74,If the second test passed in master then I don't consider it to be a good test of your fix. Can you make a test where the PLs with and without span del would lead to different QUALs in the old version? Maybe a case with a confident deletion in sample1 and a low quality SNP in sample2 that causes a spanning deletion in sample1. You could probably do this by modifying your first test by adding a sample that has A or B called with low GQ with respect to the reference.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391455182
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391455182:107,Testability,test,test,107,If the second test passed in master then I don't consider it to be a good test of your fix. Can you make a test where the PLs with and without span del would lead to different QUALs in the old version? Maybe a case with a confident deletion in sample1 and a low quality SNP in sample2 that causes a spanning deletion in sample1. You could probably do this by modifying your first test by adding a sample that has A or B called with low GQ with respect to the reference.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391455182
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391455182:380,Testability,test,test,380,If the second test passed in master then I don't consider it to be a good test of your fix. Can you make a test where the PLs with and without span del would lead to different QUALs in the old version? Maybe a case with a confident deletion in sample1 and a low quality SNP in sample2 that causes a spanning deletion in sample1. You could probably do this by modifying your first test by adding a sample that has A or B called with low GQ with respect to the reference.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391455182
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391472795:382,Availability,down,down,382,"The point of the second test is to make sure that the new code path involving the spanning deletion has no effect if the spanning deletion has low likelihood. You could imagine that I introduced a bug where you lose sensitivity whenever a spanning deletion appears in the alleles list even if there's very little evidence for it, just because its potential existence sends the code down some new, incorrect, logic. I think `testSpanningDeletionIsNotConsideredVariant` is already doing what you want because before this fix the example VC is definitely considered to be variant even though the only non-ref evidence is for the symbolic `*` allele. The PLs in this example are 50, 100, 100, 0, 100, 100, which means that it is overwhelmingly likely that this sample's genotype is REF/SPAN_DEL. In the old code you get a qual of about 50 (the hom ref PL), whereas in the new code you get that the chances of a variant are about 1 in 10^10 since the only truly variant PLs are 100. Now, if you think a multi-sample case would be useful that sounds good to me. However, a ""confident deletion in sample1 and a low quality SNP in sample2"" would not look very different since the deletion would only exist in an upstream VC, not the VC being tested, so basically we would have sample1 = REF/SPAN_DEL, sample2 = REF/SNP with low quality. The current test is the same thing with just sample1. Is this what you had in mind?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391472795
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391472795:24,Testability,test,test,24,"The point of the second test is to make sure that the new code path involving the spanning deletion has no effect if the spanning deletion has low likelihood. You could imagine that I introduced a bug where you lose sensitivity whenever a spanning deletion appears in the alleles list even if there's very little evidence for it, just because its potential existence sends the code down some new, incorrect, logic. I think `testSpanningDeletionIsNotConsideredVariant` is already doing what you want because before this fix the example VC is definitely considered to be variant even though the only non-ref evidence is for the symbolic `*` allele. The PLs in this example are 50, 100, 100, 0, 100, 100, which means that it is overwhelmingly likely that this sample's genotype is REF/SPAN_DEL. In the old code you get a qual of about 50 (the hom ref PL), whereas in the new code you get that the chances of a variant are about 1 in 10^10 since the only truly variant PLs are 100. Now, if you think a multi-sample case would be useful that sounds good to me. However, a ""confident deletion in sample1 and a low quality SNP in sample2"" would not look very different since the deletion would only exist in an upstream VC, not the VC being tested, so basically we would have sample1 = REF/SPAN_DEL, sample2 = REF/SNP with low quality. The current test is the same thing with just sample1. Is this what you had in mind?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391472795
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391472795:408,Testability,log,logic,408,"The point of the second test is to make sure that the new code path involving the spanning deletion has no effect if the spanning deletion has low likelihood. You could imagine that I introduced a bug where you lose sensitivity whenever a spanning deletion appears in the alleles list even if there's very little evidence for it, just because its potential existence sends the code down some new, incorrect, logic. I think `testSpanningDeletionIsNotConsideredVariant` is already doing what you want because before this fix the example VC is definitely considered to be variant even though the only non-ref evidence is for the symbolic `*` allele. The PLs in this example are 50, 100, 100, 0, 100, 100, which means that it is overwhelmingly likely that this sample's genotype is REF/SPAN_DEL. In the old code you get a qual of about 50 (the hom ref PL), whereas in the new code you get that the chances of a variant are about 1 in 10^10 since the only truly variant PLs are 100. Now, if you think a multi-sample case would be useful that sounds good to me. However, a ""confident deletion in sample1 and a low quality SNP in sample2"" would not look very different since the deletion would only exist in an upstream VC, not the VC being tested, so basically we would have sample1 = REF/SPAN_DEL, sample2 = REF/SNP with low quality. The current test is the same thing with just sample1. Is this what you had in mind?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391472795
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391472795:424,Testability,test,testSpanningDeletionIsNotConsideredVariant,424,"The point of the second test is to make sure that the new code path involving the spanning deletion has no effect if the spanning deletion has low likelihood. You could imagine that I introduced a bug where you lose sensitivity whenever a spanning deletion appears in the alleles list even if there's very little evidence for it, just because its potential existence sends the code down some new, incorrect, logic. I think `testSpanningDeletionIsNotConsideredVariant` is already doing what you want because before this fix the example VC is definitely considered to be variant even though the only non-ref evidence is for the symbolic `*` allele. The PLs in this example are 50, 100, 100, 0, 100, 100, which means that it is overwhelmingly likely that this sample's genotype is REF/SPAN_DEL. In the old code you get a qual of about 50 (the hom ref PL), whereas in the new code you get that the chances of a variant are about 1 in 10^10 since the only truly variant PLs are 100. Now, if you think a multi-sample case would be useful that sounds good to me. However, a ""confident deletion in sample1 and a low quality SNP in sample2"" would not look very different since the deletion would only exist in an upstream VC, not the VC being tested, so basically we would have sample1 = REF/SPAN_DEL, sample2 = REF/SNP with low quality. The current test is the same thing with just sample1. Is this what you had in mind?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391472795
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391472795:1234,Testability,test,tested,1234,"The point of the second test is to make sure that the new code path involving the spanning deletion has no effect if the spanning deletion has low likelihood. You could imagine that I introduced a bug where you lose sensitivity whenever a spanning deletion appears in the alleles list even if there's very little evidence for it, just because its potential existence sends the code down some new, incorrect, logic. I think `testSpanningDeletionIsNotConsideredVariant` is already doing what you want because before this fix the example VC is definitely considered to be variant even though the only non-ref evidence is for the symbolic `*` allele. The PLs in this example are 50, 100, 100, 0, 100, 100, which means that it is overwhelmingly likely that this sample's genotype is REF/SPAN_DEL. In the old code you get a qual of about 50 (the hom ref PL), whereas in the new code you get that the chances of a variant are about 1 in 10^10 since the only truly variant PLs are 100. Now, if you think a multi-sample case would be useful that sounds good to me. However, a ""confident deletion in sample1 and a low quality SNP in sample2"" would not look very different since the deletion would only exist in an upstream VC, not the VC being tested, so basically we would have sample1 = REF/SPAN_DEL, sample2 = REF/SNP with low quality. The current test is the same thing with just sample1. Is this what you had in mind?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391472795
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391472795:1341,Testability,test,test,1341,"The point of the second test is to make sure that the new code path involving the spanning deletion has no effect if the spanning deletion has low likelihood. You could imagine that I introduced a bug where you lose sensitivity whenever a spanning deletion appears in the alleles list even if there's very little evidence for it, just because its potential existence sends the code down some new, incorrect, logic. I think `testSpanningDeletionIsNotConsideredVariant` is already doing what you want because before this fix the example VC is definitely considered to be variant even though the only non-ref evidence is for the symbolic `*` allele. The PLs in this example are 50, 100, 100, 0, 100, 100, which means that it is overwhelmingly likely that this sample's genotype is REF/SPAN_DEL. In the old code you get a qual of about 50 (the hom ref PL), whereas in the new code you get that the chances of a variant are about 1 in 10^10 since the only truly variant PLs are 100. Now, if you think a multi-sample case would be useful that sounds good to me. However, a ""confident deletion in sample1 and a low quality SNP in sample2"" would not look very different since the deletion would only exist in an upstream VC, not the VC being tested, so basically we would have sample1 = REF/SPAN_DEL, sample2 = REF/SNP with low quality. The current test is the same thing with just sample1. Is this what you had in mind?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-391472795
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-392147804:53,Testability,test,testing,53,"Let's be rigorous and use two samples so that you're testing a case that could actually occur in the wild. It should be easy enough to add sample2 with PLs like [10,0,40,100,70,300]. Then can you add a comment about where the ""magic number"" in the assert statement comes from? Or maybe to be super duper rigorous you could compare the QUAL from that variant to the QUAL for sample3 and sample4 where 3 and 4 have the same PLs as 1 and 2, but without the spanning deletion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-392147804
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-392147804:248,Testability,assert,assert,248,"Let's be rigorous and use two samples so that you're testing a case that could actually occur in the wild. It should be easy enough to add sample2 with PLs like [10,0,40,100,70,300]. Then can you add a comment about where the ""magic number"" in the assert statement comes from? Or maybe to be super duper rigorous you could compare the QUAL from that variant to the QUAL for sample3 and sample4 where 3 and 4 have the same PLs as 1 and 2, but without the spanning deletion.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-392147804
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-392303568:169,Testability,test,tests,169,"@ldgauthier I did all of those things. Note that to be precise, when you remove the spanning deletion the ""het"" REF / SPAN_DEL must be treated as haploid, which the new tests do.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-392303568
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-393173847:2849,Testability,test,test,2849, <0%> (+7%)` | |; | [...s/spark/sv/discovery/SvDiscoveryInputMetaData.java](https://codecov.io/gh/broadinstitute/gatk/pull/4801/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvU3ZEaXNjb3ZlcnlJbnB1dE1ldGFEYXRhLmphdmE=) | `100% <0%> (ø)` | `9% <0%> (+7%)` | :arrow_up: |; | [.../utils/read/markduplicates/DuplicationMetrics.java](https://codecov.io/gh/broadinstitute/gatk/pull/4801/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL0R1cGxpY2F0aW9uTWV0cmljcy5qYXZh) | `85.366% <0%> (ø)` | `13% <0%> (?)` | |; | [...covery/inference/CpxVariantReInterpreterSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4801/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL0NweFZhcmlhbnRSZUludGVycHJldGVyU3BhcmsuamF2YQ==) | `100% <0%> (ø)` | `5% <0%> (?)` | |; | [...ils/test/testers/AbstractMarkDuplicatesTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/4801/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L3Rlc3RlcnMvQWJzdHJhY3RNYXJrRHVwbGljYXRlc1Rlc3Rlci5qYXZh) | `79.487% <0%> (ø)` | `17% <0%> (?)` | |; | [...nce/SegmentedCpxVariantSimpleVariantExtractor.java](https://codecov.io/gh/broadinstitute/gatk/pull/4801/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1NlZ21lbnRlZENweFZhcmlhbnRTaW1wbGVWYXJpYW50RXh0cmFjdG9yLmphdmE=) | `93.96% <0%> (ø)` | `71% <0%> (?)` | |; | [.../sv/StructuralVariationDiscoveryPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4801/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TdHJ1Y3R1cmFsVmFyaWF0aW9uRGlzY292ZXJ5UGlwZWxpbmVTcGFyay5qYXZh) | `88.652% <0%> (+0.081%)` | `13% <0%> (ø)` | :arrow_down: |; | [...iscoverFromLocalAssemblyConti,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-393173847
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-393173847:2854,Testability,test,testers,2854, <0%> (+7%)` | |; | [...s/spark/sv/discovery/SvDiscoveryInputMetaData.java](https://codecov.io/gh/broadinstitute/gatk/pull/4801/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvU3ZEaXNjb3ZlcnlJbnB1dE1ldGFEYXRhLmphdmE=) | `100% <0%> (ø)` | `9% <0%> (+7%)` | :arrow_up: |; | [.../utils/read/markduplicates/DuplicationMetrics.java](https://codecov.io/gh/broadinstitute/gatk/pull/4801/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL0R1cGxpY2F0aW9uTWV0cmljcy5qYXZh) | `85.366% <0%> (ø)` | `13% <0%> (?)` | |; | [...covery/inference/CpxVariantReInterpreterSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4801/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL0NweFZhcmlhbnRSZUludGVycHJldGVyU3BhcmsuamF2YQ==) | `100% <0%> (ø)` | `5% <0%> (?)` | |; | [...ils/test/testers/AbstractMarkDuplicatesTester.java](https://codecov.io/gh/broadinstitute/gatk/pull/4801/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L3Rlc3RlcnMvQWJzdHJhY3RNYXJrRHVwbGljYXRlc1Rlc3Rlci5qYXZh) | `79.487% <0%> (ø)` | `17% <0%> (?)` | |; | [...nce/SegmentedCpxVariantSimpleVariantExtractor.java](https://codecov.io/gh/broadinstitute/gatk/pull/4801/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1NlZ21lbnRlZENweFZhcmlhbnRTaW1wbGVWYXJpYW50RXh0cmFjdG9yLmphdmE=) | `93.96% <0%> (ø)` | `71% <0%> (?)` | |; | [.../sv/StructuralVariationDiscoveryPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4801/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TdHJ1Y3R1cmFsVmFyaWF0aW9uRGlzY292ZXJ5UGlwZWxpbmVTcGFyay5qYXZh) | `88.652% <0%> (+0.081%)` | `13% <0%> (ø)` | :arrow_down: |; | [...iscoverFromLocalAssemblyConti,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-393173847
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-393179618:49,Energy Efficiency,green,green,49,@davidbenjamin I kicked Travis and now tests are green so you can merge.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-393179618
https://github.com/broadinstitute/gatk/pull/4801#issuecomment-393179618:39,Testability,test,tests,39,@davidbenjamin I kicked Travis and now tests are green so you can merge.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4801#issuecomment-393179618
https://github.com/broadinstitute/gatk/pull/4805#issuecomment-391588915:1676,Testability,test,test,1676,=========================================; + Hits 50679 55644 +4965 ; - Misses 8579 9335 +756 ; - Partials 3987 4253 +266; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4805?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...pleNovelAdjacencyAndChimericAlignmentEvidence.java](https://codecov.io/gh/broadinstitute/gatk/pull/4805/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1NpbXBsZU5vdmVsQWRqYWNlbmN5QW5kQ2hpbWVyaWNBbGlnbm1lbnRFdmlkZW5jZS5qYXZh) | `62.963% <0%> (-24.537%)` | `13% <0%> (+3%)` | |; | [...overy/inference/NovelAdjacencyAndAltHaplotype.java](https://codecov.io/gh/broadinstitute/gatk/pull/4805/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL05vdmVsQWRqYWNlbmN5QW5kQWx0SGFwbG90eXBlLmphdmE=) | `75.556% <0%> (-3.81%)` | `30% <0%> (+1%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4805/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `62.887% <0%> (-3.086%)` | `53% <0%> (+16%)` | |; | [...ead/markduplicates/sparkrecords/EmptyFragment.java](https://codecov.io/gh/broadinstitute/gatk/pull/4805/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9FbXB0eUZyYWdtZW50LmphdmE=) | `86.207% <0%> (-2.682%)` | `16% <0%> (+7%)` | |; | [...transforms/markduplicates/MarkDuplicatesSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4805/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay90cmFuc2Zvcm1zL21hcmtkdXBsaWNhdGVzL01hcmtEdXBsaWNhdGVzU3BhcmsuamF2YQ==) | `96.429% <0%> (-1.648%)` | `29% <0%> (+10%)` | |; | [...iscoverFromLocalAssemblyContigAlignmentsSpark.java](https://codecov.io/gh/broadinstit,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4805#issuecomment-391588915
https://github.com/broadinstitute/gatk/pull/4805#issuecomment-391588915:2966,Usability,Simpl,SimpleSVType,2966,/EmptyFragment.java](https://codecov.io/gh/broadinstitute/gatk/pull/4805/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9FbXB0eUZyYWdtZW50LmphdmE=) | `86.207% <0%> (-2.682%)` | `16% <0%> (+7%)` | |; | [...transforms/markduplicates/MarkDuplicatesSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4805/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay90cmFuc2Zvcm1zL21hcmtkdXBsaWNhdGVzL01hcmtEdXBsaWNhdGVzU3BhcmsuamF2YQ==) | `96.429% <0%> (-1.648%)` | `29% <0%> (+10%)` | |; | [...iscoverFromLocalAssemblyContigAlignmentsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4805/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvU3ZEaXNjb3ZlckZyb21Mb2NhbEFzc2VtYmx5Q29udGlnQWxpZ25tZW50c1NwYXJrLmphdmE=) | `76.048% <0%> (-0.794%)` | `6% <0%> (+4%)` | |; | [...lbender/tools/spark/sv/discovery/SimpleSVType.java](https://codecov.io/gh/broadinstitute/gatk/pull/4805/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvU2ltcGxlU1ZUeXBlLmphdmE=) | `86% <0%> (-0.667%)` | `3% <0%> (ø)` | |; | [...der/tools/walkers/mutect/M2ArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4805/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NMkFyZ3VtZW50Q29sbGVjdGlvbi5qYXZh) | `93.939% <0%> (-0.505%)` | `5% <0%> (+2%)` | |; | [...r/utils/read/markduplicates/sparkrecords/Pair.java](https://codecov.io/gh/broadinstitute/gatk/pull/4805/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9QYWlyLmphdmE=) | `96.35% <0%> (-0.316%)` | `39% <0%> (+10%)` | |; | [...rg/broadinstitute/hellbender/utils/io/IOUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4805/dif,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4805#issuecomment-391588915
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391724103:36,Usability,guid,guide,36,Something is discussed in the style guide about NIO.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391724103
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391724363:25,Modifiability,portab,portable,25,"What is the timeline for portable WDL-NIO?. This would make things like performing preliminary analyses on subsets of contigs, etc. go a bit faster. I agree that this is not a common use case, but since it's such a small amount of work, I don't see the harm. Would be nice to be consistent with other Featured WDLs, if they're all using NIO as well (is this true?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391724363
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391724363:72,Performance,perform,performing,72,"What is the timeline for portable WDL-NIO?. This would make things like performing preliminary analyses on subsets of contigs, etc. go a bit faster. I agree that this is not a common use case, but since it's such a small amount of work, I don't see the harm. Would be nice to be consistent with other Featured WDLs, if they're all using NIO as well (is this true?)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391724363
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391726161:34,Testability,test,tests,34,"For example, I'm running some WGS tests on chr20-22. The BAMs take ~1hr+ to localize but it only takes ~5 minutes to collect read/allelic counts. Getting preempted adds a huge amount of unnecessary overhead.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391726161
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391765626:275,Deployability,release,release,275,"Some of the files in the CNV workflows are not yet supported by NIO, so it won’t be as easy as changing File to String everywhere. I think the ref and the BAMs are the most important ones to change, as Lee pointed out. Let’s not worry too much about getting this in the next release or anything like that—-as soon as is convenient is fine. But let’s decide on things like whether we need a copy in this repo and perhaps have a general strategy going forward.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-391765626
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-392042783:10,Deployability,release,release,10,"FYI, next release of Cromwell is supposed to automatically interpret File types as Sting when given a gs:// bucket file. . Sent from an iPhone and typed with my thumbs. . > On May 24, 2018, at 3:48 PM, samuelklee <notifications@github.com> wrote:; > ; > @LeeTL1220 Actually, since the coverage collection itself is comparable to the time it takes to localize the BAM, this probably also helps even when you're not just running on snippets.; > ; > Building a PoN on chr20-22 took ~2.5 hours without NIO (3/50 BAMs were preempted, some more than once), but it only took ~40 minutes with an NIO WDL I quickly whipped up (and 1/4 of that was because I was unlucky enough to get preempted on one BAM after 10 minutes)!; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-392042783
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-392044533:126,Deployability,release,release,126,"That would be really bad. Are you sure?. On Fri, May 25, 2018, 08:30 sooheelee <notifications@github.com> wrote:. > FYI, next release of Cromwell is supposed to automatically interpret File; > types as Sting when given a gs:// bucket file.; >; > Sent from an iPhone and typed with my thumbs.; >; > > On May 24, 2018, at 3:48 PM, samuelklee <notifications@github.com>; > wrote:; > >; > > @LeeTL1220 Actually, since the coverage collection itself is comparable; > to the time it takes to localize the BAM, this probably also helps even; > when you're not just running on snippets.; > >; > > Building a PoN on chr20-22 took ~2.5 hours without NIO (3/50 BAMs were; > preempted, some more than once), but it only took ~40 minutes with an NIO; > WDL I quickly whipped up (and 1/4 of that was because I was unlucky enough; > to get preempted on one BAM after 10 minutes)!; > >; > > —; > > You are receiving this because you are subscribed to this thread.; > > Reply to this email directly, view it on GitHub, or mute the thread.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4806#issuecomment-392042783>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk5K6EpZQft219BT7K8DoONgZqx2hks5t1_lQgaJpZM4UMBDI>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-392044533
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-392046938:111,Energy Efficiency,efficient,efficient,111,"Actually, I'm noticing that while using NIO for the BAM for read/allelic-count collection is usually much more efficient, using NIO for the reference in other tasks can be much slower. Perhaps the access patterns for the reference (hitting ~10^5 intervals for WES PreprocessIntervals/AnnotateIntervals and ~10^6 sites for WES/WGS CollectAllelicCounts, respectively) make localization a better strategy? @droazen does that sound right to you?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-392046938
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-392046938:197,Security,access,access,197,"Actually, I'm noticing that while using NIO for the BAM for read/allelic-count collection is usually much more efficient, using NIO for the reference in other tasks can be much slower. Perhaps the access patterns for the reference (hitting ~10^5 intervals for WES PreprocessIntervals/AnnotateIntervals and ~10^6 sites for WES/WGS CollectAllelicCounts, respectively) make localization a better strategy? @droazen does that sound right to you?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-392046938
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-392064633:118,Deployability,update,updates,118,"There is a video recording of Chris L saying so. I’ll get the link to it in a bit. Better yet, I'd like to hear about updates to these plans in person. Sent from an iPhone and typed with my thumbs. . > On May 25, 2018, at 9:18 AM, Lee Lichtenstein <notifications@github.com> wrote:; > ; > @sooheelee I just confirmed that is not the case. Have not confirmed the exact change yet.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-392064633
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437037011:232,Availability,error,error,232,"@cjllanwarne, how do I opt-in per parameter? I am trying to run the v4.0.11.0 gCNV WDL using Cromwell v36 and even after changing `File` type to `String` type for the relevant BAM/BAI inputs, I am getting a file-type not recognized error.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437037011
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437178155:107,Performance,optimiz,optimizations,107,Nevermind Chris. I've found the documentation for the feature at https://cromwell.readthedocs.io/en/stable/optimizations/FileLocalization/.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437178155
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437188313:114,Availability,error,error,114,"I spoke too soon. No matter how I define the type, whether String or File, when I run womtools validate I get the error:. ```; ERROR: Value for this attribute is expected to be a string:. bam: {; ```. I added the following parameter_meta field to the task:. ![screenshot 2018-11-08 18 02 03](https://user-images.githubusercontent.com/11543866/48232693-7569ff00-e380-11e8-9dad-2eed3ca68118.png). How to correct this @cjllanwarne? Here's the relevant WDL: https://github.com/broadinstitute/gatk/blob/4.0.11.0/scripts/cnv_wdl/cnv_common_tasks.wdl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437188313
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437188313:127,Availability,ERROR,ERROR,127,"I spoke too soon. No matter how I define the type, whether String or File, when I run womtools validate I get the error:. ```; ERROR: Value for this attribute is expected to be a string:. bam: {; ```. I added the following parameter_meta field to the task:. ![screenshot 2018-11-08 18 02 03](https://user-images.githubusercontent.com/11543866/48232693-7569ff00-e380-11e8-9dad-2eed3ca68118.png). How to correct this @cjllanwarne? Here's the relevant WDL: https://github.com/broadinstitute/gatk/blob/4.0.11.0/scripts/cnv_wdl/cnv_common_tasks.wdl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437188313
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437188313:95,Security,validat,validate,95,"I spoke too soon. No matter how I define the type, whether String or File, when I run womtools validate I get the error:. ```; ERROR: Value for this attribute is expected to be a string:. bam: {; ```. I added the following parameter_meta field to the task:. ![screenshot 2018-11-08 18 02 03](https://user-images.githubusercontent.com/11543866/48232693-7569ff00-e380-11e8-9dad-2eed3ca68118.png). How to correct this @cjllanwarne? Here's the relevant WDL: https://github.com/broadinstitute/gatk/blob/4.0.11.0/scripts/cnv_wdl/cnv_common_tasks.wdl",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437188313
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437362263:431,Performance,cache,cached,431,"@sooheelee I found that on FC, it was only necessary to make the File -> String change at the task level (i.e., in CollectCounts). Not sure if this works due to the particular version of Cromwell on FC. In any case, I don't think you should stress too much about getting NIO working on your VM. Again, I'd expect the current non-NIO gCNV WDL to only take a few hours on FC, and then less than that once coverage collection is call cached. No use spending more time getting NIO working than it would save you, after all!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437362263
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437409513:100,Availability,error,error,100,I'm informed that I have access to the broad-firecloud-dsde billing account and it appears whatever error we encountered the other day was something transient.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437409513
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437409513:25,Security,access,access,25,I'm informed that I have access to the broad-firecloud-dsde billing account and it appears whatever error we encountered the other day was something transient.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-437409513
https://github.com/broadinstitute/gatk/issues/4806#issuecomment-926124144:35,Safety,avoid,avoid,35,"NIO streaming is currently used to avoid BAM localization in `CollectCounts` and `CollectAllelicCounts`, which is particularly beneficial when running on a limited set of intervals. It is also used in `JointSegmentation` to stream VCFs, which saves on disk space. Additional tasks that could benefit from streaming are those that use counts files inputs: `FilterIntervals`, `GermlineCNVCallerCaseMode`, `GermlineCNVCallerCohortMode`, `DetermineGermlineContigPloidyCaseMode`, and `DetermineGermlineContigPloidyCohortMode`, `CreateReadCountPanelOfNormals`, and `DenoiseReadCounts`. These would require that read counts are in TSV format, which we should move to using exclusively (instead of HDF5).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4806#issuecomment-926124144
https://github.com/broadinstitute/gatk/pull/4817#issuecomment-392197039:943,Usability,Simpl,SimpleKeyXsvFuncotationFactory,943,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4817?src=pr&el=h1) Report; > Merging [#4817](https://codecov.io/gh/broadinstitute/gatk/pull/4817?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/3189612e987128275b286448b3659e9bade62594?src=pr&el=desc) will **decrease** coverage by `0.116%`.; > The diff coverage is `57.971%`. ```diff; @@ Coverage Diff @@; ## master #4817 +/- ##; ==============================================; - Coverage 80.36% 80.244% -0.116% ; - Complexity 17606 17611 +5 ; ==============================================; Files 1087 1088 +1 ; Lines 63748 63849 +101 ; Branches 10262 10276 +14 ; ==============================================; + Hits 51228 51235 +7 ; - Misses 8528 8619 +91 ; - Partials 3992 3995 +3; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4817?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ataSources/xsv/SimpleKeyXsvFuncotationFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4817/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL3hzdi9TaW1wbGVLZXlYc3ZGdW5jb3RhdGlvbkZhY3RvcnkuamF2YQ==) | `87.097% <100%> (ø)` | `27 <0> (ø)` | :arrow_down: |; | [...r/dataSources/cosmic/CosmicFuncotationFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4817/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL2Nvc21pYy9Db3NtaWNGdW5jb3RhdGlvbkZhY3RvcnkuamF2YQ==) | `64.167% <20.833%> (-12.071%)` | `21 <0> (ø)` | |; | [...dataSources/gencode/GencodeFuncotationFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4817/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL2dlbmNvZGUvR2VuY29kZUZ1bmNvdGF0aW9uRmFjdG9yeS5qYXZh) | `82.504% <74.286%> (-0.478%)` | `152 <3> (ø)` | |; | [...e/hellbender/tools/funcotator/FuncotatorUtils.java](,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4817#issuecomment-392197039
https://github.com/broadinstitute/gatk/issues/4820#issuecomment-396331882:104,Deployability,release,release,104,"@Atahualkpa Could you try this again on V4.0.5.0? We have resolved a number of issues leading up to the release that might solve your problem. Next, How is the input file sorted? And how is that recorded in your bam header? Also, are you sure that file is indeed sorted correctly? You could try resorting to queryname order if its not.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820#issuecomment-396331882
https://github.com/broadinstitute/gatk/issues/4822#issuecomment-393235457:47,Deployability,install,install,47,"I've had this issue on MacOS X and was able to install the environment successfully by removing the open-mp and mkl lines from the yaml:; ```; - intel-openmp=2018.0.0; - mkl=2018.0.1; - mkl-service=1.1.2; ``` ; Then you may need to remove the partially installed environment with:; ```; conda remove --name gatk --all; ```; Then you can run:; ```; conda create -n gatk -f ./scripts/gatkcondaenv.yml; ```; Hopefully, the tools will run without openmp and mkl, but I'm sure we are taking a performance hit so we should figure out what is the right channel to point to for these packages. @erniebrau have you ever had these isssues?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4822#issuecomment-393235457
https://github.com/broadinstitute/gatk/issues/4822#issuecomment-393235457:253,Deployability,install,installed,253,"I've had this issue on MacOS X and was able to install the environment successfully by removing the open-mp and mkl lines from the yaml:; ```; - intel-openmp=2018.0.0; - mkl=2018.0.1; - mkl-service=1.1.2; ``` ; Then you may need to remove the partially installed environment with:; ```; conda remove --name gatk --all; ```; Then you can run:; ```; conda create -n gatk -f ./scripts/gatkcondaenv.yml; ```; Hopefully, the tools will run without openmp and mkl, but I'm sure we are taking a performance hit so we should figure out what is the right channel to point to for these packages. @erniebrau have you ever had these isssues?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4822#issuecomment-393235457
https://github.com/broadinstitute/gatk/issues/4822#issuecomment-393235457:488,Performance,perform,performance,488,"I've had this issue on MacOS X and was able to install the environment successfully by removing the open-mp and mkl lines from the yaml:; ```; - intel-openmp=2018.0.0; - mkl=2018.0.1; - mkl-service=1.1.2; ``` ; Then you may need to remove the partially installed environment with:; ```; conda remove --name gatk --all; ```; Then you can run:; ```; conda create -n gatk -f ./scripts/gatkcondaenv.yml; ```; Hopefully, the tools will run without openmp and mkl, but I'm sure we are taking a performance hit so we should figure out what is the right channel to point to for these packages. @erniebrau have you ever had these isssues?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4822#issuecomment-393235457
https://github.com/broadinstitute/gatk/issues/4822#issuecomment-578660611:109,Availability,error,error,109,"This is not working. Is there another way to install the python pacakges for `CNNScoreVariants`?. I get this error:. ```; conda create -n gatk -f ./scripts/gatkcondaenv.yml; Collecting package metadata (current_repodata.json): done; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - ./scripts/gatkcondaenv.yml. Current channels:. - https://conda.anaconda.org/bioconda/linux-64; - https://conda.anaconda.org/bioconda/noarch; - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch; - https://conda.anaconda.org/r/linux-64; - https://conda.anaconda.org/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4822#issuecomment-578660611
https://github.com/broadinstitute/gatk/issues/4822#issuecomment-578660611:476,Availability,avail,available,476,"This is not working. Is there another way to install the python pacakges for `CNNScoreVariants`?. I get this error:. ```; conda create -n gatk -f ./scripts/gatkcondaenv.yml; Collecting package metadata (current_repodata.json): done; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - ./scripts/gatkcondaenv.yml. Current channels:. - https://conda.anaconda.org/bioconda/linux-64; - https://conda.anaconda.org/bioconda/noarch; - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch; - https://conda.anaconda.org/r/linux-64; - https://conda.anaconda.org/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4822#issuecomment-578660611
https://github.com/broadinstitute/gatk/issues/4822#issuecomment-578660611:45,Deployability,install,install,45,"This is not working. Is there another way to install the python pacakges for `CNNScoreVariants`?. I get this error:. ```; conda create -n gatk -f ./scripts/gatkcondaenv.yml; Collecting package metadata (current_repodata.json): done; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed. PackagesNotFoundError: The following packages are not available from current channels:. - ./scripts/gatkcondaenv.yml. Current channels:. - https://conda.anaconda.org/bioconda/linux-64; - https://conda.anaconda.org/bioconda/noarch; - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch; - https://conda.anaconda.org/r/linux-64; - https://conda.anaconda.org/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. ```",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4822#issuecomment-578660611
https://github.com/broadinstitute/gatk/issues/4822#issuecomment-578755231:94,Deployability,install,installation,94,"@yeroslaviz Are you trying to create the environment from a cloned github repo, or from a zip installation of GATK ? It looks like its from a repo, in which case you need to follow the instructions [here](https://github.com/broadinstitute/gatk#python). (gatkcondaenv.yml is generated from sources by a gradle task, so running gradle is required unless you're creating the env from an install).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4822#issuecomment-578755231
https://github.com/broadinstitute/gatk/issues/4822#issuecomment-578755231:384,Deployability,install,install,384,"@yeroslaviz Are you trying to create the environment from a cloned github repo, or from a zip installation of GATK ? It looks like its from a repo, in which case you need to follow the instructions [here](https://github.com/broadinstitute/gatk#python). (gatkcondaenv.yml is generated from sources by a gradle task, so running gradle is required unless you're creating the env from an install).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4822#issuecomment-578755231
https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393174181:45,Availability,robust,robust,45,"@mbabadi We should look into ways to be more robust against NaNs, but I think we should just go ahead and blacklist these regions. This can be done from the outset of the pipeline via the `-XL` argument to PreprocessIntervals. Does SV team have a canonical list we can start recommending? Looks like http://cf.10xgenomics.com/supp/genome/GRCh38/sv_blacklist.bed may also be a good option. Perhaps we can add some padding if necessary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393174181
https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393174181:171,Deployability,pipeline,pipeline,171,"@mbabadi We should look into ways to be more robust against NaNs, but I think we should just go ahead and blacklist these regions. This can be done from the outset of the pipeline via the `-XL` argument to PreprocessIntervals. Does SV team have a canonical list we can start recommending? Looks like http://cf.10xgenomics.com/supp/genome/GRCh38/sv_blacklist.bed may also be a good option. Perhaps we can add some padding if necessary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393174181
https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393193351:236,Availability,robust,robust,236,"For SVs, we are not blacklisting any regions except sometimes gaps and centromeres. Unfortunately many of the events occur in messy areas like this and I think it’s going to be a major issue if we can’t guarantee that the model will be robust in such regions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393193351
https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393202676:149,Availability,reliab,reliable,149,"@mwalker174 the region you found above is included in the 10X SV blacklist. What list is the SV team currently using?. If the read-depth data is not reliable in these regions, I would not expect the model fit to be very good, even if we made the model more robust against nans. So I wouldn't think the results would be very useful for SV integration. Is there a way to make CNV-SV integration ""Bayesian"" in the sense that we could fall back on a prior in the case of missing CNV data?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393202676
https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393202676:257,Availability,robust,robust,257,"@mwalker174 the region you found above is included in the 10X SV blacklist. What list is the SV team currently using?. If the read-depth data is not reliable in these regions, I would not expect the model fit to be very good, even if we made the model more robust against nans. So I wouldn't think the results would be very useful for SV integration. Is there a way to make CNV-SV integration ""Bayesian"" in the sense that we could fall back on a prior in the case of missing CNV data?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393202676
https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393202676:338,Deployability,integrat,integration,338,"@mwalker174 the region you found above is included in the 10X SV blacklist. What list is the SV team currently using?. If the read-depth data is not reliable in these regions, I would not expect the model fit to be very good, even if we made the model more robust against nans. So I wouldn't think the results would be very useful for SV integration. Is there a way to make CNV-SV integration ""Bayesian"" in the sense that we could fall back on a prior in the case of missing CNV data?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393202676
https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393202676:381,Deployability,integrat,integration,381,"@mwalker174 the region you found above is included in the 10X SV blacklist. What list is the SV team currently using?. If the read-depth data is not reliable in these regions, I would not expect the model fit to be very good, even if we made the model more robust against nans. So I wouldn't think the results would be very useful for SV integration. Is there a way to make CNV-SV integration ""Bayesian"" in the sense that we could fall back on a prior in the case of missing CNV data?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393202676
https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393202676:338,Integrability,integrat,integration,338,"@mwalker174 the region you found above is included in the 10X SV blacklist. What list is the SV team currently using?. If the read-depth data is not reliable in these regions, I would not expect the model fit to be very good, even if we made the model more robust against nans. So I wouldn't think the results would be very useful for SV integration. Is there a way to make CNV-SV integration ""Bayesian"" in the sense that we could fall back on a prior in the case of missing CNV data?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393202676
https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393202676:381,Integrability,integrat,integration,381,"@mwalker174 the region you found above is included in the 10X SV blacklist. What list is the SV team currently using?. If the read-depth data is not reliable in these regions, I would not expect the model fit to be very good, even if we made the model more robust against nans. So I wouldn't think the results would be very useful for SV integration. Is there a way to make CNV-SV integration ""Bayesian"" in the sense that we could fall back on a prior in the case of missing CNV data?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393202676
https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393219474:186,Availability,error,errors,186,"@samuelklee We aren't using any blacklists currently. I am less concerned about noisy calls because they usually don't line up well with read pair evidence. That said, I did not get nan errors with 1kbp bins - perhaps we could bump up the bin sizes in regions where this happens?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824#issuecomment-393219474
https://github.com/broadinstitute/gatk/issues/4824#issuecomment-394394208:42,Availability,error,errors,42,Blacklisting centromeres resolved the NaN errors except for one block on chrY that roughly corresponds to region q11.23. I blacklisted that block and got no more errors.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824#issuecomment-394394208
https://github.com/broadinstitute/gatk/issues/4824#issuecomment-394394208:162,Availability,error,errors,162,Blacklisting centromeres resolved the NaN errors except for one block on chrY that roughly corresponds to region q11.23. I blacklisted that block and got no more errors.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824#issuecomment-394394208
https://github.com/broadinstitute/gatk/issues/4824#issuecomment-526646440:153,Availability,error,errors,153,"@samuelklee Yes. Looking forward, we will want to reduce the extent of our blacklist and interval filtering, which are currently needed to prevent these errors.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824#issuecomment-526646440
https://github.com/broadinstitute/gatk/issues/4824#issuecomment-526646440:50,Energy Efficiency,reduce,reduce,50,"@samuelklee Yes. Looking forward, we will want to reduce the extent of our blacklist and interval filtering, which are currently needed to prevent these errors.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824#issuecomment-526646440
https://github.com/broadinstitute/gatk/issues/4824#issuecomment-550347502:93,Deployability,patch,patch,93,"At least partially addressed in #6245, we can reopen if there are other NaNs that we have to patch.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4824#issuecomment-550347502
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-464181202:114,Availability,error,error,114,"@samuelklee, in the command line with `source activate gatk` on and within a tmux session. From my notes for this error, I see ; ```; (gatk) shlee@brie:~$ ; ```. So definitely not a Docker run.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-464181202
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:13897,Availability,error,error,13897,"X008409.counts.tsv --input results/200219_X008410.counts.tsv --input results/200219_X008411.counts.tsv --input results/200219_X008412.counts.tsv --input results/200219_X008415.counts.tsv --input results/200219_X008417.counts.tsv --input results/200219_X008420.counts.tsv --input results/200219_X008422.counts.tsv --input results/200219_X008423.counts.tsv --input results/200219_X008429.counts.tsv --input results/200219_X008430.counts.tsv --input results/200219_X008432.counts.tsv --input results/200219_X008458.counts.tsv --input results/200219_X008493.counts.tsv --input results/200219_X008504.counts.tsv --input results/200219_X008512.counts.tsv --input results/200219_X008522.counts.tsv --input results/200219_X008523.counts.tsv --input results/200219_X008525.counts.tsv --input results/200219_X008528.counts.tsv --input results/200219_X008543.counts.tsv --output results/190225.181217_K00178.CNVCaller --output-prefix cohort`. I created the 190225.181217_K00178.CNVCaller folder beforehand because otherwise I'd get this error:. `08:37:16.407 INFO GermlineCNVCaller - Start Date/Time: February 26, 2019 8:37:04 AM GMT; 08:37:16.407 INFO GermlineCNVCaller - ------------------------------------------------------------; 08:37:16.407 INFO GermlineCNVCaller - ------------------------------------------------------------; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Version: 2.18.2; 08:37:16.408 INFO GermlineCNVCaller - Picard Version: 2.18.25; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:37:16.408 INFO GermlineCNVCaller - Deflater: IntelDeflater; 08:37:16.409 INFO GermlineCNVCaller - Inflater: IntelInflater; 08:37:16.409 INFO GermlineCNVCaller - GCS max retries/reopens: 20;",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:15522,Availability,down,down,15522,"HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:37:16.408 INFO GermlineCNVCaller - Deflater: IntelDeflater; 08:37:16.409 INFO GermlineCNVCaller - Inflater: IntelInflater; 08:37:16.409 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 08:37:16.409 INFO GermlineCNVCaller - Requester pays: disabled; 08:37:16.409 INFO GermlineCNVCaller - Initializing engine; 08:37:21.698 INFO GermlineCNVCaller - Done initializing engine; 08:37:22.015 INFO GermlineCNVCaller - Retrieving intervals from read-count file (results/200219_X008378.counts.tsv)...; 08:37:22.119 INFO GermlineCNVCaller - No annotated intervals were provided...; 08:37:22.120 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 08:37:22.194 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 08:37:22.195 INFO GermlineCNVCaller - Shutting down engine; [February 26, 2019 8:37:22 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.29 minutes.; Runtime.totalMemory()=330301440; java.lang.IllegalArgumentException: Output directory results/190226.181217_K00178.CNVCaller does not exist.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.validateArguments(GermlineCNVCaller.java:361); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:281); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbende",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:3556,Modifiability,variab,variables,3556,"ission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 14:13:50.032 INFO cohort_denoising_calling - Loading 24 read counts file(s)...; 14:13:53.719 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 14:13:58.626 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 14:14:04.543 INFO gcnvkernel.models.fancy_model - Global model variables: {'W_tu', 'psi_t_log__', 'ard_u_log__', 'log_mean_bias_t'}; 14:14:04.544 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'z_su', 'psi_s_log__', 'read_depth_s_log__'}; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No log emission sampler given; skipping the sampling step; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No caller given; skipping the calling step; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 14:14:10.902 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up)) starting...: 0%| | 0/5000 [00:00<?, ?it/s]; 14:14:12.877 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: N/A, SNR: N/A, T: 1.50: 0%| | 1/5000 [00:01<2:44:32, 1.97s/it]; 14:14:14.753 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: -145.294 +/- 0.000, SNR: 35869952999211676.0, T: 1.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:3698,Modifiability,variab,variables,3698,"_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 14:13:50.032 INFO cohort_denoising_calling - Loading 24 read counts file(s)...; 14:13:53.719 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 14:13:58.626 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 14:14:04.543 INFO gcnvkernel.models.fancy_model - Global model variables: {'W_tu', 'psi_t_log__', 'ard_u_log__', 'log_mean_bias_t'}; 14:14:04.544 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'z_su', 'psi_s_log__', 'read_depth_s_log__'}; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No log emission sampler given; skipping the sampling step; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No caller given; skipping the calling step; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 14:14:10.902 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up)) starting...: 0%| | 0/5000 [00:00<?, ?it/s]; 14:14:12.877 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: N/A, SNR: N/A, T: 1.50: 0%| | 1/5000 [00:01<2:44:32, 1.97s/it]; 14:14:14.753 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: -145.294 +/- 0.000, SNR: 35869952999211676.0, T: 1.50: 0%| | 2/5000 [00:03<2:40:21, 1.93s/it]; 14:14:16.609 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) E",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:3231,Performance,Load,Loading,3231,"ctors=True --disable_bias_factors_in_active_class=False --p_alt=1.000000e-06 --cnv_coherence_length=1.000000e+04 --max_copy_number=5 --p_active=0.010000 --class_coherence_length=10000.000000 --learning_rate=1.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 14:13:50.032 INFO cohort_denoising_calling - Loading 24 read counts file(s)...; 14:13:53.719 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 14:13:58.626 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 14:14:04.543 INFO gcnvkernel.models.fancy_model - Global model variables: {'W_tu', 'psi_t_log__', 'ard_u_log__', 'log_mean_bias_t'}; 14:14:04.544 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'z_su', 'psi_s_log__', 'read_depth_s_log__'}; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No log emission sampler given; skipping the sampling step; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No caller given; skipping the calling step; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 14:14:10.902 INFO gcnvkernel.tasks.inference_task_base - (denoising (wa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:3312,Performance,Load,Loading,3312,"e+04 --max_copy_number=5 --p_active=0.010000 --class_coherence_length=10000.000000 --learning_rate=1.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.900000e-01 --log_emission_samples_per_round=50 --log_emission_sampling_rounds=10 --log_emission_sampling_median_rel_error=5.000000e-03 --max_advi_iter_first_epoch=5000 --max_advi_iter_subsequent_epochs=200 --min_training_epochs=10 --max_training_epochs=50 --initial_temperature=1.500000e+00 --num_thermal_advi_iters=2500 --convergence_snr_averaging_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 14:13:50.032 INFO cohort_denoising_calling - Loading 24 read counts file(s)...; 14:13:53.719 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 14:13:58.626 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 14:14:04.543 INFO gcnvkernel.models.fancy_model - Global model variables: {'W_tu', 'psi_t_log__', 'ard_u_log__', 'log_mean_bias_t'}; 14:14:04.544 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'z_su', 'psi_s_log__', 'read_depth_s_log__'}; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No log emission sampler given; skipping the sampling step; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No caller given; skipping the calling step; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 14:14:10.902 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up)) starting...: 0%| | 0/5000 [00:00<?, ?it/s]; 14:14:12.877 INFO gcnvkernel.tasks.inference_task_base ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:15383,Performance,perform,performed,15383,"08 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 08:37:16.408 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 08:37:16.408 INFO GermlineCNVCaller - Deflater: IntelDeflater; 08:37:16.409 INFO GermlineCNVCaller - Inflater: IntelInflater; 08:37:16.409 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 08:37:16.409 INFO GermlineCNVCaller - Requester pays: disabled; 08:37:16.409 INFO GermlineCNVCaller - Initializing engine; 08:37:21.698 INFO GermlineCNVCaller - Done initializing engine; 08:37:22.015 INFO GermlineCNVCaller - Retrieving intervals from read-count file (results/200219_X008378.counts.tsv)...; 08:37:22.119 INFO GermlineCNVCaller - No annotated intervals were provided...; 08:37:22.120 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 08:37:22.194 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 08:37:22.195 INFO GermlineCNVCaller - Shutting down engine; [February 26, 2019 8:37:22 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.29 minutes.; Runtime.totalMemory()=330301440; java.lang.IllegalArgumentException: Output directory results/190226.181217_K00178.CNVCaller does not exist.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.validateArguments(GermlineCNVCaller.java:361); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:281); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broa",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:15857,Security,validat,validateArg,15857,": 20; 08:37:16.409 INFO GermlineCNVCaller - Requester pays: disabled; 08:37:16.409 INFO GermlineCNVCaller - Initializing engine; 08:37:21.698 INFO GermlineCNVCaller - Done initializing engine; 08:37:22.015 INFO GermlineCNVCaller - Retrieving intervals from read-count file (results/200219_X008378.counts.tsv)...; 08:37:22.119 INFO GermlineCNVCaller - No annotated intervals were provided...; 08:37:22.120 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 08:37:22.194 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 08:37:22.195 INFO GermlineCNVCaller - Shutting down engine; [February 26, 2019 8:37:22 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.29 minutes.; Runtime.totalMemory()=330301440; java.lang.IllegalArgumentException: Output directory results/190226.181217_K00178.CNVCaller does not exist.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.validateArguments(GermlineCNVCaller.java:361); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:281); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /mnt/storage/apps/software/gatk/4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compress",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:15954,Security,validat,validateArguments,15954,"- Initializing engine; 08:37:21.698 INFO GermlineCNVCaller - Done initializing engine; 08:37:22.015 INFO GermlineCNVCaller - Retrieving intervals from read-count file (results/200219_X008378.counts.tsv)...; 08:37:22.119 INFO GermlineCNVCaller - No annotated intervals were provided...; 08:37:22.120 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 08:37:22.194 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 08:37:22.195 INFO GermlineCNVCaller - Shutting down engine; [February 26, 2019 8:37:22 AM GMT] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 0.29 minutes.; Runtime.totalMemory()=330301440; java.lang.IllegalArgumentException: Output directory results/190226.181217_K00178.CNVCaller does not exist.; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.validateArguments(GermlineCNVCaller.java:361); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:281); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /mnt/storage/apps/software/gatk/4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /mnt/storage/apps/software/gatk/4.1.0.0/gatk-package-4.1.0.0-loc`. I'm running these comm",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398:3819,Testability,log,log,3819,"ng_window=500 --convergence_snr_trigger_threshold=1.000000e-01 --convergence_snr_countdown_window=10 --max_calling_iters=10 --caller_update_convergence_threshold=1.000000e-03 --caller_internal_admixing_rate=7.500000e-01 --caller_external_admixing_rate=1.000000e+00 --disable_caller=false --disable_sampler=false --disable_annealing=false; Stdout: 14:13:50.032 INFO cohort_denoising_calling - Loading 24 read counts file(s)...; 14:13:53.719 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 14:13:58.626 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 14:14:04.543 INFO gcnvkernel.models.fancy_model - Global model variables: {'W_tu', 'psi_t_log__', 'ard_u_log__', 'log_mean_bias_t'}; 14:14:04.544 INFO gcnvkernel.models.fancy_model - Sample-specific model variables: {'z_su', 'psi_s_log__', 'read_depth_s_log__'}; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No log emission sampler given; skipping the sampling step; 14:14:04.544 WARNING gcnvkernel.tasks.inference_task_base - No caller given; skipping the calling step; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Instantiating the convergence tracker...; 14:14:04.544 INFO gcnvkernel.tasks.inference_task_base - Setting up DA-ADVI...; 14:14:10.902 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up)) starting...: 0%| | 0/5000 [00:00<?, ?it/s]; 14:14:12.877 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: N/A, SNR: N/A, T: 1.50: 0%| | 1/5000 [00:01<2:44:32, 1.97s/it]; 14:14:14.753 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: -145.294 +/- 0.000, SNR: 35869952999211676.0, T: 1.50: 0%| | 2/5000 [00:03<2:40:21, 1.93s/it]; 14:14:16.609 INFO gcnvkernel.tasks.inference_task_base - (denoising (warm-up) epoch 1) ELBO: -110.174 +/- 10.941, SNR: 568.3, T: 1.50: 0%| | 3/5000 [00:05<2:38:25, 1.90s/it]; 14:14:18.444 INFO gcnvkernel.tasks.inf",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467406398
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467447753:44,Availability,error,error,44,"The first issue looks like an out of memory error. You may need to scatter your intervals into separate shards, as is done in the WDLs: https://github.com/broadinstitute/gatk/tree/master/scripts/cnv_wdl/germline. The second issue regarding the output directory creation is by design---CNV tools require the output directory to exist beforehand.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467447753
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467450853:118,Availability,error,error,118,See discussion at https://gatkforums.broadinstitute.org/gatk/discussion/11711/germlinecnvcaller-interval-merging-rule-error#latest regarding output directory creation.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-467450853
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-468818077:154,Usability,Guid,Guidelines,154,"If you don't mind the list being public, the github wiki, e.g. like this existing article https://github.com/broadinstitute/gatk/wiki/GATK4-Documentation-Guidelines, seems like a good place to start a list.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-468818077
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-468819341:111,Testability,test,tests-guidelines,111,I made you a page to start collecting such reminders at https://github.com/broadinstitute/gatk/wiki/Checks-and-tests-guidelines.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-468819341
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-468819341:117,Usability,guid,guidelines,117,I made you a page to start collecting such reminders at https://github.com/broadinstitute/gatk/wiki/Checks-and-tests-guidelines.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-468819341
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-469067676:1208,Availability,down,downstream,1208,"Just finished switching over all of the CNV tools to fail early if directories are not writeable---or do not exist and cannot be created---only to realize that this behavior is inconsistent with that of Picard IntervalListTools (which is used in the gCNV pipeline). That tool fails early if the output directory is not writeable or does not exist, and although there is a code path later that suggests that output directories should be created, it is not reached due to this early fail. It might be that this inconsistency was introduced in https://github.com/broadinstitute/picard/pull/1208 and I did not catch it in my PR review. @yfarjoun any opinions what the intended behavior should be? Are there any conventions for Picard tools in general?. Perhaps we could enforce this at the engine level (maybe checks that are triggered by annotations such as suggested in https://github.com/broadinstitute/gatk/issues/141, if possible)? But this would only work for GATK tools and would still rely on the diligence of developers. In any case, I'll decide on and document a convention for the CNV tools, but I think it might be a quixotic dream to enforce consistent behavior---especially without breaking things downstream which may rely on existing, inconsistent behavior...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-469067676
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-469067676:255,Deployability,pipeline,pipeline,255,"Just finished switching over all of the CNV tools to fail early if directories are not writeable---or do not exist and cannot be created---only to realize that this behavior is inconsistent with that of Picard IntervalListTools (which is used in the gCNV pipeline). That tool fails early if the output directory is not writeable or does not exist, and although there is a code path later that suggests that output directories should be created, it is not reached due to this early fail. It might be that this inconsistency was introduced in https://github.com/broadinstitute/picard/pull/1208 and I did not catch it in my PR review. @yfarjoun any opinions what the intended behavior should be? Are there any conventions for Picard tools in general?. Perhaps we could enforce this at the engine level (maybe checks that are triggered by annotations such as suggested in https://github.com/broadinstitute/gatk/issues/141, if possible)? But this would only work for GATK tools and would still rely on the diligence of developers. In any case, I'll decide on and document a convention for the CNV tools, but I think it might be a quixotic dream to enforce consistent behavior---especially without breaking things downstream which may rely on existing, inconsistent behavior...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-469067676
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-470113262:262,Availability,error,error,262,"Picard tries to adhere to the following (but it's up to each tool to figure; out):. 1. Check input and output files for readability/writability as soon as; possible.; 2. Delete incomplete outputs in case of caught exception.; 3. Return non-zero value in case of error. On Sun, Mar 3, 2019 at 4:45 PM samuelklee <notifications@github.com> wrote:. > Just finished switching over all of the CNV tools to fail early if; > directories are not writeable---or do not exist and cannot be; > created---only to realize that this behavior is inconsistent with that of; > Picard IntervalListTools (which is used in the gCNV pipeline).; >; > That tool fails early if the output directory is not writeable or does not; > exist, and although there is a code path later that suggests that output; > directories should be created, it is not reached due to this early fail. It; > might be that this inconsistency was introduced in; > broadinstitute/picard#1208; > <https://github.com/broadinstitute/picard/pull/1208> and I did not catch; > it in my PR. @yfarjoun <https://github.com/yfarjoun> any opinions what; > the intended behavior should be? Are there any conventions for Picard tools; > in general?; >; > Perhaps we could enforce this at the engine level (maybe checks that are; > triggered by annotations such as suggested in #141; > <https://github.com/broadinstitute/gatk/issues/141>, if possible)? But; > this would only work for GATK tools and would still rely on the diligence; > of developers.; >; > In any case, I'll decide on and document a convention for the CNV tools,; > but I think it might be a quixotic dream to enforce consistent; > behavior---especially without breaking things downstream which may rely on; > existing, inconsistent behavior...; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4825#issuecomment-469067676>,; > or mute the thread; > <https://github.com/notifications",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-470113262
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-470113262:1683,Availability,down,downstream,1683,"):. 1. Check input and output files for readability/writability as soon as; possible.; 2. Delete incomplete outputs in case of caught exception.; 3. Return non-zero value in case of error. On Sun, Mar 3, 2019 at 4:45 PM samuelklee <notifications@github.com> wrote:. > Just finished switching over all of the CNV tools to fail early if; > directories are not writeable---or do not exist and cannot be; > created---only to realize that this behavior is inconsistent with that of; > Picard IntervalListTools (which is used in the gCNV pipeline).; >; > That tool fails early if the output directory is not writeable or does not; > exist, and although there is a code path later that suggests that output; > directories should be created, it is not reached due to this early fail. It; > might be that this inconsistency was introduced in; > broadinstitute/picard#1208; > <https://github.com/broadinstitute/picard/pull/1208> and I did not catch; > it in my PR. @yfarjoun <https://github.com/yfarjoun> any opinions what; > the intended behavior should be? Are there any conventions for Picard tools; > in general?; >; > Perhaps we could enforce this at the engine level (maybe checks that are; > triggered by annotations such as suggested in #141; > <https://github.com/broadinstitute/gatk/issues/141>, if possible)? But; > this would only work for GATK tools and would still rely on the diligence; > of developers.; >; > In any case, I'll decide on and document a convention for the CNV tools,; > but I think it might be a quixotic dream to enforce consistent; > behavior---especially without breaking things downstream which may rely on; > existing, inconsistent behavior...; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4825#issuecomment-469067676>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0knrip3kZgweCbYoBj3fwGvlDdPEks5vTEKEgaJpZM4USOnb>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-470113262
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-470113262:612,Deployability,pipeline,pipeline,612,"Picard tries to adhere to the following (but it's up to each tool to figure; out):. 1. Check input and output files for readability/writability as soon as; possible.; 2. Delete incomplete outputs in case of caught exception.; 3. Return non-zero value in case of error. On Sun, Mar 3, 2019 at 4:45 PM samuelklee <notifications@github.com> wrote:. > Just finished switching over all of the CNV tools to fail early if; > directories are not writeable---or do not exist and cannot be; > created---only to realize that this behavior is inconsistent with that of; > Picard IntervalListTools (which is used in the gCNV pipeline).; >; > That tool fails early if the output directory is not writeable or does not; > exist, and although there is a code path later that suggests that output; > directories should be created, it is not reached due to this early fail. It; > might be that this inconsistency was introduced in; > broadinstitute/picard#1208; > <https://github.com/broadinstitute/picard/pull/1208> and I did not catch; > it in my PR. @yfarjoun <https://github.com/yfarjoun> any opinions what; > the intended behavior should be? Are there any conventions for Picard tools; > in general?; >; > Perhaps we could enforce this at the engine level (maybe checks that are; > triggered by annotations such as suggested in #141; > <https://github.com/broadinstitute/gatk/issues/141>, if possible)? But; > this would only work for GATK tools and would still rely on the diligence; > of developers.; >; > In any case, I'll decide on and document a convention for the CNV tools,; > but I think it might be a quixotic dream to enforce consistent; > behavior---especially without breaking things downstream which may rely on; > existing, inconsistent behavior...; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4825#issuecomment-469067676>,; > or mute the thread; > <https://github.com/notifications",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-470113262
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-470172806:413,Availability,down,downstream,413,"Thanks, @yfarjoun, I think those are reasonable. Just to be clear, the code for the tool mentioned above is a little confusing, in that an early fail for writability when the directory does not exist prevents us from reaching code that appears to be intended to create the directory. Not a big deal in the end (and I checked that this was also the case before the PR). But minor things like this can easily break downstream scripts, etc., as was demonstrated above, so we should take some care. I agree that it's fine to leave some decisions up to each tool, but we should try to document them for the benefit of users and future devs that might need to maintain the behavior of the tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-470172806
https://github.com/broadinstitute/gatk/issues/4825#issuecomment-470172806:60,Usability,clear,clear,60,"Thanks, @yfarjoun, I think those are reasonable. Just to be clear, the code for the tool mentioned above is a little confusing, in that an early fail for writability when the directory does not exist prevents us from reaching code that appears to be intended to create the directory. Not a big deal in the end (and I checked that this was also the case before the PR). But minor things like this can easily break downstream scripts, etc., as was demonstrated above, so we should take some care. I agree that it's fine to leave some decisions up to each tool, but we should try to document them for the benefit of users and future devs that might need to maintain the behavior of the tool.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4825#issuecomment-470172806
https://github.com/broadinstitute/gatk/issues/4826#issuecomment-393224962:78,Integrability,message,messages,78,"@sooheelee, the computation engine `gcnvkernel` produces a lot of informative messages, however, they are not shown unless you run GATK with `--verbosity DEBUG`. I have filed an issue about it: #4629. @cmnbroad could you please take a look at the issue?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4826#issuecomment-393224962
https://github.com/broadinstitute/gatk/pull/4827#issuecomment-393313238:55,Testability,Log,LogNormal,55,I use this class to allow the user to pass a Normal or LogNormal expected distribution for insert sizes; notice that the String parameter constructor allows for it to use as an [at]Argument type in a tool. . @tedsharpe please review.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4827#issuecomment-393313238
https://github.com/broadinstitute/gatk/pull/4827#issuecomment-415238525:85,Deployability,pipeline,pipeline,85,Added support for the empirical distribution read-metadata txt file generated in the pipeline.; Also rebased to current master.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4827#issuecomment-415238525
https://github.com/broadinstitute/gatk/pull/4827#issuecomment-417537129:267,Testability,log,lognormal,267,"Added a pure ""empirical"" isize distribution where the prob of each size is determine by the fraction of cases with that insert size + some smoothing so that 0-count insert size don't have 0 probability. Also I moved the supported isize distribution ""shapes"" (normal, lognormal, empirical) to a enum.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4827#issuecomment-417537129
https://github.com/broadinstitute/gatk/pull/4827#issuecomment-418420706:1076,Availability,failure,failure,1076,"used. I suspect I'd have a lot of ""YAGNI"" comments if I knew.; For example, you are basing all your implementations on Apache's AbstractIntegerDistribution. That class, it seems to me, is really intended to allow you to do sampling from a distribution. But I suspect you won't be sampling, you'll only be asking questions about density. If so, there's a lot of baggage that gets pulled into your anonymous implementations of this class: random number generators, boundary information, etc. Lots of extra boilerplate. Couldn't this be clearer if reorganized as an abstract class implementing AbstractIntegerDistribution, 3 concrete classes for each case (rather than the current anonymous classes), a factory that takes a spec and returns the correct distribution, and a simple enum class?. It seems weird that the distributions you allow users to realize using a spec are both two-tailed distributions, when fragment size is a one-tailed distribution. It seems awkward that failure to parse a distribution spec leads to a code path where you try to extract a file name and read serialized read metadata. Wouldn't it be clearer to have two completely distinct code paths with a different program argument for the empirical case?. The read metadata gives per library distributions. It seems suspect that you are folding them all together. Different libraries can have rather different fragment size stats. Still don't like that you're providing the possibility of reading the metadata text file. Seems fragile. Why don't you modify the ReadMetadata code to always produce just the data you need. Then you could eliminate the text-file code. And you could simplify the code that processes the serialized ReadMetadata which now has this awkward code path: CDF -> density -> sum across libs -> density+CDF stored in memory. If you have the CDF you can trivially produce density on demand. Notwithstanding all this, if you're happy with the code as it stands, feel free to merge.; Back to you, review done.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4827#issuecomment-418420706
https://github.com/broadinstitute/gatk/pull/4827#issuecomment-418420706:636,Usability,clear,clearer,636,"This is difficult to review because there isn't any client code: I don't know how this is going to be used. I suspect I'd have a lot of ""YAGNI"" comments if I knew.; For example, you are basing all your implementations on Apache's AbstractIntegerDistribution. That class, it seems to me, is really intended to allow you to do sampling from a distribution. But I suspect you won't be sampling, you'll only be asking questions about density. If so, there's a lot of baggage that gets pulled into your anonymous implementations of this class: random number generators, boundary information, etc. Lots of extra boilerplate. Couldn't this be clearer if reorganized as an abstract class implementing AbstractIntegerDistribution, 3 concrete classes for each case (rather than the current anonymous classes), a factory that takes a spec and returns the correct distribution, and a simple enum class?. It seems weird that the distributions you allow users to realize using a spec are both two-tailed distributions, when fragment size is a one-tailed distribution. It seems awkward that failure to parse a distribution spec leads to a code path where you try to extract a file name and read serialized read metadata. Wouldn't it be clearer to have two completely distinct code paths with a different program argument for the empirical case?. The read metadata gives per library distributions. It seems suspect that you are folding them all together. Different libraries can have rather different fragment size stats. Still don't like that you're providing the possibility of reading the metadata text file. Seems fragile. Why don't you modify the ReadMetadata code to always produce just the data you need. Then you could eliminate the text-file code. And you could simplify the code that processes the serialized ReadMetadata which now has this awkward code path: CDF -> density -> sum across libs -> density+CDF stored in memory. If you have the CDF you can trivially produce density on demand. Notwithstanding",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4827#issuecomment-418420706
https://github.com/broadinstitute/gatk/pull/4827#issuecomment-418420706:872,Usability,simpl,simple,872,"This is difficult to review because there isn't any client code: I don't know how this is going to be used. I suspect I'd have a lot of ""YAGNI"" comments if I knew.; For example, you are basing all your implementations on Apache's AbstractIntegerDistribution. That class, it seems to me, is really intended to allow you to do sampling from a distribution. But I suspect you won't be sampling, you'll only be asking questions about density. If so, there's a lot of baggage that gets pulled into your anonymous implementations of this class: random number generators, boundary information, etc. Lots of extra boilerplate. Couldn't this be clearer if reorganized as an abstract class implementing AbstractIntegerDistribution, 3 concrete classes for each case (rather than the current anonymous classes), a factory that takes a spec and returns the correct distribution, and a simple enum class?. It seems weird that the distributions you allow users to realize using a spec are both two-tailed distributions, when fragment size is a one-tailed distribution. It seems awkward that failure to parse a distribution spec leads to a code path where you try to extract a file name and read serialized read metadata. Wouldn't it be clearer to have two completely distinct code paths with a different program argument for the empirical case?. The read metadata gives per library distributions. It seems suspect that you are folding them all together. Different libraries can have rather different fragment size stats. Still don't like that you're providing the possibility of reading the metadata text file. Seems fragile. Why don't you modify the ReadMetadata code to always produce just the data you need. Then you could eliminate the text-file code. And you could simplify the code that processes the serialized ReadMetadata which now has this awkward code path: CDF -> density -> sum across libs -> density+CDF stored in memory. If you have the CDF you can trivially produce density on demand. Notwithstanding",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4827#issuecomment-418420706
https://github.com/broadinstitute/gatk/pull/4827#issuecomment-418420706:1221,Usability,clear,clearer,1221,"used. I suspect I'd have a lot of ""YAGNI"" comments if I knew.; For example, you are basing all your implementations on Apache's AbstractIntegerDistribution. That class, it seems to me, is really intended to allow you to do sampling from a distribution. But I suspect you won't be sampling, you'll only be asking questions about density. If so, there's a lot of baggage that gets pulled into your anonymous implementations of this class: random number generators, boundary information, etc. Lots of extra boilerplate. Couldn't this be clearer if reorganized as an abstract class implementing AbstractIntegerDistribution, 3 concrete classes for each case (rather than the current anonymous classes), a factory that takes a spec and returns the correct distribution, and a simple enum class?. It seems weird that the distributions you allow users to realize using a spec are both two-tailed distributions, when fragment size is a one-tailed distribution. It seems awkward that failure to parse a distribution spec leads to a code path where you try to extract a file name and read serialized read metadata. Wouldn't it be clearer to have two completely distinct code paths with a different program argument for the empirical case?. The read metadata gives per library distributions. It seems suspect that you are folding them all together. Different libraries can have rather different fragment size stats. Still don't like that you're providing the possibility of reading the metadata text file. Seems fragile. Why don't you modify the ReadMetadata code to always produce just the data you need. Then you could eliminate the text-file code. And you could simplify the code that processes the serialized ReadMetadata which now has this awkward code path: CDF -> density -> sum across libs -> density+CDF stored in memory. If you have the CDF you can trivially produce density on demand. Notwithstanding all this, if you're happy with the code as it stands, feel free to merge.; Back to you, review done.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4827#issuecomment-418420706
https://github.com/broadinstitute/gatk/pull/4827#issuecomment-418420706:1755,Usability,simpl,simplify,1755,"used. I suspect I'd have a lot of ""YAGNI"" comments if I knew.; For example, you are basing all your implementations on Apache's AbstractIntegerDistribution. That class, it seems to me, is really intended to allow you to do sampling from a distribution. But I suspect you won't be sampling, you'll only be asking questions about density. If so, there's a lot of baggage that gets pulled into your anonymous implementations of this class: random number generators, boundary information, etc. Lots of extra boilerplate. Couldn't this be clearer if reorganized as an abstract class implementing AbstractIntegerDistribution, 3 concrete classes for each case (rather than the current anonymous classes), a factory that takes a spec and returns the correct distribution, and a simple enum class?. It seems weird that the distributions you allow users to realize using a spec are both two-tailed distributions, when fragment size is a one-tailed distribution. It seems awkward that failure to parse a distribution spec leads to a code path where you try to extract a file name and read serialized read metadata. Wouldn't it be clearer to have two completely distinct code paths with a different program argument for the empirical case?. The read metadata gives per library distributions. It seems suspect that you are folding them all together. Different libraries can have rather different fragment size stats. Still don't like that you're providing the possibility of reading the metadata text file. Seems fragile. Why don't you modify the ReadMetadata code to always produce just the data you need. Then you could eliminate the text-file code. And you could simplify the code that processes the serialized ReadMetadata which now has this awkward code path: CDF -> density -> sum across libs -> density+CDF stored in memory. If you have the CDF you can trivially produce density on demand. Notwithstanding all this, if you're happy with the code as it stands, feel free to merge.; Back to you, review done.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4827#issuecomment-418420706
https://github.com/broadinstitute/gatk/issues/4828#issuecomment-394809501:158,Usability,simpl,simple,158,"@jean-philippe-martin It's looking like @Horneth won't have time to implement this in the short-term future -- since you mentioned that the change was fairly simple, would you be able to submit a PR yourself? We'd be happy to help review.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-394809501
https://github.com/broadinstitute/gatk/issues/4828#issuecomment-394839598:212,Deployability,toggle,toggle,212,"@Horneth Ideally we'd just check up-front whether the bucket has requester pays enabled, and specify the user's default project as the billing project if it is. . It would also be good, I think, if we included a toggle that allows the client to tell the library to throw and refuse to proceed if an attempt is made to access requester-pays data, so that users who don't want to incur GCS access charges can get a hard guarantee that they won't.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-394839598
https://github.com/broadinstitute/gatk/issues/4828#issuecomment-394839598:395,Energy Efficiency,charge,charges,395,"@Horneth Ideally we'd just check up-front whether the bucket has requester pays enabled, and specify the user's default project as the billing project if it is. . It would also be good, I think, if we included a toggle that allows the client to tell the library to throw and refuse to proceed if an attempt is made to access requester-pays data, so that users who don't want to incur GCS access charges can get a hard guarantee that they won't.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-394839598
https://github.com/broadinstitute/gatk/issues/4828#issuecomment-394839598:318,Security,access,access,318,"@Horneth Ideally we'd just check up-front whether the bucket has requester pays enabled, and specify the user's default project as the billing project if it is. . It would also be good, I think, if we included a toggle that allows the client to tell the library to throw and refuse to proceed if an attempt is made to access requester-pays data, so that users who don't want to incur GCS access charges can get a hard guarantee that they won't.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-394839598
https://github.com/broadinstitute/gatk/issues/4828#issuecomment-394839598:388,Security,access,access,388,"@Horneth Ideally we'd just check up-front whether the bucket has requester pays enabled, and specify the user's default project as the billing project if it is. . It would also be good, I think, if we included a toggle that allows the client to tell the library to throw and refuse to proceed if an attempt is made to access requester-pays data, so that users who don't want to incur GCS access charges can get a hard guarantee that they won't.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-394839598
https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757:514,Availability,rollback,rollback,514,"@jean-philippe-martin has added support for requester pays to gcloud. ; See https://github.com/GoogleCloudPlatform/google-cloud-java/pull/3406 . I've set up a new fork of the project at https://github.com/broadinstitute/google-cloud-java. I have a branch https://github.com/broadinstitute/google-cloud-java/tree/lb_update_pom_to_publish_to_orgbroad which I believe should make the changes necessary to run on dataproc and avoid https://github.com/GoogleCloudPlatform/google-cloud-java/issues/2453. However, if you rollback the dependencies the project no longer compiles. You can compile the nio-subproject, but the parent project can no longer build against the old dependencies. That makes me very nervous because it seems likely that we will encounter runtime errors if we substitute them. . JP created a small test case to reproduce the error and it seems like the dataproc team is looking at it. Hopefully they can resolve the issue and we can switch to the base library instead of needing to publish an additional sketchy version of it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757
https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757:763,Availability,error,errors,763,"@jean-philippe-martin has added support for requester pays to gcloud. ; See https://github.com/GoogleCloudPlatform/google-cloud-java/pull/3406 . I've set up a new fork of the project at https://github.com/broadinstitute/google-cloud-java. I have a branch https://github.com/broadinstitute/google-cloud-java/tree/lb_update_pom_to_publish_to_orgbroad which I believe should make the changes necessary to run on dataproc and avoid https://github.com/GoogleCloudPlatform/google-cloud-java/issues/2453. However, if you rollback the dependencies the project no longer compiles. You can compile the nio-subproject, but the parent project can no longer build against the old dependencies. That makes me very nervous because it seems likely that we will encounter runtime errors if we substitute them. . JP created a small test case to reproduce the error and it seems like the dataproc team is looking at it. Hopefully they can resolve the issue and we can switch to the base library instead of needing to publish an additional sketchy version of it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757
https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757:841,Availability,error,error,841,"@jean-philippe-martin has added support for requester pays to gcloud. ; See https://github.com/GoogleCloudPlatform/google-cloud-java/pull/3406 . I've set up a new fork of the project at https://github.com/broadinstitute/google-cloud-java. I have a branch https://github.com/broadinstitute/google-cloud-java/tree/lb_update_pom_to_publish_to_orgbroad which I believe should make the changes necessary to run on dataproc and avoid https://github.com/GoogleCloudPlatform/google-cloud-java/issues/2453. However, if you rollback the dependencies the project no longer compiles. You can compile the nio-subproject, but the parent project can no longer build against the old dependencies. That makes me very nervous because it seems likely that we will encounter runtime errors if we substitute them. . JP created a small test case to reproduce the error and it seems like the dataproc team is looking at it. Hopefully they can resolve the issue and we can switch to the base library instead of needing to publish an additional sketchy version of it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757
https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757:514,Deployability,rollback,rollback,514,"@jean-philippe-martin has added support for requester pays to gcloud. ; See https://github.com/GoogleCloudPlatform/google-cloud-java/pull/3406 . I've set up a new fork of the project at https://github.com/broadinstitute/google-cloud-java. I have a branch https://github.com/broadinstitute/google-cloud-java/tree/lb_update_pom_to_publish_to_orgbroad which I believe should make the changes necessary to run on dataproc and avoid https://github.com/GoogleCloudPlatform/google-cloud-java/issues/2453. However, if you rollback the dependencies the project no longer compiles. You can compile the nio-subproject, but the parent project can no longer build against the old dependencies. That makes me very nervous because it seems likely that we will encounter runtime errors if we substitute them. . JP created a small test case to reproduce the error and it seems like the dataproc team is looking at it. Hopefully they can resolve the issue and we can switch to the base library instead of needing to publish an additional sketchy version of it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757
https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757:527,Integrability,depend,dependencies,527,"@jean-philippe-martin has added support for requester pays to gcloud. ; See https://github.com/GoogleCloudPlatform/google-cloud-java/pull/3406 . I've set up a new fork of the project at https://github.com/broadinstitute/google-cloud-java. I have a branch https://github.com/broadinstitute/google-cloud-java/tree/lb_update_pom_to_publish_to_orgbroad which I believe should make the changes necessary to run on dataproc and avoid https://github.com/GoogleCloudPlatform/google-cloud-java/issues/2453. However, if you rollback the dependencies the project no longer compiles. You can compile the nio-subproject, but the parent project can no longer build against the old dependencies. That makes me very nervous because it seems likely that we will encounter runtime errors if we substitute them. . JP created a small test case to reproduce the error and it seems like the dataproc team is looking at it. Hopefully they can resolve the issue and we can switch to the base library instead of needing to publish an additional sketchy version of it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757
https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757:667,Integrability,depend,dependencies,667,"@jean-philippe-martin has added support for requester pays to gcloud. ; See https://github.com/GoogleCloudPlatform/google-cloud-java/pull/3406 . I've set up a new fork of the project at https://github.com/broadinstitute/google-cloud-java. I have a branch https://github.com/broadinstitute/google-cloud-java/tree/lb_update_pom_to_publish_to_orgbroad which I believe should make the changes necessary to run on dataproc and avoid https://github.com/GoogleCloudPlatform/google-cloud-java/issues/2453. However, if you rollback the dependencies the project no longer compiles. You can compile the nio-subproject, but the parent project can no longer build against the old dependencies. That makes me very nervous because it seems likely that we will encounter runtime errors if we substitute them. . JP created a small test case to reproduce the error and it seems like the dataproc team is looking at it. Hopefully they can resolve the issue and we can switch to the base library instead of needing to publish an additional sketchy version of it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757
https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757:422,Safety,avoid,avoid,422,"@jean-philippe-martin has added support for requester pays to gcloud. ; See https://github.com/GoogleCloudPlatform/google-cloud-java/pull/3406 . I've set up a new fork of the project at https://github.com/broadinstitute/google-cloud-java. I have a branch https://github.com/broadinstitute/google-cloud-java/tree/lb_update_pom_to_publish_to_orgbroad which I believe should make the changes necessary to run on dataproc and avoid https://github.com/GoogleCloudPlatform/google-cloud-java/issues/2453. However, if you rollback the dependencies the project no longer compiles. You can compile the nio-subproject, but the parent project can no longer build against the old dependencies. That makes me very nervous because it seems likely that we will encounter runtime errors if we substitute them. . JP created a small test case to reproduce the error and it seems like the dataproc team is looking at it. Hopefully they can resolve the issue and we can switch to the base library instead of needing to publish an additional sketchy version of it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757
https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757:814,Testability,test,test,814,"@jean-philippe-martin has added support for requester pays to gcloud. ; See https://github.com/GoogleCloudPlatform/google-cloud-java/pull/3406 . I've set up a new fork of the project at https://github.com/broadinstitute/google-cloud-java. I have a branch https://github.com/broadinstitute/google-cloud-java/tree/lb_update_pom_to_publish_to_orgbroad which I believe should make the changes necessary to run on dataproc and avoid https://github.com/GoogleCloudPlatform/google-cloud-java/issues/2453. However, if you rollback the dependencies the project no longer compiles. You can compile the nio-subproject, but the parent project can no longer build against the old dependencies. That makes me very nervous because it seems likely that we will encounter runtime errors if we substitute them. . JP created a small test case to reproduce the error and it seems like the dataproc team is looking at it. Hopefully they can resolve the issue and we can switch to the base library instead of needing to publish an additional sketchy version of it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4828#issuecomment-404322757
https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393417801:2093,Testability,log,log,2093,"cness of the site is relevant to the false positive deletion.; >; > One could ask what in the GATK engine is responsible.; >; > - The assembly engine, perhaps? No, it is the assembly engine's job to; > propose possible haplotypes, not to call them. In any case, there *is*; > one spanning read with the deletion above the reads shown, so it is a valid; > path in the graph.; > - Pair-HMM? This one confused me for a while, but no. The engine is; > *not* saying that these reads' best alignment to the reference has a; > deletion, which would be false because there is a gap opening penalty.; > Rather, it says that they align equally well (with no deletions) to the ref; > haplotype and to the deletion haplotype. The deletion shown in IGV is the; > deletion of the alt haplotype relative to the reference, not of the reads; > relative to their best haplotype.; > - The bamout writer? Nope, that code is really straightforward and; > does the right thing.; >; > So what's the issue? Well, the bamout writer gets its read alignments from; > the readLikelihoods after the reads have been realigned to their best; > haplotype. In these cases, it turns out that the alignment of the reads to; > their best haplotype, the deletion has a log likelihood better than the; > alignment to the ref haplotype by about 0.00001. The simplest solution; > would be to give an extremely modest prior in favor of the reference and; > break these near-ties in favor of the reference. @droazen; > <https://github.com/droazen> @ldgauthier <https://github.com/ldgauthier>; > @yfarjoun <https://github.com/yfarjoun> if you think this is a good idea; > I can fix it for both HC and M2. Otherwise I'll do an M2-only fix.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4829>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0lv9nhpu6C8LQCF9jGJoX4UAmfJEks5t32IhgaJpZM4UUW5o>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393417801
https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393417801:2180,Usability,simpl,simplest,2180,"cness of the site is relevant to the false positive deletion.; >; > One could ask what in the GATK engine is responsible.; >; > - The assembly engine, perhaps? No, it is the assembly engine's job to; > propose possible haplotypes, not to call them. In any case, there *is*; > one spanning read with the deletion above the reads shown, so it is a valid; > path in the graph.; > - Pair-HMM? This one confused me for a while, but no. The engine is; > *not* saying that these reads' best alignment to the reference has a; > deletion, which would be false because there is a gap opening penalty.; > Rather, it says that they align equally well (with no deletions) to the ref; > haplotype and to the deletion haplotype. The deletion shown in IGV is the; > deletion of the alt haplotype relative to the reference, not of the reads; > relative to their best haplotype.; > - The bamout writer? Nope, that code is really straightforward and; > does the right thing.; >; > So what's the issue? Well, the bamout writer gets its read alignments from; > the readLikelihoods after the reads have been realigned to their best; > haplotype. In these cases, it turns out that the alignment of the reads to; > their best haplotype, the deletion has a log likelihood better than the; > alignment to the ref haplotype by about 0.00001. The simplest solution; > would be to give an extremely modest prior in favor of the reference and; > break these near-ties in favor of the reference. @droazen; > <https://github.com/droazen> @ldgauthier <https://github.com/ldgauthier>; > @yfarjoun <https://github.com/yfarjoun> if you think this is a good idea; > I can fix it for both HC and M2. Otherwise I'll do an M2-only fix.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4829>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0lv9nhpu6C8LQCF9jGJoX4UAmfJEks5t32IhgaJpZM4UUW5o>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393417801
https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393528876:713,Testability,test,test,713,"My first question is does this reproduce with a different PairHMM implementation? It might be a precision problem, in which case the prior should do a good job. But we've also seen cases of the PairHMM giving significantly higher likelihoods for one alt over the reference (a difference of about 0.07 for the bottom gray read in the example below):; ![image](https://user-images.githubusercontent.com/6578548/40784632-34eb920e-64b4-11e8-9575-c26180808172.png); When @vruano dug into that one, the likelihoods were correct as evaluated by the PairHMM because we compare the likelihood sums over all alignments supporting each allele rather than the maximum likelihoods. I'll try to find the data to reproduce that test case.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393528876
https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393531485:565,Testability,test,test,565,"I found the data from yet another example of this behavior (see poorly aligned reads in the bamout -- somehow the big fat deletion is preferred over the reference):; ![dlbcl-c_d_pair13 nkap x_119059262 frame_shift_del ctctcttcttggttaaaggatgcaaggg_-](https://user-images.githubusercontent.com/6578548/40785072-7fdff344-64b5-11e8-8539-78267b9a2950.png). tumor exome: /seq/picard_aggregation/C1637/c_D_pair13_Dx/v3/c_D_pair13_Dx.bam; normal exome: /seq/picard_aggregation/C1637/RICOVER_134_N/v2/RICOVER_134_N.bam; M2 tumor bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/c_D_pair13_Dx.bam; M2 normal bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/RICOVER_134_N.bam; M2 combined bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/DLBCL-c_D_pair13.M2_validation_bam.bam; I can't guarantee the data is still around and I don't have the exact intervals, but if it doesn't reproduce in the latest GATK4 M2 then we can forget about it. If it does reproduce then it would put my mind at ease if the prior fixes this case too (and you could let Chip know that it got fixed 2.5 years later :-P)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393531485
https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393531485:644,Testability,test,test,644,"I found the data from yet another example of this behavior (see poorly aligned reads in the bamout -- somehow the big fat deletion is preferred over the reference):; ![dlbcl-c_d_pair13 nkap x_119059262 frame_shift_del ctctcttcttggttaaaggatgcaaggg_-](https://user-images.githubusercontent.com/6578548/40785072-7fdff344-64b5-11e8-8539-78267b9a2950.png). tumor exome: /seq/picard_aggregation/C1637/c_D_pair13_Dx/v3/c_D_pair13_Dx.bam; normal exome: /seq/picard_aggregation/C1637/RICOVER_134_N/v2/RICOVER_134_N.bam; M2 tumor bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/c_D_pair13_Dx.bam; M2 normal bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/RICOVER_134_N.bam; M2 combined bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/DLBCL-c_D_pair13.M2_validation_bam.bam; I can't guarantee the data is still around and I don't have the exact intervals, but if it doesn't reproduce in the latest GATK4 M2 then we can forget about it. If it does reproduce then it would put my mind at ease if the prior fixes this case too (and you could let Chip know that it got fixed 2.5 years later :-P)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393531485
https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393531485:725,Testability,test,test,725,"I found the data from yet another example of this behavior (see poorly aligned reads in the bamout -- somehow the big fat deletion is preferred over the reference):; ![dlbcl-c_d_pair13 nkap x_119059262 frame_shift_del ctctcttcttggttaaaggatgcaaggg_-](https://user-images.githubusercontent.com/6578548/40785072-7fdff344-64b5-11e8-8539-78267b9a2950.png). tumor exome: /seq/picard_aggregation/C1637/c_D_pair13_Dx/v3/c_D_pair13_Dx.bam; normal exome: /seq/picard_aggregation/C1637/RICOVER_134_N/v2/RICOVER_134_N.bam; M2 tumor bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/c_D_pair13_Dx.bam; M2 normal bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/RICOVER_134_N.bam; M2 combined bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/DLBCL-c_D_pair13.M2_validation_bam.bam; I can't guarantee the data is still around and I don't have the exact intervals, but if it doesn't reproduce in the latest GATK4 M2 then we can forget about it. If it does reproduce then it would put my mind at ease if the prior fixes this case too (and you could let Chip know that it got fixed 2.5 years later :-P)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393531485
https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393547216:1294,Testability,log,log,1294,"The marginalization over all paths, not the the likeliest one, would make a small difference, but I'm not sure in which direction. In fact, I would even guess it would tend to favor the reference over a deletion because there are more contributions from things like TTTTT -> TT(DELETE A T)TT(INSERT A T)T that don't actually change the bases but do contribute to the sum. That is, if the reference has a longer poly-T, there are more places to put the cancelling deletion(s) and insertion(s). However, all this stuff incurs gap opening penalties and I would be surprised if it could make a difference as big as 0.07. I think I know what can, however. If you take a look at the top of page 3 of our pair-HMM docs https://github.com/broadinstitute/gatk/blob/master/docs/pair_hmm.pdf, you will see, as @yfarjoun guessed, that there is a preference for shorter haplotypes in the form of a 1/haplotype length factor. And in fact, this amount seems to be in the right ballpark. For example, @ldgauthier's case is a 28-base deletion. If you calculate the log_10 ratio of the 1/length factors assuming a 100-base deletion haplotype, you get log_10(128/100) = 0.107. This, by the way, would explain why we don't see this sort of thing with insertions. We could simply say that any uninformative read (a log likelihood difference of 0.2) should go to the reference if that is the second best. That's basically the prior I was talking about. The M2-only solution would be to use the somatic likelihoods model on the haplotypes and remove unsupported haplotypes from the likelihoods matrix. Similarly, I could use it to align reads to the haplotype with the greatest posterior probability.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393547216
https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393547216:1252,Usability,simpl,simply,1252,"The marginalization over all paths, not the the likeliest one, would make a small difference, but I'm not sure in which direction. In fact, I would even guess it would tend to favor the reference over a deletion because there are more contributions from things like TTTTT -> TT(DELETE A T)TT(INSERT A T)T that don't actually change the bases but do contribute to the sum. That is, if the reference has a longer poly-T, there are more places to put the cancelling deletion(s) and insertion(s). However, all this stuff incurs gap opening penalties and I would be surprised if it could make a difference as big as 0.07. I think I know what can, however. If you take a look at the top of page 3 of our pair-HMM docs https://github.com/broadinstitute/gatk/blob/master/docs/pair_hmm.pdf, you will see, as @yfarjoun guessed, that there is a preference for shorter haplotypes in the form of a 1/haplotype length factor. And in fact, this amount seems to be in the right ballpark. For example, @ldgauthier's case is a 28-base deletion. If you calculate the log_10 ratio of the 1/length factors assuming a 100-base deletion haplotype, you get log_10(128/100) = 0.107. This, by the way, would explain why we don't see this sort of thing with insertions. We could simply say that any uninformative read (a log likelihood difference of 0.2) should go to the reference if that is the second best. That's basically the prior I was talking about. The M2-only solution would be to use the somatic likelihoods model on the haplotypes and remove unsupported haplotypes from the likelihoods matrix. Similarly, I could use it to align reads to the haplotype with the greatest posterior probability.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393547216
https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393559160:106,Integrability,depend,depends,106,"is there a haplotype-based prior that uses the alignment of the haplotype; to the reference? A prior that depends on the alignment might be better; than a flat pro-reference prior. On Thu, May 31, 2018 at 9:34 AM, ldgauthier <notifications@github.com>; wrote:. > I found the data from yet another example of this behavior (see poorly; > aligned reads in the bamout -- somehow the big fat deletion is preferred; > over the reference):; > [image: dlbcl-c_d_pair13 nkap x_119059262 frame_shift_del; > ctctcttcttggttaaaggatgcaaggg_-]; > <https://user-images.githubusercontent.com/6578548/40785072-7fdff344-64b5-11e8-8539-78267b9a2950.png>; >; > tumor exome: /seq/picard_aggregation/C1637/c_D_pair13_Dx/v3/c_D_pair13_; > Dx.bam; > normal exome: /seq/picard_aggregation/C1637/RICOVER_134_N/v2/RICOVER_134_; > N.bam; > M2 tumor bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/c_D_; > pair13_Dx.bam; > M2 normal bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/RICOVER_; > 134_N.bam; > M2 combined bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/DLBCL-c_; > D_pair13.M2_validation_bam.bam; > I can't guarantee the data is still around and I don't have the exact; > intervals, but if it doesn't reproduce in the latest GATK4 M2 then we can; > forget about it. If it does reproduce then it would put my mind at ease if; > the prior fixes this case too (and you could let Chip know that it got; > fixed 2.5 years later :-P); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393531485>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0qP335PYrMmALyjp96ZdOw_hv03Mks5t3_FQgaJpZM4UUW5o>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393559160
https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393559160:866,Testability,test,test,866,"is there a haplotype-based prior that uses the alignment of the haplotype; to the reference? A prior that depends on the alignment might be better; than a flat pro-reference prior. On Thu, May 31, 2018 at 9:34 AM, ldgauthier <notifications@github.com>; wrote:. > I found the data from yet another example of this behavior (see poorly; > aligned reads in the bamout -- somehow the big fat deletion is preferred; > over the reference):; > [image: dlbcl-c_d_pair13 nkap x_119059262 frame_shift_del; > ctctcttcttggttaaaggatgcaaggg_-]; > <https://user-images.githubusercontent.com/6578548/40785072-7fdff344-64b5-11e8-8539-78267b9a2950.png>; >; > tumor exome: /seq/picard_aggregation/C1637/c_D_pair13_Dx/v3/c_D_pair13_; > Dx.bam; > normal exome: /seq/picard_aggregation/C1637/RICOVER_134_N/v2/RICOVER_134_; > N.bam; > M2 tumor bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/c_D_; > pair13_Dx.bam; > M2 normal bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/RICOVER_; > 134_N.bam; > M2 combined bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/DLBCL-c_; > D_pair13.M2_validation_bam.bam; > I can't guarantee the data is still around and I don't have the exact; > intervals, but if it doesn't reproduce in the latest GATK4 M2 then we can; > forget about it. If it does reproduce then it would put my mind at ease if; > the prior fixes this case too (and you could let Chip know that it got; > fixed 2.5 years later :-P); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393531485>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0qP335PYrMmALyjp96ZdOw_hv03Mks5t3_FQgaJpZM4UUW5o>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393559160
https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393559160:951,Testability,test,test,951,"is there a haplotype-based prior that uses the alignment of the haplotype; to the reference? A prior that depends on the alignment might be better; than a flat pro-reference prior. On Thu, May 31, 2018 at 9:34 AM, ldgauthier <notifications@github.com>; wrote:. > I found the data from yet another example of this behavior (see poorly; > aligned reads in the bamout -- somehow the big fat deletion is preferred; > over the reference):; > [image: dlbcl-c_d_pair13 nkap x_119059262 frame_shift_del; > ctctcttcttggttaaaggatgcaaggg_-]; > <https://user-images.githubusercontent.com/6578548/40785072-7fdff344-64b5-11e8-8539-78267b9a2950.png>; >; > tumor exome: /seq/picard_aggregation/C1637/c_D_pair13_Dx/v3/c_D_pair13_; > Dx.bam; > normal exome: /seq/picard_aggregation/C1637/RICOVER_134_N/v2/RICOVER_134_; > N.bam; > M2 tumor bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/c_D_; > pair13_Dx.bam; > M2 normal bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/RICOVER_; > 134_N.bam; > M2 combined bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/DLBCL-c_; > D_pair13.M2_validation_bam.bam; > I can't guarantee the data is still around and I don't have the exact; > intervals, but if it doesn't reproduce in the latest GATK4 M2 then we can; > forget about it. If it does reproduce then it would put my mind at ease if; > the prior fixes this case too (and you could let Chip know that it got; > fixed 2.5 years later :-P); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393531485>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0qP335PYrMmALyjp96ZdOw_hv03Mks5t3_FQgaJpZM4UUW5o>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393559160
https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393559160:1038,Testability,test,test,1038,"is there a haplotype-based prior that uses the alignment of the haplotype; to the reference? A prior that depends on the alignment might be better; than a flat pro-reference prior. On Thu, May 31, 2018 at 9:34 AM, ldgauthier <notifications@github.com>; wrote:. > I found the data from yet another example of this behavior (see poorly; > aligned reads in the bamout -- somehow the big fat deletion is preferred; > over the reference):; > [image: dlbcl-c_d_pair13 nkap x_119059262 frame_shift_del; > ctctcttcttggttaaaggatgcaaggg_-]; > <https://user-images.githubusercontent.com/6578548/40785072-7fdff344-64b5-11e8-8539-78267b9a2950.png>; >; > tumor exome: /seq/picard_aggregation/C1637/c_D_pair13_Dx/v3/c_D_pair13_; > Dx.bam; > normal exome: /seq/picard_aggregation/C1637/RICOVER_134_N/v2/RICOVER_134_; > N.bam; > M2 tumor bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/c_D_; > pair13_Dx.bam; > M2 normal bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/RICOVER_; > 134_N.bam; > M2 combined bam: /xchip/cga_home/stewart/Cancer/DLBCL/M2/test/DLBCL-c_; > D_pair13.M2_validation_bam.bam; > I can't guarantee the data is still around and I don't have the exact; > intervals, but if it doesn't reproduce in the latest GATK4 M2 then we can; > forget about it. If it does reproduce then it would put my mind at ease if; > the prior fixes this case too (and you could let Chip know that it got; > fixed 2.5 years later :-P); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393531485>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0qP335PYrMmALyjp96ZdOw_hv03Mks5t3_FQgaJpZM4UUW5o>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4829#issuecomment-393559160
https://github.com/broadinstitute/gatk/pull/4832#issuecomment-394917958:75,Security,validat,validations,75,"Back to you @takutosato. You caught a couple of whoppers. Fortunately, the validations still look good after fixing them.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4832#issuecomment-394917958
https://github.com/broadinstitute/gatk/pull/4838#issuecomment-393659921:38,Testability,test,tested,38,@jonn-smith Please review that I have tested to your satisfaction. Particularly around `FuncotationMap`. Please note that several issues are closed with this PR.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4838#issuecomment-393659921
https://github.com/broadinstitute/gatk/issues/4840#issuecomment-394395632:258,Availability,failure,failure,258,"I think we can start by looking at the coverage files. We don't typically use alt/decoy contigs, so I wonder if unusual coverage on them is causing the model to misbehave. We should take a look at the model and call directories for the shard that caused the failure as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4840#issuecomment-394395632
https://github.com/broadinstitute/gatk/pull/4841#issuecomment-403081626:9,Availability,ping,ping,9,@droazen ping,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4841#issuecomment-403081626
https://github.com/broadinstitute/gatk/pull/4841#issuecomment-409585231:20,Performance,optimiz,optimized,20,"Thanks @cmnbroad! I optimized the imports, now waiting for Travis.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4841#issuecomment-409585231
https://github.com/broadinstitute/gatk/pull/4843#issuecomment-394557363:3731,Deployability,pipeline,pipelines,3731,| `27% <0%> (+12%)` | |; | [...org/broadinstitute/hellbender/engine/GATKTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/4843/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvR0FUS1Rvb2wuamF2YQ==) | `91.384% <0%> (-0.126%)` | `163% <0%> (+69%)` | |; | [...otypecaller/HaplotypeCallerArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4843/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJBcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `100% <0%> (ø)` | `4% <0%> (+2%)` | :arrow_up: |; | [...efaultGATKVariantAnnotationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4843/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vRGVmYXVsdEdBVEtWYXJpYW50QW5ub3RhdGlvbkFyZ3VtZW50Q29sbGVjdGlvbi5qYXZh) | `100% <0%> (ø)` | `11% <0%> (+6%)` | :arrow_up: |; | [...institute/hellbender/engine/VariantWalkerBase.java](https://codecov.io/gh/broadinstitute/gatk/pull/4843/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvVmFyaWFudFdhbGtlckJhc2UuamF2YQ==) | `100% <0%> (ø)` | `21% <0%> (+10%)` | :arrow_up: |; | [...titute/hellbender/tools/walkers/GenotypeGVCFs.java](https://codecov.io/gh/broadinstitute/gatk/pull/4843/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0dlbm90eXBlR1ZDRnMuamF2YQ==) | `90.551% <0%> (+0.469%)` | `50% <0%> (+3%)` | :arrow_up: |; | [...nder/tools/spark/pipelines/ReadsPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4843/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `90.476% <0%> (+0.68%)` | `25% <0%> (+11%)` | :arrow_up: |; | ... and [10 more](https://codecov.io/gh/broadinstitute/gatk/pull/4843/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4843#issuecomment-394557363
https://github.com/broadinstitute/gatk/pull/4844#issuecomment-393939720:3611,Deployability,pipeline,pipelines,3611,"ller/HaplotypeCallerArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXJBcmd1bWVudENvbGxlY3Rpb24uamF2YQ==) | `100% <0%> (ø)` | `4% <0%> (+2%)` | :arrow_up: |; | [...efaultGATKVariantAnnotationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vRGVmYXVsdEdBVEtWYXJpYW50QW5ub3RhdGlvbkFyZ3VtZW50Q29sbGVjdGlvbi5qYXZh) | `100% <0%> (ø)` | `11% <0%> (+6%)` | :arrow_up: |; | [...titute/hellbender/tools/walkers/GenotypeGVCFs.java](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0dlbm90eXBlR1ZDRnMuamF2YQ==) | `90.55% <0%> (+0.46%)` | `50% <0%> (+3%)` | :arrow_up: |; | [...nder/tools/spark/pipelines/ReadsPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `90.47% <0%> (+0.68%)` | `25% <0%> (+11%)` | :arrow_up: |; | [...stitute/hellbender/tools/HaplotypeCallerSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9IYXBsb3R5cGVDYWxsZXJTcGFyay5qYXZh) | `84.17% <0%> (+1.01%)` | `40% <0%> (+15%)` | :arrow_up: |; | ... and [10 more](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4844?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codec",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4844#issuecomment-393939720
https://github.com/broadinstitute/gatk/pull/4844#issuecomment-393939720:4697,Deployability,update,update,4697,"3Rpb24uamF2YQ==) | `100% <0%> (ø)` | `4% <0%> (+2%)` | :arrow_up: |; | [...efaultGATKVariantAnnotationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vRGVmYXVsdEdBVEtWYXJpYW50QW5ub3RhdGlvbkFyZ3VtZW50Q29sbGVjdGlvbi5qYXZh) | `100% <0%> (ø)` | `11% <0%> (+6%)` | :arrow_up: |; | [...titute/hellbender/tools/walkers/GenotypeGVCFs.java](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0dlbm90eXBlR1ZDRnMuamF2YQ==) | `90.55% <0%> (+0.46%)` | `50% <0%> (+3%)` | :arrow_up: |; | [...nder/tools/spark/pipelines/ReadsPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `90.47% <0%> (+0.68%)` | `25% <0%> (+11%)` | :arrow_up: |; | [...stitute/hellbender/tools/HaplotypeCallerSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9IYXBsb3R5cGVDYWxsZXJTcGFyay5qYXZh) | `84.17% <0%> (+1.01%)` | `40% <0%> (+15%)` | :arrow_up: |; | ... and [10 more](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4844?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4844?src=pr&el=footer). Last update [7628cc9...fc61689](https://codecov.io/gh/broadinstitute/gatk/pull/4844?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4844#issuecomment-393939720
https://github.com/broadinstitute/gatk/pull/4844#issuecomment-393939720:4600,Energy Efficiency,Power,Powered,4600,"3Rpb24uamF2YQ==) | `100% <0%> (ø)` | `4% <0%> (+2%)` | :arrow_up: |; | [...efaultGATKVariantAnnotationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vRGVmYXVsdEdBVEtWYXJpYW50QW5ub3RhdGlvbkFyZ3VtZW50Q29sbGVjdGlvbi5qYXZh) | `100% <0%> (ø)` | `11% <0%> (+6%)` | :arrow_up: |; | [...titute/hellbender/tools/walkers/GenotypeGVCFs.java](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0dlbm90eXBlR1ZDRnMuamF2YQ==) | `90.55% <0%> (+0.46%)` | `50% <0%> (+3%)` | :arrow_up: |; | [...nder/tools/spark/pipelines/ReadsPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `90.47% <0%> (+0.68%)` | `25% <0%> (+11%)` | :arrow_up: |; | [...stitute/hellbender/tools/HaplotypeCallerSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9IYXBsb3R5cGVDYWxsZXJTcGFyay5qYXZh) | `84.17% <0%> (+1.01%)` | `40% <0%> (+15%)` | :arrow_up: |; | ... and [10 more](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4844?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4844?src=pr&el=footer). Last update [7628cc9...fc61689](https://codecov.io/gh/broadinstitute/gatk/pull/4844?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4844#issuecomment-393939720
https://github.com/broadinstitute/gatk/pull/4844#issuecomment-393939720:4463,Usability,learn,learn,4463,"3Rpb24uamF2YQ==) | `100% <0%> (ø)` | `4% <0%> (+2%)` | :arrow_up: |; | [...efaultGATKVariantAnnotationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL0dBVEtQbHVnaW4vRGVmYXVsdEdBVEtWYXJpYW50QW5ub3RhdGlvbkFyZ3VtZW50Q29sbGVjdGlvbi5qYXZh) | `100% <0%> (ø)` | `11% <0%> (+6%)` | :arrow_up: |; | [...titute/hellbender/tools/walkers/GenotypeGVCFs.java](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0dlbm90eXBlR1ZDRnMuamF2YQ==) | `90.55% <0%> (+0.46%)` | `50% <0%> (+3%)` | :arrow_up: |; | [...nder/tools/spark/pipelines/ReadsPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `90.47% <0%> (+0.68%)` | `25% <0%> (+11%)` | :arrow_up: |; | [...stitute/hellbender/tools/HaplotypeCallerSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9IYXBsb3R5cGVDYWxsZXJTcGFyay5qYXZh) | `84.17% <0%> (+1.01%)` | `40% <0%> (+15%)` | :arrow_up: |; | ... and [10 more](https://codecov.io/gh/broadinstitute/gatk/pull/4844/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4844?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4844?src=pr&el=footer). Last update [7628cc9...fc61689](https://codecov.io/gh/broadinstitute/gatk/pull/4844?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4844#issuecomment-393939720
https://github.com/broadinstitute/gatk/pull/4846#issuecomment-394016528:1266,Usability,Learn,LearnReadOrientationModel,1266,7ac7cd?src=pr&el=desc) will **increase** coverage by `0.023%`.; > The diff coverage is `94.318%`. ```diff; @@ Coverage Diff @@; ## master #4846 +/- ##; ===============================================; + Coverage 86.661% 86.684% +0.023% ; - Complexity 29043 29179 +136 ; ===============================================; Files 1808 1808 ; Lines 134662 135021 +359 ; Branches 14935 14986 +51 ; ===============================================; + Hits 116700 117042 +342 ; - Misses 12550 12563 +13 ; - Partials 5412 5416 +4; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4846?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...llbender/utils/reference/FastaReferenceWriter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4846/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWZlcmVuY2UvRmFzdGFSZWZlcmVuY2VXcml0ZXIuamF2YQ==) | `92.414% <0%> (ø)` | `51 <0> (ø)` | :arrow_down: |; | [...ers/readorientation/LearnReadOrientationModel.java](https://codecov.io/gh/broadinstitute/gatk/pull/4846/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3JlYWRvcmllbnRhdGlvbi9MZWFyblJlYWRPcmllbnRhdGlvbk1vZGVsLmphdmE=) | `96% <100%> (ø)` | `30 <0> (ø)` | :arrow_down: |; | [...org/broadinstitute/hellbender/utils/RandomDNA.java](https://codecov.io/gh/broadinstitute/gatk/pull/4846/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9SYW5kb21ETkEuamF2YQ==) | `95.556% <100%> (ø)` | `23 <0> (ø)` | :arrow_down: |; | [...er/formats/collections/AllelicCountCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4846/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2Zvcm1hdHMvY29sbGVjdGlvbnMvQWxsZWxpY0NvdW50Q29sbGVjdGlvbi5qYXZh) | `100% <100%> (ø)` | `5 <0> (ø)` | :arrow_down: |; | [...lbender/tools/copynumber/CollectAllelicCounts.java](https://codecov.io/gh/broadi,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4846#issuecomment-394016528
https://github.com/broadinstitute/gatk/pull/4846#issuecomment-394016528:3102,Usability,Learn,LearnReadOrientationModelEngine,3102,9sbGVjdGlvbi5qYXZh) | `100% <100%> (ø)` | `5 <0> (ø)` | :arrow_down: |; | [...lbender/tools/copynumber/CollectAllelicCounts.java](https://codecov.io/gh/broadinstitute/gatk/pull/4846/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL0NvbGxlY3RBbGxlbGljQ291bnRzLmphdmE=) | `92% <100%> (ø)` | `9 <0> (ø)` | :arrow_down: |; | [...dinstitute/hellbender/utils/RandomDNAUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4846/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9SYW5kb21ETkFVbml0VGVzdC5qYXZh) | `93.151% <100%> (ø)` | `37 <0> (ø)` | :arrow_down: |; | [...llbender/tools/copynumber/PreprocessIntervals.java](https://codecov.io/gh/broadinstitute/gatk/pull/4846/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL1ByZXByb2Nlc3NJbnRlcnZhbHMuamF2YQ==) | `100% <100%> (ø)` | `19 <2> (ø)` | :arrow_down: |; | [...adorientation/LearnReadOrientationModelEngine.java](https://codecov.io/gh/broadinstitute/gatk/pull/4846/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3JlYWRvcmllbnRhdGlvbi9MZWFyblJlYWRPcmllbnRhdGlvbk1vZGVsRW5naW5lLmphdmE=) | `94.872% <100%> (ø)` | `37 <0> (ø)` | :arrow_down: |; | [...ols/walkers/readorientation/CollectF1R2Counts.java](https://codecov.io/gh/broadinstitute/gatk/pull/4846/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3JlYWRvcmllbnRhdGlvbi9Db2xsZWN0RjFSMkNvdW50cy5qYXZh) | `75.641% <100%> (ø)` | `29 <6> (ø)` | :arrow_down: |; | [...s/walkers/readorientation/F1R2FilterConstants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4846/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3JlYWRvcmllbnRhdGlvbi9GMVIyRmlsdGVyQ29uc3RhbnRzLmphdmE=) | `83.333% <100%> (ø)` | `4 <0> (ø)` | :arrow_down: |; | ... and [5 more](https://codecov.io/gh/br,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4846#issuecomment-394016528
https://github.com/broadinstitute/gatk/pull/4846#issuecomment-414776060:78,Performance,perform,performance,78,A couple more minor comments. I will take your word that your changes improve performance!,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4846#issuecomment-414776060
https://github.com/broadinstitute/gatk/issues/4849#issuecomment-394446988:205,Deployability,configurat,configuration,205,"@magicDGS I like your idea of making `TwoPassReadWalker` more configurable, with the ability to specify different filters/transformers/intervals per-pass, provided that tools that don't need this level of configuration don't have to override any additional methods.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4849#issuecomment-394446988
https://github.com/broadinstitute/gatk/issues/4849#issuecomment-394446988:62,Modifiability,config,configurable,62,"@magicDGS I like your idea of making `TwoPassReadWalker` more configurable, with the ability to specify different filters/transformers/intervals per-pass, provided that tools that don't need this level of configuration don't have to override any additional methods.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4849#issuecomment-394446988
https://github.com/broadinstitute/gatk/issues/4849#issuecomment-394446988:205,Modifiability,config,configuration,205,"@magicDGS I like your idea of making `TwoPassReadWalker` more configurable, with the ability to specify different filters/transformers/intervals per-pass, provided that tools that don't need this level of configuration don't have to override any additional methods.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4849#issuecomment-394446988
https://github.com/broadinstitute/gatk/pull/4851#issuecomment-395186239:74,Usability,simpl,simpler,74,@davidbenjamin As per offline conversation and that this code is actually simpler.... feel free to merge.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4851#issuecomment-395186239
https://github.com/broadinstitute/gatk/pull/4853#issuecomment-395207710:933,Deployability,pipeline,pipelines,933,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4853?src=pr&el=h1) Report; > Merging [#4853](https://codecov.io/gh/broadinstitute/gatk/pull/4853?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/140d76cad6c00bd902243b83935e8225da328ae4?src=pr&el=desc) will **increase** coverage by `0.006%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #4853 +/- ##; ===============================================; + Coverage 80.421% 80.427% +0.006% ; - Complexity 17820 17821 +1 ; ===============================================; Files 1089 1089 ; Lines 64161 64159 -2 ; Branches 10344 10344 ; ===============================================; + Hits 51599 51601 +2 ; + Misses 8501 8497 -4 ; Partials 4061 4061; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4853?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...lbender/tools/spark/pipelines/PrintReadsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4853/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRSZWFkc1NwYXJrLmphdmE=) | `100% <ø> (ø)` | `3 <0> (-1)` | :arrow_down: |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/4853/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `90% <0%> (+40%)` | `3% <0%> (+2%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4853#issuecomment-395207710
https://github.com/broadinstitute/gatk/issues/4856#issuecomment-395425880:297,Energy Efficiency,power,powerful,297,"Could be. I would be very inclined to believe it especially if it's particularly uninformative for SNPs, since those would tend to have smaller assembly regions and hence more trimming. I'm interested in dealing with this because we have an immediate need to make the M2 read position filter more powerful for FFPE data without hurting sensitivity.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4856#issuecomment-395425880
https://github.com/broadinstitute/gatk/issues/4856#issuecomment-395438288:32,Integrability,depend,depends,32,"Given that Sam's 1D model still depends on ReadPosRankSum and his 2D model takes annotations plus the trimmed bamout regions, the CNN isn't getting the best read position info either. I think this would definitely be useful on the germline side.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4856#issuecomment-395438288
https://github.com/broadinstitute/gatk/issues/4857#issuecomment-395447566:16,Testability,test,tests,16,Expand the unit tests so that it checks for the values of the new metadata.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4857#issuecomment-395447566
https://github.com/broadinstitute/gatk/pull/4862#issuecomment-395775236:38,Deployability,update,update,38,@takutosato A very quick PR before we update the Firecloud M2.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4862#issuecomment-395775236
https://github.com/broadinstitute/gatk/issues/4863#issuecomment-430312996:76,Safety,detect,detect,76,"This should be straight-forward once the discussion above is resolved. Just detect if the variant is on a known Mitochondrial contig. If so, use the MT codon table, otherwise use the regular codon table.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4863#issuecomment-430312996
https://github.com/broadinstitute/gatk/issues/4863#issuecomment-431140730:39,Testability,test,testing,39,I actually will implement special case testing for each different genus that is listed on the wiki page for now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4863#issuecomment-431140730
https://github.com/broadinstitute/gatk/issues/4865#issuecomment-395890633:53,Safety,avoid,avoid,53,@droazen That is correct. It was a necessary step to avoid having to make a class equality check on the likelihoods object itself. @davidbenjamin I am open to suggestions if you have an idea of how better to encapsulate the separation between these two likelihood objects.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4865#issuecomment-395890633
https://github.com/broadinstitute/gatk/issues/4865#issuecomment-396369856:193,Integrability,wrap,wrapper,193,"Thanks for indulging me on this. To me it seems like `UnfilledReadsLikelihoods` diverges too much from `ReadsLikelihoods` to extend it. In effect it's letting `ReadsLikelihoods` sometimes be a wrapper for something that is not a `ReadsLikelihoods`. I haven't worked this out but I would hope that it's possible to construct a `ReadsLikelihoods` from a pileup. I mean, the idea of pileup calling is that you use just a single base for the likelihoods and not the whole read (via Pair-HMM), so we should be able to fill the likelihoods from the base qualities.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4865#issuecomment-396369856
https://github.com/broadinstitute/gatk/issues/4865#issuecomment-396369856:125,Modifiability,extend,extend,125,"Thanks for indulging me on this. To me it seems like `UnfilledReadsLikelihoods` diverges too much from `ReadsLikelihoods` to extend it. In effect it's letting `ReadsLikelihoods` sometimes be a wrapper for something that is not a `ReadsLikelihoods`. I haven't worked this out but I would hope that it's possible to construct a `ReadsLikelihoods` from a pileup. I mean, the idea of pileup calling is that you use just a single base for the likelihoods and not the whole read (via Pair-HMM), so we should be able to fill the likelihoods from the base qualities.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4865#issuecomment-396369856
https://github.com/broadinstitute/gatk/issues/4865#issuecomment-396600621:33,Testability,test,test,33,"I have some code to do that in a test project - if you are interested on it, I can submit a PR with my proposal. It will be nice for an idea that I have in mind, and if it is used also in GATK it would have more support (as a single developer, my reviews are not as good as in a team). Just let me know if you wanna port the code to some utility class!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4865#issuecomment-396600621
https://github.com/broadinstitute/gatk/issues/4866#issuecomment-395839722:227,Testability,test,tests,227,"I haven't seen those before and have no emotional investment. On Fri, Jun 8, 2018 at 11:56 AM, David Benjamin <notifications@github.com>; wrote:. > DiploidGenotype and ProbabilityVector are not used except in their own; > unit tests. Delete? @ldgauthier <https://github.com/ldgauthier>; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4866>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdGmGPyRVd9KwG3OauuYs9TDz26F0ks5t6p6TgaJpZM4UgjvA>; > .; >. -- ; Laura Doyle Gauthier, Ph.D.; Group Leader, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4866#issuecomment-395839722
https://github.com/broadinstitute/gatk/pull/4872#issuecomment-396381943:1587,Usability,Simpl,SimpleKeyXsvFuncotationFactory,1587,089 1092 +3 ; Lines 64168 64761 +593 ; Branches 10338 10491 +153 ; ===============================================; + Hits 51625 52171 +546 ; - Misses 8486 8513 +27 ; - Partials 4057 4077 +20; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4872?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tils/annotatedinterval/AnnotatedIntervalCodec.java](https://codecov.io/gh/broadinstitute/gatk/pull/4872/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL3V0aWxzL2Fubm90YXRlZGludGVydmFsL0Fubm90YXRlZEludGVydmFsQ29kZWMuamF2YQ==) | `79.31% <ø> (ø)` | `18 <0> (ø)` | :arrow_down: |; | [...itute/hellbender/tools/funcotator/Funcotation.java](https://codecov.io/gh/broadinstitute/gatk/pull/4872/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0aW9uLmphdmE=) | `33.333% <ø> (ø)` | `3 <0> (ø)` | :arrow_down: |; | [...ataSources/xsv/SimpleKeyXsvFuncotationFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4872/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL3hzdi9TaW1wbGVLZXlYc3ZGdW5jb3RhdGlvbkZhY3RvcnkuamF2YQ==) | `87.097% <100%> (ø)` | `27 <0> (ø)` | :arrow_down: |; | [...dataSources/gencode/GencodeFuncotationBuilder.java](https://codecov.io/gh/broadinstitute/gatk/pull/4872/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL2dlbmNvZGUvR2VuY29kZUZ1bmNvdGF0aW9uQnVpbGRlci5qYXZh) | `96.774% <100%> (+0.053%)` | `30 <1> (ø)` | :arrow_down: |; | [...dataSources/gencode/GencodeFuncotationFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4872/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL2dlbmNvZGUvR2VuY29kZUZ1bmNvdGF0aW9uRmFjdG9yeS5qYXZh) | `83.634% <100%> (+0.025%)` | `164 <5> (+1)` | :arr,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4872#issuecomment-396381943
https://github.com/broadinstitute/gatk/pull/4872#issuecomment-397418459:25,Testability,test,tests,25,@jonn-smith I'll fix the tests and notify you.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4872#issuecomment-397418459
https://github.com/broadinstitute/gatk/pull/4872#issuecomment-397490924:25,Testability,test,tests,25,"@jonn-smith Assuming the tests pass, please review latest changes. If the tests have failed, I will see it tomorrow and address.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4872#issuecomment-397490924
https://github.com/broadinstitute/gatk/pull/4872#issuecomment-397490924:74,Testability,test,tests,74,"@jonn-smith Assuming the tests pass, please review latest changes. If the tests have failed, I will see it tomorrow and address.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4872#issuecomment-397490924
https://github.com/broadinstitute/gatk/pull/4872#issuecomment-397658942:43,Testability,test,tests,43,Offline...Was given permission to merge if tests pass.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4872#issuecomment-397658942
https://github.com/broadinstitute/gatk/pull/4874#issuecomment-416339183:78,Modifiability,config,configurable,78,The idea behind this branch: make the output to readsSparkSort consistent and configurable. So that if a tool alters reads without changing their sort order then no sort will be performed by default. It also means that if you request sharded output there is the ability to ask reasSparkSource to sort the file for you.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4874#issuecomment-416339183
https://github.com/broadinstitute/gatk/pull/4874#issuecomment-416339183:178,Performance,perform,performed,178,The idea behind this branch: make the output to readsSparkSort consistent and configurable. So that if a tool alters reads without changing their sort order then no sort will be performed by default. It also means that if you request sharded output there is the ability to ask reasSparkSource to sort the file for you.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4874#issuecomment-416339183
https://github.com/broadinstitute/gatk/pull/4874#issuecomment-461951346:1870,Deployability,pipeline,pipelines,1870,erage Δ | Complexity Δ | |; |---|---|---|---|; | [...nder/tools/spark/pathseq/PathSeqPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4874/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BhdGhTZXFQaXBlbGluZVNwYXJrLmphdmE=) | `81.25% <ø> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...lbender/tools/spark/pathseq/PathSeqScoreSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4874/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BhdGhTZXFTY29yZVNwYXJrLmphdmE=) | `57.407% <ø> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...llbender/engine/spark/DataprocIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4874/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvRGF0YXByb2NJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `1.786% <0%> (+0.062%)` | `1 <0> (ø)` | :arrow_down: |; | [...hellbender/tools/spark/pipelines/SortSamSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4874/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvU29ydFNhbVNwYXJrLmphdmE=) | `100% <100%> (ø)` | `5 <0> (-1)` | :arrow_down: |; | [...ellbender/tools/spark/pathseq/PathSeqBwaSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4874/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BhdGhTZXFCd2FTcGFyay5qYXZh) | `67.391% <100%> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...ls/ExtractOriginalAlignmentRecordsByNameSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4874/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi91dGlscy9FeHRyYWN0T3JpZ2luYWxBbGlnbm1lbnRSZWNvcmRzQnlOYW1lU3BhcmsuamF2YQ==) | `90.909% <100%> (ø)` | `10 <0> (ø)` | :arrow_down: |; | [...stitute/hellbender/tools/spark/RevertSamSpark.java](https://codecov.io/g,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4874#issuecomment-461951346
https://github.com/broadinstitute/gatk/pull/4874#issuecomment-461951346:3698,Deployability,pipeline,pipelines,3698, | `100% <100%> (ø)` | `5 <0> (-1)` | :arrow_down: |; | [...ellbender/tools/spark/pathseq/PathSeqBwaSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4874/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9wYXRoc2VxL1BhdGhTZXFCd2FTcGFyay5qYXZh) | `67.391% <100%> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...ls/ExtractOriginalAlignmentRecordsByNameSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4874/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi91dGlscy9FeHRyYWN0T3JpZ2luYWxBbGlnbm1lbnRSZWNvcmRzQnlOYW1lU3BhcmsuamF2YQ==) | `90.909% <100%> (ø)` | `10 <0> (ø)` | :arrow_down: |; | [...stitute/hellbender/tools/spark/RevertSamSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4874/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9SZXZlcnRTYW1TcGFyay5qYXZh) | `83.895% <100%> (ø)` | `86 <0> (ø)` | :arrow_down: |; | [...institute/hellbender/tools/spark/bwa/BwaSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4874/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9id2EvQndhU3BhcmsuamF2YQ==) | `78.947% <100%> (+1.17%)` | `7 <0> (ø)` | :arrow_down: |; | [...stitute/hellbender/engine/spark/GATKSparkTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/4874/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1NwYXJrVG9vbC5qYXZh) | `82.418% <100%> (ø)` | `78 <1> (ø)` | :arrow_down: |; | [...nder/tools/spark/pipelines/ReadsPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4874/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUmVhZHNQaXBlbGluZVNwYXJrLmphdmE=) | `90.741% <100%> (ø)` | `13 <0> (ø)` | :arrow_down: |; | ... and [12 more](https://codecov.io/gh/broadinstitute/gatk/pull/4874/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4874#issuecomment-461951346
https://github.com/broadinstitute/gatk/issues/4875#issuecomment-396286813:253,Availability,error,error,253,FWIW I'm having the exact same issue with HaplotypeCaller. It's not quite limited to just requests for help (`-h`). Any completely valid command line works. Any incorrect command line that also triggers display of the help text also fails with the same error message.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4875#issuecomment-396286813
https://github.com/broadinstitute/gatk/issues/4875#issuecomment-396286813:259,Integrability,message,message,259,FWIW I'm having the exact same issue with HaplotypeCaller. It's not quite limited to just requests for help (`-h`). Any completely valid command line works. Any incorrect command line that also triggers display of the help text also fails with the same error message.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4875#issuecomment-396286813
https://github.com/broadinstitute/gatk/issues/4875#issuecomment-396309460:22,Deployability,release,release,22,We are going to cut a release now to push out the fix for this issue.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4875#issuecomment-396309460
https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396338920:1555,Security,validat,validation,1555,====================; Files 1089 1090 +1 ; Lines 64159 64937 +778 ; Branches 10344 10510 +166 ; ===============================================; + Hits 51600 52279 +679 ; - Misses 8498 8569 +71 ; - Partials 4061 4089 +28; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4878?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...s/read/markduplicates/sparkrecords/PairedEnds.java](https://codecov.io/gh/broadinstitute/gatk/pull/4878/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9QYWlyZWRFbmRzLmphdmE=) | `100% <ø> (ø)` | `1 <0> (ø)` | :arrow_down: |; | [...roadinstitute/hellbender/utils/read/ReadUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4878/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL1JlYWRVdGlscy5qYXZh) | `80% <100%> (+0.142%)` | `202 <3> (+3)` | :arrow_up: |; | [...tools/spark/validation/CompareDuplicatesSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4878/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay92YWxpZGF0aW9uL0NvbXBhcmVEdXBsaWNhdGVzU3BhcmsuamF2YQ==) | `84.946% <100%> (+0.502%)` | `24 <3> (ø)` | :arrow_down: |; | [...itute/hellbender/engine/spark/GATKRegistrator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4878/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvR0FUS1JlZ2lzdHJhdG9yLmphdmE=) | `100% <100%> (ø)` | `3 <0> (ø)` | :arrow_down: |; | [...icates/sparkrecords/MarkDuplicatesSparkRecord.java](https://codecov.io/gh/broadinstitute/gatk/pull/4878/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9NYXJrRHVwbGljYXRlc1NwYXJrUmVjb3JkLmphdmE=) | `100% <100%> (ø)` | `7 <3> (ø)` | :arrow_down: |; | [...ils/read/markduplicates/sparkrecords/Fragment.java](https://co,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396338920
https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396645011:55,Performance,perform,performance,55,I'm posting an experimental version of this branch for performance purposes. This is no longer in a state where it should be merged.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396645011
https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396730222:14,Deployability,update,updated,14,@lbergelson I updated this branch with the new key representation. After some performance runs it appears that these lead to a slightly faster mapping operation and approximately 15% less serialization for the step where they are used. Can you take a look?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396730222
https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396730222:78,Performance,perform,performance,78,@lbergelson I updated this branch with the new key representation. After some performance runs it appears that these lead to a slightly faster mapping operation and approximately 15% less serialization for the step where they are used. Can you take a look?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4878#issuecomment-396730222
https://github.com/broadinstitute/gatk/pull/4881#issuecomment-396381499:3139,Deployability,pipeline,pipelines,3139,bWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (-30%)` | `1% <0%> (-2%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4881/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `60.714% <0%> (-5.258%)` | `41% <0%> (+4%)` | |; | [...bender/tools/walkers/annotator/StrandBiasTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4881/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9TdHJhbmRCaWFzVGVzdC5qYXZh) | `90.226% <0%> (-0.2%)` | `52% <0%> (+10%)` | |; | [...oadinstitute/hellbender/utils/read/CigarUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4881/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL0NpZ2FyVXRpbHMuamF2YQ==) | `93.827% <0%> (-0.112%)` | `159% <0%> (+68%)` | |; | [...lbender/tools/spark/pipelines/PrintReadsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4881/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRSZWFkc1NwYXJrLmphdmE=) | `100% <0%> (ø)` | `5% <0%> (+1%)` | :arrow_up: |; | [...bender/utils/runtime/AsynchronousStreamWriter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4881/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9ydW50aW1lL0FzeW5jaHJvbm91c1N0cmVhbVdyaXRlci5qYXZh) | `83.673% <0%> (ø)` | `11% <0%> (?)` | |; | [...er/utils/python/StreamingPythonScriptExecutor.java](https://codecov.io/gh/broadinstitute/gatk/pull/4881/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9weXRob24vU3RyZWFtaW5nUHl0aG9uU2NyaXB0RXhlY3V0b3IuamF2YQ==) | `84.028% <0%> (+0.028%)` | `34% <0%> (+19%)` | :arrow_up: |; | ... and [10 more](https://codecov.io/gh/broadinstitute/gatk/pull/4881/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4881#issuecomment-396381499
https://github.com/broadinstitute/gatk/pull/4881#issuecomment-396381499:2282,Testability,test,test,2282,ools/funcotator/mafOutput/MafOutputRenderer.java](https://codecov.io/gh/broadinstitute/gatk/pull/4881/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL21hZk91dHB1dC9NYWZPdXRwdXRSZW5kZXJlci5qYXZh) | `95.664% <100%> (+0.024%)` | `52 <0> (+1)` | :arrow_up: |; | [...dataSources/gencode/GencodeFuncotationFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4881/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL2RhdGFTb3VyY2VzL2dlbmNvZGUvR2VuY29kZUZ1bmNvdGF0aW9uRmFjdG9yeS5qYXZh) | `83.609% <91.304%> (+0.25%)` | `163 <7> (+2)` | :arrow_up: |; | [...utils/smithwaterman/SmithWatermanIntelAligner.java](https://codecov.io/gh/broadinstitute/gatk/pull/4881/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NtaXRoV2F0ZXJtYW5JbnRlbEFsaWduZXIuamF2YQ==) | `50% <0%> (-30%)` | `1% <0%> (-2%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4881/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `60.714% <0%> (-5.258%)` | `41% <0%> (+4%)` | |; | [...bender/tools/walkers/annotator/StrandBiasTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4881/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9TdHJhbmRCaWFzVGVzdC5qYXZh) | `90.226% <0%> (-0.2%)` | `52% <0%> (+10%)` | |; | [...oadinstitute/hellbender/utils/read/CigarUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4881/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL0NpZ2FyVXRpbHMuamF2YQ==) | `93.827% <0%> (-0.112%)` | `159% <0%> (+68%)` | |; | [...lbender/tools/spark/pipelines/PrintReadsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4881/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvY,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4881#issuecomment-396381499
https://github.com/broadinstitute/gatk/pull/4881#issuecomment-397036412:15,Testability,test,tests,15,"@jonn-smith If tests pass, can I merge?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4881#issuecomment-397036412
https://github.com/broadinstitute/gatk/issues/4882#issuecomment-396389331:169,Availability,error,error,169,"This occurs when there is an issue with the coding sequence for a given transcript in Gencode. It's a problem with the Gencode data itself. I would prefer to keep as an error so the user is more likely to see it. There is some work to be done here, though - the error message should also contain a description of an exception that was caught to produce this log statement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4882#issuecomment-396389331
https://github.com/broadinstitute/gatk/issues/4882#issuecomment-396389331:262,Availability,error,error,262,"This occurs when there is an issue with the coding sequence for a given transcript in Gencode. It's a problem with the Gencode data itself. I would prefer to keep as an error so the user is more likely to see it. There is some work to be done here, though - the error message should also contain a description of an exception that was caught to produce this log statement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4882#issuecomment-396389331
https://github.com/broadinstitute/gatk/issues/4882#issuecomment-396389331:268,Integrability,message,message,268,"This occurs when there is an issue with the coding sequence for a given transcript in Gencode. It's a problem with the Gencode data itself. I would prefer to keep as an error so the user is more likely to see it. There is some work to be done here, though - the error message should also contain a description of an exception that was caught to produce this log statement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4882#issuecomment-396389331
https://github.com/broadinstitute/gatk/issues/4882#issuecomment-396389331:358,Testability,log,log,358,"This occurs when there is an issue with the coding sequence for a given transcript in Gencode. It's a problem with the Gencode data itself. I would prefer to keep as an error so the user is more likely to see it. There is some work to be done here, though - the error message should also contain a description of an exception that was caught to produce this log statement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4882#issuecomment-396389331
https://github.com/broadinstitute/gatk/issues/4887#issuecomment-396687918:8,Deployability,update,updated,8,"Keep me updated. I think a multi-sample version of ModelSegments would be pretty easy to implement and would hopefully share a similar command-line scheme for specifying allelic-count and denoised-copy-ratio files for the normal/tumors. Something to think about is that the modeling step probably can be done on a per-sample basis after multi-sample segmentation, and it would be nice to scatter per sample for WGS. There are probably a few ways we can implement this, but let me know if you're planning something similar for M2.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4887#issuecomment-396687918
https://github.com/broadinstitute/gatk/issues/4887#issuecomment-430817379:109,Usability,simpl,simply,109,"I am very interested in this feature. In my current workflow I am using `freebayes` (and in an older version simply `pileup`) to query mutation sites (after doing the actual mutation calling using gATK4 M2) on a cohort level and would be very interested in a GATK naive approach. P.s. personally more interested in a multi-step mutation calling method than multi-sample calling, where one first calls mutation in multiple samples that are then joined together in a consensus callset, after which the consensus callset is queried individually across the entire cohort of patient resulting in genotype and allele frequencies for each variant across the entire cohort",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4887#issuecomment-430817379
https://github.com/broadinstitute/gatk/issues/4887#issuecomment-431005889:64,Deployability,pipeline,pipeline,64,"Since we're using Mutect2 for our mitochondrial variant calling pipeline that's in development, and we want to joint call mitochondria, I'm working on ""somatic joint calling"". It won't have a joint likelihood model (yet?) the way that germline SNP and indel joint calling does, but it will be able to give you a ""squared-off"" matrix of calls for each sample at each site that's variant in any sample.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4887#issuecomment-431005889
https://github.com/broadinstitute/gatk/issues/4888#issuecomment-396632448:34,Deployability,patch,patch,34,@lbergelson I'd prefer a targeted patch to the retry code in our fork as the lowest-risk option for now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4888#issuecomment-396632448
https://github.com/broadinstitute/gatk/issues/4888#issuecomment-396632448:84,Safety,risk,risk,84,@lbergelson I'd prefer a targeted patch to the retry code in our fork as the lowest-risk option for now.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4888#issuecomment-396632448
https://github.com/broadinstitute/gatk/issues/4888#issuecomment-397427772:262,Availability,error,error,262,"After discussing with @cwhelan, the `UnknownHostException` has not yet been shown to actually be retryable -- ie., we've never had a run in the exact same environment that passed where previously we saw `UnknownHostException`. He's only been able to resolve the error by changing the runtime environment (cromwell -> non-cromwell), and in that case a retry in `google-cloud-java` wouldn't help.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4888#issuecomment-397427772
https://github.com/broadinstitute/gatk/pull/4889#issuecomment-396706300:1573,Deployability,pipeline,pipelines,1573,==; Files 1089 1089 ; Lines 64161 64161 ; Branches 10344 10344 ; ===============================================; - Hits 51599 51434 -165 ; - Misses 8501 8672 +171 ; + Partials 4061 4055 -6; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4889?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ools/walkers/haplotypecaller/graphs/BaseGraph.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvQmFzZUdyYXBoLmphdmE=) | `82.239% <50%> (ø)` | `94 <2> (ø)` | :arrow_down: |; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-74.257%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-10%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4889#issuecomment-396706300
https://github.com/broadinstitute/gatk/pull/4889#issuecomment-396706300:1889,Testability,test,test,1889,exity Δ | |; |---|---|---|---|; | [...ools/walkers/haplotypecaller/graphs/BaseGraph.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9ncmFwaHMvQmFzZUdyYXBoLmphdmE=) | `82.239% <50%> (ø)` | `94 <2> (ø)` | :arrow_down: |; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-74.257%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-10%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <0%> (-22.807%)` | `0% <0%> (-2%)` | |; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4889#issuecomment-396706300
https://github.com/broadinstitute/gatk/pull/4889#issuecomment-396706300:2756,Testability,test,test,2756,Fyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-10%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <0%> (-22.807%)` | `0% <0%> (-2%)` | |; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L01pbmlDbHVzdGVyVXRpbHMuamF2YQ==) | `78.947% <0%> (-10.526%)` | `6% <0%> (-1%)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `77.083% <0%> (-3.125%)` | `31% <0%> (ø)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `89.394% <0%> (-3.03%)` | `61% <0%> (-2%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4va,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4889#issuecomment-396706300
https://github.com/broadinstitute/gatk/pull/4889#issuecomment-396706300:3645,Testability,test,test,3645,-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-10%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <0%> (-22.807%)` | `0% <0%> (-2%)` | |; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L01pbmlDbHVzdGVyVXRpbHMuamF2YQ==) | `78.947% <0%> (-10.526%)` | `6% <0%> (-1%)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `77.083% <0%> (-3.125%)` | `31% <0%> (ø)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `89.394% <0%> (-3.03%)` | `61% <0%> (-2%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `62.838% <0%> (-2.703%)` | `36% <0%> (-3%)` | |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/4889/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4889#issuecomment-396706300
https://github.com/broadinstitute/gatk/issues/4891#issuecomment-420247697:55,Deployability,pipeline,pipeline,55,"Please count me also. It is very useful tool. My whole pipeline is with GATK4, so it would be best to have this tool too.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4891#issuecomment-420247697
https://github.com/broadinstitute/gatk/issues/4891#issuecomment-435474208:15,Energy Efficiency,schedul,scheduled,15,"@ryfa5051 It's scheduled to be ported this quarter (so, either this month or next month).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4891#issuecomment-435474208
https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397066880:58,Testability,test,test,58,@lbergelson Responded to comments. I added an albeit weak test of the output for sanity. Also fixed some stuff i neglected when pushing this before,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397066880
https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397343622:28,Availability,failure,failures,28,@jamesemery Still some test failures https://storage.googleapis.com/hellbender-test-logs/build_reports/master_19811.2/tests/test/classes/org.broadinstitute.hellbender.tools.spark.validation.CompareDuplicatesSparkIntegrationTest.html#testOutputFile,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397343622
https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397343622:179,Security,validat,validation,179,@jamesemery Still some test failures https://storage.googleapis.com/hellbender-test-logs/build_reports/master_19811.2/tests/test/classes/org.broadinstitute.hellbender.tools.spark.validation.CompareDuplicatesSparkIntegrationTest.html#testOutputFile,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397343622
https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397343622:23,Testability,test,test,23,@jamesemery Still some test failures https://storage.googleapis.com/hellbender-test-logs/build_reports/master_19811.2/tests/test/classes/org.broadinstitute.hellbender.tools.spark.validation.CompareDuplicatesSparkIntegrationTest.html#testOutputFile,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397343622
https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397343622:79,Testability,test,test-logs,79,@jamesemery Still some test failures https://storage.googleapis.com/hellbender-test-logs/build_reports/master_19811.2/tests/test/classes/org.broadinstitute.hellbender.tools.spark.validation.CompareDuplicatesSparkIntegrationTest.html#testOutputFile,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397343622
https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397343622:118,Testability,test,tests,118,@jamesemery Still some test failures https://storage.googleapis.com/hellbender-test-logs/build_reports/master_19811.2/tests/test/classes/org.broadinstitute.hellbender.tools.spark.validation.CompareDuplicatesSparkIntegrationTest.html#testOutputFile,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397343622
https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397343622:124,Testability,test,test,124,@jamesemery Still some test failures https://storage.googleapis.com/hellbender-test-logs/build_reports/master_19811.2/tests/test/classes/org.broadinstitute.hellbender.tools.spark.validation.CompareDuplicatesSparkIntegrationTest.html#testOutputFile,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397343622
https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397343622:233,Testability,test,testOutputFile,233,@jamesemery Still some test failures https://storage.googleapis.com/hellbender-test-logs/build_reports/master_19811.2/tests/test/classes/org.broadinstitute.hellbender.tools.spark.validation.CompareDuplicatesSparkIntegrationTest.html#testOutputFile,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397343622
https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397710551:55,Testability,test,tests,55,"@lbergelson Responded to comments and fixed the broken tests. It now strips out non-RG annotations before it goes to write the output, which is slightly more useful from a space/speed perspective.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397710551
https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397746010:949,Security,validat,validation,949,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4894?src=pr&el=h1) Report; > Merging [#4894](https://codecov.io/gh/broadinstitute/gatk/pull/4894?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/41788f5de7711bf46222bb19c139d84946b40d70?src=pr&el=desc) will **increase** coverage by `0.191%`.; > The diff coverage is `94.231%`. ```diff; @@ Coverage Diff @@; ## master #4894 +/- ##; ===============================================; + Coverage 80.453% 80.644% +0.191% ; - Complexity 17837 18463 +626 ; ===============================================; Files 1092 1092 ; Lines 64231 65676 +1445 ; Branches 10348 10735 +387 ; ===============================================; + Hits 51676 52964 +1288 ; - Misses 8504 8601 +97 ; - Partials 4051 4111 +60; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4894?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tools/spark/validation/CompareDuplicatesSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4894/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay92YWxpZGF0aW9uL0NvbXBhcmVEdXBsaWNhdGVzU3BhcmsuamF2YQ==) | `89.63% <94.231%> (+4.683%)` | `40 <26> (+16)` | :arrow_up: |; | [...e/hellbender/tools/funcotator/FuncotatorUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4894/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3JVdGlscy5qYXZh) | `80.176% <0%> (-0.154%)` | `328% <0%> (+160%)` | |; | [...roadinstitute/hellbender/utils/read/ReadUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4894/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL1JlYWRVdGlscy5qYXZh) | `80% <0%> (ø)` | `200% <0%> (-2%)` | :arrow_down: |; | [...dataSources/gencode/GencodeFuncotationBuilder.java](https://codecov.io/gh/broadinstitute/gatk/pull/4894/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4894#issuecomment-397746010
https://github.com/broadinstitute/gatk/pull/4895#issuecomment-408528614:117,Energy Efficiency,monitor,monitor,117,"@davidbenjamin I made the requested changes and submitted novaseq validation jobs. They haven't failed yet, but I'll monitor the jobs and make changes to the wdl as needed. Will let you know when they finish.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4895#issuecomment-408528614
https://github.com/broadinstitute/gatk/pull/4895#issuecomment-408528614:66,Security,validat,validation,66,"@davidbenjamin I made the requested changes and submitted novaseq validation jobs. They haven't failed yet, but I'll monitor the jobs and make changes to the wdl as needed. Will let you know when they finish.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4895#issuecomment-408528614
https://github.com/broadinstitute/gatk/pull/4895#issuecomment-408650061:31,Testability,test,tests,31,@takutosato You can merge once tests pass.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4895#issuecomment-408650061
https://github.com/broadinstitute/gatk/pull/4895#issuecomment-409356556:1553,Energy Efficiency,Power,PowerCalculationUtils,1553,===================================; Files 1791 1813 +22 ; Lines 133696 137705 +4009 ; Branches 14896 15619 +723 ; ===============================================; + Hits 115496 119272 +3776 ; - Misses 12797 12889 +92 ; - Partials 5403 5544 +141; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4895?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nstitute/hellbender/engine/filters/ReadFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4895/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZmlsdGVycy9SZWFkRmlsdGVyLmphdmE=) | `100% <ø> (ø)` | `10 <0> (ø)` | :arrow_down: |; | [...ute/hellbender/utils/variant/GATKVCFConstants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4895/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWQ0ZDb25zdGFudHMuamF2YQ==) | `80% <ø> (ø)` | `4 <0> (ø)` | :arrow_down: |; | [...ion/basicshortmutpileup/PowerCalculationUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4895/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9Qb3dlckNhbGN1bGF0aW9uVXRpbHMuamF2YQ==) | `92.683% <0%> (-3.984%)` | `31 <0> (+13)` | |; | [...hellbender/tools/walkers/mutect/Mutect2Engine.java](https://codecov.io/gh/broadinstitute/gatk/pull/4895/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL211dGVjdC9NdXRlY3QyRW5naW5lLmphdmE=) | `93.396% <0%> (+2.005%)` | `71 <1> (+16)` | :arrow_up: |; | [...ientation/ReadOrientationModelIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4895/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3JlYWRvcmllbnRhdGlvbi9SZWFkT3JpZW50YXRpb25Nb2RlbEludGVncmF0aW9uVGVzdC5qYXZh) | `100% <100%> (ø)` | `6 <6> (?)` | |; | [...dinstitute/hellbender/utils/MathUtilsUnitTes,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4895#issuecomment-409356556
https://github.com/broadinstitute/gatk/issues/4898#issuecomment-397597920:114,Availability,error,error,114,"@MigleSur Looks like you created an issue using just the template. I'll close this out assuming it was created in error. If you do have an issue, feel free to re-open and edit to include the details. thx.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4898#issuecomment-397597920
https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741:2336,Performance,cache,cache,2336,/codecov.io/gh/broadinstitute/gatk/pull/4902?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | Coverage Δ | |; |---|---|---|; | [...e/hellbender/engine/FeatureDataSourceUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2VVbml0VGVzdC5qYXZh) | `88.318% <ø> (ø)` | |; | [...institute/hellbender/engine/FeatureDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvRmVhdHVyZURhdGFTb3VyY2UuamF2YQ==) | `74.603% <57.143%> (+24.603%)` | :arrow_up: |; | [...ender/engine/cache/SideReadInputCacheStrategy.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvU2lkZVJlYWRJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `81.481% <81.481%> (ø)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `91.667% <84.615%> (+33.333%)` | :arrow_up: |; | [...ellbender/engine/VariantWalkerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3J,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741
https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741:3501,Performance,cache,cache,3501,=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvU2lkZVJlYWRJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `81.481% <81.481%> (ø)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `91.667% <84.615%> (+33.333%)` | :arrow_up: |; | [...ellbender/engine/VariantWalkerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvVmFyaWFudFdhbGtlckludGVncmF0aW9uVGVzdC5qYXZh) | `87.288% <86.667%> (ø)` | |; | [...engine/cache/DrivingFeatureInputCacheStrategy.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvRHJpdmluZ0ZlYXR1cmVJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `88.000% <88.000%> (ø)` | |; | [...ellbender/engine/cache/LocatableCacheUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvTG9jYXRhYmxlQ2FjaGVVbml0VGVzdC5qYXZh) | `96.471% <96.471%> (ø)` | |; | [...gumentcollections/ReadInputArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#di,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741
https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741:3914,Performance,cache,cache,3914,ents&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `91.667% <84.615%> (+33.333%)` | :arrow_up: |; | [...ellbender/engine/VariantWalkerIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvVmFyaWFudFdhbGtlckludGVncmF0aW9uVGVzdC5qYXZh) | `87.288% <86.667%> (ø)` | |; | [...engine/cache/DrivingFeatureInputCacheStrategy.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvRHJpdmluZ0ZlYXR1cmVJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `88.000% <88.000%> (ø)` | |; | [...ellbender/engine/cache/LocatableCacheUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvTG9jYXRhYmxlQ2FjaGVVbml0VGVzdC5qYXZh) | `96.471% <96.471%> (ø)` | |; | [...gumentcollections/ReadInputArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL2FyZ3VtZW50Y29sbGVjdGlvbnMvUmVhZElucHV0QXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `87.500% <100.000%> (+20.833%)` | :arrow_up: |; | [...org/broadinstitute/hellbender/engine/GATKTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comm,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741
https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741:5125,Performance,cache,cache,5125,ign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvRHJpdmluZ0ZlYXR1cmVJbnB1dENhY2hlU3RyYXRlZ3kuamF2YQ==) | `88.000% <88.000%> (ø)` | |; | [...ellbender/engine/cache/LocatableCacheUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvTG9jYXRhYmxlQ2FjaGVVbml0VGVzdC5qYXZh) | `96.471% <96.471%> (ø)` | |; | [...gumentcollections/ReadInputArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL2FyZ3VtZW50Y29sbGVjdGlvbnMvUmVhZElucHV0QXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `87.500% <100.000%> (+20.833%)` | :arrow_up: |; | [...org/broadinstitute/hellbender/engine/GATKTool.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvR0FUS1Rvb2wuamF2YQ==) | `87.719% <100.000%> (+19.135%)` | :arrow_up: |; | [...titute/hellbender/engine/cache/LocatableCache.java](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvY2FjaGUvTG9jYXRhYmxlQ2FjaGUuamF2YQ==) | `100.000% <100.000%> (ø)` | |; | ... and [1863 more](https://codecov.io/gh/broadinstitute/gatk/pull/4902/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=broadinstitute) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-397744741
https://github.com/broadinstitute/gatk/pull/4902#issuecomment-439202268:24,Testability,test,testing,24,Rebased this to do some testing with current master.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4902#issuecomment-439202268
https://github.com/broadinstitute/gatk/issues/4903#issuecomment-459443188:84,Testability,test,tests,84,"I think this is still worth pursuing, but we need to do some legwork to set up good tests.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4903#issuecomment-459443188
https://github.com/broadinstitute/gatk/pull/4904#issuecomment-398181642:1583,Deployability,pipeline,pipelines,1583,1092 1092 ; Lines 64239 64239 ; Branches 10350 10350 ; ===============================================; - Hits 51685 51520 -165 ; - Misses 8504 8675 +171 ; + Partials 4050 4044 -6; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4904?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [.../sv/StructuralVariationDiscoveryPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TdHJ1Y3R1cmFsVmFyaWF0aW9uRGlzY292ZXJ5UGlwZWxpbmVTcGFyay5qYXZh) | `88.652% <ø> (ø)` | `13 <0> (ø)` | :arrow_down: |; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-74.257%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-10%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4904#issuecomment-398181642
https://github.com/broadinstitute/gatk/pull/4904#issuecomment-398181642:1899,Testability,test,test,1899,|; |---|---|---|---|; | [.../sv/StructuralVariationDiscoveryPipelineSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9TdHJ1Y3R1cmFsVmFyaWF0aW9uRGlzY292ZXJ5UGlwZWxpbmVTcGFyay5qYXZh) | `88.652% <ø> (ø)` | `13 <0> (ø)` | :arrow_down: |; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-74.257%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-10%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <0%> (-22.807%)` | `0% <0%> (-2%)` | |; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4904#issuecomment-398181642
https://github.com/broadinstitute/gatk/pull/4904#issuecomment-398181642:2766,Testability,test,test,2766,Fyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-10%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <0%> (-22.807%)` | `0% <0%> (-2%)` | |; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L01pbmlDbHVzdGVyVXRpbHMuamF2YQ==) | `78.947% <0%> (-10.526%)` | `6% <0%> (-1%)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `77.083% <0%> (-3.125%)` | `31% <0%> (ø)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `89.394% <0%> (-3.03%)` | `61% <0%> (-2%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4va,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4904#issuecomment-398181642
https://github.com/broadinstitute/gatk/pull/4904#issuecomment-398181642:3655,Testability,test,test,3655,-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-25.806%)` | `30% <0%> (-10%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <0%> (-22.807%)` | `0% <0%> (-2%)` | |; | [...titute/hellbender/utils/test/MiniClusterUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L01pbmlDbHVzdGVyVXRpbHMuamF2YQ==) | `78.947% <0%> (-10.526%)` | `6% <0%> (-1%)` | |; | [...der/engine/spark/datasources/ReadsSparkSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NvdXJjZS5qYXZh) | `77.083% <0%> (-3.125%)` | `31% <0%> (ø)` | |; | [...adinstitute/hellbender/engine/ReadsDataSource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvUmVhZHNEYXRhU291cmNlLmphdmE=) | `89.394% <0%> (-3.03%)` | `61% <0%> (-2%)` | |; | [...broadinstitute/hellbender/utils/test/BaseTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0Jhc2VUZXN0LmphdmE=) | `62.838% <0%> (-2.703%)` | `36% <0%> (-3%)` | |; | ... and [3 more](https://codecov.io/gh/broadinstitute/gatk/pull/4904/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4904#issuecomment-398181642
https://github.com/broadinstitute/gatk/pull/4904#issuecomment-398397571:45,Deployability,update,update,45,"👍 . Looks good, thanks for the documentation update.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4904#issuecomment-398397571
https://github.com/broadinstitute/gatk/issues/4908#issuecomment-398452676:5,Testability,log,logged,5,"I am logged in to an old account, that's why I've got no info! I'll henceforth be contribution from @kohlkopf. The contact information is the same. I need to deactivate this ratty old account.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4908#issuecomment-398452676
https://github.com/broadinstitute/gatk/pull/4915#issuecomment-398549491:51,Testability,test,tests,51,Yeah - the files I needed are all in there and the tests that rely on them pass. Merging...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4915#issuecomment-398549491
https://github.com/broadinstitute/gatk/pull/4916#issuecomment-398593023:961,Testability,test,test,961,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4916?src=pr&el=h1) Report; > Merging [#4916](https://codecov.io/gh/broadinstitute/gatk/pull/4916?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/b225d665f92fad1ab5ffb9d0030be02f29f2bb77?src=pr&el=desc) will **increase** coverage by `0.001%`.; > The diff coverage is `88.889%`. ```diff; @@ Coverage Diff @@; ## master #4916 +/- ##; ===============================================; + Coverage 86.478% 86.479% +0.001% ; - Complexity 29152 29165 +13 ; ===============================================; Files 1813 1814 +1 ; Lines 135227 135305 +78 ; Branches 15020 15023 +3 ; ===============================================; + Hits 116942 117011 +69 ; - Misses 12825 12831 +6 ; - Partials 5460 5463 +3; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4916?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...titute/hellbender/utils/test/ArgumentsBuilder.java](https://codecov.io/gh/broadinstitute/gatk/pull/4916/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0FyZ3VtZW50c0J1aWxkZXIuamF2YQ==) | `90.909% <ø> (ø)` | `20 <0> (ø)` | :arrow_down: |; | [...e/hellbender/engine/ArgumentsBuilderWithJIMFS.java](https://codecov.io/gh/broadinstitute/gatk/pull/4916/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvQXJndW1lbnRzQnVpbGRlcldpdGhKSU1GUy5qYXZh) | `82.5% <82.5%> (ø)` | `8 <8> (?)` | |; | [.../hellbender/engine/CRAMSupportIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4916/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvQ1JBTVN1cHBvcnRJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `92.969% <95.122%> (+0.747%)` | `24 <5> (+5)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4916#issuecomment-398593023
https://github.com/broadinstitute/gatk/pull/4917#issuecomment-398951356:83,Testability,test,test,83,"@jonn-smith I am going to need to discuss with you how to fix the remaining broken test. I believe that the the issue is with the design of the test itself, not the code in this PR. There is an implicit assumption that the MafOutputRenderer will never create a Funcotation, which this PR violates. I believe that we should eliminate this assumption.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4917#issuecomment-398951356
https://github.com/broadinstitute/gatk/pull/4917#issuecomment-398951356:144,Testability,test,test,144,"@jonn-smith I am going to need to discuss with you how to fix the remaining broken test. I believe that the the issue is with the design of the test itself, not the code in this PR. There is an implicit assumption that the MafOutputRenderer will never create a Funcotation, which this PR violates. I believe that we should eliminate this assumption.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4917#issuecomment-398951356
https://github.com/broadinstitute/gatk/pull/4917#issuecomment-399122987:3724,Deployability,update,update,3724,"r&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL21hZk91dHB1dC9NYWZPdXRwdXRSZW5kZXJlckNvbnN0YW50cy5qYXZh) | `99.01% <100%> (+0.04%)` | `1 <0> (ø)` | :arrow_down: |; | [...der/tools/funcotator/metadata/TumorNormalPair.java](https://codecov.io/gh/broadinstitute/gatk/pull/4917/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL21ldGFkYXRhL1R1bW9yTm9ybWFsUGFpci5qYXZh) | `63.63% <63.63%> (ø)` | `5 <5> (?)` | |; | [...cotator/mafOutput/CustomMafFuncotationCreator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4917/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL21hZk91dHB1dC9DdXN0b21NYWZGdW5jb3RhdGlvbkNyZWF0b3IuamF2YQ==) | `90% <90%> (ø)` | `17 <17> (?)` | |; | [...tools/funcotator/metadata/SamplePairExtractor.java](https://codecov.io/gh/broadinstitute/gatk/pull/4917/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL21ldGFkYXRhL1NhbXBsZVBhaXJFeHRyYWN0b3IuamF2YQ==) | `95% <95%> (ø)` | `9 <9> (?)` | |; | [...titute/hellbender/tools/funcotator/Funcotator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4917/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3IuamF2YQ==) | `86.18% <0%> (+0.65%)` | `43% <0%> (+1%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4917?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4917?src=pr&el=footer). Last update [9d8fdac...0e8a045](https://codecov.io/gh/broadinstitute/gatk/pull/4917?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4917#issuecomment-399122987
https://github.com/broadinstitute/gatk/pull/4917#issuecomment-399122987:3627,Energy Efficiency,Power,Powered,3627,"r&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL21hZk91dHB1dC9NYWZPdXRwdXRSZW5kZXJlckNvbnN0YW50cy5qYXZh) | `99.01% <100%> (+0.04%)` | `1 <0> (ø)` | :arrow_down: |; | [...der/tools/funcotator/metadata/TumorNormalPair.java](https://codecov.io/gh/broadinstitute/gatk/pull/4917/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL21ldGFkYXRhL1R1bW9yTm9ybWFsUGFpci5qYXZh) | `63.63% <63.63%> (ø)` | `5 <5> (?)` | |; | [...cotator/mafOutput/CustomMafFuncotationCreator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4917/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL21hZk91dHB1dC9DdXN0b21NYWZGdW5jb3RhdGlvbkNyZWF0b3IuamF2YQ==) | `90% <90%> (ø)` | `17 <17> (?)` | |; | [...tools/funcotator/metadata/SamplePairExtractor.java](https://codecov.io/gh/broadinstitute/gatk/pull/4917/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL21ldGFkYXRhL1NhbXBsZVBhaXJFeHRyYWN0b3IuamF2YQ==) | `95% <95%> (ø)` | `9 <9> (?)` | |; | [...titute/hellbender/tools/funcotator/Funcotator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4917/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3IuamF2YQ==) | `86.18% <0%> (+0.65%)` | `43% <0%> (+1%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4917?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4917?src=pr&el=footer). Last update [9d8fdac...0e8a045](https://codecov.io/gh/broadinstitute/gatk/pull/4917?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4917#issuecomment-399122987
https://github.com/broadinstitute/gatk/pull/4917#issuecomment-399122987:3490,Usability,learn,learn,3490,"r&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL21hZk91dHB1dC9NYWZPdXRwdXRSZW5kZXJlckNvbnN0YW50cy5qYXZh) | `99.01% <100%> (+0.04%)` | `1 <0> (ø)` | :arrow_down: |; | [...der/tools/funcotator/metadata/TumorNormalPair.java](https://codecov.io/gh/broadinstitute/gatk/pull/4917/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL21ldGFkYXRhL1R1bW9yTm9ybWFsUGFpci5qYXZh) | `63.63% <63.63%> (ø)` | `5 <5> (?)` | |; | [...cotator/mafOutput/CustomMafFuncotationCreator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4917/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL21hZk91dHB1dC9DdXN0b21NYWZGdW5jb3RhdGlvbkNyZWF0b3IuamF2YQ==) | `90% <90%> (ø)` | `17 <17> (?)` | |; | [...tools/funcotator/metadata/SamplePairExtractor.java](https://codecov.io/gh/broadinstitute/gatk/pull/4917/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL21ldGFkYXRhL1NhbXBsZVBhaXJFeHRyYWN0b3IuamF2YQ==) | `95% <95%> (ø)` | `9 <9> (?)` | |; | [...titute/hellbender/tools/funcotator/Funcotator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4917/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3IuamF2YQ==) | `86.18% <0%> (+0.65%)` | `43% <0%> (+1%)` | :arrow_up: |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4917?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4917?src=pr&el=footer). Last update [9d8fdac...0e8a045](https://codecov.io/gh/broadinstitute/gatk/pull/4917?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4917#issuecomment-399122987
https://github.com/broadinstitute/gatk/issues/4918#issuecomment-398595262:16,Testability,test,tests,16,And need to add tests?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4918#issuecomment-398595262
https://github.com/broadinstitute/gatk/issues/4919#issuecomment-400757635:20,Usability,learn,learn,20,"Note: this has the ""learn GATK"" label because it is self-contained, but it is not easy.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4919#issuecomment-400757635
https://github.com/broadinstitute/gatk/pull/4926#issuecomment-398931509:1310,Usability,Simpl,SimpleSVD,1310,**decrease** coverage by `21.285%`.; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## master #4926 +/- ##; ================================================; - Coverage 80.435% 59.149% -21.285% ; + Complexity 17792 12568 -5224 ; ================================================; Files 1090 1095 +5 ; Lines 64154 64606 +452 ; Branches 10342 10395 +53 ; ================================================; - Hits 51602 38214 -13388 ; - Misses 8503 22145 +13642 ; - Partials 4049 4247 +198; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4926?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...kers/haplotypecaller/AssemblyBasedCallerUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4926/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9Bc3NlbWJseUJhc2VkQ2FsbGVyVXRpbHMuamF2YQ==) | `63.366% <100%> (-6.931%)` | `22 <0> (-3)` | |; | [...broadinstitute/hellbender/utils/svd/SimpleSVD.java](https://codecov.io/gh/broadinstitute/gatk/pull/4926/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zdmQvU2ltcGxlU1ZELmphdmE=) | `0% <0%> (-100%)` | `0% <0%> (-5%)` | |; | [...ogramgroups/ShortVariantDiscoveryProgramGroup.java](https://codecov.io/gh/broadinstitute/gatk/pull/4926/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL3Byb2dyYW1ncm91cHMvU2hvcnRWYXJpYW50RGlzY292ZXJ5UHJvZ3JhbUdyb3VwLmphdmE=) | `0% <0%> (-100%)` | `0% <0%> (-3%)` | |; | [...adinstitute/hellbender/utils/R/RScriptLibrary.java](https://codecov.io/gh/broadinstitute/gatk/pull/4926/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9SL1JTY3JpcHRMaWJyYXJ5LmphdmE=) | `0% <0%> (-100%)` | `0% <0%> (-6%)` | |; | [...ls/walkers/mutect/M2FiltersArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4926/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYn,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-398931509
https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562:213,Integrability,contract,contract,213,"@DonFreed, I agree with @magicDGS's assessment about it. This feels like a fix that was applied to Gatk3 but doesn't translate to 4? Of course, there could be implementations of GATKRead that don't obey the given contract about copying, but it's worth fixing those since we were more careful to think about copy/no copy when we wrote the new interface. . Of note: if you haven't seen it, `GATKRead` provides a set of unsafe `getBaseQualitiesNoCopy()` methods for times when the copy is a performance bottleneck and you can guarantee safe use of the underlying array. . I'm going to close this. Feel free to reopen if you disagree / can provide a unit test that demonstrates the issue still exists.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562
https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562:342,Integrability,interface,interface,342,"@DonFreed, I agree with @magicDGS's assessment about it. This feels like a fix that was applied to Gatk3 but doesn't translate to 4? Of course, there could be implementations of GATKRead that don't obey the given contract about copying, but it's worth fixing those since we were more careful to think about copy/no copy when we wrote the new interface. . Of note: if you haven't seen it, `GATKRead` provides a set of unsafe `getBaseQualitiesNoCopy()` methods for times when the copy is a performance bottleneck and you can guarantee safe use of the underlying array. . I'm going to close this. Feel free to reopen if you disagree / can provide a unit test that demonstrates the issue still exists.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562
https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562:488,Performance,perform,performance,488,"@DonFreed, I agree with @magicDGS's assessment about it. This feels like a fix that was applied to Gatk3 but doesn't translate to 4? Of course, there could be implementations of GATKRead that don't obey the given contract about copying, but it's worth fixing those since we were more careful to think about copy/no copy when we wrote the new interface. . Of note: if you haven't seen it, `GATKRead` provides a set of unsafe `getBaseQualitiesNoCopy()` methods for times when the copy is a performance bottleneck and you can guarantee safe use of the underlying array. . I'm going to close this. Feel free to reopen if you disagree / can provide a unit test that demonstrates the issue still exists.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562
https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562:500,Performance,bottleneck,bottleneck,500,"@DonFreed, I agree with @magicDGS's assessment about it. This feels like a fix that was applied to Gatk3 but doesn't translate to 4? Of course, there could be implementations of GATKRead that don't obey the given contract about copying, but it's worth fixing those since we were more careful to think about copy/no copy when we wrote the new interface. . Of note: if you haven't seen it, `GATKRead` provides a set of unsafe `getBaseQualitiesNoCopy()` methods for times when the copy is a performance bottleneck and you can guarantee safe use of the underlying array. . I'm going to close this. Feel free to reopen if you disagree / can provide a unit test that demonstrates the issue still exists.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562
https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562:417,Safety,unsafe,unsafe,417,"@DonFreed, I agree with @magicDGS's assessment about it. This feels like a fix that was applied to Gatk3 but doesn't translate to 4? Of course, there could be implementations of GATKRead that don't obey the given contract about copying, but it's worth fixing those since we were more careful to think about copy/no copy when we wrote the new interface. . Of note: if you haven't seen it, `GATKRead` provides a set of unsafe `getBaseQualitiesNoCopy()` methods for times when the copy is a performance bottleneck and you can guarantee safe use of the underlying array. . I'm going to close this. Feel free to reopen if you disagree / can provide a unit test that demonstrates the issue still exists.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562
https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562:533,Safety,safe,safe,533,"@DonFreed, I agree with @magicDGS's assessment about it. This feels like a fix that was applied to Gatk3 but doesn't translate to 4? Of course, there could be implementations of GATKRead that don't obey the given contract about copying, but it's worth fixing those since we were more careful to think about copy/no copy when we wrote the new interface. . Of note: if you haven't seen it, `GATKRead` provides a set of unsafe `getBaseQualitiesNoCopy()` methods for times when the copy is a performance bottleneck and you can guarantee safe use of the underlying array. . I'm going to close this. Feel free to reopen if you disagree / can provide a unit test that demonstrates the issue still exists.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562
https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562:651,Testability,test,test,651,"@DonFreed, I agree with @magicDGS's assessment about it. This feels like a fix that was applied to Gatk3 but doesn't translate to 4? Of course, there could be implementations of GATKRead that don't obey the given contract about copying, but it's worth fixing those since we were more careful to think about copy/no copy when we wrote the new interface. . Of note: if you haven't seen it, `GATKRead` provides a set of unsafe `getBaseQualitiesNoCopy()` methods for times when the copy is a performance bottleneck and you can guarantee safe use of the underlying array. . I'm going to close this. Feel free to reopen if you disagree / can provide a unit test that demonstrates the issue still exists.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-399219562
https://github.com/broadinstitute/gatk/pull/4926#issuecomment-402295512:56,Availability,down,down,56,"@lbergelson Could we reopen this? We were able to track down the source of the modified BQs across active regions. At the end of the function `adjustQualsOfOverlappingPairedFragments(final GATKRead clippedFirstRead, final GATKRead clippedSecondRead)`, the base qualities are set in the clipped reads:; ```; clippedFirstRead.setBaseQualities(firstReadQuals);; clippedSecondRead.setBaseQualities(secondReadQuals);; ```. In some cases, the clipped read is actually the original read, modifying the original read's BQ. I've committed a simple fix to my branch that ensures that the clipped read is never the original read.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-402295512
https://github.com/broadinstitute/gatk/pull/4926#issuecomment-402295512:532,Usability,simpl,simple,532,"@lbergelson Could we reopen this? We were able to track down the source of the modified BQs across active regions. At the end of the function `adjustQualsOfOverlappingPairedFragments(final GATKRead clippedFirstRead, final GATKRead clippedSecondRead)`, the base qualities are set in the clipped reads:; ```; clippedFirstRead.setBaseQualities(firstReadQuals);; clippedSecondRead.setBaseQualities(secondReadQuals);; ```. In some cases, the clipped read is actually the original read, modifying the original read's BQ. I've committed a simple fix to my branch that ensures that the clipped read is never the original read.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-402295512
https://github.com/broadinstitute/gatk/pull/4926#issuecomment-404248365:53,Testability,test,test,53,"@droazen, thanks for the comments. I've added a unit test to demonstrate the issue. The test shows that a shallow copy is fine, so that has been changed as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-404248365
https://github.com/broadinstitute/gatk/pull/4926#issuecomment-404248365:88,Testability,test,test,88,"@droazen, thanks for the comments. I've added a unit test to demonstrate the issue. The test shows that a shallow copy is fine, so that has been changed as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-404248365
https://github.com/broadinstitute/gatk/pull/4926#issuecomment-418507292:25,Deployability,update,updated,25,Thanks @jamesemery! I've updated the PR to address your comments.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4926#issuecomment-418507292
https://github.com/broadinstitute/gatk/pull/4931#issuecomment-399546983:5,Testability,test,tests,5,"Once tests pass, feel free to merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4931#issuecomment-399546983
https://github.com/broadinstitute/gatk/pull/4931#issuecomment-399568778:5,Availability,failure,failure,5,Test failure is false alarm? @cmnbroad,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4931#issuecomment-399568778
https://github.com/broadinstitute/gatk/pull/4931#issuecomment-399568778:0,Testability,Test,Test,0,Test failure is false alarm? @cmnbroad,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4931#issuecomment-399568778
https://github.com/broadinstitute/gatk/pull/4931#issuecomment-399570983:111,Testability,test,test,111,@cmnbroad I am going to restart and see if it goes away. I am only bugging you about it since it is the python test. But my PR goes nowhere near that code.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4931#issuecomment-399570983
https://github.com/broadinstitute/gatk/pull/4931#issuecomment-399577738:80,Testability,test,test,80,@LeeTL1220 Sorry - I didn't see the before it was restarted so I'm unsure which test failed. Looks like its nearly finished.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4931#issuecomment-399577738
https://github.com/broadinstitute/gatk/pull/4931#issuecomment-399580051:221,Testability,test,test,221,"Only the python one... On Fri, Jun 22, 2018 at 4:49 PM, Chris Norman <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Sorry - I didn't see the before; > it was restarted so I'm unsure which test failed. Looks like its nearly; > finished.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4931#issuecomment-399577738>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk7mrwN0gSPmXwZ7rs21ET7LGdoKJks5t_VhVgaJpZM4UynET>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 8011A; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4931#issuecomment-399580051
https://github.com/broadinstitute/gatk/pull/4931#issuecomment-399581084:21,Testability,test,test,21,"Assuming failed push test is transient, since this PR does not affect the python env.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4931#issuecomment-399581084
https://github.com/broadinstitute/gatk/issues/4932#issuecomment-412920923:62,Integrability,message,message,62,It's useful to put something like `fixes #5104` in the commit message. That way it automatically closes the issue and shows a link from the PR to the Issue.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4932#issuecomment-412920923
https://github.com/broadinstitute/gatk/issues/4932#issuecomment-412923593:202,Integrability,message,message,202,"Yep, sorry. Just learned that as you were closing it. On Tue, Aug 14, 2018 at 11:51 AM, Louis Bergelson <notifications@github.com>; wrote:. > It's useful to put something like fixes #5104 in the commit message. That; > way it automatically closes the issue and shows a link from the PR to the; > Issue.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4932#issuecomment-412920923>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AoMkWAqeYbmnX12i6s9k_5bEWy149CPFks5uQvITgaJpZM4UyozK>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4932#issuecomment-412923593
https://github.com/broadinstitute/gatk/issues/4932#issuecomment-412923593:17,Usability,learn,learned,17,"Yep, sorry. Just learned that as you were closing it. On Tue, Aug 14, 2018 at 11:51 AM, Louis Bergelson <notifications@github.com>; wrote:. > It's useful to put something like fixes #5104 in the commit message. That; > way it automatically closes the issue and shows a link from the PR to the; > Issue.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/issues/4932#issuecomment-412920923>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AoMkWAqeYbmnX12i6s9k_5bEWy149CPFks5uQvITgaJpZM4UyozK>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4932#issuecomment-412923593
https://github.com/broadinstitute/gatk/issues/4932#issuecomment-412926174:82,Usability,learn,learn,82,"Ah, got it. By the way, @kvinter1 thanks for being the first person to take on a ""learn GATK"" issue!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4932#issuecomment-412926174
https://github.com/broadinstitute/gatk/issues/4935#issuecomment-399219739:8,Testability,test,testing,8,Do some testing on complete M2 runs.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4935#issuecomment-399219739
https://github.com/broadinstitute/gatk/issues/4938#issuecomment-1304217213:152,Integrability,wrap,wrappers,152,"[Apache Thrift](https://github.com/apache/thrift) looks like a plausible alternative to py4j. It might be possible to automate the generation of thrift wrappers. But, at the heart of any of these systems is essentially an RPC mechanism, not a binary interface, and ill suited to high-frequency function calls.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4938#issuecomment-1304217213
https://github.com/broadinstitute/gatk/issues/4938#issuecomment-1304217213:250,Integrability,interface,interface,250,"[Apache Thrift](https://github.com/apache/thrift) looks like a plausible alternative to py4j. It might be possible to automate the generation of thrift wrappers. But, at the heart of any of these systems is essentially an RPC mechanism, not a binary interface, and ill suited to high-frequency function calls.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4938#issuecomment-1304217213
https://github.com/broadinstitute/gatk/issues/4939#issuecomment-583804417:12,Deployability,integrat,integration,12,"Most of our integration tests use the non-UCSC convention. I would be shocked if that were the issue. If anyone reading this has the issue, feel free to re-open or post on the GATK forum.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4939#issuecomment-583804417
https://github.com/broadinstitute/gatk/issues/4939#issuecomment-583804417:12,Integrability,integrat,integration,12,"Most of our integration tests use the non-UCSC convention. I would be shocked if that were the issue. If anyone reading this has the issue, feel free to re-open or post on the GATK forum.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4939#issuecomment-583804417
https://github.com/broadinstitute/gatk/issues/4939#issuecomment-583804417:24,Testability,test,tests,24,"Most of our integration tests use the non-UCSC convention. I would be shocked if that were the issue. If anyone reading this has the issue, feel free to re-open or post on the GATK forum.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4939#issuecomment-583804417
https://github.com/broadinstitute/gatk/pull/4940#issuecomment-402273063:191,Deployability,update,update,191,"@lbergelson I addressed all your comments. I had some difficulty reconciling the list of Strings for annotation names to remove with the List<Annotation> for the getDefaultVariantAnnotations update, so there may be a few tests failing because the RAW_MQ header disappeared. I'll deal with those when Travis is done, but could you take a look at the other changes?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4940#issuecomment-402273063
https://github.com/broadinstitute/gatk/pull/4940#issuecomment-402273063:221,Testability,test,tests,221,"@lbergelson I addressed all your comments. I had some difficulty reconciling the list of Strings for annotation names to remove with the List<Annotation> for the getDefaultVariantAnnotations update, so there may be a few tests failing because the RAW_MQ header disappeared. I'll deal with those when Travis is done, but could you take a look at the other changes?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4940#issuecomment-402273063
https://github.com/broadinstitute/gatk/pull/4940#issuecomment-415074916:97,Deployability,update,update,97,"@lbergelson this is good to go, unless my other PR gets merged first, in which case I'll need to update the expected test data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4940#issuecomment-415074916
https://github.com/broadinstitute/gatk/pull/4940#issuecomment-415074916:117,Testability,test,test,117,"@lbergelson this is good to go, unless my other PR gets merged first, in which case I'll need to update the expected test data.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4940#issuecomment-415074916
https://github.com/broadinstitute/gatk/pull/4941#issuecomment-402846788:407,Deployability,configurat,configuration,407,"@davidbenjamin ; This will make the WDL default to producing a MAF from Funcotator instead of a VCF. There is no flag to switch between the two, so if you know of people that still want VCF output, please speak up now... Can you review the WDL and autotest-WDL changes? This has been tested in FireCloud and looks good (minus an issue that I have already filed), though I had to manually review. The Method configuration in FireCloud still uses the GATK jar override for this. Just in case you wanted to run it. Otherwise, we should blank out that parameter. As a reminder, I tested:; - mutect2.wdl: manually on local backend and FireCloud; - mutect2_nio.wdl: manually on FireCloud. Please review both WDL files. @jonn-smith Could you review the rest? I.e. the bug fixes. Apologies that I did not split these into two PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4941#issuecomment-402846788
https://github.com/broadinstitute/gatk/pull/4941#issuecomment-402846788:407,Modifiability,config,configuration,407,"@davidbenjamin ; This will make the WDL default to producing a MAF from Funcotator instead of a VCF. There is no flag to switch between the two, so if you know of people that still want VCF output, please speak up now... Can you review the WDL and autotest-WDL changes? This has been tested in FireCloud and looks good (minus an issue that I have already filed), though I had to manually review. The Method configuration in FireCloud still uses the GATK jar override for this. Just in case you wanted to run it. Otherwise, we should blank out that parameter. As a reminder, I tested:; - mutect2.wdl: manually on local backend and FireCloud; - mutect2_nio.wdl: manually on FireCloud. Please review both WDL files. @jonn-smith Could you review the rest? I.e. the bug fixes. Apologies that I did not split these into two PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4941#issuecomment-402846788
https://github.com/broadinstitute/gatk/pull/4941#issuecomment-402846788:284,Testability,test,tested,284,"@davidbenjamin ; This will make the WDL default to producing a MAF from Funcotator instead of a VCF. There is no flag to switch between the two, so if you know of people that still want VCF output, please speak up now... Can you review the WDL and autotest-WDL changes? This has been tested in FireCloud and looks good (minus an issue that I have already filed), though I had to manually review. The Method configuration in FireCloud still uses the GATK jar override for this. Just in case you wanted to run it. Otherwise, we should blank out that parameter. As a reminder, I tested:; - mutect2.wdl: manually on local backend and FireCloud; - mutect2_nio.wdl: manually on FireCloud. Please review both WDL files. @jonn-smith Could you review the rest? I.e. the bug fixes. Apologies that I did not split these into two PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4941#issuecomment-402846788
https://github.com/broadinstitute/gatk/pull/4941#issuecomment-402846788:576,Testability,test,tested,576,"@davidbenjamin ; This will make the WDL default to producing a MAF from Funcotator instead of a VCF. There is no flag to switch between the two, so if you know of people that still want VCF output, please speak up now... Can you review the WDL and autotest-WDL changes? This has been tested in FireCloud and looks good (minus an issue that I have already filed), though I had to manually review. The Method configuration in FireCloud still uses the GATK jar override for this. Just in case you wanted to run it. Otherwise, we should blank out that parameter. As a reminder, I tested:; - mutect2.wdl: manually on local backend and FireCloud; - mutect2_nio.wdl: manually on FireCloud. Please review both WDL files. @jonn-smith Could you review the rest? I.e. the bug fixes. Apologies that I did not split these into two PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4941#issuecomment-402846788
https://github.com/broadinstitute/gatk/pull/4941#issuecomment-402846895:36,Availability,failure,failure,36,@davidbenjamin @jonn-smith The test failure is a false alarm. Please review as if the tests were passing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4941#issuecomment-402846895
https://github.com/broadinstitute/gatk/pull/4941#issuecomment-402846895:31,Testability,test,test,31,@davidbenjamin @jonn-smith The test failure is a false alarm. Please review as if the tests were passing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4941#issuecomment-402846895
https://github.com/broadinstitute/gatk/pull/4941#issuecomment-402846895:86,Testability,test,tests,86,@davidbenjamin @jonn-smith The test failure is a false alarm. Please review as if the tests were passing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4941#issuecomment-402846895
https://github.com/broadinstitute/gatk/issues/4942#issuecomment-447038527:566,Deployability,release,release,566,"I evaluated an extremely naive prototype of this -- it just forced every read to begin threading at the read start -- on Mutect2 and it improves sensitivity and harms precision by small amounts. I'm pretty sure we can get the precision back by instead starting threading a before at the first unique kmer but then threading *backwards* to the beginning of the read. If we start threading forward at a non-unique kmer we're liable to start at the wrong end of the assembly region, and this modification would fix that. I think I'll try to do this in time for the 4.1 release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4942#issuecomment-447038527
https://github.com/broadinstitute/gatk/issues/4943#issuecomment-400065919:26,Testability,log,logger,26,"Ohhh, I need to lower the logger output level from the command line. I thought I remembered that there were two args for effective HC debugging and that's the second one. But the EventMap thing is still true.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4943#issuecomment-400065919
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-400384235:232,Availability,error,error-reference,232,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4947?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@39a9d13`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `6.58%`. ```diff; @@ Coverage Diff @@; ## master #4947 +/- ##; ==========================================; Coverage ? 13.338% ; Complexity ? 6396 ; ==========================================; Files ? 2016 ; Lines ? 151745 ; Branches ? 16269 ; ==========================================; Hits ? 20240 ; Misses ? 129234 ; Partials ? 2271; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4947?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ute/hellbender/utils/variant/GATKVCFConstants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4947/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWQ0ZDb25zdGFudHMuamF2YQ==) | `50% <ø> (ø)` | `2 <0> (?)` | |; | [...iantutils/PosteriorProbabilitiesUtilsUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4947/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9Qb3N0ZXJpb3JQcm9iYWJpbGl0aWVzVXRpbHNVbml0VGVzdC5qYXZh) | `4.386% <ø> (ø)` | `1 <0> (?)` | |; | [...te/hellbender/utils/variant/writers/TLODBlock.java](https://codecov.io/gh/broadinstitute/gatk/pull/4947/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L3dyaXRlcnMvVExPREJsb2NrLmphdmE=) | `0% <ø> (ø)` | `0 <0> (?)` | |; | [...notyper/GenotypeCalculationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4947/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwZUNhbGN1bGF0aW9uQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `100% <ø> (ø)` | `2 <0> (?)` | |; | [...ender/uti,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-400384235
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-400384235:180,Usability,learn,learn,180,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4947?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@39a9d13`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `6.58%`. ```diff; @@ Coverage Diff @@; ## master #4947 +/- ##; ==========================================; Coverage ? 13.338% ; Complexity ? 6396 ; ==========================================; Files ? 2016 ; Lines ? 151745 ; Branches ? 16269 ; ==========================================; Hits ? 20240 ; Misses ? 129234 ; Partials ? 2271; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4947?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ute/hellbender/utils/variant/GATKVCFConstants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4947/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L0dBVEtWQ0ZDb25zdGFudHMuamF2YQ==) | `50% <ø> (ø)` | `2 <0> (?)` | |; | [...iantutils/PosteriorProbabilitiesUtilsUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4947/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9Qb3N0ZXJpb3JQcm9iYWJpbGl0aWVzVXRpbHNVbml0VGVzdC5qYXZh) | `4.386% <ø> (ø)` | `1 <0> (?)` | |; | [...te/hellbender/utils/variant/writers/TLODBlock.java](https://codecov.io/gh/broadinstitute/gatk/pull/4947/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy92YXJpYW50L3dyaXRlcnMvVExPREJsb2NrLmphdmE=) | `0% <ø> (ø)` | `0 <0> (?)` | |; | [...notyper/GenotypeCalculationArgumentCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/4947/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2dlbm90eXBlci9HZW5vdHlwZUNhbGN1bGF0aW9uQXJndW1lbnRDb2xsZWN0aW9uLmphdmE=) | `100% <ø> (ø)` | `2 <0> (?)` | |; | [...ender/uti,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-400384235
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-403224932:6,Modifiability,refactor,refactor,6,TODO: refactor duplicated VC generation code from PosteriorProbabilitiesUtilsUnitTest in ReblockGVCFUnitTest by extracting to VariantContextTestUtils,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-403224932
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-435972625:131,Deployability,release,release,131,@droazen can someone take a look soon? Eric tells me we need to start the CCDG 60K callset next week and I'd rather run that off a release.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-435972625
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-453157113:49,Testability,test,test,49,"@lbergelson I fixed the PL wiggle in the failing test. Think you could review before 4.1? Or maybe @droazen will look again? (Getting it in would be nice, but not critical.)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-453157113
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-469412816:954,Integrability,inject,inject,954,"That should work for both my cases. It could be nice for SelectVariants to; be able to specify whether genotypes should be called or not too. Other; tools might want the sites-only option. On Mon, Mar 4, 2019 at 12:40 PM droazen <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In; > src/main/java/org/broadinstitute/hellbender/tools/genomicsdb/GenomicsDBUtils.java; > <https://github.com/broadinstitute/gatk/pull/4947#discussion_r262167602>:; >; > > @@ -40,7 +40,7 @@; > */; > public static GenomicsDBExportConfiguration.ExportConfiguration createExportConfiguration(final File reference, final String workspace,; > final String callsetJson, final String vidmapJson,; > - final String vcfHeader) {; > + final String vcfHeader, final boolean doGnarlyGenotyping) {; >; > @lbergelson <https://github.com/lbergelson> @ldgauthier; > <https://github.com/ldgauthier> If tools had a way to inject custom GDB; > config (eg., via an overridable method in GATKTool), and the engine used; > this config when creating the Feature Manager on startup, would that solve; > the problem here?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4947#discussion_r262167602>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdOOjGpZBu39mqk7jekA7iOzWDTFrks5vTVqFgaJpZM4U4KK0>; > .; >. -- ; Laura Doyle Gauthier, Ph.D.; Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-469412816
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-469412816:975,Modifiability,config,config,975,"That should work for both my cases. It could be nice for SelectVariants to; be able to specify whether genotypes should be called or not too. Other; tools might want the sites-only option. On Mon, Mar 4, 2019 at 12:40 PM droazen <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In; > src/main/java/org/broadinstitute/hellbender/tools/genomicsdb/GenomicsDBUtils.java; > <https://github.com/broadinstitute/gatk/pull/4947#discussion_r262167602>:; >; > > @@ -40,7 +40,7 @@; > */; > public static GenomicsDBExportConfiguration.ExportConfiguration createExportConfiguration(final File reference, final String workspace,; > final String callsetJson, final String vidmapJson,; > - final String vcfHeader) {; > + final String vcfHeader, final boolean doGnarlyGenotyping) {; >; > @lbergelson <https://github.com/lbergelson> @ldgauthier; > <https://github.com/ldgauthier> If tools had a way to inject custom GDB; > config (eg., via an overridable method in GATKTool), and the engine used; > this config when creating the Feature Manager on startup, would that solve; > the problem here?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4947#discussion_r262167602>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdOOjGpZBu39mqk7jekA7iOzWDTFrks5vTVqFgaJpZM4U4KK0>; > .; >. -- ; Laura Doyle Gauthier, Ph.D.; Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-469412816
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-469412816:1056,Modifiability,config,config,1056,"That should work for both my cases. It could be nice for SelectVariants to; be able to specify whether genotypes should be called or not too. Other; tools might want the sites-only option. On Mon, Mar 4, 2019 at 12:40 PM droazen <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In; > src/main/java/org/broadinstitute/hellbender/tools/genomicsdb/GenomicsDBUtils.java; > <https://github.com/broadinstitute/gatk/pull/4947#discussion_r262167602>:; >; > > @@ -40,7 +40,7 @@; > */; > public static GenomicsDBExportConfiguration.ExportConfiguration createExportConfiguration(final File reference, final String workspace,; > final String callsetJson, final String vidmapJson,; > - final String vcfHeader) {; > + final String vcfHeader, final boolean doGnarlyGenotyping) {; >; > @lbergelson <https://github.com/lbergelson> @ldgauthier; > <https://github.com/ldgauthier> If tools had a way to inject custom GDB; > config (eg., via an overridable method in GATKTool), and the engine used; > this config when creating the Feature Manager on startup, would that solve; > the problem here?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4947#discussion_r262167602>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdOOjGpZBu39mqk7jekA7iOzWDTFrks5vTVqFgaJpZM4U4KK0>; > .; >. -- ; Laura Doyle Gauthier, Ph.D.; Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-469412816
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-469412816:954,Security,inject,inject,954,"That should work for both my cases. It could be nice for SelectVariants to; be able to specify whether genotypes should be called or not too. Other; tools might want the sites-only option. On Mon, Mar 4, 2019 at 12:40 PM droazen <notifications@github.com> wrote:. > *@droazen* commented on this pull request.; > ------------------------------; >; > In; > src/main/java/org/broadinstitute/hellbender/tools/genomicsdb/GenomicsDBUtils.java; > <https://github.com/broadinstitute/gatk/pull/4947#discussion_r262167602>:; >; > > @@ -40,7 +40,7 @@; > */; > public static GenomicsDBExportConfiguration.ExportConfiguration createExportConfiguration(final File reference, final String workspace,; > final String callsetJson, final String vidmapJson,; > - final String vcfHeader) {; > + final String vcfHeader, final boolean doGnarlyGenotyping) {; >; > @lbergelson <https://github.com/lbergelson> @ldgauthier; > <https://github.com/ldgauthier> If tools had a way to inject custom GDB; > config (eg., via an overridable method in GATKTool), and the engine used; > this config when creating the Feature Manager on startup, would that solve; > the problem here?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4947#discussion_r262167602>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdOOjGpZBu39mqk7jekA7iOzWDTFrks5vTVqFgaJpZM4U4KK0>; > .; >. -- ; Laura Doyle Gauthier, Ph.D.; Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-469412816
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-477720872:113,Safety,avoid,avoid,113,"Yes please. Where we left it I think Louis was happy, but we wanted to ask Nalini if; she had any suggestions to avoid threading the argument for genotypes all; the way through the engine. On Thu, Mar 28, 2019 at 11:49 AM droazen <notifications@github.com> wrote:. > What's the status of this one @ldgauthier <https://github.com/ldgauthier>; > ? Do you need a new reviewer with @lbergelson; > <https://github.com/lbergelson> out?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4947#issuecomment-477654500>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdJBn0j6n9Dfp-sz763M3mP5b1oyDks5vbOSHgaJpZM4U4KK0>; > .; >. -- ; Laura Doyle Gauthier, Ph.D.; Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-477720872
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-478089357:114,Safety,avoid,avoid,114,"> Yes please. Where we left it I think Louis was happy, but we wanted to ask Nalini if she had any suggestions to avoid threading the argument for genotypes all the way through the engine. Not sure we can avoid threading the argument for genotypes, but would using GenomicsDBOptions instead work?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-478089357
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-478089357:205,Safety,avoid,avoid,205,"> Yes please. Where we left it I think Louis was happy, but we wanted to ask Nalini if she had any suggestions to avoid threading the argument for genotypes all the way through the engine. Not sure we can avoid threading the argument for genotypes, but would using GenomicsDBOptions instead work?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-478089357
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-495344843:678,Deployability,update,updated,678,"@ldgauthier As discussed in person, could you please pull out a `GnarlyGenotyperEngine` with a `VariantContext finalizeGenotypes(VariantContext)` public entry point, so that we can do gnarly genotyping in BigQuery? You can see the version @jonn-smith wrote here: . https://github.com/broadinstitute/gatk/blob/jts_bigquery_spark_example/src/main/java/org/broadinstitute/hellbender/tools/evoquer/GnarlyGenotyperEngine.java. (But note that the version above does not exactly match the latest version of your code -- it's just an example of the refactoring we'll need in this branch). I think once this is done, and the few comments I add just now are addressed, and this branch is updated to the latest and greatest version of your tool, this can be merged",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-495344843
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-495344843:541,Modifiability,refactor,refactoring,541,"@ldgauthier As discussed in person, could you please pull out a `GnarlyGenotyperEngine` with a `VariantContext finalizeGenotypes(VariantContext)` public entry point, so that we can do gnarly genotyping in BigQuery? You can see the version @jonn-smith wrote here: . https://github.com/broadinstitute/gatk/blob/jts_bigquery_spark_example/src/main/java/org/broadinstitute/hellbender/tools/evoquer/GnarlyGenotyperEngine.java. (But note that the version above does not exactly match the latest version of your code -- it's just an example of the refactoring we'll need in this branch). I think once this is done, and the few comments I add just now are addressed, and this branch is updated to the latest and greatest version of your tool, this can be merged",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-495344843
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-499240904:86,Modifiability,refactor,refactoring,86,"@ldgauthier Let me know once you've had the chance to do that `GnarlyGenotyperEngine` refactoring discussed above, and I'd be happy to give this a (hopefully) final look.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-499240904
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-514690456:88,Modifiability,refactor,refactor,88,"@droazen a few minor tests I missed, but this should be good to go otherwise. I did the refactor for Evoquer and I think everything else has been addressed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-514690456
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-514690456:21,Testability,test,tests,21,"@droazen a few minor tests I missed, but this should be good to go otherwise. I did the refactor for Evoquer and I think everything else has been addressed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-514690456
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-515497964:73,Testability,test,test,73,@ldgauthier Can you move `spanDel.exome.chr20.vcf` (as well as any other test files over about 1-2 MB) into a directory under `src/test/resources/large` so that they'll be stored in `git lfs` rather than check in to the repo?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-515497964
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-515497964:131,Testability,test,test,131,@ldgauthier Can you move `spanDel.exome.chr20.vcf` (as well as any other test files over about 1-2 MB) into a directory under `src/test/resources/large` so that they'll be stored in `git lfs` rather than check in to the repo?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-515497964
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-517350314:596,Availability,down,down,596,"After discussing with @ldgauthier, I'm going to approve this PR as-is, and Laura will address the remaining TODOs in a separate PR. For the record, the three remaining issues that need addressing are:. * Get rid of the `instanceof VariantWalker` check in `FeatureManager` by making `GATKTool.getGenomicsDBOptions()` return null (or `new GenomicsDBOptions(referenceArguments.getReferencePath())`) instead of throwing an exception, and then having `GATKTool.initializeFeatures()` (and its overrides) pass the GenomicsDB options in to the `FeatureManager` constructor, which can then propagate them down here. * Add a simple direct integration test for the new `--floor-blocks` HaplotypeCaller arg. * Address my maintenance concerns about `AnnotationUtils.isAlleleSpecific()` by adding an empty marker interface for AS annotations (open to other ideas here if you don't like that one)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-517350314
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-517350314:709,Availability,mainten,maintenance,709,"After discussing with @ldgauthier, I'm going to approve this PR as-is, and Laura will address the remaining TODOs in a separate PR. For the record, the three remaining issues that need addressing are:. * Get rid of the `instanceof VariantWalker` check in `FeatureManager` by making `GATKTool.getGenomicsDBOptions()` return null (or `new GenomicsDBOptions(referenceArguments.getReferencePath())`) instead of throwing an exception, and then having `GATKTool.initializeFeatures()` (and its overrides) pass the GenomicsDB options in to the `FeatureManager` constructor, which can then propagate them down here. * Add a simple direct integration test for the new `--floor-blocks` HaplotypeCaller arg. * Address my maintenance concerns about `AnnotationUtils.isAlleleSpecific()` by adding an empty marker interface for AS annotations (open to other ideas here if you don't like that one)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-517350314
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-517350314:629,Deployability,integrat,integration,629,"After discussing with @ldgauthier, I'm going to approve this PR as-is, and Laura will address the remaining TODOs in a separate PR. For the record, the three remaining issues that need addressing are:. * Get rid of the `instanceof VariantWalker` check in `FeatureManager` by making `GATKTool.getGenomicsDBOptions()` return null (or `new GenomicsDBOptions(referenceArguments.getReferencePath())`) instead of throwing an exception, and then having `GATKTool.initializeFeatures()` (and its overrides) pass the GenomicsDB options in to the `FeatureManager` constructor, which can then propagate them down here. * Add a simple direct integration test for the new `--floor-blocks` HaplotypeCaller arg. * Address my maintenance concerns about `AnnotationUtils.isAlleleSpecific()` by adding an empty marker interface for AS annotations (open to other ideas here if you don't like that one)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-517350314
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-517350314:629,Integrability,integrat,integration,629,"After discussing with @ldgauthier, I'm going to approve this PR as-is, and Laura will address the remaining TODOs in a separate PR. For the record, the three remaining issues that need addressing are:. * Get rid of the `instanceof VariantWalker` check in `FeatureManager` by making `GATKTool.getGenomicsDBOptions()` return null (or `new GenomicsDBOptions(referenceArguments.getReferencePath())`) instead of throwing an exception, and then having `GATKTool.initializeFeatures()` (and its overrides) pass the GenomicsDB options in to the `FeatureManager` constructor, which can then propagate them down here. * Add a simple direct integration test for the new `--floor-blocks` HaplotypeCaller arg. * Address my maintenance concerns about `AnnotationUtils.isAlleleSpecific()` by adding an empty marker interface for AS annotations (open to other ideas here if you don't like that one)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-517350314
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-517350314:799,Integrability,interface,interface,799,"After discussing with @ldgauthier, I'm going to approve this PR as-is, and Laura will address the remaining TODOs in a separate PR. For the record, the three remaining issues that need addressing are:. * Get rid of the `instanceof VariantWalker` check in `FeatureManager` by making `GATKTool.getGenomicsDBOptions()` return null (or `new GenomicsDBOptions(referenceArguments.getReferencePath())`) instead of throwing an exception, and then having `GATKTool.initializeFeatures()` (and its overrides) pass the GenomicsDB options in to the `FeatureManager` constructor, which can then propagate them down here. * Add a simple direct integration test for the new `--floor-blocks` HaplotypeCaller arg. * Address my maintenance concerns about `AnnotationUtils.isAlleleSpecific()` by adding an empty marker interface for AS annotations (open to other ideas here if you don't like that one)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-517350314
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-517350314:641,Testability,test,test,641,"After discussing with @ldgauthier, I'm going to approve this PR as-is, and Laura will address the remaining TODOs in a separate PR. For the record, the three remaining issues that need addressing are:. * Get rid of the `instanceof VariantWalker` check in `FeatureManager` by making `GATKTool.getGenomicsDBOptions()` return null (or `new GenomicsDBOptions(referenceArguments.getReferencePath())`) instead of throwing an exception, and then having `GATKTool.initializeFeatures()` (and its overrides) pass the GenomicsDB options in to the `FeatureManager` constructor, which can then propagate them down here. * Add a simple direct integration test for the new `--floor-blocks` HaplotypeCaller arg. * Address my maintenance concerns about `AnnotationUtils.isAlleleSpecific()` by adding an empty marker interface for AS annotations (open to other ideas here if you don't like that one)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-517350314
https://github.com/broadinstitute/gatk/pull/4947#issuecomment-517350314:615,Usability,simpl,simple,615,"After discussing with @ldgauthier, I'm going to approve this PR as-is, and Laura will address the remaining TODOs in a separate PR. For the record, the three remaining issues that need addressing are:. * Get rid of the `instanceof VariantWalker` check in `FeatureManager` by making `GATKTool.getGenomicsDBOptions()` return null (or `new GenomicsDBOptions(referenceArguments.getReferencePath())`) instead of throwing an exception, and then having `GATKTool.initializeFeatures()` (and its overrides) pass the GenomicsDB options in to the `FeatureManager` constructor, which can then propagate them down here. * Add a simple direct integration test for the new `--floor-blocks` HaplotypeCaller arg. * Address my maintenance concerns about `AnnotationUtils.isAlleleSpecific()` by adding an empty marker interface for AS annotations (open to other ideas here if you don't like that one)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4947#issuecomment-517350314
https://github.com/broadinstitute/gatk/issues/4948#issuecomment-400417990:62,Deployability,release,release,62,@LeeTL1220 When will Firecloud support Cromwell v33? We can't release an unforked M2 WDL until then.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4948#issuecomment-400417990
https://github.com/broadinstitute/gatk/pull/4949#issuecomment-400469718:51,Deployability,release,release,51,@droazen We had a request to get this in before we release.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4949#issuecomment-400469718
https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400749796:255,Availability,down,down,255,"@lbergelson ; now: 3.9GB (without ~0.3-5 GB extra off from the base image changes currently in the branch); then: 5.22GB; A moderate but not irrelevant reduction in size. With these changes some more drastic changes will be easier, namely trying to slice down the python packages or refactoring the build jars should be easier. I intend to open another PR to upgrade the base image shortly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400749796
https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400749796:359,Deployability,upgrade,upgrade,359,"@lbergelson ; now: 3.9GB (without ~0.3-5 GB extra off from the base image changes currently in the branch); then: 5.22GB; A moderate but not irrelevant reduction in size. With these changes some more drastic changes will be easier, namely trying to slice down the python packages or refactoring the build jars should be easier. I intend to open another PR to upgrade the base image shortly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400749796
https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400749796:283,Modifiability,refactor,refactoring,283,"@lbergelson ; now: 3.9GB (without ~0.3-5 GB extra off from the base image changes currently in the branch); then: 5.22GB; A moderate but not irrelevant reduction in size. With these changes some more drastic changes will be easier, namely trying to slice down the python packages or refactoring the build jars should be easier. I intend to open another PR to upgrade the base image shortly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400749796
https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400789881:963,Testability,Test,TestProgramGroup,963,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4955?src=pr&el=h1) Report; > Merging [#4955](https://codecov.io/gh/broadinstitute/gatk/pull/4955?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/4521d32e6aca6f2e98dc4ce3b7807057cd407998?src=pr&el=desc) will **decrease** coverage by `20.624%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #4955 +/- ##; ================================================; - Coverage 80.797% 60.173% -20.624% ; + Complexity 17952 12779 -5173 ; ================================================; Files 1095 1095 ; Lines 64609 64609 ; Branches 10394 10394 ; ================================================; - Hits 52202 38877 -13325 ; - Misses 8379 21497 +13118 ; - Partials 4028 4235 +207; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4955?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...institute/hellbender/cmdline/TestProgramGroup.java](https://codecov.io/gh/broadinstitute/gatk/pull/4955/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9jbWRsaW5lL1Rlc3RQcm9ncmFtR3JvdXAuamF2YQ==) | `0% <0%> (-100%)` | `0% <0%> (-3%)` | |; | [...er/utils/smithwaterman/SWNativeAlignerWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/4955/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9zbWl0aHdhdGVybWFuL1NXTmF0aXZlQWxpZ25lcldyYXBwZXIuamF2YQ==) | `0% <0%> (-100%)` | `0% <0%> (-6%)` | |; | [...llbender/engine/filters/CountingVariantFilter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4955/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvZmlsdGVycy9Db3VudGluZ1ZhcmlhbnRGaWx0ZXIuamF2YQ==) | `0% <0%> (-100%)` | `0% <0%> (-18%)` | |; | [...ne/programgroups/CoverageAnalysisProgramGroup.java](https://codecov.io/gh/broadinstitute/gatk/pull/4955/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlc,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400789881
https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666:60,Availability,down,down,60,"@droazen correct. ; Generally, one issue is that this slows down the docker image creation in a somewhat substantial way. Around 10 minutes currently. Half of this is unzipping the bundled jar, and another piece is some redundant gradle downloading that can be alleviated with cache shenanigans.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666
https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666:220,Availability,redundant,redundant,220,"@droazen correct. ; Generally, one issue is that this slows down the docker image creation in a somewhat substantial way. Around 10 minutes currently. Half of this is unzipping the bundled jar, and another piece is some redundant gradle downloading that can be alleviated with cache shenanigans.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666
https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666:237,Availability,down,downloading,237,"@droazen correct. ; Generally, one issue is that this slows down the docker image creation in a somewhat substantial way. Around 10 minutes currently. Half of this is unzipping the bundled jar, and another piece is some redundant gradle downloading that can be alleviated with cache shenanigans.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666
https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666:277,Performance,cache,cache,277,"@droazen correct. ; Generally, one issue is that this slows down the docker image creation in a somewhat substantial way. Around 10 minutes currently. Half of this is unzipping the bundled jar, and another piece is some redundant gradle downloading that can be alleviated with cache shenanigans.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666
https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666:220,Safety,redund,redundant,220,"@droazen correct. ; Generally, one issue is that this slows down the docker image creation in a somewhat substantial way. Around 10 minutes currently. Half of this is unzipping the bundled jar, and another piece is some redundant gradle downloading that can be alleviated with cache shenanigans.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4955#issuecomment-400797666
https://github.com/broadinstitute/gatk/pull/4956#issuecomment-400755806:1076,Availability,error,errors,1076,"@ldgauthier Tests are still failing because I haven't updated test vcfs. Before I do so could I get your opinion? Here's a summary of the rationale behind each commit:. * fa34758 When we get to `realignReadToBestHaplotype` we have discarded bases outside of the assembly region but we still have hard clips in the read Cigar. This change forces us to keep those Cigar elements after realigning to the best haplotype so that we know how far past the assembly window the read extended.; * 6af7ad4 After the above change, `BaseQualityRankSum` was liable to look for discarded bases in the hard-clipped part of the read. This fixes that.; * 952d217 The `Clipping` annotation doesn't do anything. It counts the number of hard clips, but pre-this PR there are no hard clips because those bases don't get realigned to the best haplotype and post-this PR the ""hard clips"" are just bases outside the assembly region. As a placeholder I'm setting it to zero (note how this doesn't break any tests!) but really I think we should just get rid of it.; * This PR introduced some off-by-one errors in the depth annotation (but not the ADs). While looking into this I found an apparent bug where some reads that don't overlap a variant get counted in the depth. The issue was that we were counting clipped bases in the overlap. I don't think this is correct because by this point in the code we have unclipped soft clips and gone through local reassembly. Therefore, anything clipped here is a part of the read we truly don't believe belongs anywhere near the assembly region. This change alone breaks the tests with a few off-by-one DP fields.; * 8c51c0a Uses hard clips in the Cigar to correct read position annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4956#issuecomment-400755806
https://github.com/broadinstitute/gatk/pull/4956#issuecomment-400755806:54,Deployability,update,updated,54,"@ldgauthier Tests are still failing because I haven't updated test vcfs. Before I do so could I get your opinion? Here's a summary of the rationale behind each commit:. * fa34758 When we get to `realignReadToBestHaplotype` we have discarded bases outside of the assembly region but we still have hard clips in the read Cigar. This change forces us to keep those Cigar elements after realigning to the best haplotype so that we know how far past the assembly window the read extended.; * 6af7ad4 After the above change, `BaseQualityRankSum` was liable to look for discarded bases in the hard-clipped part of the read. This fixes that.; * 952d217 The `Clipping` annotation doesn't do anything. It counts the number of hard clips, but pre-this PR there are no hard clips because those bases don't get realigned to the best haplotype and post-this PR the ""hard clips"" are just bases outside the assembly region. As a placeholder I'm setting it to zero (note how this doesn't break any tests!) but really I think we should just get rid of it.; * This PR introduced some off-by-one errors in the depth annotation (but not the ADs). While looking into this I found an apparent bug where some reads that don't overlap a variant get counted in the depth. The issue was that we were counting clipped bases in the overlap. I don't think this is correct because by this point in the code we have unclipped soft clips and gone through local reassembly. Therefore, anything clipped here is a part of the read we truly don't believe belongs anywhere near the assembly region. This change alone breaks the tests with a few off-by-one DP fields.; * 8c51c0a Uses hard clips in the Cigar to correct read position annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4956#issuecomment-400755806
https://github.com/broadinstitute/gatk/pull/4956#issuecomment-400755806:474,Modifiability,extend,extended,474,"@ldgauthier Tests are still failing because I haven't updated test vcfs. Before I do so could I get your opinion? Here's a summary of the rationale behind each commit:. * fa34758 When we get to `realignReadToBestHaplotype` we have discarded bases outside of the assembly region but we still have hard clips in the read Cigar. This change forces us to keep those Cigar elements after realigning to the best haplotype so that we know how far past the assembly window the read extended.; * 6af7ad4 After the above change, `BaseQualityRankSum` was liable to look for discarded bases in the hard-clipped part of the read. This fixes that.; * 952d217 The `Clipping` annotation doesn't do anything. It counts the number of hard clips, but pre-this PR there are no hard clips because those bases don't get realigned to the best haplotype and post-this PR the ""hard clips"" are just bases outside the assembly region. As a placeholder I'm setting it to zero (note how this doesn't break any tests!) but really I think we should just get rid of it.; * This PR introduced some off-by-one errors in the depth annotation (but not the ADs). While looking into this I found an apparent bug where some reads that don't overlap a variant get counted in the depth. The issue was that we were counting clipped bases in the overlap. I don't think this is correct because by this point in the code we have unclipped soft clips and gone through local reassembly. Therefore, anything clipped here is a part of the read we truly don't believe belongs anywhere near the assembly region. This change alone breaks the tests with a few off-by-one DP fields.; * 8c51c0a Uses hard clips in the Cigar to correct read position annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4956#issuecomment-400755806
https://github.com/broadinstitute/gatk/pull/4956#issuecomment-400755806:12,Testability,Test,Tests,12,"@ldgauthier Tests are still failing because I haven't updated test vcfs. Before I do so could I get your opinion? Here's a summary of the rationale behind each commit:. * fa34758 When we get to `realignReadToBestHaplotype` we have discarded bases outside of the assembly region but we still have hard clips in the read Cigar. This change forces us to keep those Cigar elements after realigning to the best haplotype so that we know how far past the assembly window the read extended.; * 6af7ad4 After the above change, `BaseQualityRankSum` was liable to look for discarded bases in the hard-clipped part of the read. This fixes that.; * 952d217 The `Clipping` annotation doesn't do anything. It counts the number of hard clips, but pre-this PR there are no hard clips because those bases don't get realigned to the best haplotype and post-this PR the ""hard clips"" are just bases outside the assembly region. As a placeholder I'm setting it to zero (note how this doesn't break any tests!) but really I think we should just get rid of it.; * This PR introduced some off-by-one errors in the depth annotation (but not the ADs). While looking into this I found an apparent bug where some reads that don't overlap a variant get counted in the depth. The issue was that we were counting clipped bases in the overlap. I don't think this is correct because by this point in the code we have unclipped soft clips and gone through local reassembly. Therefore, anything clipped here is a part of the read we truly don't believe belongs anywhere near the assembly region. This change alone breaks the tests with a few off-by-one DP fields.; * 8c51c0a Uses hard clips in the Cigar to correct read position annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4956#issuecomment-400755806
https://github.com/broadinstitute/gatk/pull/4956#issuecomment-400755806:62,Testability,test,test,62,"@ldgauthier Tests are still failing because I haven't updated test vcfs. Before I do so could I get your opinion? Here's a summary of the rationale behind each commit:. * fa34758 When we get to `realignReadToBestHaplotype` we have discarded bases outside of the assembly region but we still have hard clips in the read Cigar. This change forces us to keep those Cigar elements after realigning to the best haplotype so that we know how far past the assembly window the read extended.; * 6af7ad4 After the above change, `BaseQualityRankSum` was liable to look for discarded bases in the hard-clipped part of the read. This fixes that.; * 952d217 The `Clipping` annotation doesn't do anything. It counts the number of hard clips, but pre-this PR there are no hard clips because those bases don't get realigned to the best haplotype and post-this PR the ""hard clips"" are just bases outside the assembly region. As a placeholder I'm setting it to zero (note how this doesn't break any tests!) but really I think we should just get rid of it.; * This PR introduced some off-by-one errors in the depth annotation (but not the ADs). While looking into this I found an apparent bug where some reads that don't overlap a variant get counted in the depth. The issue was that we were counting clipped bases in the overlap. I don't think this is correct because by this point in the code we have unclipped soft clips and gone through local reassembly. Therefore, anything clipped here is a part of the read we truly don't believe belongs anywhere near the assembly region. This change alone breaks the tests with a few off-by-one DP fields.; * 8c51c0a Uses hard clips in the Cigar to correct read position annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4956#issuecomment-400755806
https://github.com/broadinstitute/gatk/pull/4956#issuecomment-400755806:981,Testability,test,tests,981,"@ldgauthier Tests are still failing because I haven't updated test vcfs. Before I do so could I get your opinion? Here's a summary of the rationale behind each commit:. * fa34758 When we get to `realignReadToBestHaplotype` we have discarded bases outside of the assembly region but we still have hard clips in the read Cigar. This change forces us to keep those Cigar elements after realigning to the best haplotype so that we know how far past the assembly window the read extended.; * 6af7ad4 After the above change, `BaseQualityRankSum` was liable to look for discarded bases in the hard-clipped part of the read. This fixes that.; * 952d217 The `Clipping` annotation doesn't do anything. It counts the number of hard clips, but pre-this PR there are no hard clips because those bases don't get realigned to the best haplotype and post-this PR the ""hard clips"" are just bases outside the assembly region. As a placeholder I'm setting it to zero (note how this doesn't break any tests!) but really I think we should just get rid of it.; * This PR introduced some off-by-one errors in the depth annotation (but not the ADs). While looking into this I found an apparent bug where some reads that don't overlap a variant get counted in the depth. The issue was that we were counting clipped bases in the overlap. I don't think this is correct because by this point in the code we have unclipped soft clips and gone through local reassembly. Therefore, anything clipped here is a part of the read we truly don't believe belongs anywhere near the assembly region. This change alone breaks the tests with a few off-by-one DP fields.; * 8c51c0a Uses hard clips in the Cigar to correct read position annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4956#issuecomment-400755806
https://github.com/broadinstitute/gatk/pull/4956#issuecomment-400755806:1590,Testability,test,tests,1590,"@ldgauthier Tests are still failing because I haven't updated test vcfs. Before I do so could I get your opinion? Here's a summary of the rationale behind each commit:. * fa34758 When we get to `realignReadToBestHaplotype` we have discarded bases outside of the assembly region but we still have hard clips in the read Cigar. This change forces us to keep those Cigar elements after realigning to the best haplotype so that we know how far past the assembly window the read extended.; * 6af7ad4 After the above change, `BaseQualityRankSum` was liable to look for discarded bases in the hard-clipped part of the read. This fixes that.; * 952d217 The `Clipping` annotation doesn't do anything. It counts the number of hard clips, but pre-this PR there are no hard clips because those bases don't get realigned to the best haplotype and post-this PR the ""hard clips"" are just bases outside the assembly region. As a placeholder I'm setting it to zero (note how this doesn't break any tests!) but really I think we should just get rid of it.; * This PR introduced some off-by-one errors in the depth annotation (but not the ADs). While looking into this I found an apparent bug where some reads that don't overlap a variant get counted in the depth. The issue was that we were counting clipped bases in the overlap. I don't think this is correct because by this point in the code we have unclipped soft clips and gone through local reassembly. Therefore, anything clipped here is a part of the read we truly don't believe belongs anywhere near the assembly region. This change alone breaks the tests with a few off-by-one DP fields.; * 8c51c0a Uses hard clips in the Cigar to correct read position annotations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4956#issuecomment-400755806
https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400792191:676,Availability,error,error,676,"first of all, this looks like a nice easter-egg to find! congrats!. I agree with you that this is a real problem. I don't understand the logic, why **HALF** of PCR_ERROR_QUAL? if that's really 20, then it's way too low!! . I also object to the second part (when the bases disagree.) imagine that one base is A@Q2 and the other is T@Q60...why would you put both bases to Q0 in that case?. I think that when the bases agree they should essentially add up, but no more than ""PCR_ERROR"" (but not lose q-score if they are already higher then PCR_ERROR). If they differ, the bigger one should lose the points that the lower one has.....though that doesn't take into account the PCR error which needs to be thought out more carefully. We should take some time to figure out the model that allows for PCR error and then derive the posterior posteriors from that....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400792191
https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400792191:797,Availability,error,error,797,"first of all, this looks like a nice easter-egg to find! congrats!. I agree with you that this is a real problem. I don't understand the logic, why **HALF** of PCR_ERROR_QUAL? if that's really 20, then it's way too low!! . I also object to the second part (when the bases disagree.) imagine that one base is A@Q2 and the other is T@Q60...why would you put both bases to Q0 in that case?. I think that when the bases agree they should essentially add up, but no more than ""PCR_ERROR"" (but not lose q-score if they are already higher then PCR_ERROR). If they differ, the bigger one should lose the points that the lower one has.....though that doesn't take into account the PCR error which needs to be thought out more carefully. We should take some time to figure out the model that allows for PCR error and then derive the posterior posteriors from that....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400792191
https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400792191:137,Testability,log,logic,137,"first of all, this looks like a nice easter-egg to find! congrats!. I agree with you that this is a real problem. I don't understand the logic, why **HALF** of PCR_ERROR_QUAL? if that's really 20, then it's way too low!! . I also object to the second part (when the bases disagree.) imagine that one base is A@Q2 and the other is T@Q60...why would you put both bases to Q0 in that case?. I think that when the bases agree they should essentially add up, but no more than ""PCR_ERROR"" (but not lose q-score if they are already higher then PCR_ERROR). If they differ, the bigger one should lose the points that the lower one has.....though that doesn't take into account the PCR error which needs to be thought out more carefully. We should take some time to figure out the model that allows for PCR error and then derive the posterior posteriors from that....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400792191
https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400799135:184,Availability,down,downstream,184,"> I agree with you that this is a real problem. I don't understand the logic, why HALF of PCR_ERROR_QUAL? if that's really 20, then it's way too low!!. The intent of this code is that downstream code will not do anything special to avoid over-counting ; overlapping mates, so that assigning half of the PCR qual effectively gives the full PCR qual for the fragment. Of course, this assumption is wrong in the case of M2. > I also object to the second part (when the bases disagree.) imagine that one base is A@Q2 and the other is T@Q60...why would you put both bases to Q0 in that case?. Good point. > We should take some time to figure out the model that allows for PCR error and then derive the posterior posteriors from that... Are you suggesting something like there are binary indicators for PCR error, read 1 sequencing error, read 2 sequencing error, with priors given by the PCR and base qualities, and we want the posteriors of these indicators given that the bases agree / disagree?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400799135
https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400799135:671,Availability,error,error,671,"> I agree with you that this is a real problem. I don't understand the logic, why HALF of PCR_ERROR_QUAL? if that's really 20, then it's way too low!!. The intent of this code is that downstream code will not do anything special to avoid over-counting ; overlapping mates, so that assigning half of the PCR qual effectively gives the full PCR qual for the fragment. Of course, this assumption is wrong in the case of M2. > I also object to the second part (when the bases disagree.) imagine that one base is A@Q2 and the other is T@Q60...why would you put both bases to Q0 in that case?. Good point. > We should take some time to figure out the model that allows for PCR error and then derive the posterior posteriors from that... Are you suggesting something like there are binary indicators for PCR error, read 1 sequencing error, read 2 sequencing error, with priors given by the PCR and base qualities, and we want the posteriors of these indicators given that the bases agree / disagree?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400799135
https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400799135:801,Availability,error,error,801,"> I agree with you that this is a real problem. I don't understand the logic, why HALF of PCR_ERROR_QUAL? if that's really 20, then it's way too low!!. The intent of this code is that downstream code will not do anything special to avoid over-counting ; overlapping mates, so that assigning half of the PCR qual effectively gives the full PCR qual for the fragment. Of course, this assumption is wrong in the case of M2. > I also object to the second part (when the bases disagree.) imagine that one base is A@Q2 and the other is T@Q60...why would you put both bases to Q0 in that case?. Good point. > We should take some time to figure out the model that allows for PCR error and then derive the posterior posteriors from that... Are you suggesting something like there are binary indicators for PCR error, read 1 sequencing error, read 2 sequencing error, with priors given by the PCR and base qualities, and we want the posteriors of these indicators given that the bases agree / disagree?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400799135
https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400799135:826,Availability,error,error,826,"> I agree with you that this is a real problem. I don't understand the logic, why HALF of PCR_ERROR_QUAL? if that's really 20, then it's way too low!!. The intent of this code is that downstream code will not do anything special to avoid over-counting ; overlapping mates, so that assigning half of the PCR qual effectively gives the full PCR qual for the fragment. Of course, this assumption is wrong in the case of M2. > I also object to the second part (when the bases disagree.) imagine that one base is A@Q2 and the other is T@Q60...why would you put both bases to Q0 in that case?. Good point. > We should take some time to figure out the model that allows for PCR error and then derive the posterior posteriors from that... Are you suggesting something like there are binary indicators for PCR error, read 1 sequencing error, read 2 sequencing error, with priors given by the PCR and base qualities, and we want the posteriors of these indicators given that the bases agree / disagree?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400799135
https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400799135:851,Availability,error,error,851,"> I agree with you that this is a real problem. I don't understand the logic, why HALF of PCR_ERROR_QUAL? if that's really 20, then it's way too low!!. The intent of this code is that downstream code will not do anything special to avoid over-counting ; overlapping mates, so that assigning half of the PCR qual effectively gives the full PCR qual for the fragment. Of course, this assumption is wrong in the case of M2. > I also object to the second part (when the bases disagree.) imagine that one base is A@Q2 and the other is T@Q60...why would you put both bases to Q0 in that case?. Good point. > We should take some time to figure out the model that allows for PCR error and then derive the posterior posteriors from that... Are you suggesting something like there are binary indicators for PCR error, read 1 sequencing error, read 2 sequencing error, with priors given by the PCR and base qualities, and we want the posteriors of these indicators given that the bases agree / disagree?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400799135
https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400799135:232,Safety,avoid,avoid,232,"> I agree with you that this is a real problem. I don't understand the logic, why HALF of PCR_ERROR_QUAL? if that's really 20, then it's way too low!!. The intent of this code is that downstream code will not do anything special to avoid over-counting ; overlapping mates, so that assigning half of the PCR qual effectively gives the full PCR qual for the fragment. Of course, this assumption is wrong in the case of M2. > I also object to the second part (when the bases disagree.) imagine that one base is A@Q2 and the other is T@Q60...why would you put both bases to Q0 in that case?. Good point. > We should take some time to figure out the model that allows for PCR error and then derive the posterior posteriors from that... Are you suggesting something like there are binary indicators for PCR error, read 1 sequencing error, read 2 sequencing error, with priors given by the PCR and base qualities, and we want the posteriors of these indicators given that the bases agree / disagree?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400799135
https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400799135:71,Testability,log,logic,71,"> I agree with you that this is a real problem. I don't understand the logic, why HALF of PCR_ERROR_QUAL? if that's really 20, then it's way too low!!. The intent of this code is that downstream code will not do anything special to avoid over-counting ; overlapping mates, so that assigning half of the PCR qual effectively gives the full PCR qual for the fragment. Of course, this assumption is wrong in the case of M2. > I also object to the second part (when the bases disagree.) imagine that one base is A@Q2 and the other is T@Q60...why would you put both bases to Q0 in that case?. Good point. > We should take some time to figure out the model that allows for PCR error and then derive the posterior posteriors from that... Are you suggesting something like there are binary indicators for PCR error, read 1 sequencing error, read 2 sequencing error, with priors given by the PCR and base qualities, and we want the posteriors of these indicators given that the bases agree / disagree?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400799135
https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400815571:31,Availability,error,error,31,"Yes. We should expect that PCR error shouldn't affect the base-quality, so two high quality, disagreeing bases are an indication of a PCR error, while one low-quality base, and one high quality base that have differing qualities looks more like a sequencing error. We might be able to obtain a data-driven model for that using the overlapping bases themselves (over monomorphic sites). The only problem is that this is only true when the reads haven't been processed by Consensus calling....but if we have a good model for consensus calling within haplotype caller we could avoid doing that upfront and simply deal with everything within haplotype caller. **That** would be ideal!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400815571
https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400815571:138,Availability,error,error,138,"Yes. We should expect that PCR error shouldn't affect the base-quality, so two high quality, disagreeing bases are an indication of a PCR error, while one low-quality base, and one high quality base that have differing qualities looks more like a sequencing error. We might be able to obtain a data-driven model for that using the overlapping bases themselves (over monomorphic sites). The only problem is that this is only true when the reads haven't been processed by Consensus calling....but if we have a good model for consensus calling within haplotype caller we could avoid doing that upfront and simply deal with everything within haplotype caller. **That** would be ideal!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400815571
https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400815571:258,Availability,error,error,258,"Yes. We should expect that PCR error shouldn't affect the base-quality, so two high quality, disagreeing bases are an indication of a PCR error, while one low-quality base, and one high quality base that have differing qualities looks more like a sequencing error. We might be able to obtain a data-driven model for that using the overlapping bases themselves (over monomorphic sites). The only problem is that this is only true when the reads haven't been processed by Consensus calling....but if we have a good model for consensus calling within haplotype caller we could avoid doing that upfront and simply deal with everything within haplotype caller. **That** would be ideal!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400815571
https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400815571:574,Safety,avoid,avoid,574,"Yes. We should expect that PCR error shouldn't affect the base-quality, so two high quality, disagreeing bases are an indication of a PCR error, while one low-quality base, and one high quality base that have differing qualities looks more like a sequencing error. We might be able to obtain a data-driven model for that using the overlapping bases themselves (over monomorphic sites). The only problem is that this is only true when the reads haven't been processed by Consensus calling....but if we have a good model for consensus calling within haplotype caller we could avoid doing that upfront and simply deal with everything within haplotype caller. **That** would be ideal!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400815571
https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400815571:603,Usability,simpl,simply,603,"Yes. We should expect that PCR error shouldn't affect the base-quality, so two high quality, disagreeing bases are an indication of a PCR error, while one low-quality base, and one high quality base that have differing qualities looks more like a sequencing error. We might be able to obtain a data-driven model for that using the overlapping bases themselves (over monomorphic sites). The only problem is that this is only true when the reads haven't been processed by Consensus calling....but if we have a good model for consensus calling within haplotype caller we could avoid doing that upfront and simply deal with everything within haplotype caller. **That** would be ideal!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400815571
https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400965092:408,Testability,log,logic,408,"Time ago, I added a method to fix the qualities of overlapping mates in the same way as `samtools` because I needed it for other reasons: . https://github.com/broadinstitute/gatk/blob/8d929ed89984137fc00009b322074f7a0908d2a5/src/main/java/org/broadinstitute/hellbender/utils/pileup/ReadPileup.java#L310-L349. Maybe you can have a look to it as a starting point. Although I am not sure about which one is the logic for the agreement between bases (sum of qualities) or disagreement (`SAMTOOLS_OVERLAP_LOW_CONFIDENCE` factor), I think that what they are doing is more reasonable the `FragmentUtils.adjustQualsOfOverlappingPairedFragments`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4958#issuecomment-400965092
https://github.com/broadinstitute/gatk/pull/4960#issuecomment-400812242:1267,Modifiability,config,config,1267,b3e464b642?src=pr&el=desc) will **increase** coverage by `0.036%`.; > The diff coverage is `84.211%`. ```diff; @@ Coverage Diff @@; ## master #4960 +/- ##; ==============================================; + Coverage 80.784% 80.82% +0.036% ; - Complexity 17957 17967 +10 ; ==============================================; Files 1095 1095 ; Lines 64587 64600 +13 ; Branches 10392 10394 +2 ; ==============================================; + Hits 52176 52210 +34 ; + Misses 8388 8372 -16 ; + Partials 4023 4018 -5; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4960?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ools/funcotator/FuncotatorArgumentDefinitions.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3JBcmd1bWVudERlZmluaXRpb25zLmphdmE=) | `86.364% <ø> (ø)` | `1 <0> (ø)` | :arrow_down: |; | [...stitute/hellbender/utils/config/ConfigFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb25maWcvQ29uZmlnRmFjdG9yeS5qYXZh) | `77.64% <100%> (+1.242%)` | `45 <0> (ø)` | :arrow_down: |; | [...titute/hellbender/tools/funcotator/Funcotator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3IuamF2YQ==) | `90.556% <81.25%> (+4.927%)` | `53 <7> (+6)` | :arrow_up: |; | [...e/hellbender/tools/funcotator/FuncotatorUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3JVdGlscy5qYXZh) | `80.491% <0%> (+0.546%)` | `170% <0%> (+2%)` | :arrow_up: |; | [...nder/utils/runtime/StreamingProcessController.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960#issuecomment-400812242
https://github.com/broadinstitute/gatk/pull/4960#issuecomment-400812242:1274,Modifiability,Config,ConfigFactory,1274,b3e464b642?src=pr&el=desc) will **increase** coverage by `0.036%`.; > The diff coverage is `84.211%`. ```diff; @@ Coverage Diff @@; ## master #4960 +/- ##; ==============================================; + Coverage 80.784% 80.82% +0.036% ; - Complexity 17957 17967 +10 ; ==============================================; Files 1095 1095 ; Lines 64587 64600 +13 ; Branches 10392 10394 +2 ; ==============================================; + Hits 52176 52210 +34 ; + Misses 8388 8372 -16 ; + Partials 4023 4018 -5; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4960?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...ools/funcotator/FuncotatorArgumentDefinitions.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3JBcmd1bWVudERlZmluaXRpb25zLmphdmE=) | `86.364% <ø> (ø)` | `1 <0> (ø)` | :arrow_down: |; | [...stitute/hellbender/utils/config/ConfigFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9jb25maWcvQ29uZmlnRmFjdG9yeS5qYXZh) | `77.64% <100%> (+1.242%)` | `45 <0> (ø)` | :arrow_down: |; | [...titute/hellbender/tools/funcotator/Funcotator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3IuamF2YQ==) | `90.556% <81.25%> (+4.927%)` | `53 <7> (+6)` | :arrow_up: |; | [...e/hellbender/tools/funcotator/FuncotatorUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9mdW5jb3RhdG9yL0Z1bmNvdGF0b3JVdGlscy5qYXZh) | `80.491% <0%> (+0.546%)` | `170% <0%> (+2%)` | :arrow_up: |; | [...nder/utils/runtime/StreamingProcessController.java](https://codecov.io/gh/broadinstitute/gatk/pull/4960/diff?src=,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960#issuecomment-400812242
https://github.com/broadinstitute/gatk/pull/4960#issuecomment-461937685:90,Deployability,configurat,configuration,90,The Engine Team discussed this internally and we're going to pull out a subset of all the configuration options into the config file. These options should be those that will change only infrequently (like the data sources directory).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960#issuecomment-461937685
https://github.com/broadinstitute/gatk/pull/4960#issuecomment-461937685:90,Modifiability,config,configuration,90,The Engine Team discussed this internally and we're going to pull out a subset of all the configuration options into the config file. These options should be those that will change only infrequently (like the data sources directory).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960#issuecomment-461937685
https://github.com/broadinstitute/gatk/pull/4960#issuecomment-461937685:121,Modifiability,config,config,121,The Engine Team discussed this internally and we're going to pull out a subset of all the configuration options into the config file. These options should be those that will change only infrequently (like the data sources directory).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4960#issuecomment-461937685
https://github.com/broadinstitute/gatk/pull/4962#issuecomment-401084153:1629,Security,validat,validation,1629,ranches 15020 15023 +3 ; ================================================; - Hits 116942 98913 -18029 ; - Misses 12825 31555 +18730 ; + Partials 5460 4761 -699; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4962?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...overy/inference/CpxVariantInterpreterUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4962/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL0NweFZhcmlhbnRJbnRlcnByZXRlclVuaXRUZXN0LmphdmE=) | `100% <100%> (ø)` | `10 <2> (+1)` | :arrow_up: |; | [.../sv/discovery/inference/CpxVariantInterpreter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4962/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL0NweFZhcmlhbnRJbnRlcnByZXRlci5qYXZh) | `74.603% <75%> (-5.236%)` | `26 <0> (ø)` | |; | [.../tools/walkers/validation/FalsePositiveRecord.java](https://codecov.io/gh/broadinstitute/gatk/pull/4962/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vRmFsc2VQb3NpdGl2ZVJlY29yZC5qYXZh) | `0% <0%> (-100%)` | `0% <0%> (-7%)` | |; | [...metrics/multi/ExampleCollectMultiMetricsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4962/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9leGFtcGxlcy9tZXRyaWNzL211bHRpL0V4YW1wbGVDb2xsZWN0TXVsdGlNZXRyaWNzU3BhcmsuamF2YQ==) | `0% <0%> (-100%)` | `0% <0%> (-7%)` | |; | [...sv/discovery/alignment/AlignedContigGenerator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4962/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvYWxpZ25tZW50L0FsaWduZWRDb250aWdHZW5lcmF0b3IuamF2YQ==) | `0% <0%> (-100%)` | `0% <0%> (-1%)` | |; | [...alignmentfilter/RealignmentArgumentCollection.java](https://codecov,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-401084153
https://github.com/broadinstitute/gatk/pull/4962#issuecomment-402765218:229,Availability,down,downstream,229,"Would it make more sense to have a much more stringent cutoff for alignment size after de-overlap? For example, I'm not sure I'd really trust a 5, 7, or even 8 base alignment. Have you experimented with higher values? What's the downstream effect of changing this cutoff that you're proposing here; and would it make sense to make it something much higher, like say 19 to match the minimum BWA-MEM seed length?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-402765218
https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403600389:622,Availability,down,downstream,622,"Thanks for the suggestion!. To answer the questions:; >Would it make more sense to have a much more stringent cutoff for alignment size after de-overlap? . Yes! The filtering step is actually done in the class `AssemblyContigAlignmentsConfigPicker` in the `alignment` package, where the unique read span length filter is defaulted to 10 base. I put it this way so that the contigs won't be ""re-classified"" in `CpxVariantInterpreter` as having a simple chimera and having to be sent back to `SimpleNovelAdjacencyInterpreter`. So, the idea was to separate the concerns of alignment picking from type inference. > What's the downstream effect of changing this cutoff that you're proposing here; and would it make sense to make it something much higher, like say 19 to match the minimum BWA-MEM seed length?. I'll experiment with the new suggested length.; The idea behind settling on this size-10 filter was to be more permissive when it comes to alignment filtering in `AssemblyContigAlignmentsConfigPicker`, and filter variants later in VCF.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403600389
https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403600389:445,Usability,simpl,simple,445,"Thanks for the suggestion!. To answer the questions:; >Would it make more sense to have a much more stringent cutoff for alignment size after de-overlap? . Yes! The filtering step is actually done in the class `AssemblyContigAlignmentsConfigPicker` in the `alignment` package, where the unique read span length filter is defaulted to 10 base. I put it this way so that the contigs won't be ""re-classified"" in `CpxVariantInterpreter` as having a simple chimera and having to be sent back to `SimpleNovelAdjacencyInterpreter`. So, the idea was to separate the concerns of alignment picking from type inference. > What's the downstream effect of changing this cutoff that you're proposing here; and would it make sense to make it something much higher, like say 19 to match the minimum BWA-MEM seed length?. I'll experiment with the new suggested length.; The idea behind settling on this size-10 filter was to be more permissive when it comes to alignment filtering in `AssemblyContigAlignmentsConfigPicker`, and filter variants later in VCF.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403600389
https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403600389:491,Usability,Simpl,SimpleNovelAdjacencyInterpreter,491,"Thanks for the suggestion!. To answer the questions:; >Would it make more sense to have a much more stringent cutoff for alignment size after de-overlap? . Yes! The filtering step is actually done in the class `AssemblyContigAlignmentsConfigPicker` in the `alignment` package, where the unique read span length filter is defaulted to 10 base. I put it this way so that the contigs won't be ""re-classified"" in `CpxVariantInterpreter` as having a simple chimera and having to be sent back to `SimpleNovelAdjacencyInterpreter`. So, the idea was to separate the concerns of alignment picking from type inference. > What's the downstream effect of changing this cutoff that you're proposing here; and would it make sense to make it something much higher, like say 19 to match the minimum BWA-MEM seed length?. I'll experiment with the new suggested length.; The idea behind settling on this size-10 filter was to be more permissive when it comes to alignment filtering in `AssemblyContigAlignmentsConfigPicker`, and filter variants later in VCF.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403600389
https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403890890:257,Deployability,configurat,configuration,257,"Results are in:. Using the branch for PR #4971 with the value `ALIGNMENT_LOW_READ_UNIQUENESS_THRESHOLD` set to 10 and 19, while keeping the gap split children together (that is, method ; `private static GoodAndBadMappings splitGaps(final GoodAndBadMappings configuration, final boolean keepSplitChildrenTogether)` is called with `false` for its second parameter). Here are the comparisons:; ```; simple variants unique TP unique FP; size-10 filter: 10756 24 101; size-19 filter: 10755 1 0; ```. So I think your suggestion is a better trade off!. What I'll do is make that parameter an (advanced) CLI argument in PR #4971 , and experiment more to settle on a good default value.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403890890
https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403890890:257,Modifiability,config,configuration,257,"Results are in:. Using the branch for PR #4971 with the value `ALIGNMENT_LOW_READ_UNIQUENESS_THRESHOLD` set to 10 and 19, while keeping the gap split children together (that is, method ; `private static GoodAndBadMappings splitGaps(final GoodAndBadMappings configuration, final boolean keepSplitChildrenTogether)` is called with `false` for its second parameter). Here are the comparisons:; ```; simple variants unique TP unique FP; size-10 filter: 10756 24 101; size-19 filter: 10755 1 0; ```. So I think your suggestion is a better trade off!. What I'll do is make that parameter an (advanced) CLI argument in PR #4971 , and experiment more to settle on a good default value.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403890890
https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403890890:396,Usability,simpl,simple,396,"Results are in:. Using the branch for PR #4971 with the value `ALIGNMENT_LOW_READ_UNIQUENESS_THRESHOLD` set to 10 and 19, while keeping the gap split children together (that is, method ; `private static GoodAndBadMappings splitGaps(final GoodAndBadMappings configuration, final boolean keepSplitChildrenTogether)` is called with `false` for its second parameter). Here are the comparisons:; ```; simple variants unique TP unique FP; size-10 filter: 10756 24 101; size-19 filter: 10755 1 0; ```. So I think your suggestion is a better trade off!. What I'll do is make that parameter an (advanced) CLI argument in PR #4971 , and experiment more to settle on a good default value.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-403890890
https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405610320:256,Availability,down,downstream,256,"It's OK, I can still look at this. Sorry for the delay. I don't want @TedBrookings to be distracted from trying to wrap up his change set. If I understand correctly, you'd like to keep the lower threshold of 2 here and then apply the more strict threshold downstream, in `AssemblyContigAlignmentsConfigPicker`? In that case, why make it 2 and not 1, ie. not have a limit at all here?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405610320
https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405610320:115,Integrability,wrap,wrap,115,"It's OK, I can still look at this. Sorry for the delay. I don't want @TedBrookings to be distracted from trying to wrap up his change set. If I understand correctly, you'd like to keep the lower threshold of 2 here and then apply the more strict threshold downstream, in `AssemblyContigAlignmentsConfigPicker`? In that case, why make it 2 and not 1, ie. not have a limit at all here?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405610320
https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353:651,Availability,down,downstream,651,"I see. . Yes. That's what I'm planning on (except that `AssemblyContigAlignmentsConfigPicker` is upstream of this unit), and here's the thought for why:; * I'd try to place the alignment picking step in a single place as much as possible, this makes improvements to the alignment picking/filtering step easier; * the size-based filter can be tuned, even by an CLI argument, this would affect the number of segments in the CPX logic, and the alt_arrangment annotations, and the simple variants re-interpreted by `CpxVariantReInterpreterSpark`, but it won't affect the alt haplotype sequence, which IMO is what really is important. ; * I'm developing a downstream variant filter, which hopefully can cut down the false-positives. And for the question of ""why 2 instead of 1"", I think what you are suggesting is to change; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 2;; if (one.getSizeOnRead() >= MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; to; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 1;; if (one.getSizeOnRead() > MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; Am i right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353
https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353:702,Availability,down,down,702,"I see. . Yes. That's what I'm planning on (except that `AssemblyContigAlignmentsConfigPicker` is upstream of this unit), and here's the thought for why:; * I'd try to place the alignment picking step in a single place as much as possible, this makes improvements to the alignment picking/filtering step easier; * the size-based filter can be tuned, even by an CLI argument, this would affect the number of segments in the CPX logic, and the alt_arrangment annotations, and the simple variants re-interpreted by `CpxVariantReInterpreterSpark`, but it won't affect the alt haplotype sequence, which IMO is what really is important. ; * I'm developing a downstream variant filter, which hopefully can cut down the false-positives. And for the question of ""why 2 instead of 1"", I think what you are suggesting is to change; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 2;; if (one.getSizeOnRead() >= MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; to; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 1;; if (one.getSizeOnRead() > MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; Am i right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353
https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353:342,Performance,tune,tuned,342,"I see. . Yes. That's what I'm planning on (except that `AssemblyContigAlignmentsConfigPicker` is upstream of this unit), and here's the thought for why:; * I'd try to place the alignment picking step in a single place as much as possible, this makes improvements to the alignment picking/filtering step easier; * the size-based filter can be tuned, even by an CLI argument, this would affect the number of segments in the CPX logic, and the alt_arrangment annotations, and the simple variants re-interpreted by `CpxVariantReInterpreterSpark`, but it won't affect the alt haplotype sequence, which IMO is what really is important. ; * I'm developing a downstream variant filter, which hopefully can cut down the false-positives. And for the question of ""why 2 instead of 1"", I think what you are suggesting is to change; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 2;; if (one.getSizeOnRead() >= MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; to; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 1;; if (one.getSizeOnRead() > MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; Am i right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353
https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353:426,Testability,log,logic,426,"I see. . Yes. That's what I'm planning on (except that `AssemblyContigAlignmentsConfigPicker` is upstream of this unit), and here's the thought for why:; * I'd try to place the alignment picking step in a single place as much as possible, this makes improvements to the alignment picking/filtering step easier; * the size-based filter can be tuned, even by an CLI argument, this would affect the number of segments in the CPX logic, and the alt_arrangment annotations, and the simple variants re-interpreted by `CpxVariantReInterpreterSpark`, but it won't affect the alt haplotype sequence, which IMO is what really is important. ; * I'm developing a downstream variant filter, which hopefully can cut down the false-positives. And for the question of ""why 2 instead of 1"", I think what you are suggesting is to change; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 2;; if (one.getSizeOnRead() >= MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; to; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 1;; if (one.getSizeOnRead() > MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; Am i right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353
https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353:477,Usability,simpl,simple,477,"I see. . Yes. That's what I'm planning on (except that `AssemblyContigAlignmentsConfigPicker` is upstream of this unit), and here's the thought for why:; * I'd try to place the alignment picking step in a single place as much as possible, this makes improvements to the alignment picking/filtering step easier; * the size-based filter can be tuned, even by an CLI argument, this would affect the number of segments in the CPX logic, and the alt_arrangment annotations, and the simple variants re-interpreted by `CpxVariantReInterpreterSpark`, but it won't affect the alt haplotype sequence, which IMO is what really is important. ; * I'm developing a downstream variant filter, which hopefully can cut down the false-positives. And for the question of ""why 2 instead of 1"", I think what you are suggesting is to change; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 2;; if (one.getSizeOnRead() >= MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; to; ```java; public static final int MIN_READ_SPAN_AFTER_DEOVERLAP = 1;; if (one.getSizeOnRead() > MIN_READ_SPAN_AFTER_DEOVERLAP) result.add(one);; ```; Am i right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-405619353
https://github.com/broadinstitute/gatk/pull/4962#issuecomment-406061907:487,Availability,down,downstream,487,"> Is the 1bp alignment really zero length somehow -- is that the issue?. Yes! In a different sense.; The explanation below is a bit mind-numbing, so please bear with me:. --------. So, to illustrate further, this is the alignment that prompted this PR: [ribbon plot](http://www.genomeribbon.com/?perma=Vg2BPO7Yay). Turning off the filter step in `AssemblyContigAlignmentsConfigPicker` by setting `ALIGNMENT_LOW_READ_UNIQUENESS_THRESHOLD = 0;` (hence the comment _""The aim is to make the downstream logic agnostic to upstream filtering detail""_), the middle alignment--actually only 2 alignment but we do gap-split--is uniquely covering only 1 base. It would generate the following without this PR; ```; chr21	42283722	CPX_chr21:42283722-42284027	C	<CPX>	.	.	ALIGN_LENGTHS=733;ALT_ARRANGEMENT;CTG_NAMES=asm029245:tig00000;END=42284027;HQ_MAPPINGS=1;MAPPING_QUALITIES=60;MAX_ALIGN_LENGTH=733;SEGMENTS=chr21:42283722-42283784,chr21:42283784-42284027;SEQ_ALT_HAPLOTYPE=CAC;SVLEN=-303;SVTYPE=CPX;TOTAL_MAPPINGS=1; ```; with this PR, it would generate:; ```; chr21	42283722	CPX_chr21:42283722-42284027	C	<CPX>	.	.	ALIGN_LENGTHS=733;ALT_ARRANGEMENT=UINS-1;CTG_NAMES=asm029245:tig00000;END=42284027;HQ_MAPPINGS=1;MAPPING_QUALITIES=60;MAX_ALIGN_LENGTH=733;SEGMENTS=chr21:42283722-42284027;SEQ_ALT_HAPLOTYPE=CAC;SVLEN=-303;SVTYPE=CPX;TOTAL_MAPPINGS=1; ```; The differences are in annotations `ALT_ARRANGEMENT` and `SEGMENTS`:; * without the PR, the code hyper-segments the region into 2 segments, with no `ALT_ARRANGEMENT`. This is technically wrong.; * with the PR, the two segmentations are ""merged"" into one, and the `ALT_ARRANGEMENT` annotation correctly says there's one ""unmapped"" base (technically not unmapped, but it would bring in more noise because a single base could map to 1/4 of the genome). And yes, the hyper-segmentation problem is still there for cases where de-overlapped alignments are 2bp long (or very short, 3bp, 5bp, etc.), but in that case the `ALT_ARRANGEMENT` won't be empty. And the",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-406061907
https://github.com/broadinstitute/gatk/pull/4962#issuecomment-406061907:498,Testability,log,logic,498,"> Is the 1bp alignment really zero length somehow -- is that the issue?. Yes! In a different sense.; The explanation below is a bit mind-numbing, so please bear with me:. --------. So, to illustrate further, this is the alignment that prompted this PR: [ribbon plot](http://www.genomeribbon.com/?perma=Vg2BPO7Yay). Turning off the filter step in `AssemblyContigAlignmentsConfigPicker` by setting `ALIGNMENT_LOW_READ_UNIQUENESS_THRESHOLD = 0;` (hence the comment _""The aim is to make the downstream logic agnostic to upstream filtering detail""_), the middle alignment--actually only 2 alignment but we do gap-split--is uniquely covering only 1 base. It would generate the following without this PR; ```; chr21	42283722	CPX_chr21:42283722-42284027	C	<CPX>	.	.	ALIGN_LENGTHS=733;ALT_ARRANGEMENT;CTG_NAMES=asm029245:tig00000;END=42284027;HQ_MAPPINGS=1;MAPPING_QUALITIES=60;MAX_ALIGN_LENGTH=733;SEGMENTS=chr21:42283722-42283784,chr21:42283784-42284027;SEQ_ALT_HAPLOTYPE=CAC;SVLEN=-303;SVTYPE=CPX;TOTAL_MAPPINGS=1; ```; with this PR, it would generate:; ```; chr21	42283722	CPX_chr21:42283722-42284027	C	<CPX>	.	.	ALIGN_LENGTHS=733;ALT_ARRANGEMENT=UINS-1;CTG_NAMES=asm029245:tig00000;END=42284027;HQ_MAPPINGS=1;MAPPING_QUALITIES=60;MAX_ALIGN_LENGTH=733;SEGMENTS=chr21:42283722-42284027;SEQ_ALT_HAPLOTYPE=CAC;SVLEN=-303;SVTYPE=CPX;TOTAL_MAPPINGS=1; ```; The differences are in annotations `ALT_ARRANGEMENT` and `SEGMENTS`:; * without the PR, the code hyper-segments the region into 2 segments, with no `ALT_ARRANGEMENT`. This is technically wrong.; * with the PR, the two segmentations are ""merged"" into one, and the `ALT_ARRANGEMENT` annotation correctly says there's one ""unmapped"" base (technically not unmapped, but it would bring in more noise because a single base could map to 1/4 of the genome). And yes, the hyper-segmentation problem is still there for cases where de-overlapped alignments are 2bp long (or very short, 3bp, 5bp, etc.), but in that case the `ALT_ARRANGEMENT` won't be empty. And the",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4962#issuecomment-406061907
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-402264086:212,Deployability,update,update,212,@davidbenjamin Thanks for your review; I've tried to address your comments but let me know if anything still looks like it could use some work or if you spot anything new. Still waiting for tests to pass on this update (bar the GenomicsDB tests that I expect to fail).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-402264086
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-402264086:190,Testability,test,tests,190,@davidbenjamin Thanks for your review; I've tried to address your comments but let me know if anything still looks like it could use some work or if you spot anything new. Still waiting for tests to pass on this update (bar the GenomicsDB tests that I expect to fail).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-402264086
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-402264086:239,Testability,test,tests,239,@davidbenjamin Thanks for your review; I've tried to address your comments but let me know if anything still looks like it could use some work or if you spot anything new. Still waiting for tests to pass on this update (bar the GenomicsDB tests that I expect to fail).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-402264086
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172:87,Testability,test,tests,87,"I've rebased on the most recent GenomicsDB changes but am still having trouble getting tests to pass. Not sure yet what the solution is but here is the issue for documentation:. With these new changes HaplotypeCaller produces the following GVCF records on our chr20 test data:. ```; 20	10068158	.	GTGTATATATATA	G,<NON_REF>	66.73	.	BaseQRankSum=-0.652;ClippingRankSum=0.000;DP=29;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-0.253	GT:AD:DP:GQ:PL:SB	0/1:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20	10068160	.	GTATATATATATGTA	G,*,<NON_REF>	697.73	.	DP=28;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQ=87005.00	GT:AD:DP:GQ:PL:SB	1/2:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,2,4; ```. If I run this through CombineGVCFs like this:. ```; ./gatk CombineGVCFs -V src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf -O test_gdb_import_combine.g.vcf -R src/test/resources/large/human_g1k_v37.20.21.fasta; ```. The resulting GVCF has these records:. ```; 20 10068158 . GTGTATATATATA G,<NON_REF> . . BaseQRankSum=-6.520e-01;ClippingRankSum=0.00;DP=29;ExcessHet=3.01;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-2.530e-01 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068159 . T *,<NON_REF> . . DP=29 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068160 . GTATATATATATGTA G,*,<NON_REF> . . DP=28;ExcessHet=3.01;RAW_MQ=87005.00 GT:AD:DP:GQ:PL:SB ./.:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,2,4; 20 10068161 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068162 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068163 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068164 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068165 . T *,<NON_R",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172:266,Testability,test,test,266,"I've rebased on the most recent GenomicsDB changes but am still having trouble getting tests to pass. Not sure yet what the solution is but here is the issue for documentation:. With these new changes HaplotypeCaller produces the following GVCF records on our chr20 test data:. ```; 20	10068158	.	GTGTATATATATA	G,<NON_REF>	66.73	.	BaseQRankSum=-0.652;ClippingRankSum=0.000;DP=29;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-0.253	GT:AD:DP:GQ:PL:SB	0/1:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20	10068160	.	GTATATATATATGTA	G,*,<NON_REF>	697.73	.	DP=28;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQ=87005.00	GT:AD:DP:GQ:PL:SB	1/2:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,2,4; ```. If I run this through CombineGVCFs like this:. ```; ./gatk CombineGVCFs -V src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf -O test_gdb_import_combine.g.vcf -R src/test/resources/large/human_g1k_v37.20.21.fasta; ```. The resulting GVCF has these records:. ```; 20 10068158 . GTGTATATATATA G,<NON_REF> . . BaseQRankSum=-6.520e-01;ClippingRankSum=0.00;DP=29;ExcessHet=3.01;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-2.530e-01 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068159 . T *,<NON_REF> . . DP=29 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068160 . GTATATATATATGTA G,*,<NON_REF> . . DP=28;ExcessHet=3.01;RAW_MQ=87005.00 GT:AD:DP:GQ:PL:SB ./.:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,2,4; 20 10068161 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068162 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068163 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068164 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068165 . T *,<NON_R",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172:831,Testability,test,test,831,"I've rebased on the most recent GenomicsDB changes but am still having trouble getting tests to pass. Not sure yet what the solution is but here is the issue for documentation:. With these new changes HaplotypeCaller produces the following GVCF records on our chr20 test data:. ```; 20	10068158	.	GTGTATATATATA	G,<NON_REF>	66.73	.	BaseQRankSum=-0.652;ClippingRankSum=0.000;DP=29;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-0.253	GT:AD:DP:GQ:PL:SB	0/1:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20	10068160	.	GTATATATATATGTA	G,*,<NON_REF>	697.73	.	DP=28;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQ=87005.00	GT:AD:DP:GQ:PL:SB	1/2:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,2,4; ```. If I run this through CombineGVCFs like this:. ```; ./gatk CombineGVCFs -V src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf -O test_gdb_import_combine.g.vcf -R src/test/resources/large/human_g1k_v37.20.21.fasta; ```. The resulting GVCF has these records:. ```; 20 10068158 . GTGTATATATATA G,<NON_REF> . . BaseQRankSum=-6.520e-01;ClippingRankSum=0.00;DP=29;ExcessHet=3.01;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-2.530e-01 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068159 . T *,<NON_REF> . . DP=29 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068160 . GTATATATATATGTA G,*,<NON_REF> . . DP=28;ExcessHet=3.01;RAW_MQ=87005.00 GT:AD:DP:GQ:PL:SB ./.:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,2,4; 20 10068161 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068162 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068163 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068164 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068165 . T *,<NON_R",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172:907,Testability,test,testGVCFMode,907,"I've rebased on the most recent GenomicsDB changes but am still having trouble getting tests to pass. Not sure yet what the solution is but here is the issue for documentation:. With these new changes HaplotypeCaller produces the following GVCF records on our chr20 test data:. ```; 20	10068158	.	GTGTATATATATA	G,<NON_REF>	66.73	.	BaseQRankSum=-0.652;ClippingRankSum=0.000;DP=29;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-0.253	GT:AD:DP:GQ:PL:SB	0/1:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20	10068160	.	GTATATATATATGTA	G,*,<NON_REF>	697.73	.	DP=28;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQ=87005.00	GT:AD:DP:GQ:PL:SB	1/2:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,2,4; ```. If I run this through CombineGVCFs like this:. ```; ./gatk CombineGVCFs -V src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf -O test_gdb_import_combine.g.vcf -R src/test/resources/large/human_g1k_v37.20.21.fasta; ```. The resulting GVCF has these records:. ```; 20 10068158 . GTGTATATATATA G,<NON_REF> . . BaseQRankSum=-6.520e-01;ClippingRankSum=0.00;DP=29;ExcessHet=3.01;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-2.530e-01 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068159 . T *,<NON_REF> . . DP=29 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068160 . GTATATATATATGTA G,*,<NON_REF> . . DP=28;ExcessHet=3.01;RAW_MQ=87005.00 GT:AD:DP:GQ:PL:SB ./.:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,2,4; 20 10068161 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068162 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068163 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068164 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068165 . T *,<NON_R",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172:972,Testability,test,test,972,"I've rebased on the most recent GenomicsDB changes but am still having trouble getting tests to pass. Not sure yet what the solution is but here is the issue for documentation:. With these new changes HaplotypeCaller produces the following GVCF records on our chr20 test data:. ```; 20	10068158	.	GTGTATATATATA	G,<NON_REF>	66.73	.	BaseQRankSum=-0.652;ClippingRankSum=0.000;DP=29;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-0.253	GT:AD:DP:GQ:PL:SB	0/1:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20	10068160	.	GTATATATATATGTA	G,*,<NON_REF>	697.73	.	DP=28;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQ=87005.00	GT:AD:DP:GQ:PL:SB	1/2:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,2,4; ```. If I run this through CombineGVCFs like this:. ```; ./gatk CombineGVCFs -V src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf -O test_gdb_import_combine.g.vcf -R src/test/resources/large/human_g1k_v37.20.21.fasta; ```. The resulting GVCF has these records:. ```; 20 10068158 . GTGTATATATATA G,<NON_REF> . . BaseQRankSum=-6.520e-01;ClippingRankSum=0.00;DP=29;ExcessHet=3.01;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-2.530e-01 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068159 . T *,<NON_REF> . . DP=29 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068160 . GTATATATATATGTA G,*,<NON_REF> . . DP=28;ExcessHet=3.01;RAW_MQ=87005.00 GT:AD:DP:GQ:PL:SB ./.:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,2,4; 20 10068161 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068162 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068163 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068164 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068165 . T *,<NON_R",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172:3194,Testability,test,test,3194," T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068168 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068169 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068170 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068171 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068172 . G *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068173 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068174 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068175 . T <NON_REF> . . . GT:DP:GQ:MIN_DP:PL ./.:20:17:20:0,17,729; ```. When the original GVCF is imported into GenomicsDB and then extracted:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.g.vcf -L 20; ```. It contains the following records in this region:. ```; 20 10068158 . GTGTATATATATA G,<NON_REF> . . BaseQRankSum=-6.520e-01;ClippingRankSum=0.00;DP=29;ExcessHet=3.01;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-2.530e-0; 1 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068159 . T *,<NON_REF> . . DP=29 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068160 . GTATATATATATGTA G,*,<NON_REF> . . DP=28;ExcessHet=3.01;RAW_MQ=87005.00 GT:AD:DP:GQ:PL:SB ./.:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,; 2,4; 20 10068161 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 1006",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172:3254,Testability,test,test,3254,",4; 20 10068168 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068169 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068170 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068171 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068172 . G *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068173 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068174 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068175 . T <NON_REF> . . . GT:DP:GQ:MIN_DP:PL ./.:20:17:20:0,17,729; ```. When the original GVCF is imported into GenomicsDB and then extracted:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.g.vcf -L 20; ```. It contains the following records in this region:. ```; 20 10068158 . GTGTATATATATA G,<NON_REF> . . BaseQRankSum=-6.520e-01;ClippingRankSum=0.00;DP=29;ExcessHet=3.01;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-2.530e-0; 1 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068159 . T *,<NON_REF> . . DP=29 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068160 . GTATATATATATGTA G,*,<NON_REF> . . DP=28;ExcessHet=3.01;RAW_MQ=87005.00 GT:AD:DP:GQ:PL:SB ./.:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,; 2,4; 20 10068161 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068162 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,47",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172:3330,Testability,test,testGVCFMode,3330,"PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068169 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068170 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068171 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068172 . G *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068173 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068174 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068175 . T <NON_REF> . . . GT:DP:GQ:MIN_DP:PL ./.:20:17:20:0,17,729; ```. When the original GVCF is imported into GenomicsDB and then extracted:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.g.vcf -L 20; ```. It contains the following records in this region:. ```; 20 10068158 . GTGTATATATATA G,<NON_REF> . . BaseQRankSum=-6.520e-01;ClippingRankSum=0.00;DP=29;ExcessHet=3.01;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-2.530e-0; 1 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068159 . T *,<NON_REF> . . DP=29 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068160 . GTATATATATATGTA G,*,<NON_REF> . . DP=28;ExcessHet=3.01;RAW_MQ=87005.00 GT:AD:DP:GQ:PL:SB ./.:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,; 2,4; 20 10068161 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068162 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068163 . T *,<NON_REF> . . DP=28 GT:AD",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172:3457,Testability,test,test,3457,"/.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068170 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068171 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068172 . G *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068173 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068174 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068175 . T <NON_REF> . . . GT:DP:GQ:MIN_DP:PL ./.:20:17:20:0,17,729; ```. When the original GVCF is imported into GenomicsDB and then extracted:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.g.vcf -L 20; ```. It contains the following records in this region:. ```; 20 10068158 . GTGTATATATATA G,<NON_REF> . . BaseQRankSum=-6.520e-01;ClippingRankSum=0.00;DP=29;ExcessHet=3.01;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-2.530e-0; 1 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068159 . T *,<NON_REF> . . DP=29 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068160 . GTATATATATATGTA G,*,<NON_REF> . . DP=28;ExcessHet=3.01;RAW_MQ=87005.00 GT:AD:DP:GQ:PL:SB ./.:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,; 2,4; 20 10068161 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068162 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068163 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068164 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172:3507,Testability,test,test,3507," A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068171 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068172 . G *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068173 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068174 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,4,0:6:53:735,102,53,507,108,472:0,0,2,4; 20 10068175 . T <NON_REF> . . . GT:DP:GQ:MIN_DP:PL ./.:20:17:20:0,17,729; ```. When the original GVCF is imported into GenomicsDB and then extracted:. ```; ./gatk GenomicsDBImport -R src/test/resources/large/human_g1k_v37.20.21.fasta -L 20 -V src/test/resources/org/broadinstitute/hellbender/tools/haplotypecaller/expected.testGVCFMode.gatk4.g.vcf -genomicsdb-workspace-path spanDelWorkspace; ./gatk SelectVariants -V gendb://spanDelWorkspace -R src/test/resources/large/human_g1k_v37.20.21.fasta -O test.g.vcf -L 20; ```. It contains the following records in this region:. ```; 20 10068158 . GTGTATATATATA G,<NON_REF> . . BaseQRankSum=-6.520e-01;ClippingRankSum=0.00;DP=29;ExcessHet=3.01;MQRankSum=0.328;RAW_MQ=93364.00;ReadPosRankSum=-2.530e-0; 1 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068159 . T *,<NON_REF> . . DP=29 GT:AD:DP:GQ:PL:SB ./.:3,4,0:7:57:104,0,57,114,69,183:0,3,2,2; 20 10068160 . GTATATATATATGTA G,*,<NON_REF> . . DP=28;ExcessHet=3.01;RAW_MQ=87005.00 GT:AD:DP:GQ:PL:SB ./.:0,2,4,0:6:53:735,162,131,102,0,53,507,174,108,472:0,0,; 2,4; 20 10068161 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068162 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068163 . T *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 10068164 . A *,<NON_REF> . . DP=28 GT:AD:DP:GQ:PL:SB ./.:0,2,0:6:53:735,162,131,507,174,472:0,0,2,4; 20 ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-403519172
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-404310473:159,Testability,log,logic,159,"I'll try to explain my understanding of how spanning deletions and the associated attributes are computed in GenomicsDB - you can let me know what part of the logic I should fix. 1. The spanning deletion allele at 20 : 10068160 corresponds to the deletion at 20 : 10068158; 1. The spanning deletion alleles at positions 20 : 10068160-10068175 correspond to the deletion GTATATATATATGTA -> G at 20 : 10068160.; 1. In GenomicsDB:; 1. ALT allele GTATATATATATGTA -> G at 20 : 10068160 is considered a deletion.; 1. GTATATATATATGTA -> * at 20 : 10068160 is NOT considered a deletion. Should it be considered a deletion ALT allele and be part of the min PL computation?; 1. Hence, GTATATATATATGTA -> G is the deletion ALT allele with min PL value. All the AD and PL values in the subsequent positions correspond to this ALT allele. Let me know if I should fix 3.ii",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-404310473
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-413701822:23,Deployability,release,release,23,"Thanks Laura, the next release will incorporate both your suggestions.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-413701822
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-415848233:95,Deployability,update,update,95,@cwhelan my understanding is that this is blocked because tests won't pass until there's a GDB update -- yes?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-415848233
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-415848233:58,Testability,test,tests,58,@cwhelan my understanding is that this is blocked because tests won't pass until there's a GDB update -- yes?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-415848233
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-415852291:125,Deployability,update,update,125,"@ldgauthier Yes, this was just waiting on some resolution to the double-spanning deletion issue reported above. If a new GDB update makes the output of `GenomicsDBImport` concordant with that of `CombineGVCFs` in terms of the DP and AD's listed above, I think I can update the test files to reflect the new behavior. I still think that the behavior of Combine and GDBI is not exactly right as I mentioned in the last line of my long comment above, but maybe that's something that could be spun out into its own issue to fix independently, as long as we're OK with running with the current CombineGVCFs behavior in the meantime.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-415852291
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-415852291:266,Deployability,update,update,266,"@ldgauthier Yes, this was just waiting on some resolution to the double-spanning deletion issue reported above. If a new GDB update makes the output of `GenomicsDBImport` concordant with that of `CombineGVCFs` in terms of the DP and AD's listed above, I think I can update the test files to reflect the new behavior. I still think that the behavior of Combine and GDBI is not exactly right as I mentioned in the last line of my long comment above, but maybe that's something that could be spun out into its own issue to fix independently, as long as we're OK with running with the current CombineGVCFs behavior in the meantime.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-415852291
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-415852291:277,Testability,test,test,277,"@ldgauthier Yes, this was just waiting on some resolution to the double-spanning deletion issue reported above. If a new GDB update makes the output of `GenomicsDBImport` concordant with that of `CombineGVCFs` in terms of the DP and AD's listed above, I think I can update the test files to reflect the new behavior. I still think that the behavior of Combine and GDBI is not exactly right as I mentioned in the last line of my long comment above, but maybe that's something that could be spun out into its own issue to fix independently, as long as we're OK with running with the current CombineGVCFs behavior in the meantime.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-415852291
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-418369155:17,Deployability,update,updated,17,"@ldgauthier I've updated `GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithNonDiploidData` to use its own expected output VCF rather than running `CombineGVCFS` and comparing to the output of that. Let me know if you want to take one more look, or have someone else do so, before giving this a final thumb.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-418369155
https://github.com/broadinstitute/gatk/pull/4963#issuecomment-418369155:58,Testability,test,testGenomicsDBImportFileInputsAgainstCombineGVCFWithNonDiploidData,58,"@ldgauthier I've updated `GenomicsDBImportIntegrationTest.testGenomicsDBImportFileInputsAgainstCombineGVCFWithNonDiploidData` to use its own expected output VCF rather than running `CombineGVCFS` and comparing to the output of that. Let me know if you want to take one more look, or have someone else do so, before giving this a final thumb.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4963#issuecomment-418369155
https://github.com/broadinstitute/gatk/pull/4964#issuecomment-401423305:714,Modifiability,extend,extend,714,"@magicDGS The problem with exposing the datasources to walkers is that they would be able to invalidate the entire traversal. For example, a `ReadWalker` could alter the traversal intervals on the reads datasource mid-way through traversal from within `apply()`, or it could cause the reads iterator used by the engine to get closed by issuing a separate `iterator()` call on the datasource, which would cause the rest of the traversal to fail. This is why I feel strongly that the datasource objects should not be directly accessible to walker-based tools. Note that it's still possible for walkers to create their own, separate datasources without reaching into the ones used by the engine, or a tool author can extend `GATKTool` directly rather than one of the walker base classes and have the freedom to access everything (which was not possible before this PR).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4964#issuecomment-401423305
https://github.com/broadinstitute/gatk/pull/4964#issuecomment-401423305:524,Security,access,accessible,524,"@magicDGS The problem with exposing the datasources to walkers is that they would be able to invalidate the entire traversal. For example, a `ReadWalker` could alter the traversal intervals on the reads datasource mid-way through traversal from within `apply()`, or it could cause the reads iterator used by the engine to get closed by issuing a separate `iterator()` call on the datasource, which would cause the rest of the traversal to fail. This is why I feel strongly that the datasource objects should not be directly accessible to walker-based tools. Note that it's still possible for walkers to create their own, separate datasources without reaching into the ones used by the engine, or a tool author can extend `GATKTool` directly rather than one of the walker base classes and have the freedom to access everything (which was not possible before this PR).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4964#issuecomment-401423305
https://github.com/broadinstitute/gatk/pull/4964#issuecomment-401423305:808,Security,access,access,808,"@magicDGS The problem with exposing the datasources to walkers is that they would be able to invalidate the entire traversal. For example, a `ReadWalker` could alter the traversal intervals on the reads datasource mid-way through traversal from within `apply()`, or it could cause the reads iterator used by the engine to get closed by issuing a separate `iterator()` call on the datasource, which would cause the rest of the traversal to fail. This is why I feel strongly that the datasource objects should not be directly accessible to walker-based tools. Note that it's still possible for walkers to create their own, separate datasources without reaching into the ones used by the engine, or a tool author can extend `GATKTool` directly rather than one of the walker base classes and have the freedom to access everything (which was not possible before this PR).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4964#issuecomment-401423305
https://github.com/broadinstitute/gatk/pull/4964#issuecomment-404134765:206,Security,expose,exposed,206,"I am aware that those methods should be definetively implemented in the abstract class - but it could be recommended *NOT TO OVERRIDE* unless you know what you are doing. I know the problems of having them exposed, but also I can see some potential to be accessible from an implemented walker when the user knows what is doing...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4964#issuecomment-404134765
https://github.com/broadinstitute/gatk/pull/4964#issuecomment-404134765:255,Security,access,accessible,255,"I am aware that those methods should be definetively implemented in the abstract class - but it could be recommended *NOT TO OVERRIDE* unless you know what you are doing. I know the problems of having them exposed, but also I can see some potential to be accessible from an implemented walker when the user knows what is doing...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4964#issuecomment-404134765
https://github.com/broadinstitute/gatk/issues/4965#issuecomment-413284005:323,Integrability,depend,dependencies,323,"The builds have been broken as a result of the new mechanism for constructing smaller docker images. Namely no longer is the gatk clone and dockerfile sufficient to construct the docker image which has resulted in dockerhubs automated builds feature failing to construct the image successfully. Since there are a number of dependencies that our build has (notably things like java) and the vm dockers build runs is out of our control, a better approach would appear to be constructing the image with a cron job and pushing it to dockerhub manually.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4965#issuecomment-413284005
https://github.com/broadinstitute/gatk/pull/4969#issuecomment-401443363:72,Deployability,release,release,72,@jamesemery Can you take a look? (This doesn't have to go into the next release.),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4969#issuecomment-401443363
https://github.com/broadinstitute/gatk/pull/4969#issuecomment-401457112:4700,Deployability,update,update,4700,"zdC5qYXZh) | `97.82% <100%> (+0.26%)` | `8 <1> (+1)` | :arrow_up: |; | [...er/tools/walkers/GenotypeGVCFsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4969/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0dlbm90eXBlR1ZDRnNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `78.26% <100%> (+2.45%)` | `25 <6> (+6)` | :arrow_up: |; | [...bender/tools/walkers/variantutils/ReblockGVCF.java](https://codecov.io/gh/broadinstitute/gatk/pull/4969/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9SZWJsb2NrR1ZDRi5qYXZh) | `81.52% <100%> (+1.17%)` | `46 <0> (+3)` | :arrow_up: |; | [...der/tools/walkers/annotator/RMSMappingQuality.java](https://codecov.io/gh/broadinstitute/gatk/pull/4969/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9STVNNYXBwaW5nUXVhbGl0eS5qYXZh) | `83.6% <77.35%> (-10.21%)` | `41 <25> (+1)` | |; | [...der/tools/walkers/CombineGVCFsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4969/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0NvbWJpbmVHVkNGc0ludGVncmF0aW9uVGVzdC5qYXZh) | `87.44% <83.33%> (+0.15%)` | `24 <2> (ø)` | :arrow_down: |; | ... and [19 more](https://codecov.io/gh/broadinstitute/gatk/pull/4969/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4969?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4969?src=pr&el=footer). Last update [868a32e...49c474d](https://codecov.io/gh/broadinstitute/gatk/pull/4969?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4969#issuecomment-401457112
https://github.com/broadinstitute/gatk/pull/4969#issuecomment-401457112:4603,Energy Efficiency,Power,Powered,4603,"zdC5qYXZh) | `97.82% <100%> (+0.26%)` | `8 <1> (+1)` | :arrow_up: |; | [...er/tools/walkers/GenotypeGVCFsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4969/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0dlbm90eXBlR1ZDRnNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `78.26% <100%> (+2.45%)` | `25 <6> (+6)` | :arrow_up: |; | [...bender/tools/walkers/variantutils/ReblockGVCF.java](https://codecov.io/gh/broadinstitute/gatk/pull/4969/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9SZWJsb2NrR1ZDRi5qYXZh) | `81.52% <100%> (+1.17%)` | `46 <0> (+3)` | :arrow_up: |; | [...der/tools/walkers/annotator/RMSMappingQuality.java](https://codecov.io/gh/broadinstitute/gatk/pull/4969/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9STVNNYXBwaW5nUXVhbGl0eS5qYXZh) | `83.6% <77.35%> (-10.21%)` | `41 <25> (+1)` | |; | [...der/tools/walkers/CombineGVCFsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4969/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0NvbWJpbmVHVkNGc0ludGVncmF0aW9uVGVzdC5qYXZh) | `87.44% <83.33%> (+0.15%)` | `24 <2> (ø)` | :arrow_down: |; | ... and [19 more](https://codecov.io/gh/broadinstitute/gatk/pull/4969/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4969?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4969?src=pr&el=footer). Last update [868a32e...49c474d](https://codecov.io/gh/broadinstitute/gatk/pull/4969?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4969#issuecomment-401457112
https://github.com/broadinstitute/gatk/pull/4969#issuecomment-401457112:4466,Usability,learn,learn,4466,"zdC5qYXZh) | `97.82% <100%> (+0.26%)` | `8 <1> (+1)` | :arrow_up: |; | [...er/tools/walkers/GenotypeGVCFsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4969/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0dlbm90eXBlR1ZDRnNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `78.26% <100%> (+2.45%)` | `25 <6> (+6)` | :arrow_up: |; | [...bender/tools/walkers/variantutils/ReblockGVCF.java](https://codecov.io/gh/broadinstitute/gatk/pull/4969/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhcmlhbnR1dGlscy9SZWJsb2NrR1ZDRi5qYXZh) | `81.52% <100%> (+1.17%)` | `46 <0> (+3)` | :arrow_up: |; | [...der/tools/walkers/annotator/RMSMappingQuality.java](https://codecov.io/gh/broadinstitute/gatk/pull/4969/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9STVNNYXBwaW5nUXVhbGl0eS5qYXZh) | `83.6% <77.35%> (-10.21%)` | `41 <25> (+1)` | |; | [...der/tools/walkers/CombineGVCFsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4969/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL0NvbWJpbmVHVkNGc0ludGVncmF0aW9uVGVzdC5qYXZh) | `87.44% <83.33%> (+0.15%)` | `24 <2> (ø)` | :arrow_down: |; | ... and [19 more](https://codecov.io/gh/broadinstitute/gatk/pull/4969/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4969?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4969?src=pr&el=footer). Last update [868a32e...49c474d](https://codecov.io/gh/broadinstitute/gatk/pull/4969?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4969#issuecomment-401457112
https://github.com/broadinstitute/gatk/pull/4969#issuecomment-415069583:149,Energy Efficiency,allocate,allocated,149,"The AS_MQ never suffered from this issue because it uses AD for (allele-specific) depth instead of the INFO DP. The sum of the squared MQs there was allocated based on informative reads and the AD represents informative reads, so the data there was always in lock-step.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4969#issuecomment-415069583
https://github.com/broadinstitute/gatk/pull/4969#issuecomment-422509124:75,Availability,error,error,75,I put this into a different branch because I upgraded GDB to fix the weird error. I don't want this feature to go into the 4.0.9.0 release so I'll do a PR of the new branch after.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4969#issuecomment-422509124
https://github.com/broadinstitute/gatk/pull/4969#issuecomment-422509124:45,Deployability,upgrade,upgraded,45,I put this into a different branch because I upgraded GDB to fix the weird error. I don't want this feature to go into the 4.0.9.0 release so I'll do a PR of the new branch after.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4969#issuecomment-422509124
https://github.com/broadinstitute/gatk/pull/4969#issuecomment-422509124:131,Deployability,release,release,131,I put this into a different branch because I upgraded GDB to fix the weird error. I don't want this feature to go into the 4.0.9.0 release so I'll do a PR of the new branch after.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4969#issuecomment-422509124
https://github.com/broadinstitute/gatk/pull/4969#issuecomment-427444343:16,Testability,test,tests,16,"Provided Travis tests pass, does this still have your :+1: @jamesemery ? The ReblockGVCF tool had a less elegant MQ solution so now I'm having it output both versions so that we don't need to re-reprocess the gnomAD v3 GVCFs.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4969#issuecomment-427444343
https://github.com/broadinstitute/gatk/pull/4969#issuecomment-439642813:452,Availability,error,error,452,"Just as a note. This change in line 251 of [`gatk/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/RMSMappingQuality.java`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/RMSMappingQuality.java) of function `parseRawDataString`:. ```; final int squareSum = Integer.parseInt(parsed[SUM_OF_SQUARES_INDEX]);; ```. results in the following type of GVCF/VCF parsing error:. ```; A USER ERROR has occurred: Bad input: malformed RAW_MQ annotation: 3415207168,1749038; ```. when the `RAW_MQ` first index is greater than `Integer.MAX_VALUE`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4969#issuecomment-439642813
https://github.com/broadinstitute/gatk/pull/4969#issuecomment-439642813:472,Availability,ERROR,ERROR,472,"Just as a note. This change in line 251 of [`gatk/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/RMSMappingQuality.java`](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/RMSMappingQuality.java) of function `parseRawDataString`:. ```; final int squareSum = Integer.parseInt(parsed[SUM_OF_SQUARES_INDEX]);; ```. results in the following type of GVCF/VCF parsing error:. ```; A USER ERROR has occurred: Bad input: malformed RAW_MQ annotation: 3415207168,1749038; ```. when the `RAW_MQ` first index is greater than `Integer.MAX_VALUE`.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4969#issuecomment-439642813
https://github.com/broadinstitute/gatk/pull/4970#issuecomment-401503431:3926,Deployability,Integrat,IntegrationTestSpec,3926,..llbender/tools/genomicsdb/GenomicsDBConstants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJDb25zdGFudHMuamF2YQ==) | `0% <0%> (ø)` | `0% <0%> (ø)` | :arrow_down: |; | [...r/tools/walkers/annotator/ClippingRankSumTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9DbGlwcGluZ1JhbmtTdW1UZXN0LmphdmE=) | `100% <0%> (ø)` | `4% <0%> (ø)` | :arrow_down: |; | [...ender/engine/spark/datasources/ReadsSparkSink.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NpbmsuamF2YQ==) | `77.027% <0%> (+0.676%)` | `33% <0%> (+1%)` | :arrow_up: |; | [...spark/sv/evidence/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `69.485% <0%> (+0.825%)` | `61% <0%> (+1%)` | :arrow_up: |; | [...utils/test/ReadsPreprocessingPipelineTestData.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1JlYWRzUHJlcHJvY2Vzc2luZ1BpcGVsaW5lVGVzdERhdGEuamF2YQ==) | `0.862% <0%> (+0.862%)` | `1% <0%> (+1%)` | :arrow_up: |; | [...ute/hellbender/utils/test/IntegrationTestSpec.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0ludGVncmF0aW9uVGVzdFNwZWMuamF2YQ==) | `73.118% <0%> (+1.075%)` | `25% <0%> (ø)` | :arrow_down: |; | ... and [588 more](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4970#issuecomment-401503431
https://github.com/broadinstitute/gatk/pull/4970#issuecomment-401503431:3926,Integrability,Integrat,IntegrationTestSpec,3926,..llbender/tools/genomicsdb/GenomicsDBConstants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJDb25zdGFudHMuamF2YQ==) | `0% <0%> (ø)` | `0% <0%> (ø)` | :arrow_down: |; | [...r/tools/walkers/annotator/ClippingRankSumTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9DbGlwcGluZ1JhbmtTdW1UZXN0LmphdmE=) | `100% <0%> (ø)` | `4% <0%> (ø)` | :arrow_down: |; | [...ender/engine/spark/datasources/ReadsSparkSink.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NpbmsuamF2YQ==) | `77.027% <0%> (+0.676%)` | `33% <0%> (+1%)` | :arrow_up: |; | [...spark/sv/evidence/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `69.485% <0%> (+0.825%)` | `61% <0%> (+1%)` | :arrow_up: |; | [...utils/test/ReadsPreprocessingPipelineTestData.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1JlYWRzUHJlcHJvY2Vzc2luZ1BpcGVsaW5lVGVzdERhdGEuamF2YQ==) | `0.862% <0%> (+0.862%)` | `1% <0%> (+1%)` | :arrow_up: |; | [...ute/hellbender/utils/test/IntegrationTestSpec.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0ludGVncmF0aW9uVGVzdFNwZWMuamF2YQ==) | `73.118% <0%> (+1.075%)` | `25% <0%> (ø)` | :arrow_down: |; | ... and [588 more](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4970#issuecomment-401503431
https://github.com/broadinstitute/gatk/pull/4970#issuecomment-401503431:1658,Security,Validat,ValidateBasicSomaticShortMutations,1658,====================================; + Hits 38890 52186 +13296 ; + Misses 21482 8384 -13098 ; + Partials 4232 4024 -208; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4970?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...nce/SegmentedCpxVariantSimpleVariantExtractor.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1NlZ21lbnRlZENweFZhcmlhbnRTaW1wbGVWYXJpYW50RXh0cmFjdG9yLmphdmE=) | `93.96% <100%> (+8.949%)` | `71 <0> (+5)` | :arrow_up: |; | [.../sv/discovery/inference/CpxVariantInterpreter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL0NweFZhcmlhbnRJbnRlcnByZXRlci5qYXZh) | `79.839% <100%> (+74.921%)` | `26 <0> (+25)` | :arrow_up: |; | [...tmutpileup/ValidateBasicSomaticShortMutations.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `85.965% <0%> (-0.94%)` | `7% <0%> (-6%)` | |; | [.../hellbender/tools/genomicsdb/GenomicsDBImport.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJJbXBvcnQuamF2YQ==) | `75.758% <0%> (-0.591%)` | `53% <0%> (+7%)` | |; | [...llbender/tools/genomicsdb/GenomicsDBConstants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJDb25zdGFudHMuamF2YQ==) | `0% <0%> (ø)` | `0% <0%> (ø)` | :arrow_down: |; | [...r/tools/walkers/annotator/ClippingRankSumTest.java](https://co,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4970#issuecomment-401503431
https://github.com/broadinstitute/gatk/pull/4970#issuecomment-401503431:3581,Testability,test,test,3581,..llbender/tools/genomicsdb/GenomicsDBConstants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJDb25zdGFudHMuamF2YQ==) | `0% <0%> (ø)` | `0% <0%> (ø)` | :arrow_down: |; | [...r/tools/walkers/annotator/ClippingRankSumTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9DbGlwcGluZ1JhbmtTdW1UZXN0LmphdmE=) | `100% <0%> (ø)` | `4% <0%> (ø)` | :arrow_down: |; | [...ender/engine/spark/datasources/ReadsSparkSink.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NpbmsuamF2YQ==) | `77.027% <0%> (+0.676%)` | `33% <0%> (+1%)` | :arrow_up: |; | [...spark/sv/evidence/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `69.485% <0%> (+0.825%)` | `61% <0%> (+1%)` | :arrow_up: |; | [...utils/test/ReadsPreprocessingPipelineTestData.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1JlYWRzUHJlcHJvY2Vzc2luZ1BpcGVsaW5lVGVzdERhdGEuamF2YQ==) | `0.862% <0%> (+0.862%)` | `1% <0%> (+1%)` | :arrow_up: |; | [...ute/hellbender/utils/test/IntegrationTestSpec.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0ludGVncmF0aW9uVGVzdFNwZWMuamF2YQ==) | `73.118% <0%> (+1.075%)` | `25% <0%> (ø)` | :arrow_down: |; | ... and [588 more](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4970#issuecomment-401503431
https://github.com/broadinstitute/gatk/pull/4970#issuecomment-401503431:3921,Testability,test,test,3921,..llbender/tools/genomicsdb/GenomicsDBConstants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJDb25zdGFudHMuamF2YQ==) | `0% <0%> (ø)` | `0% <0%> (ø)` | :arrow_down: |; | [...r/tools/walkers/annotator/ClippingRankSumTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Fubm90YXRvci9DbGlwcGluZ1JhbmtTdW1UZXN0LmphdmE=) | `100% <0%> (ø)` | `4% <0%> (ø)` | :arrow_down: |; | [...ender/engine/spark/datasources/ReadsSparkSink.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvZGF0YXNvdXJjZXMvUmVhZHNTcGFya1NpbmsuamF2YQ==) | `77.027% <0%> (+0.676%)` | `33% <0%> (+1%)` | :arrow_up: |; | [...spark/sv/evidence/FindBreakpointEvidenceSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9ldmlkZW5jZS9GaW5kQnJlYWtwb2ludEV2aWRlbmNlU3BhcmsuamF2YQ==) | `69.485% <0%> (+0.825%)` | `61% <0%> (+1%)` | :arrow_up: |; | [...utils/test/ReadsPreprocessingPipelineTestData.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1JlYWRzUHJlcHJvY2Vzc2luZ1BpcGVsaW5lVGVzdERhdGEuamF2YQ==) | `0.862% <0%> (+0.862%)` | `1% <0%> (+1%)` | :arrow_up: |; | [...ute/hellbender/utils/test/IntegrationTestSpec.java](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L0ludGVncmF0aW9uVGVzdFNwZWMuamF2YQ==) | `73.118% <0%> (+1.075%)` | `25% <0%> (ø)` | :arrow_down: |; | ... and [588 more](https://codecov.io/gh/broadinstitute/gatk/pull/4970/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4970#issuecomment-401503431
https://github.com/broadinstitute/gatk/pull/4971#issuecomment-401503759:952,Testability,Test,TestUtilsForAssemblyBasedSVDiscovery,952,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4971?src=pr&el=h1) Report; > Merging [#4971](https://codecov.io/gh/broadinstitute/gatk/pull/4971?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/e4668126a05727e8b8dac63aa5e223a6fd1b904a?src=pr&el=desc) will **decrease** coverage by `0.157%`.; > The diff coverage is `97.222%`. ```diff; @@ Coverage Diff @@; ## master #4971 +/- ##; ===============================================; - Coverage 86.352% 86.194% -0.157% ; - Complexity 28592 29040 +448 ; ===============================================; Files 1782 1782 ; Lines 132367 133721 +1354 ; Branches 14746 15130 +384 ; ===============================================; + Hits 114301 115260 +959 ; - Misses 12753 13065 +312 ; - Partials 5313 5396 +83; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4971?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...iscovery/TestUtilsForAssemblyBasedSVDiscovery.java](https://codecov.io/gh/broadinstitute/gatk/pull/4971/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvVGVzdFV0aWxzRm9yQXNzZW1ibHlCYXNlZFNWRGlzY292ZXJ5LmphdmE=) | `95.522% <100%> (+10.448%)` | `13 <0> (+1)` | :arrow_up: |; | [...covery/inference/TestUtilsCpxVariantInference.java](https://codecov.io/gh/broadinstitute/gatk/pull/4971/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1Rlc3RVdGlsc0NweFZhcmlhbnRJbmZlcmVuY2UuamF2YQ==) | `99.512% <100%> (-0.021%)` | `7 <1> (ø)` | |; | [...lignment/AssemblyContigAlignmentsConfigPicker.java](https://codecov.io/gh/broadinstitute/gatk/pull/4971/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvYWxpZ25tZW50L0Fzc2VtYmx5Q29udGlnQWxpZ25tZW50c0NvbmZpZ1BpY2tlci5qYXZh) | `93.515% <91.667%> (+0.526%)` | `95 <27> (+8)` | :arrow_up: |; | [.../AssemblyCon,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4971#issuecomment-401503759
https://github.com/broadinstitute/gatk/pull/4971#issuecomment-401503759:1307,Testability,Test,TestUtilsCpxVariantInference,1307,erage by `0.157%`.; > The diff coverage is `97.222%`. ```diff; @@ Coverage Diff @@; ## master #4971 +/- ##; ===============================================; - Coverage 86.352% 86.194% -0.157% ; - Complexity 28592 29040 +448 ; ===============================================; Files 1782 1782 ; Lines 132367 133721 +1354 ; Branches 14746 15130 +384 ; ===============================================; + Hits 114301 115260 +959 ; - Misses 12753 13065 +312 ; - Partials 5313 5396 +83; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4971?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...iscovery/TestUtilsForAssemblyBasedSVDiscovery.java](https://codecov.io/gh/broadinstitute/gatk/pull/4971/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvVGVzdFV0aWxzRm9yQXNzZW1ibHlCYXNlZFNWRGlzY292ZXJ5LmphdmE=) | `95.522% <100%> (+10.448%)` | `13 <0> (+1)` | :arrow_up: |; | [...covery/inference/TestUtilsCpxVariantInference.java](https://codecov.io/gh/broadinstitute/gatk/pull/4971/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1Rlc3RVdGlsc0NweFZhcmlhbnRJbmZlcmVuY2UuamF2YQ==) | `99.512% <100%> (-0.021%)` | `7 <1> (ø)` | |; | [...lignment/AssemblyContigAlignmentsConfigPicker.java](https://codecov.io/gh/broadinstitute/gatk/pull/4971/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvYWxpZ25tZW50L0Fzc2VtYmx5Q29udGlnQWxpZ25tZW50c0NvbmZpZ1BpY2tlci5qYXZh) | `93.515% <91.667%> (+0.526%)` | `95 <27> (+8)` | :arrow_up: |; | [.../AssemblyContigAlignmentsConfigPickerUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4971/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvYWxpZ25tZW50L0Fzc2VtYmx5Q29udGlnQWxpZ25tZW50c0NvbmZpZ1BpY2tlclVuaXRUZXN0LmphdmE=) | `99.605% <99.429%> ,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4971#issuecomment-401503759
https://github.com/broadinstitute/gatk/issues/4975#issuecomment-401872559:165,Deployability,release,releases,165,"@davidbenjamin Can you think of any changes that went in between `4.0.4.0` and `4.0.5.0` that could have caused this? The major changes to newQual between those two releases were in https://github.com/broadinstitute/gatk/pull/4801, I believe.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975#issuecomment-401872559
https://github.com/broadinstitute/gatk/issues/4975#issuecomment-401933028:1452,Availability,down,down,1452,"ssion.so from jar:file:/home/vojta/bin/gatk/gatk-package-4.0.5.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:45:47.648 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:45:47.649 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.5.2; 22:45:47.649 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:45:47.800 INFO GenotypeGVCFs - Initializing engine; 22:45:48.331 INFO FeatureManager - Using codec VCFCodec to read file file:///home/vojta/dokumenty/fakulta/botanika/arabidopsis/samples/lib_2018_06/4_joined/rad34test.comb2.raw.vcf.gz; 22:45:48.467 INFO GenotypeGVCFs - Done initializing engine; 22:45:48.555 INFO ProgressMeter - Starting traversal; 22:45:48.556 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 22:45:51.038 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 22:45:51.045 INFO GenotypeGVCFs - Shutting down engine; [2. července 2018 22:45:51 CEST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=528482304; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 6.911788849595091E-17,NaN; at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.getLog10PNonRef(AlleleFrequencyCalculator.java:143); at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:255); at org.broadinstitute.hellbender.tools.walkers.genotyper.GenotypingEngine.calculateGenotypes(GenotypingEngine.java:210); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.calculateGenotypes(GenotypeGVCFs.java:266); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.regenotypeVC(GenotypeGVCFs.java:222); at org.broadinsti",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975#issuecomment-401933028
https://github.com/broadinstitute/gatk/issues/4975#issuecomment-401933028:3191,Integrability,wrap,wrapAndCopyInto,3191,ine.java:210); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.calculateGenotypes(GenotypeGVCFs.java:266); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.regenotypeVC(GenotypeGVCFs.java:222); at org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs.apply(GenotypeGVCFs.java:201); at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:149); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:984); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hel,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975#issuecomment-401933028
https://github.com/broadinstitute/gatk/issues/4975#issuecomment-401933028:422,Performance,Load,Loading,422,"I see same problem in `4.0.5.2`:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/vojta/bin/gatk/gatk-package-4.0.5.2-local.jar GenotypeGVCFs -O rad34test.comb2.raw.g.vcf.gz -R ../../../jic_reference/alygenomes.fasta -V rad34test.comb2.raw.vcf.gz --new-qual; 22:45:47.050 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/vojta/bin/gatk/gatk-package-4.0.5.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; 22:45:47.648 INFO GenotypeGVCFs - ------------------------------------------------------------; 22:45:47.649 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.5.2; 22:45:47.649 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 22:45:47.800 INFO GenotypeGVCFs - Initializing engine; 22:45:48.331 INFO FeatureManager - Using codec VCFCodec to read file file:///home/vojta/dokumenty/fakulta/botanika/arabidopsis/samples/lib_2018_06/4_joined/rad34test.comb2.raw.vcf.gz; 22:45:48.467 INFO GenotypeGVCFs - Done initializing engine; 22:45:48.555 INFO ProgressMeter - Starting traversal; 22:45:48.556 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 22:45:51.038 WARN InbreedingCoeff - Annotation will not be calculated, must provide at least 10 samples; 22:45:51.045 INFO GenotypeGVCFs - Shutting down engine; [2. července 2018 22:45:51 CEST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=528482304; java.lang.IllegalArgumentException: log10LikelihoodsOfAC are bad 6.911788849595091E-17,NaN; at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AFCalculationResult.<init>(AFCalculationResult.java:72); at org.broadinstitute.hellbender.tools.walkers.genotyper.afcalc.AlleleFrequencyCalculator.getLog10PNonRef(AlleleFrequencyCalculator.java:143); at org.broadinstitute",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975#issuecomment-401933028
https://github.com/broadinstitute/gatk/issues/4975#issuecomment-402175972:60,Availability,error,error,60,"@droazen I think I see how #4801 could introduce a rounding error that creates an extremely small positive log10 probability, which triggers the error. The old code was ; ```; log10PNoVariant += log10GenotypePosteriors[HOM_REF_GENOTYPE_INDEX]; ```. and the new code to handle spanning deletion is; ```; log10PNoVariant += MathUtils.log10SumLog10(nonVariantLog10Posteriors); ```; where `nonVariantLog10Posteriors` includes but the hom ref posterior *and* the posteriors of ref / span del het genotypes. So instead of A, where A is the log 10 hom ref posterior, we have log10(10^A + 10^B), where B is the ref/span del het log10 posterior. This latter quantity should never be positive, but the `log10SumLog10` method it relies on doesn't know that and has finite precision. Given that the problematic number is truly miniscule, `2.559797571100845E-21`, my money is on that explanation. I think a reasonable solution is just to replace it by zero, because we know that's where it comes from. That is, the code should become; ```; log10PNoVariant += Math.min(MathUtils.log10SumLog10(nonVariantLog10Posteriors), 0);; ```. If there is a way for me to debug without having to learn to use GenomicsDB I would like to confirm this myself. Otherwise, @sooheelee can I give you a jar to try out on the tutorial data where you spotted the problem earlier?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975#issuecomment-402175972
https://github.com/broadinstitute/gatk/issues/4975#issuecomment-402175972:145,Availability,error,error,145,"@droazen I think I see how #4801 could introduce a rounding error that creates an extremely small positive log10 probability, which triggers the error. The old code was ; ```; log10PNoVariant += log10GenotypePosteriors[HOM_REF_GENOTYPE_INDEX]; ```. and the new code to handle spanning deletion is; ```; log10PNoVariant += MathUtils.log10SumLog10(nonVariantLog10Posteriors); ```; where `nonVariantLog10Posteriors` includes but the hom ref posterior *and* the posteriors of ref / span del het genotypes. So instead of A, where A is the log 10 hom ref posterior, we have log10(10^A + 10^B), where B is the ref/span del het log10 posterior. This latter quantity should never be positive, but the `log10SumLog10` method it relies on doesn't know that and has finite precision. Given that the problematic number is truly miniscule, `2.559797571100845E-21`, my money is on that explanation. I think a reasonable solution is just to replace it by zero, because we know that's where it comes from. That is, the code should become; ```; log10PNoVariant += Math.min(MathUtils.log10SumLog10(nonVariantLog10Posteriors), 0);; ```. If there is a way for me to debug without having to learn to use GenomicsDB I would like to confirm this myself. Otherwise, @sooheelee can I give you a jar to try out on the tutorial data where you spotted the problem earlier?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975#issuecomment-402175972
https://github.com/broadinstitute/gatk/issues/4975#issuecomment-402175972:534,Testability,log,log,534,"@droazen I think I see how #4801 could introduce a rounding error that creates an extremely small positive log10 probability, which triggers the error. The old code was ; ```; log10PNoVariant += log10GenotypePosteriors[HOM_REF_GENOTYPE_INDEX]; ```. and the new code to handle spanning deletion is; ```; log10PNoVariant += MathUtils.log10SumLog10(nonVariantLog10Posteriors); ```; where `nonVariantLog10Posteriors` includes but the hom ref posterior *and* the posteriors of ref / span del het genotypes. So instead of A, where A is the log 10 hom ref posterior, we have log10(10^A + 10^B), where B is the ref/span del het log10 posterior. This latter quantity should never be positive, but the `log10SumLog10` method it relies on doesn't know that and has finite precision. Given that the problematic number is truly miniscule, `2.559797571100845E-21`, my money is on that explanation. I think a reasonable solution is just to replace it by zero, because we know that's where it comes from. That is, the code should become; ```; log10PNoVariant += Math.min(MathUtils.log10SumLog10(nonVariantLog10Posteriors), 0);; ```. If there is a way for me to debug without having to learn to use GenomicsDB I would like to confirm this myself. Otherwise, @sooheelee can I give you a jar to try out on the tutorial data where you spotted the problem earlier?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975#issuecomment-402175972
https://github.com/broadinstitute/gatk/issues/4975#issuecomment-402175972:1169,Usability,learn,learn,1169,"@droazen I think I see how #4801 could introduce a rounding error that creates an extremely small positive log10 probability, which triggers the error. The old code was ; ```; log10PNoVariant += log10GenotypePosteriors[HOM_REF_GENOTYPE_INDEX]; ```. and the new code to handle spanning deletion is; ```; log10PNoVariant += MathUtils.log10SumLog10(nonVariantLog10Posteriors); ```; where `nonVariantLog10Posteriors` includes but the hom ref posterior *and* the posteriors of ref / span del het genotypes. So instead of A, where A is the log 10 hom ref posterior, we have log10(10^A + 10^B), where B is the ref/span del het log10 posterior. This latter quantity should never be positive, but the `log10SumLog10` method it relies on doesn't know that and has finite precision. Given that the problematic number is truly miniscule, `2.559797571100845E-21`, my money is on that explanation. I think a reasonable solution is just to replace it by zero, because we know that's where it comes from. That is, the code should become; ```; log10PNoVariant += Math.min(MathUtils.log10SumLog10(nonVariantLog10Posteriors), 0);; ```. If there is a way for me to debug without having to learn to use GenomicsDB I would like to confirm this myself. Otherwise, @sooheelee can I give you a jar to try out on the tutorial data where you spotted the problem earlier?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975#issuecomment-402175972
https://github.com/broadinstitute/gatk/issues/4975#issuecomment-402184817:19,Availability,error,error,19,"@davidbenjamin The error is not specific to GenomicsDB. @V-Z got the same error with a VCF as input (see https://github.com/broadinstitute/gatk/issues/4544#issuecomment-401904928), so that should make it slightly easier to replicate.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975#issuecomment-402184817
https://github.com/broadinstitute/gatk/issues/4975#issuecomment-402184817:74,Availability,error,error,74,"@davidbenjamin The error is not specific to GenomicsDB. @V-Z got the same error with a VCF as input (see https://github.com/broadinstitute/gatk/issues/4544#issuecomment-401904928), so that should make it slightly easier to replicate.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975#issuecomment-402184817
https://github.com/broadinstitute/gatk/issues/4976#issuecomment-404949363:1098,Safety,detect,detect,1098,"@LeeTL1220 Having started to implement this. I have a number of design questions that would be informed by your usecases. . Firstly, is there a reason to preserve symbolic alleles? It seems as though spanning deletions could/should be dropped as in most cases there is another variant context representing that deletion elsewhere in your file? Should there be validation around dropping spanning deletion symbolic alleles to ensure we aren't dropping a spanning deletion that isn't represented anywhere else? What about nocalls? . Your example suggests that we rely on the header line counts for subsetting annotations, if there is a disagreement in the header do you want any more sophisticated behavior than just throwing? My understanding is that we are lenient with splitting in htsjdk and there have been some mislabeled header lines in the past that would make this an expected state. Furthermore, most allele specific annotators are of type string because there is no standard for ""|"" delimiters which makes them hard to handle properly. @ldgauthier do you have any suggestions as to how to detect and handle allele specific annotations?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4976#issuecomment-404949363
https://github.com/broadinstitute/gatk/issues/4976#issuecomment-404949363:360,Security,validat,validation,360,"@LeeTL1220 Having started to implement this. I have a number of design questions that would be informed by your usecases. . Firstly, is there a reason to preserve symbolic alleles? It seems as though spanning deletions could/should be dropped as in most cases there is another variant context representing that deletion elsewhere in your file? Should there be validation around dropping spanning deletion symbolic alleles to ensure we aren't dropping a spanning deletion that isn't represented anywhere else? What about nocalls? . Your example suggests that we rely on the header line counts for subsetting annotations, if there is a disagreement in the header do you want any more sophisticated behavior than just throwing? My understanding is that we are lenient with splitting in htsjdk and there have been some mislabeled header lines in the past that would make this an expected state. Furthermore, most allele specific annotators are of type string because there is no standard for ""|"" delimiters which makes them hard to handle properly. @ldgauthier do you have any suggestions as to how to detect and handle allele specific annotations?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4976#issuecomment-404949363
https://github.com/broadinstitute/gatk/pull/4977#issuecomment-401860633:21,Availability,failure,failure,21,@droazen I think the failure is transient,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4977#issuecomment-401860633
https://github.com/broadinstitute/gatk/issues/4978#issuecomment-503219497:148,Modifiability,config,configured,148,A holdover for this is currently in place where we detect if no funcotations were produced at all. In that case we warn the user that they may have configured the reference version and data sources incorrectly.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4978#issuecomment-503219497
https://github.com/broadinstitute/gatk/issues/4978#issuecomment-503219497:51,Safety,detect,detect,51,A holdover for this is currently in place where we detect if no funcotations were produced at all. In that case we warn the user that they may have configured the reference version and data sources incorrectly.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4978#issuecomment-503219497
https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402623148:188,Availability,error,error,188,https://github.com/broadinstitute/gatk/commit/9a4fb6d4e5fd2a226db820a8b4062c139b66e2ef; There is a standard math variant of log(x) for calculating the common log(1+x). It avoids numerical error for small x.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402623148
https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402623148:171,Safety,avoid,avoids,171,https://github.com/broadinstitute/gatk/commit/9a4fb6d4e5fd2a226db820a8b4062c139b66e2ef; There is a standard math variant of log(x) for calculating the common log(1+x). It avoids numerical error for small x.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402623148
https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402623148:124,Testability,log,log,124,https://github.com/broadinstitute/gatk/commit/9a4fb6d4e5fd2a226db820a8b4062c139b66e2ef; There is a standard math variant of log(x) for calculating the common log(1+x). It avoids numerical error for small x.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402623148
https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402623148:158,Testability,log,log,158,https://github.com/broadinstitute/gatk/commit/9a4fb6d4e5fd2a226db820a8b4062c139b66e2ef; There is a standard math variant of log(x) for calculating the common log(1+x). It avoids numerical error for small x.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402623148
https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402746850:1226,Energy Efficiency,efficient,efficient,1226,"@EvanTheB You have a point, because the implementation of `logSumLog` (okay it's log10 but that's ugly so let's pretend we live in a parallel universe where the GATK uses only natural log) is (assume that `B < A` below); ```; log (10^A + 10^B . . .) = A + log(1 + 10^(B - A) . . .); ```; the accuracy of which could certainly be improved via the dedicated `log1p` method, especially when `B << A`. But accuracy alone doesn't necessarily address the issue. I mean, the results we're getting are like 10^-21 instead of 0. If the numerical answer is epsilon, the crucial thing is that epsilon must be negative, not that abs(epsilon) be minimized. Now, one could always truncated the (alternating) Taylor series for `log(1 + 10^(B - A) . . .)` at some fixed order that guarantees it's an underestimate, but first, even if that's the implementation of `log1p` it's brittle to rely on that; and second, it's not guaranteed that the leading term in dominant. Another alternative is instead of calculating the probability of all the non-variant (that is, hom ref but also het ref / span del and hom span del) genotypes add up just the variant genotypes and subtract those probabilities from 1. I hesitate to do this because it's less efficient when there are many alt alleles (there are many more variant genotypes than non-variant). Granted, it's only when span dels are present, but I still don't like it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402746850
https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402746850:59,Testability,log,logSumLog,59,"@EvanTheB You have a point, because the implementation of `logSumLog` (okay it's log10 but that's ugly so let's pretend we live in a parallel universe where the GATK uses only natural log) is (assume that `B < A` below); ```; log (10^A + 10^B . . .) = A + log(1 + 10^(B - A) . . .); ```; the accuracy of which could certainly be improved via the dedicated `log1p` method, especially when `B << A`. But accuracy alone doesn't necessarily address the issue. I mean, the results we're getting are like 10^-21 instead of 0. If the numerical answer is epsilon, the crucial thing is that epsilon must be negative, not that abs(epsilon) be minimized. Now, one could always truncated the (alternating) Taylor series for `log(1 + 10^(B - A) . . .)` at some fixed order that guarantees it's an underestimate, but first, even if that's the implementation of `log1p` it's brittle to rely on that; and second, it's not guaranteed that the leading term in dominant. Another alternative is instead of calculating the probability of all the non-variant (that is, hom ref but also het ref / span del and hom span del) genotypes add up just the variant genotypes and subtract those probabilities from 1. I hesitate to do this because it's less efficient when there are many alt alleles (there are many more variant genotypes than non-variant). Granted, it's only when span dels are present, but I still don't like it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402746850
https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402746850:184,Testability,log,log,184,"@EvanTheB You have a point, because the implementation of `logSumLog` (okay it's log10 but that's ugly so let's pretend we live in a parallel universe where the GATK uses only natural log) is (assume that `B < A` below); ```; log (10^A + 10^B . . .) = A + log(1 + 10^(B - A) . . .); ```; the accuracy of which could certainly be improved via the dedicated `log1p` method, especially when `B << A`. But accuracy alone doesn't necessarily address the issue. I mean, the results we're getting are like 10^-21 instead of 0. If the numerical answer is epsilon, the crucial thing is that epsilon must be negative, not that abs(epsilon) be minimized. Now, one could always truncated the (alternating) Taylor series for `log(1 + 10^(B - A) . . .)` at some fixed order that guarantees it's an underestimate, but first, even if that's the implementation of `log1p` it's brittle to rely on that; and second, it's not guaranteed that the leading term in dominant. Another alternative is instead of calculating the probability of all the non-variant (that is, hom ref but also het ref / span del and hom span del) genotypes add up just the variant genotypes and subtract those probabilities from 1. I hesitate to do this because it's less efficient when there are many alt alleles (there are many more variant genotypes than non-variant). Granted, it's only when span dels are present, but I still don't like it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402746850
https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402746850:226,Testability,log,log,226,"@EvanTheB You have a point, because the implementation of `logSumLog` (okay it's log10 but that's ugly so let's pretend we live in a parallel universe where the GATK uses only natural log) is (assume that `B < A` below); ```; log (10^A + 10^B . . .) = A + log(1 + 10^(B - A) . . .); ```; the accuracy of which could certainly be improved via the dedicated `log1p` method, especially when `B << A`. But accuracy alone doesn't necessarily address the issue. I mean, the results we're getting are like 10^-21 instead of 0. If the numerical answer is epsilon, the crucial thing is that epsilon must be negative, not that abs(epsilon) be minimized. Now, one could always truncated the (alternating) Taylor series for `log(1 + 10^(B - A) . . .)` at some fixed order that guarantees it's an underestimate, but first, even if that's the implementation of `log1p` it's brittle to rely on that; and second, it's not guaranteed that the leading term in dominant. Another alternative is instead of calculating the probability of all the non-variant (that is, hom ref but also het ref / span del and hom span del) genotypes add up just the variant genotypes and subtract those probabilities from 1. I hesitate to do this because it's less efficient when there are many alt alleles (there are many more variant genotypes than non-variant). Granted, it's only when span dels are present, but I still don't like it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402746850
https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402746850:256,Testability,log,log,256,"@EvanTheB You have a point, because the implementation of `logSumLog` (okay it's log10 but that's ugly so let's pretend we live in a parallel universe where the GATK uses only natural log) is (assume that `B < A` below); ```; log (10^A + 10^B . . .) = A + log(1 + 10^(B - A) . . .); ```; the accuracy of which could certainly be improved via the dedicated `log1p` method, especially when `B << A`. But accuracy alone doesn't necessarily address the issue. I mean, the results we're getting are like 10^-21 instead of 0. If the numerical answer is epsilon, the crucial thing is that epsilon must be negative, not that abs(epsilon) be minimized. Now, one could always truncated the (alternating) Taylor series for `log(1 + 10^(B - A) . . .)` at some fixed order that guarantees it's an underestimate, but first, even if that's the implementation of `log1p` it's brittle to rely on that; and second, it's not guaranteed that the leading term in dominant. Another alternative is instead of calculating the probability of all the non-variant (that is, hom ref but also het ref / span del and hom span del) genotypes add up just the variant genotypes and subtract those probabilities from 1. I hesitate to do this because it's less efficient when there are many alt alleles (there are many more variant genotypes than non-variant). Granted, it's only when span dels are present, but I still don't like it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402746850
https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402746850:713,Testability,log,log,713,"@EvanTheB You have a point, because the implementation of `logSumLog` (okay it's log10 but that's ugly so let's pretend we live in a parallel universe where the GATK uses only natural log) is (assume that `B < A` below); ```; log (10^A + 10^B . . .) = A + log(1 + 10^(B - A) . . .); ```; the accuracy of which could certainly be improved via the dedicated `log1p` method, especially when `B << A`. But accuracy alone doesn't necessarily address the issue. I mean, the results we're getting are like 10^-21 instead of 0. If the numerical answer is epsilon, the crucial thing is that epsilon must be negative, not that abs(epsilon) be minimized. Now, one could always truncated the (alternating) Taylor series for `log(1 + 10^(B - A) . . .)` at some fixed order that guarantees it's an underestimate, but first, even if that's the implementation of `log1p` it's brittle to rely on that; and second, it's not guaranteed that the leading term in dominant. Another alternative is instead of calculating the probability of all the non-variant (that is, hom ref but also het ref / span del and hom span del) genotypes add up just the variant genotypes and subtract those probabilities from 1. I hesitate to do this because it's less efficient when there are many alt alleles (there are many more variant genotypes than non-variant). Granted, it's only when span dels are present, but I still don't like it.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4980#issuecomment-402746850
https://github.com/broadinstitute/gatk/pull/4981#issuecomment-405582729:36,Modifiability,refactor,refactoring,36,"@cmnbroad , I've done the suggested refactoring and documentation changes.; Can you take a look again please? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4981#issuecomment-405582729
https://github.com/broadinstitute/gatk/pull/4982#issuecomment-402356444:938,Security,Validat,ValidateBasicSomaticShortMutations,938,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/4982?src=pr&el=h1) Report; > Merging [#4982](https://codecov.io/gh/broadinstitute/gatk/pull/4982?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/ddf042a04ad02f2e3207ff71bdace1bd7cec9a49?src=pr&el=desc) will **increase** coverage by `0.003%`.; > The diff coverage is `88.889%`. ```diff; @@ Coverage Diff @@; ## master #4982 +/- ##; ===============================================; + Coverage 80.784% 80.787% +0.003% ; - Complexity 17960 17966 +6 ; ===============================================; Files 1095 1095 ; Lines 64592 64619 +27 ; Branches 10392 10400 +8 ; ===============================================; + Hits 52180 52204 +24 ; - Misses 8388 8389 +1 ; - Partials 4024 4026 +2; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4982?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tmutpileup/ValidateBasicSomaticShortMutations.java](https://codecov.io/gh/broadinstitute/gatk/pull/4982/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `86.905% <88.889%> (+0.94%)` | `13 <0> (+6)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4982#issuecomment-402356444
https://github.com/broadinstitute/gatk/issues/4986#issuecomment-403542398:150,Deployability,update,update,150,"Upgrading directly to the latest `google-cloud-java` is blocked by https://github.com/GoogleCloudPlatform/google-cloud-java/issues/2453, but we could update our fork to include the built-in prefetching.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4986#issuecomment-403542398
https://github.com/broadinstitute/gatk/issues/4987#issuecomment-403487947:493,Safety,safe,safe,493,"It looks like this is happening because the `GATKAnnotationPluginDescriptor` only propagates pedigree arguments to annotations that derive from the `PedigreeAnnotation` class. In the forum post comments, the one report that includes an input command line specifies the `PossibleDeNovo` annotation, which isn't part of the`PedigreeAnnotation` hierarchy. So it doesn't get properly populated. I assume the other case is similar. This fix is change the hierarchy (or better yet, find a more type-safe way to identify pedigree annotations). Also, the tool doesn't appear in the docs because it doesn't have a `@DocumentedFeature` annotation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-403487947
https://github.com/broadinstitute/gatk/issues/4987#issuecomment-403538495:306,Performance,perform,perform,306,"@cmnbroad right, at the time when we impemented the pedigree annotations we made the decision that we didn't really expect the `PossibleDeNovo` to be used in the same way as the other pedigree annotations, as it seemed to be a specific case. It would be possible to add the ability for `PossibleDeNovo` to perform generate a SampleDB from its pedigree file at construction so it will be compatible. We still couldn't use the variant annotator for `CalculateGenotypePosteriors` though as we still can't have the same argument name on the path in multiple places",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-403538495
https://github.com/broadinstitute/gatk/issues/4987#issuecomment-403562512:18,Deployability,pipeline,pipeline,18,In the (proposed) pipeline it was run separately with VariantAnnotator. It could probably be rolled into CalculateGenotypePosteriors. Admittedly even four years later we still don't use it in production.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-403562512
https://github.com/broadinstitute/gatk/issues/4987#issuecomment-439235429:276,Availability,error,error,276,"We have another researcher asking about this feature at <https://gatkforums.broadinstitute.org/gatk/discussion/14299/gatk4-variantannotator-de-novo-ped-workshop-1809#latest>. If annotating de novos will not be a feature, then please can we remove the option and/or update the error message to be more informative? Would appreciate it. Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-439235429
https://github.com/broadinstitute/gatk/issues/4987#issuecomment-439235429:265,Deployability,update,update,265,"We have another researcher asking about this feature at <https://gatkforums.broadinstitute.org/gatk/discussion/14299/gatk4-variantannotator-de-novo-ped-workshop-1809#latest>. If annotating de novos will not be a feature, then please can we remove the option and/or update the error message to be more informative? Would appreciate it. Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-439235429
https://github.com/broadinstitute/gatk/issues/4987#issuecomment-439235429:282,Integrability,message,message,282,"We have another researcher asking about this feature at <https://gatkforums.broadinstitute.org/gatk/discussion/14299/gatk4-variantannotator-de-novo-ped-workshop-1809#latest>. If annotating de novos will not be a feature, then please can we remove the option and/or update the error message to be more informative? Would appreciate it. Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-439235429
https://github.com/broadinstitute/gatk/issues/4987#issuecomment-449426668:52,Availability,error,error,52,"Hello, we have another researcher asking about this error here: https://gatkforums.broadinstitute.org/gatk/discussion/comment/54993#Comment_54993",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-449426668
https://github.com/broadinstitute/gatk/issues/4987#issuecomment-462433251:21,Testability,test,test,21,"@jamesemery requests test files that show what a possible de novo looks like and I have agreed to prep some data towards this. Here is the data bundle that includes a GATK3.7 result.; [VariantAnnotator_PossibleDeNovo_sooheelee.zip](https://github.com/broadinstitute/gatk/files/2852379/VariantAnnotator_PossibleDeNovo_sooheelee.zip). The command I used with GATK3.7:; ```; java -jar $GATK -T VariantAnnotator \; -R ref/ref.fasta \; -V precomputed/trioGGVCF.vcf.gz \; -o trioVA.denovo.vcf.gz \; -A PossibleDeNovo ; -ped trio.ped; ```. Here are the three records that then show either `loConfDeNovo` or `hiConfDeNovo` annotations as a result:; ![screenshot 2019-02-11 13 10 16](https://user-images.githubusercontent.com/11543866/52583757-64f17680-2dfe-11e9-8c8e-11a9751f881c.png). For all of the records that change, see https://gatkforums.broadinstitute.org/gatk/discussion/comment/56268#Comment_56268.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-462433251
https://github.com/broadinstitute/gatk/issues/4987#issuecomment-462866609:129,Deployability,release,release,129,#5663 Hooks up the `PossibleDeNovo` to the annotation engine and should hopefully resolve this issue when it gets in by the next release.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4987#issuecomment-462866609
https://github.com/broadinstitute/gatk/pull/4988#issuecomment-403460454:232,Performance,perform,performance,232,"This looks OK to me. If I understand correctly, the decaying probability to stay in a class/state now only applies to the silent class and the baseline copy-number state?. @asmirnov239 Let's try to do a quick run on GPC2 to see how performance changes. You might want to sweep `p-alt`, `p-active`, and `cnv-coherence-length` again, since I don't think we should expect the optimal parameters to be the same across both HMMs; perhaps just sample the corners of the hypercube. Then let's rerun a SFARI cohort with 1) the filtered target list, 2) this HMM change, and 3) both, using optimal parameters.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4988#issuecomment-403460454
https://github.com/broadinstitute/gatk/pull/4988#issuecomment-407141271:84,Testability,test,test,84,"@asmirnov239 Evaluations can wait until they've been automated, but don't forget to test this change at some point.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4988#issuecomment-407141271
https://github.com/broadinstitute/gatk/pull/4988#issuecomment-736834077:116,Testability,benchmark,benchmark,116,"@ldgauthier Yes, definitely. I believe Jack has done some evaluations in the past that were inconclusive, but I can benchmark this branch now and see if we get improvements with certain types of CNV categories.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4988#issuecomment-736834077
https://github.com/broadinstitute/gatk/pull/4992#issuecomment-403523290:1237,Testability,test,test,1237,r&el=h1) Report; > Merging [#4992](https://codecov.io/gh/broadinstitute/gatk/pull/4992?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/d6a7fcd895677424b491df8c4ee6227e34b52251?src=pr&el=desc) will **decrease** coverage by `0.037%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #4992 +/- ##; ===============================================; - Coverage 60.188% 60.151% -0.037% ; + Complexity 12780 12767 -13 ; ===============================================; Files 1095 1095 ; Lines 64609 64609 ; Branches 10394 10394 ; ===============================================; - Hits 38887 38863 -24 ; - Misses 21488 21507 +19 ; - Partials 4234 4239 +5; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4992?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [.../hellbender/tools/genomicsdb/GenomicsDBImport.java](https://codecov.io/gh/broadinstitute/gatk/pull/4992/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9nZW5vbWljc2RiL0dlbm9taWNzREJJbXBvcnQuamF2YQ==) | `70.954% <0%> (-5.394%)` | `40% <0%> (-6%)` | |; | [...hellbender/utils/test/VariantContextTestUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4992/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1ZhcmlhbnRDb250ZXh0VGVzdFV0aWxzLmphdmE=) | `66.26% <0%> (-2.439%)` | `48% <0%> (-5%)` | |; | [...rg/broadinstitute/hellbender/utils/io/IOUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4992/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pby9JT1V0aWxzLmphdmE=) | `43.548% <0%> (-2.151%)` | `30% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4992/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `74.194% <0%> (-0.645%)` | `35% <0%> (ø)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4992#issuecomment-403523290
https://github.com/broadinstitute/gatk/pull/4992#issuecomment-403528803:9,Deployability,Integrat,Integration,9,@droazen Integration tests went from ~58 minutes to ~50 minutes as a result of this branch.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4992#issuecomment-403528803
https://github.com/broadinstitute/gatk/pull/4992#issuecomment-403528803:9,Integrability,Integrat,Integration,9,@droazen Integration tests went from ~58 minutes to ~50 minutes as a result of this branch.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4992#issuecomment-403528803
https://github.com/broadinstitute/gatk/pull/4992#issuecomment-403528803:21,Testability,test,tests,21,@droazen Integration tests went from ~58 minutes to ~50 minutes as a result of this branch.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4992#issuecomment-403528803
https://github.com/broadinstitute/gatk/pull/4993#issuecomment-415137763:92,Deployability,update,update,92,"@kgururaj I just tried this out with my annotations and it worked right out of the box! The update was very simple on my end. Ideally it might be nice to define the combine operations as static Strings in the annotation classes, but we can do that on the GATK side.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4993#issuecomment-415137763
https://github.com/broadinstitute/gatk/pull/4993#issuecomment-415137763:108,Usability,simpl,simple,108,"@kgururaj I just tried this out with my annotations and it worked right out of the box! The update was very simple on my end. Ideally it might be nice to define the combine operations as static Strings in the annotation classes, but we can do that on the GATK side.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4993#issuecomment-415137763
https://github.com/broadinstitute/gatk/pull/4993#issuecomment-415407026:147,Deployability,update,updateINFOFieldLengthDescriptor,147,"So if I needed a new combine operation for an allele-specific annotation, how would I specify that the annotation is allele-specific? Do we need a updateINFOFieldLengthDescriptor like updateINFOFieldCombineOperation?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4993#issuecomment-415407026
https://github.com/broadinstitute/gatk/pull/4993#issuecomment-415407026:184,Deployability,update,updateINFOFieldCombineOperation,184,"So if I needed a new combine operation for an allele-specific annotation, how would I specify that the annotation is allele-specific? Do we need a updateINFOFieldLengthDescriptor like updateINFOFieldCombineOperation?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4993#issuecomment-415407026
https://github.com/broadinstitute/gatk/pull/4993#issuecomment-415509584:76,Deployability,update,updateINFOFieldDescriptor,76,"* If you are adding a new allele specific INFO field, yeah, we would need a updateINFOFieldDescriptor() function. You would need to call this the first time an array is created. You can take the vid that GenomicsDB creates and modify the specific INFO fields of interest.; * If you wish to specify a new combine operation that doesn't exist in GenomicsDB yet (say element_wise_median), that would involved modifying the C++ code. I haven't documented that anywhere. If this is something that you wish to do, please let me know. I'll try to find a way of supporting such operations.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4993#issuecomment-415509584
https://github.com/broadinstitute/gatk/pull/4993#issuecomment-415611462:318,Usability,simpl,simply,318,"* If you are planning to add more allele specific annotations (other than the ones listed [here](https://github.com/Intel-HLS/GenomicsDB/blob/master/src/main/java/com/intel/genomicsdb/importer/Constants.java)), then I can provide more example code in GATK showing how to set the type and length descriptors.; * If you simply wish to change the combine operation for existing annotations, the example code in this PR should suffice",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4993#issuecomment-415611462
https://github.com/broadinstitute/gatk/pull/4993#issuecomment-415804596:578,Usability,simpl,simply,578,"I would like to know how to add more allele-specific annotations. Example; code would be great. On Thu, Aug 23, 2018 at 8:12 PM, Karthik Gururaj <notifications@github.com>; wrote:. >; > - If you are planning to add more allele specific annotations (other; > than the ones listed [here](For the allele specific annotation fields; > that we know; > <https://github.com/Intel-HLS/GenomicsDB/blob/master/src/main/java/com/intel/genomicsdb/importer/Constants.java>)),; > then I can provide more example code in GATK showing how to set the type; > and length descriptors.; > - If you simply wish to change the combine operation for existing; > annotations, the example code in this PR should suffice; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk/pull/4993#issuecomment-415611462>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdOgKRPyTCP0gxEq0Ye1b4Q5CZ8HFks5uT0TWgaJpZM4VJ9WN>; > .; >. -- ; Laura Doyle Gauthier, Ph.D.; Associate Director, Germline Methods; Data Sciences Platform; gauthier@broadinstitute.org; Broad Institute of MIT & Harvard; 320 Charles St.; Cambridge MA 0214",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4993#issuecomment-415804596
https://github.com/broadinstitute/gatk/pull/4993#issuecomment-426013613:135,Deployability,patch,patch,135,@ldgauthier Could you rebase this branch and make any changes you require? Then we can merge it and rebase your other branch onto this patch.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4993#issuecomment-426013613
https://github.com/broadinstitute/gatk/pull/4993#issuecomment-426369168:162,Testability,test,tests,162,"I started looking into allele-specific annotation combine setting and it should be possible from the GATK-side, but then I realized I don't want to write all the tests for it right now. I'll put it in a branch and open an issue, but the gist is that this PR is useful, I want it merged, and my feature request can be addressed by a GATK dev at some point in the future if necessary.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4993#issuecomment-426369168
https://github.com/broadinstitute/gatk/issues/4994#issuecomment-403951324:73,Testability,log,log,73,It also looks like the method in question is O(n^2) when it could be O(n log n) if it sorted the interval list first...,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4994#issuecomment-403951324
https://github.com/broadinstitute/gatk/issues/4994#issuecomment-429935301:20,Deployability,patch,patched,20,Closing -- this was patched.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4994#issuecomment-429935301
https://github.com/broadinstitute/gatk/pull/4996#issuecomment-404549419:2002,Usability,Simpl,SimpleNovelAdjacencyInterpreter,2002,h/broadinstitute/gatk/pull/4996/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvYWxpZ25tZW50L0Fzc2VtYmx5Q29udGlnQWxpZ25tZW50c0NvbmZpZ1BpY2tlclVuaXRUZXN0LmphdmE=) | `99.057% <ø> (ø)` | `30 <0> (ø)` | :arrow_down: |; | [...ry/alignment/ContigAlignmentsModifierUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4996/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvYWxpZ25tZW50L0NvbnRpZ0FsaWdubWVudHNNb2RpZmllclVuaXRUZXN0LmphdmE=) | `99.194% <ø> (ø)` | `19 <0> (ø)` | :arrow_down: |; | [...very/alignment/AlignedContigGeneratorUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4996/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvYWxpZ25tZW50L0FsaWduZWRDb250aWdHZW5lcmF0b3JVbml0VGVzdC5qYXZh) | `97.17% <ø> (ø)` | `10 <0> (ø)` | :arrow_down: |; | [...ery/inference/SimpleNovelAdjacencyInterpreter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4996/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL1NpbXBsZU5vdmVsQWRqYWNlbmN5SW50ZXJwcmV0ZXIuamF2YQ==) | `80.882% <ø> (ø)` | `11 <0> (ø)` | :arrow_down: |; | [...ender/tools/spark/sv/utils/GATKSVVCFConstants.java](https://codecov.io/gh/broadinstitute/gatk/pull/4996/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi91dGlscy9HQVRLU1ZWQ0ZDb25zdGFudHMuamF2YQ==) | `0% <ø> (-75%)` | `0 <0> (-1)` | |; | [.../sv/discovery/inference/CpxVariantInterpreter.java](https://codecov.io/gh/broadinstitute/gatk/pull/4996/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9kaXNjb3ZlcnkvaW5mZXJlbmNlL0NweFZhcmlhbnRJbnRlcnByZXRlci5qYXZh) | `79.839% <ø> (ø)` | `26 <0> (ø)` | :arrow_down: |; | [.../DiscoverVariantsFromContigAlignmentsSAMSpark.java](,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4996#issuecomment-404549419
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-404577493:28,Deployability,release,release,28,I'd suggest doing a bug fix release @lbergelson once this fix goes in.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-404577493
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-404596841:0,Testability,Test,Tested,0,Tested with 10K intervals and 100 WES samples,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-404596841
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-404596846:2425,Deployability,integrat,integration,2425,bmRlci91dGlscy9SL1JTY3JpcHRFeGVjdXRvci5qYXZh) | `80.282% <0%> (-8.451%)` | `17% <0%> (-3%)` | |; | [...g/broadinstitute/hellbender/utils/io/Resource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4997/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pby9SZXNvdXJjZS5qYXZh) | `55.556% <0%> (-7.407%)` | `6% <0%> (-1%)` | |; | [...llbender/tools/walkers/bqsr/AnalyzeCovariates.java](https://codecov.io/gh/broadinstitute/gatk/pull/4997/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQW5hbHl6ZUNvdmFyaWF0ZXMuamF2YQ==) | `67.593% <0%> (-4.63%)` | `29% <0%> (-1%)` | |; | [...ute/hellbender/utils/recalibration/RecalUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4997/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWNhbGlicmF0aW9uL1JlY2FsVXRpbHMuamF2YQ==) | `89.407% <0%> (-3.814%)` | `52% <0%> (-1%)` | |; | [.../sv/integration/SVIntegrationTestDataProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/4997/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9pbnRlZ3JhdGlvbi9TVkludGVncmF0aW9uVGVzdERhdGFQcm92aWRlci5qYXZh) | `90.909% <0%> (-3.209%)` | `2% <0%> (+1%)` | |; | [...te/hellbender/tools/spark/sv/utils/SVInterval.java](https://codecov.io/gh/broadinstitute/gatk/pull/4997/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi91dGlscy9TVkludGVydmFsLmphdmE=) | `85.507% <0%> (-1.993%)` | `64% <0%> (+28%)` | |; | [...walkers/bqsr/AnalyzeCovariatesIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4997/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQW5hbHl6ZUNvdmFyaWF0ZXNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `92.063% <0%> (-1.783%)` | `22% <0%> (-2%)` | |; | [...on/FindBreakpointEvidenceSparkIntegrationTest.java](https://codecov.io/gh/b,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-404596846
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-404596846:2425,Integrability,integrat,integration,2425,bmRlci91dGlscy9SL1JTY3JpcHRFeGVjdXRvci5qYXZh) | `80.282% <0%> (-8.451%)` | `17% <0%> (-3%)` | |; | [...g/broadinstitute/hellbender/utils/io/Resource.java](https://codecov.io/gh/broadinstitute/gatk/pull/4997/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9pby9SZXNvdXJjZS5qYXZh) | `55.556% <0%> (-7.407%)` | `6% <0%> (-1%)` | |; | [...llbender/tools/walkers/bqsr/AnalyzeCovariates.java](https://codecov.io/gh/broadinstitute/gatk/pull/4997/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQW5hbHl6ZUNvdmFyaWF0ZXMuamF2YQ==) | `67.593% <0%> (-4.63%)` | `29% <0%> (-1%)` | |; | [...ute/hellbender/utils/recalibration/RecalUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/4997/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWNhbGlicmF0aW9uL1JlY2FsVXRpbHMuamF2YQ==) | `89.407% <0%> (-3.814%)` | `52% <0%> (-1%)` | |; | [.../sv/integration/SVIntegrationTestDataProvider.java](https://codecov.io/gh/broadinstitute/gatk/pull/4997/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi9pbnRlZ3JhdGlvbi9TVkludGVncmF0aW9uVGVzdERhdGFQcm92aWRlci5qYXZh) | `90.909% <0%> (-3.209%)` | `2% <0%> (+1%)` | |; | [...te/hellbender/tools/spark/sv/utils/SVInterval.java](https://codecov.io/gh/broadinstitute/gatk/pull/4997/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9zdi91dGlscy9TVkludGVydmFsLmphdmE=) | `85.507% <0%> (-1.993%)` | `64% <0%> (+28%)` | |; | [...walkers/bqsr/AnalyzeCovariatesIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4997/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQW5hbHl6ZUNvdmFyaWF0ZXNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `92.063% <0%> (-1.783%)` | `22% <0%> (-2%)` | |; | [...on/FindBreakpointEvidenceSparkIntegrationTest.java](https://codecov.io/gh/b,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-404596846
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-405982928:53,Deployability,release,release,53,@lbergelson Do you want to look at this ? I can do a release tomorrow if we can get it in.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-405982928
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-406751089:34,Availability,error,error,34,How do you want to test this? The error was triggered only if a large number of intervals (~1000) was imported by the tool.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-406751089
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-406751089:19,Testability,test,test,19,How do you want to test this? The error was triggered only if a large number of intervals (~1000) was imported by the tool.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-406751089
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407056469:41,Deployability,integrat,integration,41,"@kguraj It looks like there are existing integration tests that use intervals that cover a pretty wide genomic range. It should be easy to write a test that programmatically generates a large set of (10000) or so very small intervals (1bp) with small (1bp) gaps between them (the gaps are necessary since otherwise the intervals will be merged together by the engine) that fails without this change and passes with it. It doesn't necessarily have to verify the results, just successfully complete.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407056469
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407056469:41,Integrability,integrat,integration,41,"@kguraj It looks like there are existing integration tests that use intervals that cover a pretty wide genomic range. It should be easy to write a test that programmatically generates a large set of (10000) or so very small intervals (1bp) with small (1bp) gaps between them (the gaps are necessary since otherwise the intervals will be merged together by the engine) that fails without this change and passes with it. It doesn't necessarily have to verify the results, just successfully complete.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407056469
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407056469:53,Testability,test,tests,53,"@kguraj It looks like there are existing integration tests that use intervals that cover a pretty wide genomic range. It should be easy to write a test that programmatically generates a large set of (10000) or so very small intervals (1bp) with small (1bp) gaps between them (the gaps are necessary since otherwise the intervals will be merged together by the engine) that fails without this change and passes with it. It doesn't necessarily have to verify the results, just successfully complete.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407056469
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407056469:147,Testability,test,test,147,"@kguraj It looks like there are existing integration tests that use intervals that cover a pretty wide genomic range. It should be easy to write a test that programmatically generates a large set of (10000) or so very small intervals (1bp) with small (1bp) gaps between them (the gaps are necessary since otherwise the intervals will be merged together by the engine) that fails without this change and passes with it. It doesn't necessarily have to verify the results, just successfully complete.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407056469
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407169334:9,Testability,test,test,9,FYI: the test will take a long time to run. Added the requested test.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407169334
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407169334:64,Testability,test,test,64,FYI: the test will take a long time to run. Added the requested test.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407169334
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031:479,Availability,failure,failure,479,"@kgururaj Thanks for adding the test. Running it locally on my laptop (without your fix) succeeds though - I have to bump it up from 1000 intervals to 9000 to reproduce the stack overflow. But if I do that, it takes a long time to run, since it appears to be creating lots of small partitions. Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?. Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:. `[GenomicsDB::VariantStorageManager] INFO: ignore message ""[TileDB::StorageManager] Error: Cannot list TileDB directory; Directory buffer overflow."" in the previous line`. followed by a failure that ends like this:. `[TileDB::StorageManager] Error: Cannot store schema; Too many open files in system.; libc++abi.dylib: terminating with uncaught exception of type LoadOperatorException: LoadOperatorException : Could not define TileDB array; TileDB error message : [TileDB::StorageManager] Error: Cannot store schema; Too many open files in system`. Can you reproduce that ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031:626,Availability,Error,Error,626,"@kgururaj Thanks for adding the test. Running it locally on my laptop (without your fix) succeeds though - I have to bump it up from 1000 intervals to 9000 to reproduce the stack overflow. But if I do that, it takes a long time to run, since it appears to be creating lots of small partitions. Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?. Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:. `[GenomicsDB::VariantStorageManager] INFO: ignore message ""[TileDB::StorageManager] Error: Cannot list TileDB directory; Directory buffer overflow."" in the previous line`. followed by a failure that ends like this:. `[TileDB::StorageManager] Error: Cannot store schema; Too many open files in system.; libc++abi.dylib: terminating with uncaught exception of type LoadOperatorException: LoadOperatorException : Could not define TileDB array; TileDB error message : [TileDB::StorageManager] Error: Cannot store schema; Too many open files in system`. Can you reproduce that ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031:728,Availability,failure,failure,728,"@kgururaj Thanks for adding the test. Running it locally on my laptop (without your fix) succeeds though - I have to bump it up from 1000 intervals to 9000 to reproduce the stack overflow. But if I do that, it takes a long time to run, since it appears to be creating lots of small partitions. Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?. Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:. `[GenomicsDB::VariantStorageManager] INFO: ignore message ""[TileDB::StorageManager] Error: Cannot list TileDB directory; Directory buffer overflow."" in the previous line`. followed by a failure that ends like this:. `[TileDB::StorageManager] Error: Cannot store schema; Too many open files in system.; libc++abi.dylib: terminating with uncaught exception of type LoadOperatorException: LoadOperatorException : Could not define TileDB array; TileDB error message : [TileDB::StorageManager] Error: Cannot store schema; Too many open files in system`. Can you reproduce that ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031:784,Availability,Error,Error,784,"@kgururaj Thanks for adding the test. Running it locally on my laptop (without your fix) succeeds though - I have to bump it up from 1000 intervals to 9000 to reproduce the stack overflow. But if I do that, it takes a long time to run, since it appears to be creating lots of small partitions. Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?. Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:. `[GenomicsDB::VariantStorageManager] INFO: ignore message ""[TileDB::StorageManager] Error: Cannot list TileDB directory; Directory buffer overflow."" in the previous line`. followed by a failure that ends like this:. `[TileDB::StorageManager] Error: Cannot store schema; Too many open files in system.; libc++abi.dylib: terminating with uncaught exception of type LoadOperatorException: LoadOperatorException : Could not define TileDB array; TileDB error message : [TileDB::StorageManager] Error: Cannot store schema; Too many open files in system`. Can you reproduce that ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031:990,Availability,error,error,990,"@kgururaj Thanks for adding the test. Running it locally on my laptop (without your fix) succeeds though - I have to bump it up from 1000 intervals to 9000 to reproduce the stack overflow. But if I do that, it takes a long time to run, since it appears to be creating lots of small partitions. Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?. Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:. `[GenomicsDB::VariantStorageManager] INFO: ignore message ""[TileDB::StorageManager] Error: Cannot list TileDB directory; Directory buffer overflow."" in the previous line`. followed by a failure that ends like this:. `[TileDB::StorageManager] Error: Cannot store schema; Too many open files in system.; libc++abi.dylib: terminating with uncaught exception of type LoadOperatorException: LoadOperatorException : Could not define TileDB array; TileDB error message : [TileDB::StorageManager] Error: Cannot store schema; Too many open files in system`. Can you reproduce that ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031:1031,Availability,Error,Error,1031,"@kgururaj Thanks for adding the test. Running it locally on my laptop (without your fix) succeeds though - I have to bump it up from 1000 intervals to 9000 to reproduce the stack overflow. But if I do that, it takes a long time to run, since it appears to be creating lots of small partitions. Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?. Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:. `[GenomicsDB::VariantStorageManager] INFO: ignore message ""[TileDB::StorageManager] Error: Cannot list TileDB directory; Directory buffer overflow."" in the previous line`. followed by a failure that ends like this:. `[TileDB::StorageManager] Error: Cannot store schema; Too many open files in system.; libc++abi.dylib: terminating with uncaught exception of type LoadOperatorException: LoadOperatorException : Could not define TileDB array; TileDB error message : [TileDB::StorageManager] Error: Cannot store schema; Too many open files in system`. Can you reproduce that ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031:531,Integrability,message,messages,531,"@kgururaj Thanks for adding the test. Running it locally on my laptop (without your fix) succeeds though - I have to bump it up from 1000 intervals to 9000 to reproduce the stack overflow. But if I do that, it takes a long time to run, since it appears to be creating lots of small partitions. Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?. Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:. `[GenomicsDB::VariantStorageManager] INFO: ignore message ""[TileDB::StorageManager] Error: Cannot list TileDB directory; Directory buffer overflow."" in the previous line`. followed by a failure that ends like this:. `[TileDB::StorageManager] Error: Cannot store schema; Too many open files in system.; libc++abi.dylib: terminating with uncaught exception of type LoadOperatorException: LoadOperatorException : Could not define TileDB array; TileDB error message : [TileDB::StorageManager] Error: Cannot store schema; Too many open files in system`. Can you reproduce that ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031:592,Integrability,message,message,592,"@kgururaj Thanks for adding the test. Running it locally on my laptop (without your fix) succeeds though - I have to bump it up from 1000 intervals to 9000 to reproduce the stack overflow. But if I do that, it takes a long time to run, since it appears to be creating lots of small partitions. Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?. Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:. `[GenomicsDB::VariantStorageManager] INFO: ignore message ""[TileDB::StorageManager] Error: Cannot list TileDB directory; Directory buffer overflow."" in the previous line`. followed by a failure that ends like this:. `[TileDB::StorageManager] Error: Cannot store schema; Too many open files in system.; libc++abi.dylib: terminating with uncaught exception of type LoadOperatorException: LoadOperatorException : Could not define TileDB array; TileDB error message : [TileDB::StorageManager] Error: Cannot store schema; Too many open files in system`. Can you reproduce that ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031:996,Integrability,message,message,996,"@kgururaj Thanks for adding the test. Running it locally on my laptop (without your fix) succeeds though - I have to bump it up from 1000 intervals to 9000 to reproduce the stack overflow. But if I do that, it takes a long time to run, since it appears to be creating lots of small partitions. Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?. Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:. `[GenomicsDB::VariantStorageManager] INFO: ignore message ""[TileDB::StorageManager] Error: Cannot list TileDB directory; Directory buffer overflow."" in the previous line`. followed by a failure that ends like this:. `[TileDB::StorageManager] Error: Cannot store schema; Too many open files in system.; libc++abi.dylib: terminating with uncaught exception of type LoadOperatorException: LoadOperatorException : Could not define TileDB array; TileDB error message : [TileDB::StorageManager] Error: Cannot store schema; Too many open files in system`. Can you reproduce that ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031:905,Performance,Load,LoadOperatorException,905,"@kgururaj Thanks for adding the test. Running it locally on my laptop (without your fix) succeeds though - I have to bump it up from 1000 intervals to 9000 to reproduce the stack overflow. But if I do that, it takes a long time to run, since it appears to be creating lots of small partitions. Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?. Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:. `[GenomicsDB::VariantStorageManager] INFO: ignore message ""[TileDB::StorageManager] Error: Cannot list TileDB directory; Directory buffer overflow."" in the previous line`. followed by a failure that ends like this:. `[TileDB::StorageManager] Error: Cannot store schema; Too many open files in system.; libc++abi.dylib: terminating with uncaught exception of type LoadOperatorException: LoadOperatorException : Could not define TileDB array; TileDB error message : [TileDB::StorageManager] Error: Cannot store schema; Too many open files in system`. Can you reproduce that ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031:928,Performance,Load,LoadOperatorException,928,"@kgururaj Thanks for adding the test. Running it locally on my laptop (without your fix) succeeds though - I have to bump it up from 1000 intervals to 9000 to reproduce the stack overflow. But if I do that, it takes a long time to run, since it appears to be creating lots of small partitions. Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?. Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:. `[GenomicsDB::VariantStorageManager] INFO: ignore message ""[TileDB::StorageManager] Error: Cannot list TileDB directory; Directory buffer overflow."" in the previous line`. followed by a failure that ends like this:. `[TileDB::StorageManager] Error: Cannot store schema; Too many open files in system.; libc++abi.dylib: terminating with uncaught exception of type LoadOperatorException: LoadOperatorException : Could not define TileDB array; TileDB error message : [TileDB::StorageManager] Error: Cannot store schema; Too many open files in system`. Can you reproduce that ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031:32,Testability,test,test,32,"@kgururaj Thanks for adding the test. Running it locally on my laptop (without your fix) succeeds though - I have to bump it up from 1000 intervals to 9000 to reproduce the stack overflow. But if I do that, it takes a long time to run, since it appears to be creating lots of small partitions. Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?. Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:. `[GenomicsDB::VariantStorageManager] INFO: ignore message ""[TileDB::StorageManager] Error: Cannot list TileDB directory; Directory buffer overflow."" in the previous line`. followed by a failure that ends like this:. `[TileDB::StorageManager] Error: Cannot store schema; Too many open files in system.; libc++abi.dylib: terminating with uncaught exception of type LoadOperatorException: LoadOperatorException : Could not define TileDB array; TileDB error message : [TileDB::StorageManager] Error: Cannot store schema; Too many open files in system`. Can you reproduce that ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407214031
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434:506,Availability,failure,failure,506,"_Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?_; No - see the PR message https://github.com/broadinstitute/gatk/pull/4645#issue-180678162. The multi-interval support in GenomicsDBImport tool is purely for convenience. For scalability with a large number of intervals and samples, you should use multiple processes, each writing to a small (1?) number of intervals. **_Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:_**; The first set of messages are spurious debug messages - no error in reality. I'll provide a jar without these messages. The second set of messages are a result of too many file handles open per process - your system is limiting the number of file handles opened by a single process. Again, this goes back to the previous statement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434:631,Availability,error,error,631,"_Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?_; No - see the PR message https://github.com/broadinstitute/gatk/pull/4645#issue-180678162. The multi-interval support in GenomicsDBImport tool is purely for convenience. For scalability with a large number of intervals and samples, you should use multiple processes, each writing to a small (1?) number of intervals. **_Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:_**; The first set of messages are spurious debug messages - no error in reality. I'll provide a jar without these messages. The second set of messages are a result of too many file handles open per process - your system is limiting the number of file handles opened by a single process. Again, this goes back to the previous statement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434:126,Integrability,message,message,126,"_Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?_; No - see the PR message https://github.com/broadinstitute/gatk/pull/4645#issue-180678162. The multi-interval support in GenomicsDBImport tool is purely for convenience. For scalability with a large number of intervals and samples, you should use multiple processes, each writing to a small (1?) number of intervals. **_Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:_**; The first set of messages are spurious debug messages - no error in reality. I'll provide a jar without these messages. The second set of messages are a result of too many file handles open per process - your system is limiting the number of file handles opened by a single process. Again, this goes back to the previous statement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434:558,Integrability,message,messages,558,"_Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?_; No - see the PR message https://github.com/broadinstitute/gatk/pull/4645#issue-180678162. The multi-interval support in GenomicsDBImport tool is purely for convenience. For scalability with a large number of intervals and samples, you should use multiple processes, each writing to a small (1?) number of intervals. **_Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:_**; The first set of messages are spurious debug messages - no error in reality. I'll provide a jar without these messages. The second set of messages are a result of too many file handles open per process - your system is limiting the number of file handles opened by a single process. Again, this goes back to the previous statement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434:589,Integrability,message,messages,589,"_Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?_; No - see the PR message https://github.com/broadinstitute/gatk/pull/4645#issue-180678162. The multi-interval support in GenomicsDBImport tool is purely for convenience. For scalability with a large number of intervals and samples, you should use multiple processes, each writing to a small (1?) number of intervals. **_Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:_**; The first set of messages are spurious debug messages - no error in reality. I'll provide a jar without these messages. The second set of messages are a result of too many file handles open per process - your system is limiting the number of file handles opened by a single process. Again, this goes back to the previous statement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434:617,Integrability,message,messages,617,"_Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?_; No - see the PR message https://github.com/broadinstitute/gatk/pull/4645#issue-180678162. The multi-interval support in GenomicsDBImport tool is purely for convenience. For scalability with a large number of intervals and samples, you should use multiple processes, each writing to a small (1?) number of intervals. **_Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:_**; The first set of messages are spurious debug messages - no error in reality. I'll provide a jar without these messages. The second set of messages are a result of too many file handles open per process - your system is limiting the number of file handles opened by a single process. Again, this goes back to the previous statement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434:682,Integrability,message,messages,682,"_Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?_; No - see the PR message https://github.com/broadinstitute/gatk/pull/4645#issue-180678162. The multi-interval support in GenomicsDBImport tool is purely for convenience. For scalability with a large number of intervals and samples, you should use multiple processes, each writing to a small (1?) number of intervals. **_Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:_**; The first set of messages are spurious debug messages - no error in reality. I'll provide a jar without these messages. The second set of messages are a result of too many file handles open per process - your system is limiting the number of file handles opened by a single process. Again, this goes back to the previous statement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434:710,Integrability,message,messages,710,"_Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?_; No - see the PR message https://github.com/broadinstitute/gatk/pull/4645#issue-180678162. The multi-interval support in GenomicsDBImport tool is purely for convenience. For scalability with a large number of intervals and samples, you should use multiple processes, each writing to a small (1?) number of intervals. **_Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:_**; The first set of messages are spurious debug messages - no error in reality. I'll provide a jar without these messages. The second set of messages are a result of too many file handles open per process - your system is limiting the number of file handles opened by a single process. Again, this goes back to the previous statement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434:283,Performance,scalab,scalability,283,"_Is there any way to get it to use fewer partitions in a case like this where there are lots of intervals ?_; No - see the PR message https://github.com/broadinstitute/gatk/pull/4645#issue-180678162. The multi-interval support in GenomicsDBImport tool is purely for convenience. For scalability with a large number of intervals and samples, you should use multiple processes, each writing to a small (1?) number of intervals. **_Somewhat more concerning is that when with 8000 intervals, I see a different failure mode. First I see lots (thousands) of these messages:_**; The first set of messages are spurious debug messages - no error in reality. I'll provide a jar without these messages. The second set of messages are a result of too many file handles open per process - your system is limiting the number of file handles opened by a single process. Again, this goes back to the previous statement.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407228434
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407394879:287,Availability,error,error,287,"@kgururaj Do those first messages originate [here](https://github.com/Intel-HLS/TileDB/blob/33ad0355db215cb528162a74c16b706a059b0346/core/src/storage_manager/storage_manager.cc#L1168) ? I'm not sure what GenomicsDB code path leads there, but it looks like TileDB considers them to be an error that results in a short-circuit return. I'd be concerned that masking them would hide some underlying error. Are there any tests that verify that data round-trips though GDB when an interval list large enough to trigger these messages is encountered ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407394879
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407394879:355,Availability,mask,masking,355,"@kgururaj Do those first messages originate [here](https://github.com/Intel-HLS/TileDB/blob/33ad0355db215cb528162a74c16b706a059b0346/core/src/storage_manager/storage_manager.cc#L1168) ? I'm not sure what GenomicsDB code path leads there, but it looks like TileDB considers them to be an error that results in a short-circuit return. I'd be concerned that masking them would hide some underlying error. Are there any tests that verify that data round-trips though GDB when an interval list large enough to trigger these messages is encountered ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407394879
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407394879:395,Availability,error,error,395,"@kgururaj Do those first messages originate [here](https://github.com/Intel-HLS/TileDB/blob/33ad0355db215cb528162a74c16b706a059b0346/core/src/storage_manager/storage_manager.cc#L1168) ? I'm not sure what GenomicsDB code path leads there, but it looks like TileDB considers them to be an error that results in a short-circuit return. I'd be concerned that masking them would hide some underlying error. Are there any tests that verify that data round-trips though GDB when an interval list large enough to trigger these messages is encountered ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407394879
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407394879:25,Integrability,message,messages,25,"@kgururaj Do those first messages originate [here](https://github.com/Intel-HLS/TileDB/blob/33ad0355db215cb528162a74c16b706a059b0346/core/src/storage_manager/storage_manager.cc#L1168) ? I'm not sure what GenomicsDB code path leads there, but it looks like TileDB considers them to be an error that results in a short-circuit return. I'd be concerned that masking them would hide some underlying error. Are there any tests that verify that data round-trips though GDB when an interval list large enough to trigger these messages is encountered ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407394879
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407394879:519,Integrability,message,messages,519,"@kgururaj Do those first messages originate [here](https://github.com/Intel-HLS/TileDB/blob/33ad0355db215cb528162a74c16b706a059b0346/core/src/storage_manager/storage_manager.cc#L1168) ? I'm not sure what GenomicsDB code path leads there, but it looks like TileDB considers them to be an error that results in a short-circuit return. I'd be concerned that masking them would hide some underlying error. Are there any tests that verify that data round-trips though GDB when an interval list large enough to trigger these messages is encountered ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407394879
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407394879:416,Testability,test,tests,416,"@kgururaj Do those first messages originate [here](https://github.com/Intel-HLS/TileDB/blob/33ad0355db215cb528162a74c16b706a059b0346/core/src/storage_manager/storage_manager.cc#L1168) ? I'm not sure what GenomicsDB code path leads there, but it looks like TileDB considers them to be an error that results in a short-circuit return. I'd be concerned that masking them would hide some underlying error. Are there any tests that verify that data round-trips though GDB when an interval list large enough to trigger these messages is encountered ?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407394879
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407440149:16,Availability,error,error,16,"Yes, that's the error message. You should see [this part of the GenomicsDB](https://github.com/Intel-HLS/GenomicsDB/blob/stack_ovf_fix/src/main/cpp/src/genomicsdb/variant_storage_manager.cc#L540) code that retries the function when an error is detected. So, you don't need to be concerned by those error messages. They are annoying though and I will disable them from being printed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407440149
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407440149:235,Availability,error,error,235,"Yes, that's the error message. You should see [this part of the GenomicsDB](https://github.com/Intel-HLS/GenomicsDB/blob/stack_ovf_fix/src/main/cpp/src/genomicsdb/variant_storage_manager.cc#L540) code that retries the function when an error is detected. So, you don't need to be concerned by those error messages. They are annoying though and I will disable them from being printed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407440149
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407440149:298,Availability,error,error,298,"Yes, that's the error message. You should see [this part of the GenomicsDB](https://github.com/Intel-HLS/GenomicsDB/blob/stack_ovf_fix/src/main/cpp/src/genomicsdb/variant_storage_manager.cc#L540) code that retries the function when an error is detected. So, you don't need to be concerned by those error messages. They are annoying though and I will disable them from being printed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407440149
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407440149:22,Integrability,message,message,22,"Yes, that's the error message. You should see [this part of the GenomicsDB](https://github.com/Intel-HLS/GenomicsDB/blob/stack_ovf_fix/src/main/cpp/src/genomicsdb/variant_storage_manager.cc#L540) code that retries the function when an error is detected. So, you don't need to be concerned by those error messages. They are annoying though and I will disable them from being printed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407440149
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407440149:304,Integrability,message,messages,304,"Yes, that's the error message. You should see [this part of the GenomicsDB](https://github.com/Intel-HLS/GenomicsDB/blob/stack_ovf_fix/src/main/cpp/src/genomicsdb/variant_storage_manager.cc#L540) code that retries the function when an error is detected. So, you don't need to be concerned by those error messages. They are annoying though and I will disable them from being printed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407440149
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407440149:244,Safety,detect,detected,244,"Yes, that's the error message. You should see [this part of the GenomicsDB](https://github.com/Intel-HLS/GenomicsDB/blob/stack_ovf_fix/src/main/cpp/src/genomicsdb/variant_storage_manager.cc#L540) code that retries the function when an error is detected. So, you don't need to be concerned by those error messages. They are annoying though and I will disable them from being printed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407440149
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407867902:839,Availability,error,error,839,"@kguraj Thanks for the response(s). The test in the PR was super useful as a temporary test, but as you mentioned it runs pretty slowly, and as it stands the test passes on current master anyway. It seems to require on the order of 9000-1000 intervals instead rather than 1000 to actually hit stack overflow. Since that would be a very slow running test, I'm inclined to back it out. Also, the user who originally reported the issue was using 11k intervals, and it seems that the stack overflow fix is unlikely to help in that case. Is there any guidance for users on what is a reasonable number of intervals per process ? It sounds like the intention was that it be used with pretty small intervals. Should we issue a warning message in GenomicsDBImport at some threshold number of intervals ?. Are you planning to produce a jar with the error messages suppressed for this PR?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407867902
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407867902:727,Integrability,message,message,727,"@kguraj Thanks for the response(s). The test in the PR was super useful as a temporary test, but as you mentioned it runs pretty slowly, and as it stands the test passes on current master anyway. It seems to require on the order of 9000-1000 intervals instead rather than 1000 to actually hit stack overflow. Since that would be a very slow running test, I'm inclined to back it out. Also, the user who originally reported the issue was using 11k intervals, and it seems that the stack overflow fix is unlikely to help in that case. Is there any guidance for users on what is a reasonable number of intervals per process ? It sounds like the intention was that it be used with pretty small intervals. Should we issue a warning message in GenomicsDBImport at some threshold number of intervals ?. Are you planning to produce a jar with the error messages suppressed for this PR?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407867902
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407867902:845,Integrability,message,messages,845,"@kguraj Thanks for the response(s). The test in the PR was super useful as a temporary test, but as you mentioned it runs pretty slowly, and as it stands the test passes on current master anyway. It seems to require on the order of 9000-1000 intervals instead rather than 1000 to actually hit stack overflow. Since that would be a very slow running test, I'm inclined to back it out. Also, the user who originally reported the issue was using 11k intervals, and it seems that the stack overflow fix is unlikely to help in that case. Is there any guidance for users on what is a reasonable number of intervals per process ? It sounds like the intention was that it be used with pretty small intervals. Should we issue a warning message in GenomicsDBImport at some threshold number of intervals ?. Are you planning to produce a jar with the error messages suppressed for this PR?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407867902
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407867902:40,Testability,test,test,40,"@kguraj Thanks for the response(s). The test in the PR was super useful as a temporary test, but as you mentioned it runs pretty slowly, and as it stands the test passes on current master anyway. It seems to require on the order of 9000-1000 intervals instead rather than 1000 to actually hit stack overflow. Since that would be a very slow running test, I'm inclined to back it out. Also, the user who originally reported the issue was using 11k intervals, and it seems that the stack overflow fix is unlikely to help in that case. Is there any guidance for users on what is a reasonable number of intervals per process ? It sounds like the intention was that it be used with pretty small intervals. Should we issue a warning message in GenomicsDBImport at some threshold number of intervals ?. Are you planning to produce a jar with the error messages suppressed for this PR?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407867902
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407867902:87,Testability,test,test,87,"@kguraj Thanks for the response(s). The test in the PR was super useful as a temporary test, but as you mentioned it runs pretty slowly, and as it stands the test passes on current master anyway. It seems to require on the order of 9000-1000 intervals instead rather than 1000 to actually hit stack overflow. Since that would be a very slow running test, I'm inclined to back it out. Also, the user who originally reported the issue was using 11k intervals, and it seems that the stack overflow fix is unlikely to help in that case. Is there any guidance for users on what is a reasonable number of intervals per process ? It sounds like the intention was that it be used with pretty small intervals. Should we issue a warning message in GenomicsDBImport at some threshold number of intervals ?. Are you planning to produce a jar with the error messages suppressed for this PR?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407867902
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407867902:158,Testability,test,test,158,"@kguraj Thanks for the response(s). The test in the PR was super useful as a temporary test, but as you mentioned it runs pretty slowly, and as it stands the test passes on current master anyway. It seems to require on the order of 9000-1000 intervals instead rather than 1000 to actually hit stack overflow. Since that would be a very slow running test, I'm inclined to back it out. Also, the user who originally reported the issue was using 11k intervals, and it seems that the stack overflow fix is unlikely to help in that case. Is there any guidance for users on what is a reasonable number of intervals per process ? It sounds like the intention was that it be used with pretty small intervals. Should we issue a warning message in GenomicsDBImport at some threshold number of intervals ?. Are you planning to produce a jar with the error messages suppressed for this PR?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407867902
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407867902:349,Testability,test,test,349,"@kguraj Thanks for the response(s). The test in the PR was super useful as a temporary test, but as you mentioned it runs pretty slowly, and as it stands the test passes on current master anyway. It seems to require on the order of 9000-1000 intervals instead rather than 1000 to actually hit stack overflow. Since that would be a very slow running test, I'm inclined to back it out. Also, the user who originally reported the issue was using 11k intervals, and it seems that the stack overflow fix is unlikely to help in that case. Is there any guidance for users on what is a reasonable number of intervals per process ? It sounds like the intention was that it be used with pretty small intervals. Should we issue a warning message in GenomicsDBImport at some threshold number of intervals ?. Are you planning to produce a jar with the error messages suppressed for this PR?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407867902
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407867902:546,Usability,guid,guidance,546,"@kguraj Thanks for the response(s). The test in the PR was super useful as a temporary test, but as you mentioned it runs pretty slowly, and as it stands the test passes on current master anyway. It seems to require on the order of 9000-1000 intervals instead rather than 1000 to actually hit stack overflow. Since that would be a very slow running test, I'm inclined to back it out. Also, the user who originally reported the issue was using 11k intervals, and it seems that the stack overflow fix is unlikely to help in that case. Is there any guidance for users on what is a reasonable number of intervals per process ? It sounds like the intention was that it be used with pretty small intervals. Should we issue a warning message in GenomicsDBImport at some threshold number of intervals ?. Are you planning to produce a jar with the error messages suppressed for this PR?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-407867902
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-408165237:91,Integrability,message,message,91,"It's your call whether you want the test or not. Yeah, it would be good to put an advisory message if the number of intervals is more than 100. I uploaded a jar yesterday, but it's not showed up in Maven central",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-408165237
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-408165237:36,Testability,test,test,36,"It's your call whether you want the test or not. Yeah, it would be good to put an advisory message if the number of intervals is more than 100. I uploaded a jar yesterday, but it's not showed up in Maven central",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-408165237
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-408189092:31,Integrability,message,messages,31,New jar without spurious debug messages,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-408189092
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-408502462:22,Deployability,update,updated,22,"@kgururaj Thx for the updated jar. Can you remove the test commit now, and then we can run this on travis once more and then we can merge ? Thx.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-408502462
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-408502462:54,Testability,test,test,54,"@kgururaj Thx for the updated jar. Can you remove the test commit now, and then we can run this on travis once more and then we can merge ? Thx.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-408502462
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-408544552:28,Testability,test,test,28,"@kgururaj It looks like the test is still included in the PR - I think it should come out since it doesn't use enough intervals to test the fix, and it would be too slow if it did.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-408544552
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-408544552:131,Testability,test,test,131,"@kgururaj It looks like the test is still included in the PR - I think it should come out since it doesn't use enough intervals to test the fix, and it would be too slow if it did.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-408544552
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-408551280:17,Testability,test,test,17,It's no longer a test - just a private function. I just kept it in case somebody wishes to make it a good test in the future.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-408551280
https://github.com/broadinstitute/gatk/pull/4997#issuecomment-408551280:106,Testability,test,test,106,It's no longer a test - just a private function. I just kept it in case somebody wishes to make it a good test in the future.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4997#issuecomment-408551280
https://github.com/broadinstitute/gatk/pull/4999#issuecomment-405370854:1680,Security,Validat,ValidateBasicSomaticShortMutations,1680,nches 14766 14773 +7 ; ===============================================; + Hits 114640 114660 +20 ; - Misses 12746 12748 +2 ; - Partials 5327 5334 +7; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/4999?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...tmutpileup/BasicSomaticShortMutationValidator.java](https://codecov.io/gh/broadinstitute/gatk/pull/4999/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9CYXNpY1NvbWF0aWNTaG9ydE11dGF0aW9uVmFsaWRhdG9yLmphdmE=) | `60.526% <0%> (-4.339%)` | `5 <3> (ø)` | |; | [...dateBasicSomaticShortMutationsIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/4999/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zSW50ZWdyYXRpb25UZXN0LmphdmE=) | `100% <100%> (ø)` | `5 <0> (ø)` | :arrow_down: |; | [...tmutpileup/ValidateBasicSomaticShortMutations.java](https://codecov.io/gh/broadinstitute/gatk/pull/4999/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vYmFzaWNzaG9ydG11dHBpbGV1cC9WYWxpZGF0ZUJhc2ljU29tYXRpY1Nob3J0TXV0YXRpb25zLmphdmE=) | `83.486% <75%> (-3.419%)` | `19 <4> (+6)` | |; | [...nder/utils/runtime/StreamingProcessController.java](https://codecov.io/gh/broadinstitute/gatk/pull/4999/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9ydW50aW1lL1N0cmVhbWluZ1Byb2Nlc3NDb250cm9sbGVyLmphdmE=) | `71.292% <0%> (-0.957%)` | `35% <0%> (-1%)` | |; | [...e/hellbender/engine/spark/SparkContextFactory.java](https://codecov.io/gh/broadinstitute/gatk/pull/4999/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvc3BhcmsvU3BhcmtDb250ZXh0RmFjdG9yeS5qYXZh) | `73.973% <0%> (+2.74%)` | `11% <0%> (ø)` | :arrow_down: |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4999#issuecomment-405370854
https://github.com/broadinstitute/gatk/issues/5001#issuecomment-404629551:262,Deployability,integrat,integration,262,"To clarify, the tests are being run. It appears to be a bug in how we have configured the jacocoTestReport job that gets executed inside the docker image which seems to result some missing xml files that codeCoverage uses to build its reports. Since we have our integration and cloud tests outside of the docker image the coverage didn't drop to zero. I am looking into reconfiguring the jacocoTestReport task to behave correctly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5001#issuecomment-404629551
https://github.com/broadinstitute/gatk/issues/5001#issuecomment-404629551:262,Integrability,integrat,integration,262,"To clarify, the tests are being run. It appears to be a bug in how we have configured the jacocoTestReport job that gets executed inside the docker image which seems to result some missing xml files that codeCoverage uses to build its reports. Since we have our integration and cloud tests outside of the docker image the coverage didn't drop to zero. I am looking into reconfiguring the jacocoTestReport task to behave correctly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5001#issuecomment-404629551
https://github.com/broadinstitute/gatk/issues/5001#issuecomment-404629551:75,Modifiability,config,configured,75,"To clarify, the tests are being run. It appears to be a bug in how we have configured the jacocoTestReport job that gets executed inside the docker image which seems to result some missing xml files that codeCoverage uses to build its reports. Since we have our integration and cloud tests outside of the docker image the coverage didn't drop to zero. I am looking into reconfiguring the jacocoTestReport task to behave correctly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5001#issuecomment-404629551
https://github.com/broadinstitute/gatk/issues/5001#issuecomment-404629551:16,Testability,test,tests,16,"To clarify, the tests are being run. It appears to be a bug in how we have configured the jacocoTestReport job that gets executed inside the docker image which seems to result some missing xml files that codeCoverage uses to build its reports. Since we have our integration and cloud tests outside of the docker image the coverage didn't drop to zero. I am looking into reconfiguring the jacocoTestReport task to behave correctly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5001#issuecomment-404629551
https://github.com/broadinstitute/gatk/issues/5001#issuecomment-404629551:284,Testability,test,tests,284,"To clarify, the tests are being run. It appears to be a bug in how we have configured the jacocoTestReport job that gets executed inside the docker image which seems to result some missing xml files that codeCoverage uses to build its reports. Since we have our integration and cloud tests outside of the docker image the coverage didn't drop to zero. I am looking into reconfiguring the jacocoTestReport task to behave correctly.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5001#issuecomment-404629551
https://github.com/broadinstitute/gatk/issues/5003#issuecomment-404287460:16,Testability,test,tests,16,This is causing tests to fail in master.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5003#issuecomment-404287460
https://github.com/broadinstitute/gatk/issues/5003#issuecomment-404316063:7,Deployability,release,released,7,Google released a hotfix which fixed the problem.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5003#issuecomment-404316063
https://github.com/broadinstitute/gatk/issues/5003#issuecomment-404316063:18,Deployability,hotfix,hotfix,18,Google released a hotfix which fixed the problem.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5003#issuecomment-404316063
https://github.com/broadinstitute/gatk/pull/5004#issuecomment-404305209:21,Deployability,release,released,21,closing since google released 208.0.1 before I could get this passing.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5004#issuecomment-404305209
https://github.com/broadinstitute/gatk/pull/5005#issuecomment-404303174:103,Testability,test,test,103,@TedBrookings feel free to cherry-pick if I cannot merge this into master today (because of the gcloud test stuff),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5005#issuecomment-404303174
https://github.com/broadinstitute/gatk/pull/5007#issuecomment-404375834:1232,Security,validat,validation,1232,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5007?src=pr&el=h1) Report; > Merging [#5007](https://codecov.io/gh/broadinstitute/gatk/pull/5007?src=pr&el=desc) into [master](https://codecov.io/gh/broadinstitute/gatk/commit/1977c537b209d25fea504d2f601af7a9731debcf?src=pr&el=desc) will **increase** coverage by `0.015%`.; > The diff coverage is `80%`. ```diff; @@ Coverage Diff @@; ## master #5007 +/- ##; ===============================================; + Coverage 60.162% 60.177% +0.015% ; - Complexity 12772 12785 +13 ; ===============================================; Files 1095 1096 +1 ; Lines 64616 64666 +50 ; Branches 10394 10397 +3 ; ===============================================; + Hits 38874 38914 +40 ; - Misses 21504 21510 +6 ; - Partials 4238 4242 +4; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5007?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...e/hellbender/engine/AbstractConcordanceWalker.java](https://codecov.io/gh/broadinstitute/gatk/pull/5007/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci9lbmdpbmUvQWJzdHJhY3RDb25jb3JkYW5jZVdhbGtlci5qYXZh) | `84.706% <50%> (-0.836%)` | `13 <0> (ø)` | |; | [...s/walkers/validation/MergeMutect2CallsWithMC3.java](https://codecov.io/gh/broadinstitute/gatk/pull/5007/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL3ZhbGlkYXRpb24vTWVyZ2VNdXRlY3QyQ2FsbHNXaXRoTUMzLmphdmE=) | `81.25% <81.25%> (ø)` | `13 <13> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5007#issuecomment-404375834
https://github.com/broadinstitute/gatk/issues/5009#issuecomment-404619628:161,Deployability,release,releases,161,Sorry for the bug @cvalenci. That looks like something we've seen before and recently fixed (#4980). Can you try 4.0.6.0: https://github.com/broadinstitute/gatk/releases/tag/4.0.6.0 ?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5009#issuecomment-404619628
https://github.com/broadinstitute/gatk/issues/5013#issuecomment-405101030:48,Modifiability,inherit,inheritance,48,"Although I was again wrong by static-blocks and inheritance (see https://github.com/broadinstitute/gatk/issues/3483), I think that in the case of the tests is better to make overridable this config - thus, I keep it open.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5013#issuecomment-405101030
https://github.com/broadinstitute/gatk/issues/5013#issuecomment-405101030:191,Modifiability,config,config,191,"Although I was again wrong by static-blocks and inheritance (see https://github.com/broadinstitute/gatk/issues/3483), I think that in the case of the tests is better to make overridable this config - thus, I keep it open.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5013#issuecomment-405101030
https://github.com/broadinstitute/gatk/issues/5013#issuecomment-405101030:150,Testability,test,tests,150,"Although I was again wrong by static-blocks and inheritance (see https://github.com/broadinstitute/gatk/issues/3483), I think that in the case of the tests is better to make overridable this config - thus, I keep it open.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5013#issuecomment-405101030
https://github.com/broadinstitute/gatk/issues/5014#issuecomment-405100769:78,Availability,down,downstream,78,"Otherwise, a possibility is to move to an static method that can be called by downstream `Main` classes, to be sure that the static initialization is done in the same way as GATK. In this case, this should also being documented.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5014#issuecomment-405100769
https://github.com/broadinstitute/gatk/issues/5014#issuecomment-405100946:45,Modifiability,inherit,inheritance,45,"Sorry, again confounded by static-blocks and inheritance (dup of my own issue https://github.com/broadinstitute/gatk/issues/3483)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5014#issuecomment-405100946
https://github.com/broadinstitute/gatk/pull/5015#issuecomment-405596508:50,Availability,down,down,50,This brings read-count + allelic-count collection down to ~34 cents / TCGA WGS BAM from ~57 cents / BAM. @LeeTL1220 TAG team said they would like to wait for this change to be in FireCloud before starting their tests. When should we make it (along with the necessary changes to get tests passing here---and should that be a fork)?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-405596508
https://github.com/broadinstitute/gatk/pull/5015#issuecomment-405596508:211,Testability,test,tests,211,This brings read-count + allelic-count collection down to ~34 cents / TCGA WGS BAM from ~57 cents / BAM. @LeeTL1220 TAG team said they would like to wait for this change to be in FireCloud before starting their tests. When should we make it (along with the necessary changes to get tests passing here---and should that be a fork)?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-405596508
https://github.com/broadinstitute/gatk/pull/5015#issuecomment-405596508:282,Testability,test,tests,282,This brings read-count + allelic-count collection down to ~34 cents / TCGA WGS BAM from ~57 cents / BAM. @LeeTL1220 TAG team said they would like to wait for this change to be in FireCloud before starting their tests. When should we make it (along with the necessary changes to get tests passing here---and should that be a fork)?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-405596508
https://github.com/broadinstitute/gatk/pull/5015#issuecomment-406044277:255,Testability,test,tests,255,"@jnktsj see this PR to get an idea of how minimal the changes would be to switch the FC Featured versions of the ModelSegments WDLs to use NIO. For various reasons, I cannot easily make the switch to the WDLs in the repo here (as you can see, this causes tests to fail). So if you would like to go ahead and start testing, I would suggest that you simply clone the Featured WDLs and make the changes yourself, if that's something you're comfortable with.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-406044277
https://github.com/broadinstitute/gatk/pull/5015#issuecomment-406044277:314,Testability,test,testing,314,"@jnktsj see this PR to get an idea of how minimal the changes would be to switch the FC Featured versions of the ModelSegments WDLs to use NIO. For various reasons, I cannot easily make the switch to the WDLs in the repo here (as you can see, this causes tests to fail). So if you would like to go ahead and start testing, I would suggest that you simply clone the Featured WDLs and make the changes yourself, if that's something you're comfortable with.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-406044277
https://github.com/broadinstitute/gatk/pull/5015#issuecomment-406044277:348,Usability,simpl,simply,348,"@jnktsj see this PR to get an idea of how minimal the changes would be to switch the FC Featured versions of the ModelSegments WDLs to use NIO. For various reasons, I cannot easily make the switch to the WDLs in the repo here (as you can see, this causes tests to fail). So if you would like to go ahead and start testing, I would suggest that you simply clone the Featured WDLs and make the changes yourself, if that's something you're comfortable with.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-406044277
https://github.com/broadinstitute/gatk/pull/5015#issuecomment-461937520:110,Deployability,update,updated,110,@samuelklee What's the status of this? Now that the default version of cromwell in gatk is v36 should this be updated?,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-461937520
https://github.com/broadinstitute/gatk/pull/5015#issuecomment-461938277:38,Deployability,update,update,38,"I think we are also waiting for FC to update, so that NIO can be call cached. Not sure what the status is on that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-461938277
https://github.com/broadinstitute/gatk/pull/5015#issuecomment-461938277:70,Performance,cache,cached,70,"I think we are also waiting for FC to update, so that NIO can be call cached. Not sure what the status is on that.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5015#issuecomment-461938277
https://github.com/broadinstitute/gatk/pull/5017#issuecomment-415611303:755,Security,validat,validation,755,"@droazen, I think I have pushed most of the changes requested -. * Moved out `appendPathToDir` from BucketUtils to IOUtils; * `appendPathToDir` now uses Path.resolve() to append a given path to dir; * If a workspace already exists and `overExistingWorkspace` is false, a `UnableToCreateGenomicsDBWorkspace` exception is thrown while creating a GenomicsDB workspace.; * Made sure all paths passed to GenomicsDB are absolute.; * Introduced `gendb.hdfs:` and `gendb.gs:` URI schemes in addition to the existing `gendb:` scheme for identifying Cloud paths in GenomicsDB with unit testing for these new schemes.; * Added unit tests to test writing to GenomicsDB workspace/arrays to GCS and then reading/querying from the same GenomicsDB instance from GCS with validation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5017#issuecomment-415611303
https://github.com/broadinstitute/gatk/pull/5017#issuecomment-415611303:576,Testability,test,testing,576,"@droazen, I think I have pushed most of the changes requested -. * Moved out `appendPathToDir` from BucketUtils to IOUtils; * `appendPathToDir` now uses Path.resolve() to append a given path to dir; * If a workspace already exists and `overExistingWorkspace` is false, a `UnableToCreateGenomicsDBWorkspace` exception is thrown while creating a GenomicsDB workspace.; * Made sure all paths passed to GenomicsDB are absolute.; * Introduced `gendb.hdfs:` and `gendb.gs:` URI schemes in addition to the existing `gendb:` scheme for identifying Cloud paths in GenomicsDB with unit testing for these new schemes.; * Added unit tests to test writing to GenomicsDB workspace/arrays to GCS and then reading/querying from the same GenomicsDB instance from GCS with validation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5017#issuecomment-415611303
https://github.com/broadinstitute/gatk/pull/5017#issuecomment-415611303:621,Testability,test,tests,621,"@droazen, I think I have pushed most of the changes requested -. * Moved out `appendPathToDir` from BucketUtils to IOUtils; * `appendPathToDir` now uses Path.resolve() to append a given path to dir; * If a workspace already exists and `overExistingWorkspace` is false, a `UnableToCreateGenomicsDBWorkspace` exception is thrown while creating a GenomicsDB workspace.; * Made sure all paths passed to GenomicsDB are absolute.; * Introduced `gendb.hdfs:` and `gendb.gs:` URI schemes in addition to the existing `gendb:` scheme for identifying Cloud paths in GenomicsDB with unit testing for these new schemes.; * Added unit tests to test writing to GenomicsDB workspace/arrays to GCS and then reading/querying from the same GenomicsDB instance from GCS with validation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5017#issuecomment-415611303
https://github.com/broadinstitute/gatk/pull/5017#issuecomment-415611303:630,Testability,test,test,630,"@droazen, I think I have pushed most of the changes requested -. * Moved out `appendPathToDir` from BucketUtils to IOUtils; * `appendPathToDir` now uses Path.resolve() to append a given path to dir; * If a workspace already exists and `overExistingWorkspace` is false, a `UnableToCreateGenomicsDBWorkspace` exception is thrown while creating a GenomicsDB workspace.; * Made sure all paths passed to GenomicsDB are absolute.; * Introduced `gendb.hdfs:` and `gendb.gs:` URI schemes in addition to the existing `gendb:` scheme for identifying Cloud paths in GenomicsDB with unit testing for these new schemes.; * Added unit tests to test writing to GenomicsDB workspace/arrays to GCS and then reading/querying from the same GenomicsDB instance from GCS with validation.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5017#issuecomment-415611303
https://github.com/broadinstitute/gatk/pull/5017#issuecomment-422559926:129,Testability,test,tests,129,"I've opened https://github.com/broadinstitute/gatk/pull/5197 with the last few comments addressed. We'll see if the travis cloud tests pass (since they were never actually run on this PR), and if they do pass we can merge.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5017#issuecomment-422559926
https://github.com/broadinstitute/gatk/pull/5017#issuecomment-423026481:338,Availability,failure,failures,338,"@droazen, I have pushed a debugging test. It just prints out all the keys and not private and not id values in the service json pointed by GOOGLE_APPLICATION_CREDENTIALS env. Would it be possible to accept this into the nalinigans_genomicsdb_uri_support branch, so I can browse through the stdout for that test on Travis? By the way, the failures in the build seem to be unrelated to my change. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5017#issuecomment-423026481
https://github.com/broadinstitute/gatk/pull/5017#issuecomment-423026481:36,Testability,test,test,36,"@droazen, I have pushed a debugging test. It just prints out all the keys and not private and not id values in the service json pointed by GOOGLE_APPLICATION_CREDENTIALS env. Would it be possible to accept this into the nalinigans_genomicsdb_uri_support branch, so I can browse through the stdout for that test on Travis? By the way, the failures in the build seem to be unrelated to my change. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5017#issuecomment-423026481
https://github.com/broadinstitute/gatk/pull/5017#issuecomment-423026481:306,Testability,test,test,306,"@droazen, I have pushed a debugging test. It just prints out all the keys and not private and not id values in the service json pointed by GOOGLE_APPLICATION_CREDENTIALS env. Would it be possible to accept this into the nalinigans_genomicsdb_uri_support branch, so I can browse through the stdout for that test on Travis? By the way, the failures in the build seem to be unrelated to my change. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5017#issuecomment-423026481
https://github.com/broadinstitute/gatk/pull/5018#issuecomment-405633317:17,Usability,clear,clear,17,Its not entirely clear to my why Codecov hasn't commented here. But here is the commit on their page: https://codecov.io/gh/broadinstitute/gatk/commit/af23590723748fa27a2d065e48c26a20d0e91488,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5018#issuecomment-405633317
https://github.com/broadinstitute/gatk/pull/5019#issuecomment-405760843:30,Testability,test,tests,30,I am curious why the WDL auto tests did not catch this..,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5019#issuecomment-405760843
https://github.com/broadinstitute/gatk/pull/5019#issuecomment-405939434:32,Testability,test,tests,32,"> I am curious why the WDL auto tests did not catch this.. @LeeTL1220 In `scripts/m2_cromwell_tests/test_m2_wdl_multi.json` we test with; ```; ""Mutect2_Multi.run_orientation_bias_filter"": true`,; ""Mutect2_Multi.artifact_modes"": [""G/T"", ""C/T""],; ```; whereas the probable bug I foresaw would only occur if these lines were omitted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5019#issuecomment-405939434
https://github.com/broadinstitute/gatk/pull/5019#issuecomment-405939434:127,Testability,test,test,127,"> I am curious why the WDL auto tests did not catch this.. @LeeTL1220 In `scripts/m2_cromwell_tests/test_m2_wdl_multi.json` we test with; ```; ""Mutect2_Multi.run_orientation_bias_filter"": true`,; ""Mutect2_Multi.artifact_modes"": [""G/T"", ""C/T""],; ```; whereas the probable bug I foresaw would only occur if these lines were omitted.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5019#issuecomment-405939434
https://github.com/broadinstitute/gatk/pull/5021#issuecomment-405984274:72,Availability,down,downloads,72,"The tests were failing for some reason, probably due to some dependency downloads failing, so I'm rerunning them. Will merge when that's done.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5021#issuecomment-405984274
https://github.com/broadinstitute/gatk/pull/5021#issuecomment-405984274:61,Integrability,depend,dependency,61,"The tests were failing for some reason, probably due to some dependency downloads failing, so I'm rerunning them. Will merge when that's done.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5021#issuecomment-405984274
https://github.com/broadinstitute/gatk/pull/5021#issuecomment-405984274:4,Testability,test,tests,4,"The tests were failing for some reason, probably due to some dependency downloads failing, so I'm rerunning them. Will merge when that's done.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5021#issuecomment-405984274
https://github.com/broadinstitute/gatk/pull/5021#issuecomment-405986092:1806,Deployability,pipeline,pipelines,1806,``. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5021?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [.../org/broadinstitute/hellbender/tools/FlagStat.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9GbGFnU3RhdC5qYXZh) | `76.404% <ø> (ø)` | `3 <0> (ø)` | :arrow_down: |; | [...rg/broadinstitute/hellbender/tools/SplitReads.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9TcGxpdFJlYWRzLmphdmE=) | `90% <ø> (ø)` | `20 <0> (ø)` | :arrow_down: |; | [...broadinstitute/hellbender/tools/GetSampleName.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9HZXRTYW1wbGVOYW1lLmphdmE=) | `66.667% <ø> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...ellbender/tools/spark/pipelines/FlagStatSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvRmxhZ1N0YXRTcGFyay5qYXZh) | `90% <ø> (ø)` | `4 <0> (ø)` | :arrow_down: |; | [...tools/walkers/haplotypecaller/HaplotypeCaller.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXIuamF2YQ==) | `0% <0%> (-76.923%)` | `0% <0%> (-17%)` | |; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-74.257%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5021#issuecomment-405986092
https://github.com/broadinstitute/gatk/pull/5021#issuecomment-405986092:2726,Deployability,pipeline,pipelines,2726,667% <ø> (ø)` | `7 <0> (ø)` | :arrow_down: |; | [...ellbender/tools/spark/pipelines/FlagStatSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvRmxhZ1N0YXRTcGFyay5qYXZh) | `90% <ø> (ø)` | `4 <0> (ø)` | :arrow_down: |; | [...tools/walkers/haplotypecaller/HaplotypeCaller.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXIuamF2YQ==) | `0% <0%> (-76.923%)` | `0% <0%> (-17%)` | |; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-74.257%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <0%> (-22.807%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3Jj,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5021#issuecomment-405986092
https://github.com/broadinstitute/gatk/pull/5021#issuecomment-405986092:3042,Testability,test,test,3042,cy9zcGFyay9waXBlbGluZXMvRmxhZ1N0YXRTcGFyay5qYXZh) | `90% <ø> (ø)` | `4 <0> (ø)` | :arrow_down: |; | [...tools/walkers/haplotypecaller/HaplotypeCaller.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2hhcGxvdHlwZWNhbGxlci9IYXBsb3R5cGVDYWxsZXIuamF2YQ==) | `0% <0%> (-76.923%)` | `0% <0%> (-17%)` | |; | [...s/spark/ParallelCopyGCSDirectoryIntoHDFSSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9QYXJhbGxlbENvcHlHQ1NEaXJlY3RvcnlJbnRvSERGU1NwYXJrLmphdmE=) | `0% <0%> (-74.257%)` | `0% <0%> (-17%)` | |; | [...nder/tools/spark/pipelines/PrintVariantsSpark.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvUHJpbnRWYXJpYW50c1NwYXJrLmphdmE=) | `0% <0%> (-66.667%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/test/XorWrapper.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy90ZXN0L1hvcldyYXBwZXIuamF2YQ==) | `13.043% <0%> (-65.217%)` | `2% <0%> (-7%)` | |; | [...nder/tools/spark/BaseRecalibratorSparkSharded.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9CYXNlUmVjYWxpYnJhdG9yU3BhcmtTaGFyZGVkLmphdmE=) | `0% <0%> (-22.807%)` | `0% <0%> (-2%)` | |; | [...oadinstitute/hellbender/utils/gcs/BucketUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9nY3MvQnVja2V0VXRpbHMuamF2YQ==) | `54.194% <0%> (-20%)` | `30% <0%> (-5%)` | |; | ... and [1236 more](https://codecov.io/gh/broadinstitute/gatk/pull/5021/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5021#issuecomment-405986092
https://github.com/broadinstitute/gatk/issues/5022#issuecomment-405647323:25,Integrability,depend,dependency,25,"Actually, is the reshape dependency even required for AnalyzeCovariates? If not, we should just remove the import statement from BQSR.R. In any case, we should still add a test to cover plotting. @droazen can you delegate this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5022#issuecomment-405647323
https://github.com/broadinstitute/gatk/issues/5022#issuecomment-405647323:172,Testability,test,test,172,"Actually, is the reshape dependency even required for AnalyzeCovariates? If not, we should just remove the import statement from BQSR.R. In any case, we should still add a test to cover plotting. @droazen can you delegate this?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5022#issuecomment-405647323
https://github.com/broadinstitute/gatk/pull/5023#issuecomment-405694045:2894,Deployability,pipeline,pipelines,2894, [.../markduplicates/MarkDuplicatesScoringStrategy.java](https://codecov.io/gh/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL01hcmtEdXBsaWNhdGVzU2NvcmluZ1N0cmF0ZWd5LmphdmE=) | `76.471% <0%> (-5.882%)` | `7 <0> (ø)` | |; | [...r/utils/read/markduplicates/sparkrecords/Pair.java](https://codecov.io/gh/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9QYWlyLmphdmE=) | `100% <100%> (ø)` | `26 <1> (-1)` | :arrow_down: |; | [...forms/markduplicates/MarkDuplicatesSparkUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay90cmFuc2Zvcm1zL21hcmtkdXBsaWNhdGVzL01hcmtEdXBsaWNhdGVzU3BhcmtVdGlscy5qYXZh) | `91.284% <100%> (-0.079%)` | `65 <0> (ø)` | |; | [.../pipelines/MarkDuplicatesSparkIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvTWFya0R1cGxpY2F0ZXNTcGFya0ludGVncmF0aW9uVGVzdC5qYXZh) | `89.933% <100%> (+1.383%)` | `27 <6> (+6)` | :arrow_up: |; | [...ils/read/markduplicates/sparkrecords/Fragment.java](https://codecov.io/gh/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9GcmFnbWVudC5qYXZh) | `92.857% <100%> (-0.893%)` | `6 <1> (-1)` | |; | [...ats/collections/SimpleCountCollectionUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2Zvcm1hdHMvY29sbGVjdGlvbnMvU2ltcGxlQ291bnRDb2xsZWN0aW9uVW5pdFRlc3QuamF2YQ==) | `83.784% <0%> (-5.105%)` | `5% <0%> (+2%)` | |; | [...ber/formats/collections/Simp,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5023#issuecomment-405694045
https://github.com/broadinstitute/gatk/pull/5023#issuecomment-405694045:3561,Usability,Simpl,SimpleCountCollectionUnitTest,3561,h/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9QYWlyLmphdmE=) | `100% <100%> (ø)` | `26 <1> (-1)` | :arrow_down: |; | [...forms/markduplicates/MarkDuplicatesSparkUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay90cmFuc2Zvcm1zL21hcmtkdXBsaWNhdGVzL01hcmtEdXBsaWNhdGVzU3BhcmtVdGlscy5qYXZh) | `91.284% <100%> (-0.079%)` | `65 <0> (ø)` | |; | [.../pipelines/MarkDuplicatesSparkIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvTWFya0R1cGxpY2F0ZXNTcGFya0ludGVncmF0aW9uVGVzdC5qYXZh) | `89.933% <100%> (+1.383%)` | `27 <6> (+6)` | :arrow_up: |; | [...ils/read/markduplicates/sparkrecords/Fragment.java](https://codecov.io/gh/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9GcmFnbWVudC5qYXZh) | `92.857% <100%> (-0.893%)` | `6 <1> (-1)` | |; | [...ats/collections/SimpleCountCollectionUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2Zvcm1hdHMvY29sbGVjdGlvbnMvU2ltcGxlQ291bnRDb2xsZWN0aW9uVW5pdFRlc3QuamF2YQ==) | `83.784% <0%> (-5.105%)` | `5% <0%> (+2%)` | |; | [...ber/formats/collections/SimpleCountCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2Zvcm1hdHMvY29sbGVjdGlvbnMvU2ltcGxlQ291bnRDb2xsZWN0aW9uLmphdmE=) | `100% <0%> (ø)` | `12% <0%> (+1%)` | :arrow_up: |; | ... and [4 more](https://codecov.io/gh/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5023#issuecomment-405694045
https://github.com/broadinstitute/gatk/pull/5023#issuecomment-405694045:3912,Usability,Simpl,SimpleCountCollection,3912,h/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9QYWlyLmphdmE=) | `100% <100%> (ø)` | `26 <1> (-1)` | :arrow_down: |; | [...forms/markduplicates/MarkDuplicatesSparkUtils.java](https://codecov.io/gh/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay90cmFuc2Zvcm1zL21hcmtkdXBsaWNhdGVzL01hcmtEdXBsaWNhdGVzU3BhcmtVdGlscy5qYXZh) | `91.284% <100%> (-0.079%)` | `65 <0> (ø)` | |; | [.../pipelines/MarkDuplicatesSparkIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9zcGFyay9waXBlbGluZXMvTWFya0R1cGxpY2F0ZXNTcGFya0ludGVncmF0aW9uVGVzdC5qYXZh) | `89.933% <100%> (+1.383%)` | `27 <6> (+6)` | :arrow_up: |; | [...ils/read/markduplicates/sparkrecords/Fragment.java](https://codecov.io/gh/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci91dGlscy9yZWFkL21hcmtkdXBsaWNhdGVzL3NwYXJrcmVjb3Jkcy9GcmFnbWVudC5qYXZh) | `92.857% <100%> (-0.893%)` | `6 <1> (-1)` | |; | [...ats/collections/SimpleCountCollectionUnitTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2Zvcm1hdHMvY29sbGVjdGlvbnMvU2ltcGxlQ291bnRDb2xsZWN0aW9uVW5pdFRlc3QuamF2YQ==) | `83.784% <0%> (-5.105%)` | `5% <0%> (+2%)` | |; | [...ber/formats/collections/SimpleCountCollection.java](https://codecov.io/gh/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree#diff-c3JjL21haW4vamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy9jb3B5bnVtYmVyL2Zvcm1hdHMvY29sbGVjdGlvbnMvU2ltcGxlQ291bnRDb2xsZWN0aW9uLmphdmE=) | `100% <0%> (ø)` | `12% <0%> (+1%)` | :arrow_up: |; | ... and [4 more](https://codecov.io/gh/broadinstitute/gatk/pull/5023/diff?src=pr&el=tree-more) | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5023#issuecomment-405694045
https://github.com/broadinstitute/gatk/pull/5023#issuecomment-413289284:31,Testability,test,tests,31,@lbergelson Removed the broken tests. The problem stemmed from markDuplicatesGATK being out of date. Since we are removing to remove it soon anyway i figured we would have the test only apply for markDuplicatesSpark,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5023#issuecomment-413289284
https://github.com/broadinstitute/gatk/pull/5023#issuecomment-413289284:176,Testability,test,test,176,@lbergelson Removed the broken tests. The problem stemmed from markDuplicatesGATK being out of date. Since we are removing to remove it soon anyway i figured we would have the test only apply for markDuplicatesSpark,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5023#issuecomment-413289284
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-405949809:25,Availability,error,error,25,@kgururaj I suspect this error is coming from genomicsDB ? @EvanTheB did you see a change when running on identical inputs (the failure case the log files you included covers quite a bit more genomic territory than the success case).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-405949809
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-405949809:128,Availability,failure,failure,128,@kgururaj I suspect this error is coming from genomicsDB ? @EvanTheB did you see a change when running on identical inputs (the failure case the log files you included covers quite a bit more genomic territory than the success case).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-405949809
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-405949809:145,Testability,log,log,145,@kgururaj I suspect this error is coming from genomicsDB ? @EvanTheB did you see a change when running on identical inputs (the failure case the log files you included covers quite a bit more genomic territory than the success case).,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-405949809
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406119705:46,Testability,log,log,46,"@cmnbroad apologies, I intended to upload the log from the same shard, the stderr is re-uploaded here. All shards of 4.0.6.0 failed, while all shards of 4.0.4.0 succeeded. . [gengvcferr.txt](https://github.com/broadinstitute/gatk/files/2207790/gengvcferr.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406119705
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406750537:168,Energy Efficiency,consumption,consumption,168,"To be honest, I don't have a clear idea of why this is happening. I tried running a query with 1000 samples using the same GenomicsDB jar that GATK uses and the memory consumption stayed below 1 GB. Some suggestions/questions:; * If you were importing/querying multiple intervals at once, I would expect #4994 to be relevant. But your script shows a single interval being imported/queried.; * Would it be possible to run the SelectVariants tool using the GenomicsDB workspace as input and see how much memory is being consumed (instead of GenotypeGVCFs)? The Select tool simply extracts the data from GenomicsDB and prints out a VCF.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406750537
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406750537:29,Usability,clear,clear,29,"To be honest, I don't have a clear idea of why this is happening. I tried running a query with 1000 samples using the same GenomicsDB jar that GATK uses and the memory consumption stayed below 1 GB. Some suggestions/questions:; * If you were importing/querying multiple intervals at once, I would expect #4994 to be relevant. But your script shows a single interval being imported/queried.; * Would it be possible to run the SelectVariants tool using the GenomicsDB workspace as input and see how much memory is being consumed (instead of GenotypeGVCFs)? The Select tool simply extracts the data from GenomicsDB and prints out a VCF.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406750537
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406750537:571,Usability,simpl,simply,571,"To be honest, I don't have a clear idea of why this is happening. I tried running a query with 1000 samples using the same GenomicsDB jar that GATK uses and the memory consumption stayed below 1 GB. Some suggestions/questions:; * If you were importing/querying multiple intervals at once, I would expect #4994 to be relevant. But your script shows a single interval being imported/queried.; * Would it be possible to run the SelectVariants tool using the GenomicsDB workspace as input and see how much memory is being consumed (instead of GenotypeGVCFs)? The Select tool simply extracts the data from GenomicsDB and prints out a VCF.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406750537
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406906206:39,Availability,error,error,39,From the stack trace I had assumed the error was likely related to loading the dbsnp vcf. From: gs://broad-references/hg38/v0/. I have attached a 1Mbase subset of my gvcf and a script that reliably reproduces the error for me. [script.txt](https://github.com/broadinstitute/gatk/files/2217511/script.txt); [NA12878.hg38.g.vcf.gz](https://github.com/broadinstitute/gatk/files/2217512/NA12878.hg38.g.vcf.gz),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406906206
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406906206:189,Availability,reliab,reliably,189,From the stack trace I had assumed the error was likely related to loading the dbsnp vcf. From: gs://broad-references/hg38/v0/. I have attached a 1Mbase subset of my gvcf and a script that reliably reproduces the error for me. [script.txt](https://github.com/broadinstitute/gatk/files/2217511/script.txt); [NA12878.hg38.g.vcf.gz](https://github.com/broadinstitute/gatk/files/2217512/NA12878.hg38.g.vcf.gz),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406906206
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406906206:213,Availability,error,error,213,From the stack trace I had assumed the error was likely related to loading the dbsnp vcf. From: gs://broad-references/hg38/v0/. I have attached a 1Mbase subset of my gvcf and a script that reliably reproduces the error for me. [script.txt](https://github.com/broadinstitute/gatk/files/2217511/script.txt); [NA12878.hg38.g.vcf.gz](https://github.com/broadinstitute/gatk/files/2217512/NA12878.hg38.g.vcf.gz),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406906206
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406906206:67,Performance,load,loading,67,From the stack trace I had assumed the error was likely related to loading the dbsnp vcf. From: gs://broad-references/hg38/v0/. I have attached a 1Mbase subset of my gvcf and a script that reliably reproduces the error for me. [script.txt](https://github.com/broadinstitute/gatk/files/2217511/script.txt); [NA12878.hg38.g.vcf.gz](https://github.com/broadinstitute/gatk/files/2217512/NA12878.hg38.g.vcf.gz),MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-406906206
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-407243127:113,Availability,error,error,113,"@kgururaj are you using the script I provided? Otherwise it must be something site specific here. . Update: This error only occurs when the genomicsdb is on the /tmp drive. I verified by copying the db back and forth, and it works when on the local drive, fails on the tmp drive. . The /tmp drive is XFS, and it does not have locking enabled. (`flock /tmp/test echo 1` fails). I suspect these other tickets may be relevant:. https://github.com/Intel-HLS/TileDB/pull/77; https://github.com/broadinstitute/gatk/issues/4753; https://gatkforums.broadinstitute.org/gatk/discussion/11184/could-not-open-array-genomicsdb-array-at-workspace-from-genotypegvcfs-in-gatk-4-0-0-0. However the error is an allocation error in a std::vector, so it could still be some other issue. Is the failure to flock not being checked for?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-407243127
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-407243127:361,Availability,echo,echo,361,"@kgururaj are you using the script I provided? Otherwise it must be something site specific here. . Update: This error only occurs when the genomicsdb is on the /tmp drive. I verified by copying the db back and forth, and it works when on the local drive, fails on the tmp drive. . The /tmp drive is XFS, and it does not have locking enabled. (`flock /tmp/test echo 1` fails). I suspect these other tickets may be relevant:. https://github.com/Intel-HLS/TileDB/pull/77; https://github.com/broadinstitute/gatk/issues/4753; https://gatkforums.broadinstitute.org/gatk/discussion/11184/could-not-open-array-genomicsdb-array-at-workspace-from-genotypegvcfs-in-gatk-4-0-0-0. However the error is an allocation error in a std::vector, so it could still be some other issue. Is the failure to flock not being checked for?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-407243127
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-407243127:681,Availability,error,error,681,"@kgururaj are you using the script I provided? Otherwise it must be something site specific here. . Update: This error only occurs when the genomicsdb is on the /tmp drive. I verified by copying the db back and forth, and it works when on the local drive, fails on the tmp drive. . The /tmp drive is XFS, and it does not have locking enabled. (`flock /tmp/test echo 1` fails). I suspect these other tickets may be relevant:. https://github.com/Intel-HLS/TileDB/pull/77; https://github.com/broadinstitute/gatk/issues/4753; https://gatkforums.broadinstitute.org/gatk/discussion/11184/could-not-open-array-genomicsdb-array-at-workspace-from-genotypegvcfs-in-gatk-4-0-0-0. However the error is an allocation error in a std::vector, so it could still be some other issue. Is the failure to flock not being checked for?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-407243127
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-407243127:704,Availability,error,error,704,"@kgururaj are you using the script I provided? Otherwise it must be something site specific here. . Update: This error only occurs when the genomicsdb is on the /tmp drive. I verified by copying the db back and forth, and it works when on the local drive, fails on the tmp drive. . The /tmp drive is XFS, and it does not have locking enabled. (`flock /tmp/test echo 1` fails). I suspect these other tickets may be relevant:. https://github.com/Intel-HLS/TileDB/pull/77; https://github.com/broadinstitute/gatk/issues/4753; https://gatkforums.broadinstitute.org/gatk/discussion/11184/could-not-open-array-genomicsdb-array-at-workspace-from-genotypegvcfs-in-gatk-4-0-0-0. However the error is an allocation error in a std::vector, so it could still be some other issue. Is the failure to flock not being checked for?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-407243127
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-407243127:774,Availability,failure,failure,774,"@kgururaj are you using the script I provided? Otherwise it must be something site specific here. . Update: This error only occurs when the genomicsdb is on the /tmp drive. I verified by copying the db back and forth, and it works when on the local drive, fails on the tmp drive. . The /tmp drive is XFS, and it does not have locking enabled. (`flock /tmp/test echo 1` fails). I suspect these other tickets may be relevant:. https://github.com/Intel-HLS/TileDB/pull/77; https://github.com/broadinstitute/gatk/issues/4753; https://gatkforums.broadinstitute.org/gatk/discussion/11184/could-not-open-array-genomicsdb-array-at-workspace-from-genotypegvcfs-in-gatk-4-0-0-0. However the error is an allocation error in a std::vector, so it could still be some other issue. Is the failure to flock not being checked for?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-407243127
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-407243127:100,Deployability,Update,Update,100,"@kgururaj are you using the script I provided? Otherwise it must be something site specific here. . Update: This error only occurs when the genomicsdb is on the /tmp drive. I verified by copying the db back and forth, and it works when on the local drive, fails on the tmp drive. . The /tmp drive is XFS, and it does not have locking enabled. (`flock /tmp/test echo 1` fails). I suspect these other tickets may be relevant:. https://github.com/Intel-HLS/TileDB/pull/77; https://github.com/broadinstitute/gatk/issues/4753; https://gatkforums.broadinstitute.org/gatk/discussion/11184/could-not-open-array-genomicsdb-array-at-workspace-from-genotypegvcfs-in-gatk-4-0-0-0. However the error is an allocation error in a std::vector, so it could still be some other issue. Is the failure to flock not being checked for?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-407243127
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-407243127:356,Testability,test,test,356,"@kgururaj are you using the script I provided? Otherwise it must be something site specific here. . Update: This error only occurs when the genomicsdb is on the /tmp drive. I verified by copying the db back and forth, and it works when on the local drive, fails on the tmp drive. . The /tmp drive is XFS, and it does not have locking enabled. (`flock /tmp/test echo 1` fails). I suspect these other tickets may be relevant:. https://github.com/Intel-HLS/TileDB/pull/77; https://github.com/broadinstitute/gatk/issues/4753; https://gatkforums.broadinstitute.org/gatk/discussion/11184/could-not-open-array-genomicsdb-array-at-workspace-from-genotypegvcfs-in-gatk-4-0-0-0. However the error is an allocation error in a std::vector, so it could still be some other issue. Is the failure to flock not being checked for?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-407243127
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-409070215:163,Availability,error,error,163,"I tried running gatk version 4.0.7.0 with the environment variable: 'TILEDB_DISABLE_FILE_LOCKING=YES' to see if that would fix the issue, but I still get the same error. I am not sure that the patch that enable that was actually in the 4.0.7.0 release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-409070215
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-409070215:193,Deployability,patch,patch,193,"I tried running gatk version 4.0.7.0 with the environment variable: 'TILEDB_DISABLE_FILE_LOCKING=YES' to see if that would fix the issue, but I still get the same error. I am not sure that the patch that enable that was actually in the 4.0.7.0 release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-409070215
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-409070215:244,Deployability,release,release,244,"I tried running gatk version 4.0.7.0 with the environment variable: 'TILEDB_DISABLE_FILE_LOCKING=YES' to see if that would fix the issue, but I still get the same error. I am not sure that the patch that enable that was actually in the 4.0.7.0 release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-409070215
https://github.com/broadinstitute/gatk/issues/5024#issuecomment-409070215:58,Modifiability,variab,variable,58,"I tried running gatk version 4.0.7.0 with the environment variable: 'TILEDB_DISABLE_FILE_LOCKING=YES' to see if that would fix the issue, but I still get the same error. I am not sure that the patch that enable that was actually in the 4.0.7.0 release.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5024#issuecomment-409070215
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406014137:53,Availability,error,errors,53,"@samuelklee This is failing with ""can't find ggplot2 errors""",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406014137
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406016353:36,Deployability,install,installed,36,"It seems to imply that ggplot isn't installed in the docker, which is confusing because it should be.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406016353
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406023338:50,Deployability,install,installed,50,"Actually, it appears that ggplot2 isn't currently installed in broadinstitute/gatk:gatkbase-2.0.0, although other packages that are installed via the install_R_packages.R script are indeed present. Is this image in a bad state?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406023338
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406023338:132,Deployability,install,installed,132,"Actually, it appears that ggplot2 isn't currently installed in broadinstitute/gatk:gatkbase-2.0.0, although other packages that are installed via the install_R_packages.R script are indeed present. Is this image in a bad state?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406023338
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:712,Availability,error,errors,712,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:1388,Availability,error,error,1388,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:59,Deployability,install,installed,59,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:126,Deployability,install,installing,126,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:360,Deployability,install,installed,360,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:447,Deployability,install,install,447,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:568,Deployability,install,installed,568,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:661,Deployability,install,installs,661,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:1056,Deployability,install,installed,1056,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:1131,Deployability,install,install,1131,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:1263,Deployability,install,installs,1263,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:1520,Deployability,install,installed,1520,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:162,Integrability,depend,dependencies,162,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:333,Integrability,depend,dependencies,333,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:464,Integrability,depend,dependencies,464,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:541,Integrability,depend,dependencies,541,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:801,Integrability,depend,dependencies,801,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:1012,Integrability,depend,dependency,1012,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:1394,Integrability,message,message,1394,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261:950,Usability,learn,learned,950,"Looks like all packages *except* ggplot2 were successfully installed. The following lines in the R script are responsible for installing 3 of the packages:. ```; dependencies = c(""ggplot2"",""gplots"",""gsalib""); repos <- c(""http://cran.cnr.Berkeley.edu"",; ""https://cran.mtu.edu"",; ""http://lib.stat.cmu.edu/R/CRAN/""); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- 1; while(length(missing)!=0 & try <= length(repos)) {; install.packages(dependencies[missing], repos = repos[try], clean = TRUE); missing <- which(!(dependencies %in% rownames(installed.packages()))); try <- try + 1; }; ```. I guess this is supposed to ensure that the installs don't fail due to intermittent connection errors, etc., but each repo is only hit once and it's possible for the loop to exit with dependencies still missing. Could this have happened when the current base image was built and pushed? @jamesemery did you push this image?. Also, I learned that *reshape2* (as opposed to reshape) is actually a dependency of ggplot2 that is automatically installed along with ggplot2. So the original removal of reshape from the `install.packages` list was fine. However, the import statement that is removed in this PR fails whether or not ggplot2 successfully installs, and is extraneous in any case. This is all consistent with the fact that the users from the forum post only get an error message about reshape and not ggplot2. Note that they are using broadinstitute/gatk:4.0.4.0, in which ggplot2 is successfully installed.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406028261
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406068437:66,Integrability,depend,dependencies,66,"See latest comments in #4250. Not sure if using conda to manage R dependencies will affect the size of the image, any idea @jamesemery?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406068437
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:45,Deployability,install,install,45,"OK, I experimented a bit with removing the R install from the base image and adding the R dependencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:748,Deployability,install,install,748,"OK, I experimented a bit with removing the R install from the base image and adding the R dependencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:893,Deployability,install,install,893,"OK, I experimented a bit with removing the R install from the base image and adding the R dependencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:1111,Deployability,install,install,1111,"endencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnondeterminism-perl; libfont-afm-perl libfontenc1 libgcc-5-dev libgdbm3 libgettextpo-dev; libget",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:1193,Deployability,install,install,1193,"re falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnondeterminism-perl; libfont-afm-perl libfontenc1 libgcc-5-dev libgdbm3 libgettextpo-dev; libgettextpo0 libgfortran-5-dev libgfortran3 libgomp1 libhtml-form-perl; libhtml-format-perl libhtml-parser-perl libhtml-tagset-perl; libhtml-tree-perl libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl; libhttp-message-perl libhttp-negotiate-perl libio-html-perl; libio-socket-ssl-perl libipc-system-simple-perl libisc-export160 libisl15; libitm1 libjpeg-dev libjpeg-turbo8-dev libjpeg8-dev liblapack-dev liblapack3; liblsan0 liblwp-mediatypes-perl liblwp-protocol-https-perl liblzma-dev; libmail-sendmail-perl libmailtools-perl libmnl0 libmpc3 libmpfr4 libmpx0; libncurses5-dev libnet-dbus-perl libnet-http-perl libnet-smtp-ssl-perl; libnet-ssleay-perl libpaper-utils libpaper1 libpcre16-3 libpcre3-dev; libpcre32-3 libpcrecpp0v5 libperl5.22 libpipeline1 libpng12-dev libquadmath0; libreadline-dev libreadline6-dev libsigsegv2 libstdc++-5-dev; libsys-hostname-long-perl libtcl8.6 libtext-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:3378,Deployability,patch,patch,3378,"-listing-perl libfile-mimeinfo-perl libfile-stripnondeterminism-perl; libfont-afm-perl libfontenc1 libgcc-5-dev libgdbm3 libgettextpo-dev; libgettextpo0 libgfortran-5-dev libgfortran3 libgomp1 libhtml-form-perl; libhtml-format-perl libhtml-parser-perl libhtml-tagset-perl; libhtml-tree-perl libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl; libhttp-message-perl libhttp-negotiate-perl libio-html-perl; libio-socket-ssl-perl libipc-system-simple-perl libisc-export160 libisl15; libitm1 libjpeg-dev libjpeg-turbo8-dev libjpeg8-dev liblapack-dev liblapack3; liblsan0 liblwp-mediatypes-perl liblwp-protocol-https-perl liblzma-dev; libmail-sendmail-perl libmailtools-perl libmnl0 libmpc3 libmpfr4 libmpx0; libncurses5-dev libnet-dbus-perl libnet-http-perl libnet-smtp-ssl-perl; libnet-ssleay-perl libpaper-utils libpaper1 libpcre16-3 libpcre3-dev; libpcre32-3 libpcrecpp0v5 libperl5.22 libpipeline1 libpng12-dev libquadmath0; libreadline-dev libreadline6-dev libsigsegv2 libstdc++-5-dev; libsys-hostname-long-perl libtcl8.6 libtext-iconv-perl libtie-ixhash-perl; libtimedate-perl libtinfo-dev libtk8.6 libtsan0 libubsan0 libunistring0; liburi-perl libwww-perl libwww-robotrules-perl libx11-protocol-perl libxaw7; libxcb-shape0 libxft2 libxml-parser-perl libxml-twig-perl; libxml-xpathengine-perl libxmu6 libxmuu1 libxpm4 libxss1 libxtables11 libxv1; libxxf86dga1 linux-libc-dev m4 make man-db manpages manpages-dev netbase; patch perl perl-modules-5.22 po-debconf python-pkg-resources python-scour; python-six r-base-core r-base-dev r-doc-html rename tzdata x11-utils; x11-xserver-utils xdg-utils zip zlib1g-dev```. -Not sure if moving the R install to the conda environment (which is not in the base image) will increase Travis time, but it doesn't appear to from the limited number of builds that have run so far. At some point we may want to move conda into the base image. However, I think that this would require that the base be rebuilt with every python code change, which is not optimal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:3596,Deployability,install,install,3596,"-listing-perl libfile-mimeinfo-perl libfile-stripnondeterminism-perl; libfont-afm-perl libfontenc1 libgcc-5-dev libgdbm3 libgettextpo-dev; libgettextpo0 libgfortran-5-dev libgfortran3 libgomp1 libhtml-form-perl; libhtml-format-perl libhtml-parser-perl libhtml-tagset-perl; libhtml-tree-perl libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl; libhttp-message-perl libhttp-negotiate-perl libio-html-perl; libio-socket-ssl-perl libipc-system-simple-perl libisc-export160 libisl15; libitm1 libjpeg-dev libjpeg-turbo8-dev libjpeg8-dev liblapack-dev liblapack3; liblsan0 liblwp-mediatypes-perl liblwp-protocol-https-perl liblzma-dev; libmail-sendmail-perl libmailtools-perl libmnl0 libmpc3 libmpfr4 libmpx0; libncurses5-dev libnet-dbus-perl libnet-http-perl libnet-smtp-ssl-perl; libnet-ssleay-perl libpaper-utils libpaper1 libpcre16-3 libpcre3-dev; libpcre32-3 libpcrecpp0v5 libperl5.22 libpipeline1 libpng12-dev libquadmath0; libreadline-dev libreadline6-dev libsigsegv2 libstdc++-5-dev; libsys-hostname-long-perl libtcl8.6 libtext-iconv-perl libtie-ixhash-perl; libtimedate-perl libtinfo-dev libtk8.6 libtsan0 libubsan0 libunistring0; liburi-perl libwww-perl libwww-robotrules-perl libx11-protocol-perl libxaw7; libxcb-shape0 libxft2 libxml-parser-perl libxml-twig-perl; libxml-xpathengine-perl libxmu6 libxmuu1 libxpm4 libxss1 libxtables11 libxv1; libxxf86dga1 linux-libc-dev m4 make man-db manpages manpages-dev netbase; patch perl perl-modules-5.22 po-debconf python-pkg-resources python-scour; python-six r-base-core r-base-dev r-doc-html rename tzdata x11-utils; x11-xserver-utils xdg-utils zip zlib1g-dev```. -Not sure if moving the R install to the conda environment (which is not in the base image) will increase Travis time, but it doesn't appear to from the limited number of builds that have run so far. At some point we may want to move conda into the base image. However, I think that this would require that the base be rebuilt with every python code change, which is not optimal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:90,Integrability,depend,dependencies,90,"OK, I experimented a bit with removing the R install from the base image and adding the R dependencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:280,Integrability,depend,depend,280,"OK, I experimented a bit with removing the R install from the base image and adding the R dependencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:638,Integrability,depend,depend,638,"OK, I experimented a bit with removing the R install from the base image and adding the R dependencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:725,Integrability,depend,dependencies,725,"OK, I experimented a bit with removing the R install from the base image and adding the R dependencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:803,Integrability,depend,dependencies,803,"OK, I experimented a bit with removing the R install from the base image and adding the R dependencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:1078,Integrability,depend,dependencies,1078,"endencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnondeterminism-perl; libfont-afm-perl libfontenc1 libgcc-5-dev libgdbm3 libgettextpo-dev; libget",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:2308,Integrability,message,message-perl,2308,"re falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnondeterminism-perl; libfont-afm-perl libfontenc1 libgcc-5-dev libgdbm3 libgettextpo-dev; libgettextpo0 libgfortran-5-dev libgfortran3 libgomp1 libhtml-form-perl; libhtml-format-perl libhtml-parser-perl libhtml-tagset-perl; libhtml-tree-perl libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl; libhttp-message-perl libhttp-negotiate-perl libio-html-perl; libio-socket-ssl-perl libipc-system-simple-perl libisc-export160 libisl15; libitm1 libjpeg-dev libjpeg-turbo8-dev libjpeg8-dev liblapack-dev liblapack3; liblsan0 liblwp-mediatypes-perl liblwp-protocol-https-perl liblzma-dev; libmail-sendmail-perl libmailtools-perl libmnl0 libmpc3 libmpfr4 libmpx0; libncurses5-dev libnet-dbus-perl libnet-http-perl libnet-smtp-ssl-perl; libnet-ssleay-perl libpaper-utils libpaper1 libpcre16-3 libpcre3-dev; libpcre32-3 libpcrecpp0v5 libperl5.22 libpipeline1 libpng12-dev libquadmath0; libreadline-dev libreadline6-dev libsigsegv2 libstdc++-5-dev; libsys-hostname-long-perl libtcl8.6 libtext-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:2530,Integrability,mediat,mediatypes-perl,2530,"re falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnondeterminism-perl; libfont-afm-perl libfontenc1 libgcc-5-dev libgdbm3 libgettextpo-dev; libgettextpo0 libgfortran-5-dev libgfortran3 libgomp1 libhtml-form-perl; libhtml-format-perl libhtml-parser-perl libhtml-tagset-perl; libhtml-tree-perl libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl; libhttp-message-perl libhttp-negotiate-perl libio-html-perl; libio-socket-ssl-perl libipc-system-simple-perl libisc-export160 libisl15; libitm1 libjpeg-dev libjpeg-turbo8-dev libjpeg8-dev liblapack-dev liblapack3; liblsan0 liblwp-mediatypes-perl liblwp-protocol-https-perl liblzma-dev; libmail-sendmail-perl libmailtools-perl libmnl0 libmpc3 libmpfr4 libmpx0; libncurses5-dev libnet-dbus-perl libnet-http-perl libnet-smtp-ssl-perl; libnet-ssleay-perl libpaper-utils libpaper1 libpcre16-3 libpcre3-dev; libpcre32-3 libpcrecpp0v5 libperl5.22 libpipeline1 libpng12-dev libquadmath0; libreadline-dev libreadline6-dev libsigsegv2 libstdc++-5-dev; libsys-hostname-long-perl libtcl8.6 libtext-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:2553,Integrability,protocol,protocol-https-perl,2553,"re falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnondeterminism-perl; libfont-afm-perl libfontenc1 libgcc-5-dev libgdbm3 libgettextpo-dev; libgettextpo0 libgfortran-5-dev libgfortran3 libgomp1 libhtml-form-perl; libhtml-format-perl libhtml-parser-perl libhtml-tagset-perl; libhtml-tree-perl libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl; libhttp-message-perl libhttp-negotiate-perl libio-html-perl; libio-socket-ssl-perl libipc-system-simple-perl libisc-export160 libisl15; libitm1 libjpeg-dev libjpeg-turbo8-dev libjpeg8-dev liblapack-dev liblapack3; liblsan0 liblwp-mediatypes-perl liblwp-protocol-https-perl liblzma-dev; libmail-sendmail-perl libmailtools-perl libmnl0 libmpc3 libmpfr4 libmpx0; libncurses5-dev libnet-dbus-perl libnet-http-perl libnet-smtp-ssl-perl; libnet-ssleay-perl libpaper-utils libpaper1 libpcre16-3 libpcre3-dev; libpcre32-3 libpcrecpp0v5 libperl5.22 libpipeline1 libpng12-dev libquadmath0; libreadline-dev libreadline6-dev libsigsegv2 libstdc++-5-dev; libsys-hostname-long-perl libtcl8.6 libtext-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:3144,Integrability,protocol,protocol-perl,3144,"-listing-perl libfile-mimeinfo-perl libfile-stripnondeterminism-perl; libfont-afm-perl libfontenc1 libgcc-5-dev libgdbm3 libgettextpo-dev; libgettextpo0 libgfortran-5-dev libgfortran3 libgomp1 libhtml-form-perl; libhtml-format-perl libhtml-parser-perl libhtml-tagset-perl; libhtml-tree-perl libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl; libhttp-message-perl libhttp-negotiate-perl libio-html-perl; libio-socket-ssl-perl libipc-system-simple-perl libisc-export160 libisl15; libitm1 libjpeg-dev libjpeg-turbo8-dev libjpeg8-dev liblapack-dev liblapack3; liblsan0 liblwp-mediatypes-perl liblwp-protocol-https-perl liblzma-dev; libmail-sendmail-perl libmailtools-perl libmnl0 libmpc3 libmpfr4 libmpx0; libncurses5-dev libnet-dbus-perl libnet-http-perl libnet-smtp-ssl-perl; libnet-ssleay-perl libpaper-utils libpaper1 libpcre16-3 libpcre3-dev; libpcre32-3 libpcrecpp0v5 libperl5.22 libpipeline1 libpng12-dev libquadmath0; libreadline-dev libreadline6-dev libsigsegv2 libstdc++-5-dev; libsys-hostname-long-perl libtcl8.6 libtext-iconv-perl libtie-ixhash-perl; libtimedate-perl libtinfo-dev libtk8.6 libtsan0 libubsan0 libunistring0; liburi-perl libwww-perl libwww-robotrules-perl libx11-protocol-perl libxaw7; libxcb-shape0 libxft2 libxml-parser-perl libxml-twig-perl; libxml-xpathengine-perl libxmu6 libxmuu1 libxpm4 libxss1 libxtables11 libxv1; libxxf86dga1 linux-libc-dev m4 make man-db manpages manpages-dev netbase; patch perl perl-modules-5.22 po-debconf python-pkg-resources python-scour; python-six r-base-core r-base-dev r-doc-html rename tzdata x11-utils; x11-xserver-utils xdg-utils zip zlib1g-dev```. -Not sure if moving the R install to the conda environment (which is not in the base image) will increase Travis time, but it doesn't appear to from the limited number of builds that have run so far. At some point we may want to move conda into the base image. However, I think that this would require that the base be rebuilt with every python code change, which is not optimal.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:269,Testability,test,tests,269,"OK, I experimented a bit with removing the R install from the base image and adding the R dependencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:310,Testability,test,test,310,"OK, I experimented a bit with removing the R install from the base image and adding the R dependencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:418,Testability,test,test,418,"OK, I experimented a bit with removing the R install from the base image and adding the R dependencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:932,Testability,test,tests,932,"OK, I experimented a bit with removing the R install from the base image and adding the R dependencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:1045,Testability,test,tests,1045,"e R install from the base image and adding the R dependencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnondeterminism-perl; libfont-afm-perl libfon",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:1101,Testability,test,tests,1101,"endencies to the conda environment in a branch and rebased on that. A few issues that I've run into or that came up in discussion with @jamesemery and @cmnbroad:. -I moved all tests that depend on R into the `python` test group (which should perhaps be renamed to `conda`). Note that some of these also fall into the `spark` test group---not sure if there is any special Spark setup done for that group, but we should make sure that they don't fail if they're not run with the conda environment. -@cmnbroad mentioned that some Picard tools that depend on R may break outside of the conda environment if the user does not have the R dependencies. -When we install R in the base image, we pull in a lot of basic dependencies (e.g., build-essential, various libraries and compilers, etc.) So when the R install is removed, it looks like many tests begin failing or hanging, perhaps because they are falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnondeterminism-perl; libfont-afm-perl libfontenc1 libgcc-5-dev libgdbm3 libgettextpo-dev; libget",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954:2397,Usability,simpl,simple-perl,2397,"re falling back on Java implementations (e.g., AVX PairHMM tests). We need to determine the dependencies for these tests and install them separately. Here is the list of packages that get pulled in by the R install: ```autoconf automake autotools-dev binutils bsdmainutils build-essential; bzip2-doc cdbs cpp cpp-5 debhelper dh-strip-nondeterminism dh-translations; dpkg-dev fakeroot g++ g++-5 gcc gcc-5 gettext gettext-base gfortran; gfortran-5 groff-base ifupdown intltool intltool-debian iproute2; isc-dhcp-client isc-dhcp-common libalgorithm-diff-perl; libalgorithm-diff-xs-perl libalgorithm-merge-perl libarchive-zip-perl; libasan2 libasprintf-dev libasprintf0v5 libatm1 libatomic1; libauthen-sasl-perl libblas-common libblas-dev libblas3 libbz2-dev; libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libcroco3 libcurl3; libdns-export162 libdpkg-perl libencode-locale-perl libfakeroot; libfile-basedir-perl libfile-desktopentry-perl libfile-fcntllock-perl; libfile-listing-perl libfile-mimeinfo-perl libfile-stripnondeterminism-perl; libfont-afm-perl libfontenc1 libgcc-5-dev libgdbm3 libgettextpo-dev; libgettextpo0 libgfortran-5-dev libgfortran3 libgomp1 libhtml-form-perl; libhtml-format-perl libhtml-parser-perl libhtml-tagset-perl; libhtml-tree-perl libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl; libhttp-message-perl libhttp-negotiate-perl libio-html-perl; libio-socket-ssl-perl libipc-system-simple-perl libisc-export160 libisl15; libitm1 libjpeg-dev libjpeg-turbo8-dev libjpeg8-dev liblapack-dev liblapack3; liblsan0 liblwp-mediatypes-perl liblwp-protocol-https-perl liblzma-dev; libmail-sendmail-perl libmailtools-perl libmnl0 libmpc3 libmpfr4 libmpx0; libncurses5-dev libnet-dbus-perl libnet-http-perl libnet-smtp-ssl-perl; libnet-ssleay-perl libpaper-utils libpaper1 libpcre16-3 libpcre3-dev; libpcre32-3 libpcrecpp0v5 libperl5.22 libpipeline1 libpng12-dev libquadmath0; libreadline-dev libreadline6-dev libsigsegv2 libstdc++-5-dev; libsys-hostname-long-perl libtcl8.6 libtext-",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406373954
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406380497:24,Testability,test,tests,24,"Note also that gCNV WDL tests are hanging because of bullet 3, since theano/PyMC3 is probably falling back on non-compiled implementations as well.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406380497
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406619450:24,Integrability,depend,dependency,24,I addressed the ggplot2 dependency in #5040 and moved the commit to address #5022 to that PR.,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-406619450
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-413599300:120,Deployability,install,installed,120,"Thanks! Just to be clear, the PR is incomplete. We need to determine the additional dependencies (which were previously installed along with R) required for AVX, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-413599300
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-413599300:84,Integrability,depend,dependencies,84,"Thanks! Just to be clear, the PR is incomplete. We need to determine the additional dependencies (which were previously installed along with R) required for AVX, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-413599300
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-413599300:19,Usability,clear,clear,19,"Thanks! Just to be clear, the PR is incomplete. We need to determine the additional dependencies (which were previously installed along with R) required for AVX, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-413599300
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-598916467:232,Availability,error,error-reference,232,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5026?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@cbbbb7a`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `0%`. ```diff; @@ Coverage Diff @@; ## master #5026 +/- ##; ==========================================; Coverage ? 80.255% ; Complexity ? 27182 ; ==========================================; Files ? 1779 ; Lines ? 132169 ; Branches ? 14721 ; ==========================================; Hits ? 106072 ; Misses ? 20971 ; Partials ? 5126; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5026?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...walkers/bqsr/AnalyzeCovariatesIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5026/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQW5hbHl6ZUNvdmFyaWF0ZXNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `12.308% <0%> (ø)` | `2 <0> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-598916467
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-598916467:180,Usability,learn,learn,180,# [Codecov](https://codecov.io/gh/broadinstitute/gatk/pull/5026?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@cbbbb7a`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `0%`. ```diff; @@ Coverage Diff @@; ## master #5026 +/- ##; ==========================================; Coverage ? 80.255% ; Complexity ? 27182 ; ==========================================; Files ? 1779 ; Lines ? 132169 ; Branches ? 14721 ; ==========================================; Hits ? 106072 ; Misses ? 20971 ; Partials ? 5126; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/gatk/pull/5026?src=pr&el=tree) | Coverage Δ | Complexity Δ | |; |---|---|---|---|; | [...walkers/bqsr/AnalyzeCovariatesIntegrationTest.java](https://codecov.io/gh/broadinstitute/gatk/pull/5026/diff?src=pr&el=tree#diff-c3JjL3Rlc3QvamF2YS9vcmcvYnJvYWRpbnN0aXR1dGUvaGVsbGJlbmRlci90b29scy93YWxrZXJzL2Jxc3IvQW5hbHl6ZUNvdmFyaWF0ZXNJbnRlZ3JhdGlvblRlc3QuamF2YQ==) | `12.308% <0%> (ø)` | `2 <0> (?)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-598916467
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-598996311:300,Integrability,depend,dependencies,300,"Rebased and tweaked a few package versions, some of which are slightly different than those currently in the base (as is the version of R). Looks like `build-essential` is all you need in the base image for tests to not fallback on slow implementations, but we might want to double check that native dependencies are correct. Not really sure how urgent this is, and I have to admit I've lost track of the remaining ways R can break our builds. The possibility of a bad Travis cache (which I think was a common issue in the past) is now prevented by #6454, right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-598996311
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-598996311:476,Performance,cache,cache,476,"Rebased and tweaked a few package versions, some of which are slightly different than those currently in the base (as is the version of R). Looks like `build-essential` is all you need in the base image for tests to not fallback on slow implementations, but we might want to double check that native dependencies are correct. Not really sure how urgent this is, and I have to admit I've lost track of the remaining ways R can break our builds. The possibility of a bad Travis cache (which I think was a common issue in the past) is now prevented by #6454, right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-598996311
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-598996311:207,Testability,test,tests,207,"Rebased and tweaked a few package versions, some of which are slightly different than those currently in the base (as is the version of R). Looks like `build-essential` is all you need in the base image for tests to not fallback on slow implementations, but we might want to double check that native dependencies are correct. Not really sure how urgent this is, and I have to admit I've lost track of the remaining ways R can break our builds. The possibility of a bad Travis cache (which I think was a common issue in the past) is now prevented by #6454, right?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-598996311
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-601891195:23,Testability,test,tests,23,"For a glorious moment, tests were passing, but it looks like I could not vanquish Xbyak in the end---and now I'm seeing https://github.com/broadinstitute/gatk/issues/6513, as an additional insult... I have made significant progress in otherwise making sure that the conda environment is consistent, but might need to confer with engine team about next steps. @droazen @lbergelson @jamesemery @cmnbroad any thoughts are appreciated! I'll also roll back the changes to the test groups, as James suggested, when I get a chance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-601891195
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-601891195:471,Testability,test,test,471,"For a glorious moment, tests were passing, but it looks like I could not vanquish Xbyak in the end---and now I'm seeing https://github.com/broadinstitute/gatk/issues/6513, as an additional insult... I have made significant progress in otherwise making sure that the conda environment is consistent, but might need to confer with engine team about next steps. @droazen @lbergelson @jamesemery @cmnbroad any thoughts are appreciated! I'll also roll back the changes to the test groups, as James suggested, when I get a chance.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-601891195
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-602140879:62,Deployability,update,update,62,"Looks like Xbyak might have been successfully squashed by the update to tensorflow 1.15.0, but it's hard to say given its intermittent nature. Not sure what to do about the Java 11 funkiness.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-602140879
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606830417:21,Availability,error,error,21,"Bleh. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f513eecd0f2, pid=6936, tid=6963; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6936); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6936.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; ```. Then exit 134 which is ""something is fucked but we don't know what"". . Looks like a seg fault somewhere with some JNI something....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606830417
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606830417:601,Availability,error,error,601,"Bleh. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f513eecd0f2, pid=6936, tid=6963; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6936); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6936.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; ```. Then exit 134 which is ""something is fucked but we don't know what"". . Looks like a seg fault somewhere with some JNI something....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606830417
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606830417:920,Availability,fault,fault,920,"Bleh. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f513eecd0f2, pid=6936, tid=6963; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6936); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6936.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; ```. Then exit 134 which is ""something is fucked but we don't know what"". . Looks like a seg fault somewhere with some JNI something....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606830417
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606830417:36,Safety,detect,detected,36,"Bleh. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f513eecd0f2, pid=6936, tid=6963; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6936); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6936.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; ```. Then exit 134 which is ""something is fucked but we don't know what"". . Looks like a seg fault somewhere with some JNI something....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606830417
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606830417:711,Testability,log,log,711,"Bleh. ```; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f513eecd0f2, pid=6936, tid=6963; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6936); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6936.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; ```. Then exit 134 which is ""something is fucked but we don't know what"". . Looks like a seg fault somewhere with some JNI something....",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606830417
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606837262:233,Availability,down,down,233,"Yeah, I think that's the same one I noted in #6513 (probably should've noted it here instead). I'm trying to make the gradle log more verbose (but not too verbose, otherwise Travis complains), but I don't know if that'll help pin it down to a particular failing test or not...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606837262
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606837262:125,Testability,log,log,125,"Yeah, I think that's the same one I noted in #6513 (probably should've noted it here instead). I'm trying to make the gradle log more verbose (but not too verbose, otherwise Travis complains), but I don't know if that'll help pin it down to a particular failing test or not...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606837262
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606837262:262,Testability,test,test,262,"Yeah, I think that's the same one I noted in #6513 (probably should've noted it here instead). I'm trying to make the gradle log more verbose (but not too verbose, otherwise Travis complains), but I don't know if that'll help pin it down to a particular failing test or not...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-606837262
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:137,Availability,down,downsampling,137,"Hmm, running `./gradlew --info ...` yields the following snippet:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.downsampling.ReservoirDownsamplerUnitTest > testReservoirDownsampler[29](TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0)) STANDARD_ERROR; 01:40:10.641 WARN gatk - Running test: TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0); Finished 130000 tests; Finished 140000 tests. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compres",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:983,Availability,error,error,983,"...` yields the following snippet:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.downsampling.ReservoirDownsamplerUnitTest > testReservoirDownsampler[29](TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0)) STANDARD_ERROR; 01:40:10.641 WARN gatk - Running test: TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0); Finished 130000 tests; Finished 140000 tests. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compression_level=2 -Dsamjdk.use_asyn",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:1563,Availability,error,error,1563," Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compression_level=2 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -javaagent:build/tmp/expandedArchives/org.jacoco.agent-0.8.4.jar_982888894296538c98d7324f3ca78d8f/jacocoagent.jar=destfile=build/jacoco/test.exec,append=true,inclnolocationclasses=false,dumponexit=true,output=file,jmx=false @/tmp/gradle-worker-classpath15215090933788021522txt -Xms500M -Xmx3500M -Dfile.encoding=UTF-8 -Duser.country=US -Duser.language=en -Duser.variant -ea worker.org.gradle.process.internal.worker.GradleWorkerMain 'Gradle Test Executor 2'; Successfully",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:772,Performance,load,load,772,"Hmm, running `./gradlew --info ...` yields the following snippet:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.downsampling.ReservoirDownsamplerUnitTest > testReservoirDownsampler[29](TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0)) STANDARD_ERROR; 01:40:10.641 WARN gatk - Running test: TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0); Finished 130000 tests; Finished 140000 tests. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compres",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:998,Safety,detect,detected,998,"...` yields the following snippet:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.downsampling.ReservoirDownsamplerUnitTest > testReservoirDownsampler[29](TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0)) STANDARD_ERROR; 01:40:10.641 WARN gatk - Running test: TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0); Finished 130000 tests; Finished 140000 tests. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compression_level=2 -Dsamjdk.use_asyn",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:94,Testability,test,test,94,"Hmm, running `./gradlew --info ...` yields the following snippet:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.downsampling.ReservoirDownsamplerUnitTest > testReservoirDownsampler[29](TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0)) STANDARD_ERROR; 01:40:10.641 WARN gatk - Running test: TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0); Finished 130000 tests; Finished 140000 tests. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compres",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:181,Testability,test,testReservoirDownsampler,181,"Hmm, running `./gradlew --info ...` yields the following snippet:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.downsampling.ReservoirDownsamplerUnitTest > testReservoirDownsampler[29](TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0)) STANDARD_ERROR; 01:40:10.641 WARN gatk - Running test: TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0); Finished 130000 tests; Finished 140000 tests. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compres",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:210,Testability,Test,TestDataProvider,210,"Hmm, running `./gradlew --info ...` yields the following snippet:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.downsampling.ReservoirDownsamplerUnitTest > testReservoirDownsampler[29](TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0)) STANDARD_ERROR; 01:40:10.641 WARN gatk - Running test: TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0); Finished 130000 tests; Finished 140000 tests. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compres",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:409,Testability,test,test,409,"Hmm, running `./gradlew --info ...` yields the following snippet:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.downsampling.ReservoirDownsamplerUnitTest > testReservoirDownsampler[29](TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0)) STANDARD_ERROR; 01:40:10.641 WARN gatk - Running test: TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0); Finished 130000 tests; Finished 140000 tests. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compres",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:415,Testability,Test,TestDataProvider,415,"Hmm, running `./gradlew --info ...` yields the following snippet:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.downsampling.ReservoirDownsamplerUnitTest > testReservoirDownsampler[29](TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0)) STANDARD_ERROR; 01:40:10.641 WARN gatk - Running test: TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0); Finished 130000 tests; Finished 140000 tests. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compres",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:581,Testability,test,tests,581,"Hmm, running `./gradlew --info ...` yields the following snippet:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.downsampling.ReservoirDownsamplerUnitTest > testReservoirDownsampler[29](TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0)) STANDARD_ERROR; 01:40:10.641 WARN gatk - Running test: TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0); Finished 130000 tests; Finished 140000 tests. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compres",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:604,Testability,test,tests,604,"Hmm, running `./gradlew --info ...` yields the following snippet:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.downsampling.ReservoirDownsamplerUnitTest > testReservoirDownsampler[29](TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0)) STANDARD_ERROR; 01:40:10.641 WARN gatk - Running test: TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0); Finished 130000 tests; Finished 140000 tests. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compres",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:633,Testability,test,test,633,"Hmm, running `./gradlew --info ...` yields the following snippet:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.downsampling.ReservoirDownsamplerUnitTest > testReservoirDownsampler[29](TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0)) STANDARD_ERROR; 01:40:10.641 WARN gatk - Running test: TestDataProvider(ReservoirDownsamplerTest: reservoirSize=10000 totalReads=10000 expectedNumReadsAfterDownsampling=10000 expectedNumDiscardedItems=0); Finished 130000 tests; Finished 140000 tests. Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest STANDARD_ERROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compres",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:1673,Testability,log,log,1673,"RROR; 01:40:14.522 WARN NativeLibraryLoader - Unable to load libgkl_pairhmm_fpga.so from native/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compression_level=2 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -javaagent:build/tmp/expandedArchives/org.jacoco.agent-0.8.4.jar_982888894296538c98d7324f3ca78d8f/jacocoagent.jar=destfile=build/jacoco/test.exec,append=true,inclnolocationclasses=false,dumponexit=true,output=file,jmx=false @/tmp/gradle-worker-classpath15215090933788021522txt -Xms500M -Xmx3500M -Dfile.encoding=UTF-8 -Duser.country=US -Duser.language=en -Duser.variant -ea worker.org.gradle.process.internal.worker.GradleWorkerMain 'Gradle Test Executor 2'; Successfully started process 'Gradle Test Executor 2'. Gradle Test Executor 2 started executing tests. > Task :test; ",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:1817,Testability,Test,Test,1817,"ve/libgkl_pairhmm_fpga.so (/tmp/libgkl_pairhmm_fpga17703278887667828152.so: libgkl_pairhmm_shacc.so: cannot open shared object file: No such file or directory); #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fe1a5cd00f2, pid=6969, tid=6997; #; # JRE version: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compression_level=2 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -javaagent:build/tmp/expandedArchives/org.jacoco.agent-0.8.4.jar_982888894296538c98d7324f3ca78d8f/jacocoagent.jar=destfile=build/jacoco/test.exec,append=true,inclnolocationclasses=false,dumponexit=true,output=file,jmx=false @/tmp/gradle-worker-classpath15215090933788021522txt -Xms500M -Xmx3500M -Dfile.encoding=UTF-8 -Duser.country=US -Duser.language=en -Duser.variant -ea worker.org.gradle.process.internal.worker.GradleWorkerMain 'Gradle Test Executor 2'; Successfully started process 'Gradle Test Executor 2'. Gradle Test Executor 2 started executing tests. > Task :test; Finished 150000 tests; ```. I've also seen a few intermittent successes. Could this be relate",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:2276,Testability,test,test,2276,"sion: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compression_level=2 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -javaagent:build/tmp/expandedArchives/org.jacoco.agent-0.8.4.jar_982888894296538c98d7324f3ca78d8f/jacocoagent.jar=destfile=build/jacoco/test.exec,append=true,inclnolocationclasses=false,dumponexit=true,output=file,jmx=false @/tmp/gradle-worker-classpath15215090933788021522txt -Xms500M -Xmx3500M -Dfile.encoding=UTF-8 -Duser.country=US -Duser.language=en -Duser.variant -ea worker.org.gradle.process.internal.worker.GradleWorkerMain 'Gradle Test Executor 2'; Successfully started process 'Gradle Test Executor 2'. Gradle Test Executor 2 started executing tests. > Task :test; Finished 150000 tests; ```. I've also seen a few intermittent successes. Could this be related to https://github.com/broadinstitute/gatk/issues/3732 or something along those lines (there are a few other issues where this test is mentioned)? I'm not sure if the `--info` output actually indicates that test is responsible for the segfault, or if it's some other test between the 140000th and 150000th...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:2581,Testability,Test,Test,2581,"sion: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compression_level=2 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -javaagent:build/tmp/expandedArchives/org.jacoco.agent-0.8.4.jar_982888894296538c98d7324f3ca78d8f/jacocoagent.jar=destfile=build/jacoco/test.exec,append=true,inclnolocationclasses=false,dumponexit=true,output=file,jmx=false @/tmp/gradle-worker-classpath15215090933788021522txt -Xms500M -Xmx3500M -Dfile.encoding=UTF-8 -Duser.country=US -Duser.language=en -Duser.variant -ea worker.org.gradle.process.internal.worker.GradleWorkerMain 'Gradle Test Executor 2'; Successfully started process 'Gradle Test Executor 2'. Gradle Test Executor 2 started executing tests. > Task :test; Finished 150000 tests; ```. I've also seen a few intermittent successes. Could this be related to https://github.com/broadinstitute/gatk/issues/3732 or something along those lines (there are a few other issues where this test is mentioned)? I'm not sure if the `--info` output actually indicates that test is responsible for the segfault, or if it's some other test between the 140000th and 150000th...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:2636,Testability,Test,Test,2636,"sion: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compression_level=2 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -javaagent:build/tmp/expandedArchives/org.jacoco.agent-0.8.4.jar_982888894296538c98d7324f3ca78d8f/jacocoagent.jar=destfile=build/jacoco/test.exec,append=true,inclnolocationclasses=false,dumponexit=true,output=file,jmx=false @/tmp/gradle-worker-classpath15215090933788021522txt -Xms500M -Xmx3500M -Dfile.encoding=UTF-8 -Duser.country=US -Duser.language=en -Duser.variant -ea worker.org.gradle.process.internal.worker.GradleWorkerMain 'Gradle Test Executor 2'; Successfully started process 'Gradle Test Executor 2'. Gradle Test Executor 2 started executing tests. > Task :test; Finished 150000 tests; ```. I've also seen a few intermittent successes. Could this be related to https://github.com/broadinstitute/gatk/issues/3732 or something along those lines (there are a few other issues where this test is mentioned)? I'm not sure if the `--info` output actually indicates that test is responsible for the segfault, or if it's some other test between the 140000th and 150000th...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:2661,Testability,Test,Test,2661,"sion: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compression_level=2 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -javaagent:build/tmp/expandedArchives/org.jacoco.agent-0.8.4.jar_982888894296538c98d7324f3ca78d8f/jacocoagent.jar=destfile=build/jacoco/test.exec,append=true,inclnolocationclasses=false,dumponexit=true,output=file,jmx=false @/tmp/gradle-worker-classpath15215090933788021522txt -Xms500M -Xmx3500M -Dfile.encoding=UTF-8 -Duser.country=US -Duser.language=en -Duser.variant -ea worker.org.gradle.process.internal.worker.GradleWorkerMain 'Gradle Test Executor 2'; Successfully started process 'Gradle Test Executor 2'. Gradle Test Executor 2 started executing tests. > Task :test; Finished 150000 tests; ```. I've also seen a few intermittent successes. Could this be related to https://github.com/broadinstitute/gatk/issues/3732 or something along those lines (there are a few other issues where this test is mentioned)? I'm not sure if the `--info` output actually indicates that test is responsible for the segfault, or if it's some other test between the 140000th and 150000th...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:2695,Testability,test,tests,2695,"sion: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compression_level=2 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -javaagent:build/tmp/expandedArchives/org.jacoco.agent-0.8.4.jar_982888894296538c98d7324f3ca78d8f/jacocoagent.jar=destfile=build/jacoco/test.exec,append=true,inclnolocationclasses=false,dumponexit=true,output=file,jmx=false @/tmp/gradle-worker-classpath15215090933788021522txt -Xms500M -Xmx3500M -Dfile.encoding=UTF-8 -Duser.country=US -Duser.language=en -Duser.variant -ea worker.org.gradle.process.internal.worker.GradleWorkerMain 'Gradle Test Executor 2'; Successfully started process 'Gradle Test Executor 2'. Gradle Test Executor 2 started executing tests. > Task :test; Finished 150000 tests; ```. I've also seen a few intermittent successes. Could this be related to https://github.com/broadinstitute/gatk/issues/3732 or something along those lines (there are a few other issues where this test is mentioned)? I'm not sure if the `--info` output actually indicates that test is responsible for the segfault, or if it's some other test between the 140000th and 150000th...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:2710,Testability,test,test,2710,"sion: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compression_level=2 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -javaagent:build/tmp/expandedArchives/org.jacoco.agent-0.8.4.jar_982888894296538c98d7324f3ca78d8f/jacocoagent.jar=destfile=build/jacoco/test.exec,append=true,inclnolocationclasses=false,dumponexit=true,output=file,jmx=false @/tmp/gradle-worker-classpath15215090933788021522txt -Xms500M -Xmx3500M -Dfile.encoding=UTF-8 -Duser.country=US -Duser.language=en -Duser.variant -ea worker.org.gradle.process.internal.worker.GradleWorkerMain 'Gradle Test Executor 2'; Successfully started process 'Gradle Test Executor 2'. Gradle Test Executor 2 started executing tests. > Task :test; Finished 150000 tests; ```. I've also seen a few intermittent successes. Could this be related to https://github.com/broadinstitute/gatk/issues/3732 or something along those lines (there are a few other issues where this test is mentioned)? I'm not sure if the `--info` output actually indicates that test is responsible for the segfault, or if it's some other test between the 140000th and 150000th...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:2732,Testability,test,tests,2732,"sion: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compression_level=2 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -javaagent:build/tmp/expandedArchives/org.jacoco.agent-0.8.4.jar_982888894296538c98d7324f3ca78d8f/jacocoagent.jar=destfile=build/jacoco/test.exec,append=true,inclnolocationclasses=false,dumponexit=true,output=file,jmx=false @/tmp/gradle-worker-classpath15215090933788021522txt -Xms500M -Xmx3500M -Dfile.encoding=UTF-8 -Duser.country=US -Duser.language=en -Duser.variant -ea worker.org.gradle.process.internal.worker.GradleWorkerMain 'Gradle Test Executor 2'; Successfully started process 'Gradle Test Executor 2'. Gradle Test Executor 2 started executing tests. > Task :test; Finished 150000 tests; ```. I've also seen a few intermittent successes. Could this be related to https://github.com/broadinstitute/gatk/issues/3732 or something along those lines (there are a few other issues where this test is mentioned)? I'm not sure if the `--info` output actually indicates that test is responsible for the segfault, or if it's some other test between the 140000th and 150000th...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:2937,Testability,test,test,2937,"sion: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compression_level=2 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -javaagent:build/tmp/expandedArchives/org.jacoco.agent-0.8.4.jar_982888894296538c98d7324f3ca78d8f/jacocoagent.jar=destfile=build/jacoco/test.exec,append=true,inclnolocationclasses=false,dumponexit=true,output=file,jmx=false @/tmp/gradle-worker-classpath15215090933788021522txt -Xms500M -Xmx3500M -Dfile.encoding=UTF-8 -Duser.country=US -Duser.language=en -Duser.variant -ea worker.org.gradle.process.internal.worker.GradleWorkerMain 'Gradle Test Executor 2'; Successfully started process 'Gradle Test Executor 2'. Gradle Test Executor 2 started executing tests. > Task :test; Finished 150000 tests; ```. I've also seen a few intermittent successes. Could this be related to https://github.com/broadinstitute/gatk/issues/3732 or something along those lines (there are a few other issues where this test is mentioned)? I'm not sure if the `--info` output actually indicates that test is responsible for the segfault, or if it's some other test between the 140000th and 150000th...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:3017,Testability,test,test,3017,"sion: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compression_level=2 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -javaagent:build/tmp/expandedArchives/org.jacoco.agent-0.8.4.jar_982888894296538c98d7324f3ca78d8f/jacocoagent.jar=destfile=build/jacoco/test.exec,append=true,inclnolocationclasses=false,dumponexit=true,output=file,jmx=false @/tmp/gradle-worker-classpath15215090933788021522txt -Xms500M -Xmx3500M -Dfile.encoding=UTF-8 -Duser.country=US -Duser.language=en -Duser.variant -ea worker.org.gradle.process.internal.worker.GradleWorkerMain 'Gradle Test Executor 2'; Successfully started process 'Gradle Test Executor 2'. Gradle Test Executor 2 started executing tests. > Task :test; Finished 150000 tests; ```. I've also seen a few intermittent successes. Could this be related to https://github.com/broadinstitute/gatk/issues/3732 or something along those lines (there are a few other issues where this test is mentioned)? I'm not sure if the `--info` output actually indicates that test is responsible for the segfault, or if it's some other test between the 140000th and 150000th...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088:3077,Testability,test,test,3077,"sion: OpenJDK Runtime Environment (11.0.2+9) (build 11.0.2+9); # Java VM: OpenJDK 64-Bit Server VM (11.0.2+9, mixed mode, tiered, compressed oops, g1 gc, linux-amd64); # Problematic frame:; # V [libjvm.so+0x8fd0f2] jni_GetByteArrayElements+0x72; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/share/apport/apport %p %s %c %d %P"" (or dumping to /home/travis/build/broadinstitute/gatk/core.6969); #; # An error report file with more information is saved as:; # /home/travis/build/broadinstitute/gatk/hs_err_pid6969.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; #; Starting process 'Gradle Test Executor 2'. Working directory: /home/travis/build/broadinstitute/gatk Command: /usr/local/lib/jvm/openjdk11/bin/java -Dgatk.spark.debug -Dorg.gradle.native=false -Dsamjdk.compression_level=2 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -javaagent:build/tmp/expandedArchives/org.jacoco.agent-0.8.4.jar_982888894296538c98d7324f3ca78d8f/jacocoagent.jar=destfile=build/jacoco/test.exec,append=true,inclnolocationclasses=false,dumponexit=true,output=file,jmx=false @/tmp/gradle-worker-classpath15215090933788021522txt -Xms500M -Xmx3500M -Dfile.encoding=UTF-8 -Duser.country=US -Duser.language=en -Duser.variant -ea worker.org.gradle.process.internal.worker.GradleWorkerMain 'Gradle Test Executor 2'; Successfully started process 'Gradle Test Executor 2'. Gradle Test Executor 2 started executing tests. > Task :test; Finished 150000 tests; ```. I've also seen a few intermittent successes. Could this be related to https://github.com/broadinstitute/gatk/issues/3732 or something along those lines (there are a few other issues where this test is mentioned)? I'm not sure if the `--info` output actually indicates that test is responsible for the segfault, or if it's some other test between the 140000th and 150000th...",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607332088
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772:35,Availability,failure,failures,35,"I can reproduce these intermittent failures locally with Ubuntu 18.04 and Java 11. Here's a snippet of one of the hs_err_pid*.logs:. ```; Stack: [0x00007f663c024000,0x00007f663c125000], sp=0x00007f663c121ee0, free space=1015k; Native frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code); V [libjvm.so+0x8c4cb0]; C [libgkl_pairhmm_omp6531264563224332212.so+0x66e92] JavaData::getData(JNIEnv_*, _jobjectArray*&, _jobjectArray*&)+0x222. Java frames: (J=compiled Java code, j=interpreted, Vv=VM code); j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoodsNative([Ljava/lang/Object;[Ljava/lang/Object;[D)V+0; j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoods([Lorg/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder;[Lorg/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder;[D)V+4; j org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM.computeLog10Likelihoods(Lorg/broadinstitute/hellbender/utils/genotyper/LikelihoodMatrix;Ljava/util/List;Ljava/util/Map;)V+221; j org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest.testLikelihoodsFromHaplotypes(Lorg/broadinstitute/hellbender/utils/pairhmm/PairHMM;Ljava/lang/Boolean;)V+297; v ~StubRoutines::call_stub; ```. I've experimented with changing the testing order of the implementations (e.g., from AVX, OMP, FPGA to OMP, AVX, FPGA), and I've seen failures in both AVX and OMP (i.e., libgkl_pairhmm*.so and libgkl_pairhmm_omp*.so). It seems like the .so that is complained about is switched (i.e, the AVX test segfaults on libgkl_pairhmm_omp*.so and vice versa), and that the test that runs first is always the failing one (if there is a failure). Not sure if this is a real pattern or if it will be useful in debugging, but it seems to be persistent.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772:1388,Availability,failure,failures,1388,"I can reproduce these intermittent failures locally with Ubuntu 18.04 and Java 11. Here's a snippet of one of the hs_err_pid*.logs:. ```; Stack: [0x00007f663c024000,0x00007f663c125000], sp=0x00007f663c121ee0, free space=1015k; Native frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code); V [libjvm.so+0x8c4cb0]; C [libgkl_pairhmm_omp6531264563224332212.so+0x66e92] JavaData::getData(JNIEnv_*, _jobjectArray*&, _jobjectArray*&)+0x222. Java frames: (J=compiled Java code, j=interpreted, Vv=VM code); j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoodsNative([Ljava/lang/Object;[Ljava/lang/Object;[D)V+0; j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoods([Lorg/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder;[Lorg/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder;[D)V+4; j org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM.computeLog10Likelihoods(Lorg/broadinstitute/hellbender/utils/genotyper/LikelihoodMatrix;Ljava/util/List;Ljava/util/Map;)V+221; j org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest.testLikelihoodsFromHaplotypes(Lorg/broadinstitute/hellbender/utils/pairhmm/PairHMM;Ljava/lang/Boolean;)V+297; v ~StubRoutines::call_stub; ```. I've experimented with changing the testing order of the implementations (e.g., from AVX, OMP, FPGA to OMP, AVX, FPGA), and I've seen failures in both AVX and OMP (i.e., libgkl_pairhmm*.so and libgkl_pairhmm_omp*.so). It seems like the .so that is complained about is switched (i.e, the AVX test segfaults on libgkl_pairhmm_omp*.so and vice versa), and that the test that runs first is always the failing one (if there is a failure). Not sure if this is a real pattern or if it will be useful in debugging, but it seems to be persistent.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772:1678,Availability,failure,failure,1678,"I can reproduce these intermittent failures locally with Ubuntu 18.04 and Java 11. Here's a snippet of one of the hs_err_pid*.logs:. ```; Stack: [0x00007f663c024000,0x00007f663c125000], sp=0x00007f663c121ee0, free space=1015k; Native frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code); V [libjvm.so+0x8c4cb0]; C [libgkl_pairhmm_omp6531264563224332212.so+0x66e92] JavaData::getData(JNIEnv_*, _jobjectArray*&, _jobjectArray*&)+0x222. Java frames: (J=compiled Java code, j=interpreted, Vv=VM code); j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoodsNative([Ljava/lang/Object;[Ljava/lang/Object;[D)V+0; j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoods([Lorg/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder;[Lorg/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder;[D)V+4; j org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM.computeLog10Likelihoods(Lorg/broadinstitute/hellbender/utils/genotyper/LikelihoodMatrix;Ljava/util/List;Ljava/util/Map;)V+221; j org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest.testLikelihoodsFromHaplotypes(Lorg/broadinstitute/hellbender/utils/pairhmm/PairHMM;Ljava/lang/Boolean;)V+297; v ~StubRoutines::call_stub; ```. I've experimented with changing the testing order of the implementations (e.g., from AVX, OMP, FPGA to OMP, AVX, FPGA), and I've seen failures in both AVX and OMP (i.e., libgkl_pairhmm*.so and libgkl_pairhmm_omp*.so). It seems like the .so that is complained about is switched (i.e, the AVX test segfaults on libgkl_pairhmm_omp*.so and vice versa), and that the test that runs first is always the failing one (if there is a failure). Not sure if this is a real pattern or if it will be useful in debugging, but it seems to be persistent.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772:126,Testability,log,logs,126,"I can reproduce these intermittent failures locally with Ubuntu 18.04 and Java 11. Here's a snippet of one of the hs_err_pid*.logs:. ```; Stack: [0x00007f663c024000,0x00007f663c125000], sp=0x00007f663c121ee0, free space=1015k; Native frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code); V [libjvm.so+0x8c4cb0]; C [libgkl_pairhmm_omp6531264563224332212.so+0x66e92] JavaData::getData(JNIEnv_*, _jobjectArray*&, _jobjectArray*&)+0x222. Java frames: (J=compiled Java code, j=interpreted, Vv=VM code); j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoodsNative([Ljava/lang/Object;[Ljava/lang/Object;[D)V+0; j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoods([Lorg/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder;[Lorg/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder;[D)V+4; j org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM.computeLog10Likelihoods(Lorg/broadinstitute/hellbender/utils/genotyper/LikelihoodMatrix;Ljava/util/List;Ljava/util/Map;)V+221; j org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest.testLikelihoodsFromHaplotypes(Lorg/broadinstitute/hellbender/utils/pairhmm/PairHMM;Ljava/lang/Boolean;)V+297; v ~StubRoutines::call_stub; ```. I've experimented with changing the testing order of the implementations (e.g., from AVX, OMP, FPGA to OMP, AVX, FPGA), and I've seen failures in both AVX and OMP (i.e., libgkl_pairhmm*.so and libgkl_pairhmm_omp*.so). It seems like the .so that is complained about is switched (i.e, the AVX test segfaults on libgkl_pairhmm_omp*.so and vice versa), and that the test that runs first is always the failing one (if there is a failure). Not sure if this is a real pattern or if it will be useful in debugging, but it seems to be persistent.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772:1111,Testability,test,testLikelihoodsFromHaplotypes,1111,"I can reproduce these intermittent failures locally with Ubuntu 18.04 and Java 11. Here's a snippet of one of the hs_err_pid*.logs:. ```; Stack: [0x00007f663c024000,0x00007f663c125000], sp=0x00007f663c121ee0, free space=1015k; Native frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code); V [libjvm.so+0x8c4cb0]; C [libgkl_pairhmm_omp6531264563224332212.so+0x66e92] JavaData::getData(JNIEnv_*, _jobjectArray*&, _jobjectArray*&)+0x222. Java frames: (J=compiled Java code, j=interpreted, Vv=VM code); j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoodsNative([Ljava/lang/Object;[Ljava/lang/Object;[D)V+0; j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoods([Lorg/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder;[Lorg/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder;[D)V+4; j org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM.computeLog10Likelihoods(Lorg/broadinstitute/hellbender/utils/genotyper/LikelihoodMatrix;Ljava/util/List;Ljava/util/Map;)V+221; j org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest.testLikelihoodsFromHaplotypes(Lorg/broadinstitute/hellbender/utils/pairhmm/PairHMM;Ljava/lang/Boolean;)V+297; v ~StubRoutines::call_stub; ```. I've experimented with changing the testing order of the implementations (e.g., from AVX, OMP, FPGA to OMP, AVX, FPGA), and I've seen failures in both AVX and OMP (i.e., libgkl_pairhmm*.so and libgkl_pairhmm_omp*.so). It seems like the .so that is complained about is switched (i.e, the AVX test segfaults on libgkl_pairhmm_omp*.so and vice versa), and that the test that runs first is always the failing one (if there is a failure). Not sure if this is a real pattern or if it will be useful in debugging, but it seems to be persistent.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772:1224,Testability,Stub,StubRoutines,1224,"I can reproduce these intermittent failures locally with Ubuntu 18.04 and Java 11. Here's a snippet of one of the hs_err_pid*.logs:. ```; Stack: [0x00007f663c024000,0x00007f663c125000], sp=0x00007f663c121ee0, free space=1015k; Native frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code); V [libjvm.so+0x8c4cb0]; C [libgkl_pairhmm_omp6531264563224332212.so+0x66e92] JavaData::getData(JNIEnv_*, _jobjectArray*&, _jobjectArray*&)+0x222. Java frames: (J=compiled Java code, j=interpreted, Vv=VM code); j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoodsNative([Ljava/lang/Object;[Ljava/lang/Object;[D)V+0; j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoods([Lorg/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder;[Lorg/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder;[D)V+4; j org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM.computeLog10Likelihoods(Lorg/broadinstitute/hellbender/utils/genotyper/LikelihoodMatrix;Ljava/util/List;Ljava/util/Map;)V+221; j org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest.testLikelihoodsFromHaplotypes(Lorg/broadinstitute/hellbender/utils/pairhmm/PairHMM;Ljava/lang/Boolean;)V+297; v ~StubRoutines::call_stub; ```. I've experimented with changing the testing order of the implementations (e.g., from AVX, OMP, FPGA to OMP, AVX, FPGA), and I've seen failures in both AVX and OMP (i.e., libgkl_pairhmm*.so and libgkl_pairhmm_omp*.so). It seems like the .so that is complained about is switched (i.e, the AVX test segfaults on libgkl_pairhmm_omp*.so and vice versa), and that the test that runs first is always the failing one (if there is a failure). Not sure if this is a real pattern or if it will be useful in debugging, but it seems to be persistent.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772:1290,Testability,test,testing,1290,"I can reproduce these intermittent failures locally with Ubuntu 18.04 and Java 11. Here's a snippet of one of the hs_err_pid*.logs:. ```; Stack: [0x00007f663c024000,0x00007f663c125000], sp=0x00007f663c121ee0, free space=1015k; Native frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code); V [libjvm.so+0x8c4cb0]; C [libgkl_pairhmm_omp6531264563224332212.so+0x66e92] JavaData::getData(JNIEnv_*, _jobjectArray*&, _jobjectArray*&)+0x222. Java frames: (J=compiled Java code, j=interpreted, Vv=VM code); j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoodsNative([Ljava/lang/Object;[Ljava/lang/Object;[D)V+0; j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoods([Lorg/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder;[Lorg/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder;[D)V+4; j org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM.computeLog10Likelihoods(Lorg/broadinstitute/hellbender/utils/genotyper/LikelihoodMatrix;Ljava/util/List;Ljava/util/Map;)V+221; j org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest.testLikelihoodsFromHaplotypes(Lorg/broadinstitute/hellbender/utils/pairhmm/PairHMM;Ljava/lang/Boolean;)V+297; v ~StubRoutines::call_stub; ```. I've experimented with changing the testing order of the implementations (e.g., from AVX, OMP, FPGA to OMP, AVX, FPGA), and I've seen failures in both AVX and OMP (i.e., libgkl_pairhmm*.so and libgkl_pairhmm_omp*.so). It seems like the .so that is complained about is switched (i.e, the AVX test segfaults on libgkl_pairhmm_omp*.so and vice versa), and that the test that runs first is always the failing one (if there is a failure). Not sure if this is a real pattern or if it will be useful in debugging, but it seems to be persistent.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772:1545,Testability,test,test,1545,"I can reproduce these intermittent failures locally with Ubuntu 18.04 and Java 11. Here's a snippet of one of the hs_err_pid*.logs:. ```; Stack: [0x00007f663c024000,0x00007f663c125000], sp=0x00007f663c121ee0, free space=1015k; Native frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code); V [libjvm.so+0x8c4cb0]; C [libgkl_pairhmm_omp6531264563224332212.so+0x66e92] JavaData::getData(JNIEnv_*, _jobjectArray*&, _jobjectArray*&)+0x222. Java frames: (J=compiled Java code, j=interpreted, Vv=VM code); j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoodsNative([Ljava/lang/Object;[Ljava/lang/Object;[D)V+0; j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoods([Lorg/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder;[Lorg/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder;[D)V+4; j org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM.computeLog10Likelihoods(Lorg/broadinstitute/hellbender/utils/genotyper/LikelihoodMatrix;Ljava/util/List;Ljava/util/Map;)V+221; j org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest.testLikelihoodsFromHaplotypes(Lorg/broadinstitute/hellbender/utils/pairhmm/PairHMM;Ljava/lang/Boolean;)V+297; v ~StubRoutines::call_stub; ```. I've experimented with changing the testing order of the implementations (e.g., from AVX, OMP, FPGA to OMP, AVX, FPGA), and I've seen failures in both AVX and OMP (i.e., libgkl_pairhmm*.so and libgkl_pairhmm_omp*.so). It seems like the .so that is complained about is switched (i.e, the AVX test segfaults on libgkl_pairhmm_omp*.so and vice versa), and that the test that runs first is always the failing one (if there is a failure). Not sure if this is a real pattern or if it will be useful in debugging, but it seems to be persistent.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772:1616,Testability,test,test,1616,"I can reproduce these intermittent failures locally with Ubuntu 18.04 and Java 11. Here's a snippet of one of the hs_err_pid*.logs:. ```; Stack: [0x00007f663c024000,0x00007f663c125000], sp=0x00007f663c121ee0, free space=1015k; Native frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code); V [libjvm.so+0x8c4cb0]; C [libgkl_pairhmm_omp6531264563224332212.so+0x66e92] JavaData::getData(JNIEnv_*, _jobjectArray*&, _jobjectArray*&)+0x222. Java frames: (J=compiled Java code, j=interpreted, Vv=VM code); j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoodsNative([Ljava/lang/Object;[Ljava/lang/Object;[D)V+0; j com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoods([Lorg/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder;[Lorg/broadinstitute/gatk/nativebindings/pairhmm/HaplotypeDataHolder;[D)V+4; j org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM.computeLog10Likelihoods(Lorg/broadinstitute/hellbender/utils/genotyper/LikelihoodMatrix;Ljava/util/List;Ljava/util/Map;)V+221; j org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest.testLikelihoodsFromHaplotypes(Lorg/broadinstitute/hellbender/utils/pairhmm/PairHMM;Ljava/lang/Boolean;)V+297; v ~StubRoutines::call_stub; ```. I've experimented with changing the testing order of the implementations (e.g., from AVX, OMP, FPGA to OMP, AVX, FPGA), and I've seen failures in both AVX and OMP (i.e., libgkl_pairhmm*.so and libgkl_pairhmm_omp*.so). It seems like the .so that is complained about is switched (i.e, the AVX test segfaults on libgkl_pairhmm_omp*.so and vice versa), and that the test that runs first is always the failing one (if there is a failure). Not sure if this is a real pattern or if it will be useful in debugging, but it seems to be persistent.",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607561772
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205:444,Availability,failure,failure,444,"Hmm, actually, could this be a problem due to the way the native libraries are loaded in the test code? Note that we first cycle through all implementations in the DataProvider, loading the respective library for each implementation via the `synchronized boolean load` method in the `NativeLibraryLoader`. I'm not really that familiar with concurrency in Java (nor loading native libraries, for that matter), but it seems that the intermittent failure goes away when I refactor the test to remove the DataProvider (by just looping through the implementations in the test method). Perhaps related to https://github.com/broadinstitute/gatk/issues/5339?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205:242,Integrability,synchroniz,synchronized,242,"Hmm, actually, could this be a problem due to the way the native libraries are loaded in the test code? Note that we first cycle through all implementations in the DataProvider, loading the respective library for each implementation via the `synchronized boolean load` method in the `NativeLibraryLoader`. I'm not really that familiar with concurrency in Java (nor loading native libraries, for that matter), but it seems that the intermittent failure goes away when I refactor the test to remove the DataProvider (by just looping through the implementations in the test method). Perhaps related to https://github.com/broadinstitute/gatk/issues/5339?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205:469,Modifiability,refactor,refactor,469,"Hmm, actually, could this be a problem due to the way the native libraries are loaded in the test code? Note that we first cycle through all implementations in the DataProvider, loading the respective library for each implementation via the `synchronized boolean load` method in the `NativeLibraryLoader`. I'm not really that familiar with concurrency in Java (nor loading native libraries, for that matter), but it seems that the intermittent failure goes away when I refactor the test to remove the DataProvider (by just looping through the implementations in the test method). Perhaps related to https://github.com/broadinstitute/gatk/issues/5339?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205:79,Performance,load,loaded,79,"Hmm, actually, could this be a problem due to the way the native libraries are loaded in the test code? Note that we first cycle through all implementations in the DataProvider, loading the respective library for each implementation via the `synchronized boolean load` method in the `NativeLibraryLoader`. I'm not really that familiar with concurrency in Java (nor loading native libraries, for that matter), but it seems that the intermittent failure goes away when I refactor the test to remove the DataProvider (by just looping through the implementations in the test method). Perhaps related to https://github.com/broadinstitute/gatk/issues/5339?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205:178,Performance,load,loading,178,"Hmm, actually, could this be a problem due to the way the native libraries are loaded in the test code? Note that we first cycle through all implementations in the DataProvider, loading the respective library for each implementation via the `synchronized boolean load` method in the `NativeLibraryLoader`. I'm not really that familiar with concurrency in Java (nor loading native libraries, for that matter), but it seems that the intermittent failure goes away when I refactor the test to remove the DataProvider (by just looping through the implementations in the test method). Perhaps related to https://github.com/broadinstitute/gatk/issues/5339?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205:263,Performance,load,load,263,"Hmm, actually, could this be a problem due to the way the native libraries are loaded in the test code? Note that we first cycle through all implementations in the DataProvider, loading the respective library for each implementation via the `synchronized boolean load` method in the `NativeLibraryLoader`. I'm not really that familiar with concurrency in Java (nor loading native libraries, for that matter), but it seems that the intermittent failure goes away when I refactor the test to remove the DataProvider (by just looping through the implementations in the test method). Perhaps related to https://github.com/broadinstitute/gatk/issues/5339?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205:340,Performance,concurren,concurrency,340,"Hmm, actually, could this be a problem due to the way the native libraries are loaded in the test code? Note that we first cycle through all implementations in the DataProvider, loading the respective library for each implementation via the `synchronized boolean load` method in the `NativeLibraryLoader`. I'm not really that familiar with concurrency in Java (nor loading native libraries, for that matter), but it seems that the intermittent failure goes away when I refactor the test to remove the DataProvider (by just looping through the implementations in the test method). Perhaps related to https://github.com/broadinstitute/gatk/issues/5339?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205:365,Performance,load,loading,365,"Hmm, actually, could this be a problem due to the way the native libraries are loaded in the test code? Note that we first cycle through all implementations in the DataProvider, loading the respective library for each implementation via the `synchronized boolean load` method in the `NativeLibraryLoader`. I'm not really that familiar with concurrency in Java (nor loading native libraries, for that matter), but it seems that the intermittent failure goes away when I refactor the test to remove the DataProvider (by just looping through the implementations in the test method). Perhaps related to https://github.com/broadinstitute/gatk/issues/5339?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205:93,Testability,test,test,93,"Hmm, actually, could this be a problem due to the way the native libraries are loaded in the test code? Note that we first cycle through all implementations in the DataProvider, loading the respective library for each implementation via the `synchronized boolean load` method in the `NativeLibraryLoader`. I'm not really that familiar with concurrency in Java (nor loading native libraries, for that matter), but it seems that the intermittent failure goes away when I refactor the test to remove the DataProvider (by just looping through the implementations in the test method). Perhaps related to https://github.com/broadinstitute/gatk/issues/5339?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205:482,Testability,test,test,482,"Hmm, actually, could this be a problem due to the way the native libraries are loaded in the test code? Note that we first cycle through all implementations in the DataProvider, loading the respective library for each implementation via the `synchronized boolean load` method in the `NativeLibraryLoader`. I'm not really that familiar with concurrency in Java (nor loading native libraries, for that matter), but it seems that the intermittent failure goes away when I refactor the test to remove the DataProvider (by just looping through the implementations in the test method). Perhaps related to https://github.com/broadinstitute/gatk/issues/5339?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205:566,Testability,test,test,566,"Hmm, actually, could this be a problem due to the way the native libraries are loaded in the test code? Note that we first cycle through all implementations in the DataProvider, loading the respective library for each implementation via the `synchronized boolean load` method in the `NativeLibraryLoader`. I'm not really that familiar with concurrency in Java (nor loading native libraries, for that matter), but it seems that the intermittent failure goes away when I refactor the test to remove the DataProvider (by just looping through the implementations in the test method). Perhaps related to https://github.com/broadinstitute/gatk/issues/5339?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607596205
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607967425:298,Deployability,install,install,298,"Uncompressed size of the Docker goes up from 4.12GB to 4.76GB. The correct Tensorflow + MKL + R packages contribute most of the extra bulk. However, using `conda clean -afy` (as opposed to `conda clean -ay`, which we were doing before) and additionally deleting *.a and *.pyc files after the conda install saves ~700MB in the end (without these, the image goes up to 5.43GB).",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-607967425
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608022086:20,Availability,failure,failures,20,Travis reported job failures from build [29858](https://travis-ci.com/broadinstitute/gatk/builds/157989813); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29858.5](https://travis-ci.com/broadinstitute/gatk/jobs/312877597) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29858.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608022086
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608022086:109,Availability,Failure,Failures,109,Travis reported job failures from build [29858](https://travis-ci.com/broadinstitute/gatk/builds/157989813); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29858.5](https://travis-ci.com/broadinstitute/gatk/jobs/312877597) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29858.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608022086
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608022086:144,Testability,Test,Test,144,Travis reported job failures from build [29858](https://travis-ci.com/broadinstitute/gatk/builds/157989813); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29858.5](https://travis-ci.com/broadinstitute/gatk/jobs/312877597) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29858.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608022086
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608022086:171,Testability,Log,Logs,171,Travis reported job failures from build [29858](https://travis-ci.com/broadinstitute/gatk/builds/157989813); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29858.5](https://travis-ci.com/broadinstitute/gatk/jobs/312877597) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29858.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608022086
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608022086:308,Testability,log,logs,308,Travis reported job failures from build [29858](https://travis-ci.com/broadinstitute/gatk/builds/157989813); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29858.5](https://travis-ci.com/broadinstitute/gatk/jobs/312877597) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29858.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608022086
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608022086:356,Testability,test,test-logs,356,Travis reported job failures from build [29858](https://travis-ci.com/broadinstitute/gatk/builds/157989813); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29858.5](https://travis-ci.com/broadinstitute/gatk/jobs/312877597) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29858.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608022086
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608022086:395,Testability,test,tests,395,Travis reported job failures from build [29858](https://travis-ci.com/broadinstitute/gatk/builds/157989813); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29858.5](https://travis-ci.com/broadinstitute/gatk/jobs/312877597) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29858.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608022086
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608022086:401,Testability,test,test,401,Travis reported job failures from build [29858](https://travis-ci.com/broadinstitute/gatk/builds/157989813); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29858.5](https://travis-ci.com/broadinstitute/gatk/jobs/312877597) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29858.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608022086
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608023918:20,Availability,failure,failures,20,Travis reported job failures from build [29860](https://travis-ci.com/broadinstitute/gatk/builds/157990750); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29860.5](https://travis-ci.com/broadinstitute/gatk/jobs/312886112) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29860.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608023918
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608023918:109,Availability,Failure,Failures,109,Travis reported job failures from build [29860](https://travis-ci.com/broadinstitute/gatk/builds/157990750); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29860.5](https://travis-ci.com/broadinstitute/gatk/jobs/312886112) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29860.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608023918
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608023918:144,Testability,Test,Test,144,Travis reported job failures from build [29860](https://travis-ci.com/broadinstitute/gatk/builds/157990750); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29860.5](https://travis-ci.com/broadinstitute/gatk/jobs/312886112) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29860.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608023918
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608023918:171,Testability,Log,Logs,171,Travis reported job failures from build [29860](https://travis-ci.com/broadinstitute/gatk/builds/157990750); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29860.5](https://travis-ci.com/broadinstitute/gatk/jobs/312886112) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29860.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608023918
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608023918:308,Testability,log,logs,308,Travis reported job failures from build [29860](https://travis-ci.com/broadinstitute/gatk/builds/157990750); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29860.5](https://travis-ci.com/broadinstitute/gatk/jobs/312886112) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29860.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608023918
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608023918:356,Testability,test,test-logs,356,Travis reported job failures from build [29860](https://travis-ci.com/broadinstitute/gatk/builds/157990750); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29860.5](https://travis-ci.com/broadinstitute/gatk/jobs/312886112) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29860.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608023918
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608023918:395,Testability,test,tests,395,Travis reported job failures from build [29860](https://travis-ci.com/broadinstitute/gatk/builds/157990750); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29860.5](https://travis-ci.com/broadinstitute/gatk/jobs/312886112) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29860.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608023918
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608023918:401,Testability,test,test,401,Travis reported job failures from build [29860](https://travis-ci.com/broadinstitute/gatk/builds/157990750); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29860.5](https://travis-ci.com/broadinstitute/gatk/jobs/312886112) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29860.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608023918
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608030301:20,Availability,failure,failures,20,Travis reported job failures from build [29862](https://travis-ci.com/broadinstitute/gatk/builds/157993177); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29862.5](https://travis-ci.com/broadinstitute/gatk/jobs/312891875) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29862.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608030301
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608030301:109,Availability,Failure,Failures,109,Travis reported job failures from build [29862](https://travis-ci.com/broadinstitute/gatk/builds/157993177); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29862.5](https://travis-ci.com/broadinstitute/gatk/jobs/312891875) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29862.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608030301
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608030301:144,Testability,Test,Test,144,Travis reported job failures from build [29862](https://travis-ci.com/broadinstitute/gatk/builds/157993177); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29862.5](https://travis-ci.com/broadinstitute/gatk/jobs/312891875) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29862.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608030301
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608030301:171,Testability,Log,Logs,171,Travis reported job failures from build [29862](https://travis-ci.com/broadinstitute/gatk/builds/157993177); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29862.5](https://travis-ci.com/broadinstitute/gatk/jobs/312891875) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29862.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608030301
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608030301:308,Testability,log,logs,308,Travis reported job failures from build [29862](https://travis-ci.com/broadinstitute/gatk/builds/157993177); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29862.5](https://travis-ci.com/broadinstitute/gatk/jobs/312891875) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29862.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608030301
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608030301:356,Testability,test,test-logs,356,Travis reported job failures from build [29862](https://travis-ci.com/broadinstitute/gatk/builds/157993177); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29862.5](https://travis-ci.com/broadinstitute/gatk/jobs/312891875) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29862.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608030301
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608030301:395,Testability,test,tests,395,Travis reported job failures from build [29862](https://travis-ci.com/broadinstitute/gatk/builds/157993177); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29862.5](https://travis-ci.com/broadinstitute/gatk/jobs/312891875) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29862.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608030301
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608030301:401,Testability,test,test,401,Travis reported job failures from build [29862](https://travis-ci.com/broadinstitute/gatk/builds/157993177); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29862.5](https://travis-ci.com/broadinstitute/gatk/jobs/312891875) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29862.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608030301
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608033310:20,Availability,failure,failures,20,Travis reported job failures from build [29864](https://travis-ci.com/broadinstitute/gatk/builds/157993670); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29864.5](https://travis-ci.com/broadinstitute/gatk/jobs/312892956) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29864.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608033310
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608033310:109,Availability,Failure,Failures,109,Travis reported job failures from build [29864](https://travis-ci.com/broadinstitute/gatk/builds/157993670); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29864.5](https://travis-ci.com/broadinstitute/gatk/jobs/312892956) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29864.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608033310
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608033310:144,Testability,Test,Test,144,Travis reported job failures from build [29864](https://travis-ci.com/broadinstitute/gatk/builds/157993670); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29864.5](https://travis-ci.com/broadinstitute/gatk/jobs/312892956) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29864.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608033310
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608033310:171,Testability,Log,Logs,171,Travis reported job failures from build [29864](https://travis-ci.com/broadinstitute/gatk/builds/157993670); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29864.5](https://travis-ci.com/broadinstitute/gatk/jobs/312892956) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29864.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608033310
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608033310:308,Testability,log,logs,308,Travis reported job failures from build [29864](https://travis-ci.com/broadinstitute/gatk/builds/157993670); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29864.5](https://travis-ci.com/broadinstitute/gatk/jobs/312892956) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29864.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608033310
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608033310:356,Testability,test,test-logs,356,Travis reported job failures from build [29864](https://travis-ci.com/broadinstitute/gatk/builds/157993670); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29864.5](https://travis-ci.com/broadinstitute/gatk/jobs/312892956) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29864.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608033310
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608033310:395,Testability,test,tests,395,Travis reported job failures from build [29864](https://travis-ci.com/broadinstitute/gatk/builds/157993670); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29864.5](https://travis-ci.com/broadinstitute/gatk/jobs/312892956) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29864.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608033310
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608033310:401,Testability,test,test,401,Travis reported job failures from build [29864](https://travis-ci.com/broadinstitute/gatk/builds/157993670); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29864.5](https://travis-ci.com/broadinstitute/gatk/jobs/312892956) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29864.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608033310
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608046067:20,Availability,failure,failures,20,Travis reported job failures from build [29868](https://travis-ci.com/broadinstitute/gatk/builds/158005638); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29868.5](https://travis-ci.com/broadinstitute/gatk/jobs/312918727) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29868.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608046067
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608046067:109,Availability,Failure,Failures,109,Travis reported job failures from build [29868](https://travis-ci.com/broadinstitute/gatk/builds/158005638); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29868.5](https://travis-ci.com/broadinstitute/gatk/jobs/312918727) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29868.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608046067
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608046067:144,Testability,Test,Test,144,Travis reported job failures from build [29868](https://travis-ci.com/broadinstitute/gatk/builds/158005638); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29868.5](https://travis-ci.com/broadinstitute/gatk/jobs/312918727) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29868.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608046067
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608046067:171,Testability,Log,Logs,171,Travis reported job failures from build [29868](https://travis-ci.com/broadinstitute/gatk/builds/158005638); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29868.5](https://travis-ci.com/broadinstitute/gatk/jobs/312918727) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29868.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608046067
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608046067:308,Testability,log,logs,308,Travis reported job failures from build [29868](https://travis-ci.com/broadinstitute/gatk/builds/158005638); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29868.5](https://travis-ci.com/broadinstitute/gatk/jobs/312918727) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29868.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608046067
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608046067:356,Testability,test,test-logs,356,Travis reported job failures from build [29868](https://travis-ci.com/broadinstitute/gatk/builds/158005638); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29868.5](https://travis-ci.com/broadinstitute/gatk/jobs/312918727) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29868.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608046067
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608046067:395,Testability,test,tests,395,Travis reported job failures from build [29868](https://travis-ci.com/broadinstitute/gatk/builds/158005638); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29868.5](https://travis-ci.com/broadinstitute/gatk/jobs/312918727) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29868.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608046067
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608046067:401,Testability,test,test,401,Travis reported job failures from build [29868](https://travis-ci.com/broadinstitute/gatk/builds/158005638); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29868.5](https://travis-ci.com/broadinstitute/gatk/jobs/312918727) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29868.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608046067
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608047246:20,Availability,failure,failures,20,Travis reported job failures from build [29870](https://travis-ci.com/broadinstitute/gatk/builds/158005875); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29870.5](https://travis-ci.com/broadinstitute/gatk/jobs/312919425) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29870.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608047246
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608047246:109,Availability,Failure,Failures,109,Travis reported job failures from build [29870](https://travis-ci.com/broadinstitute/gatk/builds/158005875); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29870.5](https://travis-ci.com/broadinstitute/gatk/jobs/312919425) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29870.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608047246
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608047246:144,Testability,Test,Test,144,Travis reported job failures from build [29870](https://travis-ci.com/broadinstitute/gatk/builds/158005875); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29870.5](https://travis-ci.com/broadinstitute/gatk/jobs/312919425) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29870.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608047246
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608047246:171,Testability,Log,Logs,171,Travis reported job failures from build [29870](https://travis-ci.com/broadinstitute/gatk/builds/158005875); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29870.5](https://travis-ci.com/broadinstitute/gatk/jobs/312919425) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29870.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608047246
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608047246:308,Testability,log,logs,308,Travis reported job failures from build [29870](https://travis-ci.com/broadinstitute/gatk/builds/158005875); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29870.5](https://travis-ci.com/broadinstitute/gatk/jobs/312919425) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29870.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608047246
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608047246:356,Testability,test,test-logs,356,Travis reported job failures from build [29870](https://travis-ci.com/broadinstitute/gatk/builds/158005875); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29870.5](https://travis-ci.com/broadinstitute/gatk/jobs/312919425) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29870.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608047246
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608047246:395,Testability,test,tests,395,Travis reported job failures from build [29870](https://travis-ci.com/broadinstitute/gatk/builds/158005875); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29870.5](https://travis-ci.com/broadinstitute/gatk/jobs/312919425) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29870.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608047246
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608047246:401,Testability,test,test,401,Travis reported job failures from build [29870](https://travis-ci.com/broadinstitute/gatk/builds/158005875); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29870.5](https://travis-ci.com/broadinstitute/gatk/jobs/312919425) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29870.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608047246
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608048022:20,Availability,failure,failures,20,Travis reported job failures from build [29875](https://travis-ci.com/broadinstitute/gatk/builds/158006000); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29875.5](https://travis-ci.com/broadinstitute/gatk/jobs/312919826) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29875.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608048022
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608048022:109,Availability,Failure,Failures,109,Travis reported job failures from build [29875](https://travis-ci.com/broadinstitute/gatk/builds/158006000); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29875.5](https://travis-ci.com/broadinstitute/gatk/jobs/312919826) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29875.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608048022
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608048022:144,Testability,Test,Test,144,Travis reported job failures from build [29875](https://travis-ci.com/broadinstitute/gatk/builds/158006000); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29875.5](https://travis-ci.com/broadinstitute/gatk/jobs/312919826) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29875.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608048022
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608048022:171,Testability,Log,Logs,171,Travis reported job failures from build [29875](https://travis-ci.com/broadinstitute/gatk/builds/158006000); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29875.5](https://travis-ci.com/broadinstitute/gatk/jobs/312919826) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29875.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608048022
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608048022:308,Testability,log,logs,308,Travis reported job failures from build [29875](https://travis-ci.com/broadinstitute/gatk/builds/158006000); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29875.5](https://travis-ci.com/broadinstitute/gatk/jobs/312919826) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29875.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608048022
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608048022:356,Testability,test,test-logs,356,Travis reported job failures from build [29875](https://travis-ci.com/broadinstitute/gatk/builds/158006000); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29875.5](https://travis-ci.com/broadinstitute/gatk/jobs/312919826) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29875.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608048022
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608048022:395,Testability,test,tests,395,Travis reported job failures from build [29875](https://travis-ci.com/broadinstitute/gatk/builds/158006000); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29875.5](https://travis-ci.com/broadinstitute/gatk/jobs/312919826) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29875.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608048022
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608048022:401,Testability,test,test,401,Travis reported job failures from build [29875](https://travis-ci.com/broadinstitute/gatk/builds/158006000); Failures in the following jobs:. | Test Type | JDK | Job ID | Logs |; | --------- |---- | ------ | ---- |; | conda | openjdk8 | [29875.5](https://travis-ci.com/broadinstitute/gatk/jobs/312919826) | [logs](https://storage.googleapis.com/hellbender-test-logs/build_reports/master_29875.5/tests/test/index.html) |,MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608048022
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608460914:366,Deployability,release,release,366,"OK, verified that numerical results in the gCNV WDL tests match 4.1.6.0. Not sure if @lucidtronix needs to run similar checks for the CNN. Other than needing to push a new base image after review, I think this branch is ready to go. However, as we discussed, we should highlight major changes (e.g., the need to run R plotting tools in the conda environment) in the release notes. @droazen can you assign a final reviewer?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608460914
https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608460914:52,Testability,test,tests,52,"OK, verified that numerical results in the gCNV WDL tests match 4.1.6.0. Not sure if @lucidtronix needs to run similar checks for the CNN. Other than needing to push a new base image after review, I think this branch is ready to go. However, as we discussed, we should highlight major changes (e.g., the need to run R plotting tools in the conda environment) in the release notes. @droazen can you assign a final reviewer?",MatchSource.ISSUE_COMMENT,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5026#issuecomment-608460914
