id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://github.com/hail-is/hail/tree/0.2.133/README.md:1478,Deployability,install,installation,1478,"e and has first-class support for multi-dimensional structured data, like the genomic data in a genome-wide association study (GWAS). Hail is exposed as a Python library, using primitives for distributed queries and linear algebra implemented in Scala, [Spark](https://spark.apache.org/docs/latest/index.html), and increasingly C++. See the [documentation](https://hail.is/docs/0.2/) for more info on using Hail. ### Community. Hail has been widely adopted in academia and industry, including as the analysis platform for the [genome aggregation database](https://gnomad.broadinstitute.org) and [UK Biobank rapid GWAS](https://www.nealelab.is/uk-biobank). Learn more about [Hail-powered science](https://hail.is/references.html). ### Contribute. If you'd like to discuss or contribute to the development of methods or infrastructure, please:. - see the [For Software Developers](https://hail.is/docs/0.2/getting_started_developing.html) section of the installation guide for info on compiling Hail; - chat with us about development in our [Zulip chatroom](https://hail.zulipchat.com); - visit the [Development Forum](https://dev.hail.is) for longer-form discussions; <!--- - read [this post]() (coming soon!) for tips on submitting a successful Pull Request to our repository --->. Hail uses a continuous deployment approach to software development, which means we frequently add new features. We update users about changes to Hail via the [Discussion Forum](https://discuss.hail.is). We recommend creating an account on the Discussion Forum so that you can subscribe to these updates as well. ### Maintainer. Hail is maintained by a team in the [Neale lab](https://nealelab.is/) at the [Stanley Center for Psychiatric Research](https://www.broadinstitute.org/stanley) of the [Broad Institute of MIT and Harvard](https://www.broadinstitute.org) and the [Analytic and Translational Genetics Unit](https://www.atgu.mgh.harvard.edu/) of [Massachusetts General Hospital](https://www.massgeneral.org/). Co",MatchSource.DOCS,README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/README.md
https://github.com/hail-is/hail/tree/0.2.133/README.md:1820,Deployability,continuous,continuous,1820,"the [documentation](https://hail.is/docs/0.2/) for more info on using Hail. ### Community. Hail has been widely adopted in academia and industry, including as the analysis platform for the [genome aggregation database](https://gnomad.broadinstitute.org) and [UK Biobank rapid GWAS](https://www.nealelab.is/uk-biobank). Learn more about [Hail-powered science](https://hail.is/references.html). ### Contribute. If you'd like to discuss or contribute to the development of methods or infrastructure, please:. - see the [For Software Developers](https://hail.is/docs/0.2/getting_started_developing.html) section of the installation guide for info on compiling Hail; - chat with us about development in our [Zulip chatroom](https://hail.zulipchat.com); - visit the [Development Forum](https://dev.hail.is) for longer-form discussions; <!--- - read [this post]() (coming soon!) for tips on submitting a successful Pull Request to our repository --->. Hail uses a continuous deployment approach to software development, which means we frequently add new features. We update users about changes to Hail via the [Discussion Forum](https://discuss.hail.is). We recommend creating an account on the Discussion Forum so that you can subscribe to these updates as well. ### Maintainer. Hail is maintained by a team in the [Neale lab](https://nealelab.is/) at the [Stanley Center for Psychiatric Research](https://www.broadinstitute.org/stanley) of the [Broad Institute of MIT and Harvard](https://www.broadinstitute.org) and the [Analytic and Translational Genetics Unit](https://www.atgu.mgh.harvard.edu/) of [Massachusetts General Hospital](https://www.massgeneral.org/). Contact the Hail team at <code><a href=""mailto:hail@broadinstitute.org"">hail@broadinstitute.org</a></code>. ### Citing Hail. If you use Hail for published work, please cite the software. You can get a; citation for the version of Hail you installed by executing:. ```python; import hail as hl; print(hl.citation()); ```. Which will look li",MatchSource.DOCS,README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/README.md
https://github.com/hail-is/hail/tree/0.2.133/README.md:1831,Deployability,deploy,deployment,1831,"the [documentation](https://hail.is/docs/0.2/) for more info on using Hail. ### Community. Hail has been widely adopted in academia and industry, including as the analysis platform for the [genome aggregation database](https://gnomad.broadinstitute.org) and [UK Biobank rapid GWAS](https://www.nealelab.is/uk-biobank). Learn more about [Hail-powered science](https://hail.is/references.html). ### Contribute. If you'd like to discuss or contribute to the development of methods or infrastructure, please:. - see the [For Software Developers](https://hail.is/docs/0.2/getting_started_developing.html) section of the installation guide for info on compiling Hail; - chat with us about development in our [Zulip chatroom](https://hail.zulipchat.com); - visit the [Development Forum](https://dev.hail.is) for longer-form discussions; <!--- - read [this post]() (coming soon!) for tips on submitting a successful Pull Request to our repository --->. Hail uses a continuous deployment approach to software development, which means we frequently add new features. We update users about changes to Hail via the [Discussion Forum](https://discuss.hail.is). We recommend creating an account on the Discussion Forum so that you can subscribe to these updates as well. ### Maintainer. Hail is maintained by a team in the [Neale lab](https://nealelab.is/) at the [Stanley Center for Psychiatric Research](https://www.broadinstitute.org/stanley) of the [Broad Institute of MIT and Harvard](https://www.broadinstitute.org) and the [Analytic and Translational Genetics Unit](https://www.atgu.mgh.harvard.edu/) of [Massachusetts General Hospital](https://www.massgeneral.org/). Contact the Hail team at <code><a href=""mailto:hail@broadinstitute.org"">hail@broadinstitute.org</a></code>. ### Citing Hail. If you use Hail for published work, please cite the software. You can get a; citation for the version of Hail you installed by executing:. ```python; import hail as hl; print(hl.citation()); ```. Which will look li",MatchSource.DOCS,README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/README.md
https://github.com/hail-is/hail/tree/0.2.133/README.md:1923,Deployability,update,update,1923,"as been widely adopted in academia and industry, including as the analysis platform for the [genome aggregation database](https://gnomad.broadinstitute.org) and [UK Biobank rapid GWAS](https://www.nealelab.is/uk-biobank). Learn more about [Hail-powered science](https://hail.is/references.html). ### Contribute. If you'd like to discuss or contribute to the development of methods or infrastructure, please:. - see the [For Software Developers](https://hail.is/docs/0.2/getting_started_developing.html) section of the installation guide for info on compiling Hail; - chat with us about development in our [Zulip chatroom](https://hail.zulipchat.com); - visit the [Development Forum](https://dev.hail.is) for longer-form discussions; <!--- - read [this post]() (coming soon!) for tips on submitting a successful Pull Request to our repository --->. Hail uses a continuous deployment approach to software development, which means we frequently add new features. We update users about changes to Hail via the [Discussion Forum](https://discuss.hail.is). We recommend creating an account on the Discussion Forum so that you can subscribe to these updates as well. ### Maintainer. Hail is maintained by a team in the [Neale lab](https://nealelab.is/) at the [Stanley Center for Psychiatric Research](https://www.broadinstitute.org/stanley) of the [Broad Institute of MIT and Harvard](https://www.broadinstitute.org) and the [Analytic and Translational Genetics Unit](https://www.atgu.mgh.harvard.edu/) of [Massachusetts General Hospital](https://www.massgeneral.org/). Contact the Hail team at <code><a href=""mailto:hail@broadinstitute.org"">hail@broadinstitute.org</a></code>. ### Citing Hail. If you use Hail for published work, please cite the software. You can get a; citation for the version of Hail you installed by executing:. ```python; import hail as hl; print(hl.citation()); ```. Which will look like:. ```; Hail Team. Hail 0.2.13-81ab564db2b4. https://github.com/hail-is/hail/releases/tag/0.2.1",MatchSource.DOCS,README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/README.md
https://github.com/hail-is/hail/tree/0.2.133/README.md:2103,Deployability,update,updates,2103,"egation database](https://gnomad.broadinstitute.org) and [UK Biobank rapid GWAS](https://www.nealelab.is/uk-biobank). Learn more about [Hail-powered science](https://hail.is/references.html). ### Contribute. If you'd like to discuss or contribute to the development of methods or infrastructure, please:. - see the [For Software Developers](https://hail.is/docs/0.2/getting_started_developing.html) section of the installation guide for info on compiling Hail; - chat with us about development in our [Zulip chatroom](https://hail.zulipchat.com); - visit the [Development Forum](https://dev.hail.is) for longer-form discussions; <!--- - read [this post]() (coming soon!) for tips on submitting a successful Pull Request to our repository --->. Hail uses a continuous deployment approach to software development, which means we frequently add new features. We update users about changes to Hail via the [Discussion Forum](https://discuss.hail.is). We recommend creating an account on the Discussion Forum so that you can subscribe to these updates as well. ### Maintainer. Hail is maintained by a team in the [Neale lab](https://nealelab.is/) at the [Stanley Center for Psychiatric Research](https://www.broadinstitute.org/stanley) of the [Broad Institute of MIT and Harvard](https://www.broadinstitute.org) and the [Analytic and Translational Genetics Unit](https://www.atgu.mgh.harvard.edu/) of [Massachusetts General Hospital](https://www.massgeneral.org/). Contact the Hail team at <code><a href=""mailto:hail@broadinstitute.org"">hail@broadinstitute.org</a></code>. ### Citing Hail. If you use Hail for published work, please cite the software. You can get a; citation for the version of Hail you installed by executing:. ```python; import hail as hl; print(hl.citation()); ```. Which will look like:. ```; Hail Team. Hail 0.2.13-81ab564db2b4. https://github.com/hail-is/hail/releases/tag/0.2.13.; ```. ##### Acknowledgements. The Hail team has several sources of funding at the Broad Institute:; - ",MatchSource.DOCS,README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/README.md
https://github.com/hail-is/hail/tree/0.2.133/README.md:2763,Deployability,install,installed,2763,"ubmitting a successful Pull Request to our repository --->. Hail uses a continuous deployment approach to software development, which means we frequently add new features. We update users about changes to Hail via the [Discussion Forum](https://discuss.hail.is). We recommend creating an account on the Discussion Forum so that you can subscribe to these updates as well. ### Maintainer. Hail is maintained by a team in the [Neale lab](https://nealelab.is/) at the [Stanley Center for Psychiatric Research](https://www.broadinstitute.org/stanley) of the [Broad Institute of MIT and Harvard](https://www.broadinstitute.org) and the [Analytic and Translational Genetics Unit](https://www.atgu.mgh.harvard.edu/) of [Massachusetts General Hospital](https://www.massgeneral.org/). Contact the Hail team at <code><a href=""mailto:hail@broadinstitute.org"">hail@broadinstitute.org</a></code>. ### Citing Hail. If you use Hail for published work, please cite the software. You can get a; citation for the version of Hail you installed by executing:. ```python; import hail as hl; print(hl.citation()); ```. Which will look like:. ```; Hail Team. Hail 0.2.13-81ab564db2b4. https://github.com/hail-is/hail/releases/tag/0.2.13.; ```. ##### Acknowledgements. The Hail team has several sources of funding at the Broad Institute:; - The Stanley Center for Psychiatric Research, which together with Neale Lab has provided an incredibly supportive and stimulating home.; - Principal Investigators Benjamin Neale and Daniel MacArthur, whose scientific leadership has been essential for solving the right problems.; - Jeremy Wertheimer, whose strategic advice and generous philanthropy have been essential for growing the impact of Hail. We are grateful for generous support from:; - The National Institute of Diabetes and Digestive and Kidney Diseases; - The National Institute of Mental Health; - The National Human Genome Research Institute; - The Chan Zuckerberg Initiative. We would like to thank <a href=""https://zu",MatchSource.DOCS,README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/README.md
https://github.com/hail-is/hail/tree/0.2.133/README.md:2942,Deployability,release,releases,2942," changes to Hail via the [Discussion Forum](https://discuss.hail.is). We recommend creating an account on the Discussion Forum so that you can subscribe to these updates as well. ### Maintainer. Hail is maintained by a team in the [Neale lab](https://nealelab.is/) at the [Stanley Center for Psychiatric Research](https://www.broadinstitute.org/stanley) of the [Broad Institute of MIT and Harvard](https://www.broadinstitute.org) and the [Analytic and Translational Genetics Unit](https://www.atgu.mgh.harvard.edu/) of [Massachusetts General Hospital](https://www.massgeneral.org/). Contact the Hail team at <code><a href=""mailto:hail@broadinstitute.org"">hail@broadinstitute.org</a></code>. ### Citing Hail. If you use Hail for published work, please cite the software. You can get a; citation for the version of Hail you installed by executing:. ```python; import hail as hl; print(hl.citation()); ```. Which will look like:. ```; Hail Team. Hail 0.2.13-81ab564db2b4. https://github.com/hail-is/hail/releases/tag/0.2.13.; ```. ##### Acknowledgements. The Hail team has several sources of funding at the Broad Institute:; - The Stanley Center for Psychiatric Research, which together with Neale Lab has provided an incredibly supportive and stimulating home.; - Principal Investigators Benjamin Neale and Daniel MacArthur, whose scientific leadership has been essential for solving the right problems.; - Jeremy Wertheimer, whose strategic advice and generous philanthropy have been essential for growing the impact of Hail. We are grateful for generous support from:; - The National Institute of Diabetes and Digestive and Kidney Diseases; - The National Institute of Mental Health; - The National Human Genome Research Institute; - The Chan Zuckerberg Initiative. We would like to thank <a href=""https://zulipchat.com/"">Zulip</a> for supporting; open-source by providing free hosting, and YourKit, LLC for generously providing; free licenses for <a href=""https://www.yourkit.com/java/profiler/"">You",MatchSource.DOCS,README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/README.md
https://github.com/hail-is/hail/tree/0.2.133/README.md:1205,Energy Efficiency,power,powered,1205,"vg)](https://zenodo.org/badge/latestdoi/45069467) [![PyPI version](https://badge.fury.io/py/hail.svg)](https://badge.fury.io/py/hail). [Hail](https://hail.is) is an open-source, general-purpose, Python-based data analysis tool with additional data types and methods for working with genomic data. Hail is built to scale and has first-class support for multi-dimensional structured data, like the genomic data in a genome-wide association study (GWAS). Hail is exposed as a Python library, using primitives for distributed queries and linear algebra implemented in Scala, [Spark](https://spark.apache.org/docs/latest/index.html), and increasingly C++. See the [documentation](https://hail.is/docs/0.2/) for more info on using Hail. ### Community. Hail has been widely adopted in academia and industry, including as the analysis platform for the [genome aggregation database](https://gnomad.broadinstitute.org) and [UK Biobank rapid GWAS](https://www.nealelab.is/uk-biobank). Learn more about [Hail-powered science](https://hail.is/references.html). ### Contribute. If you'd like to discuss or contribute to the development of methods or infrastructure, please:. - see the [For Software Developers](https://hail.is/docs/0.2/getting_started_developing.html) section of the installation guide for info on compiling Hail; - chat with us about development in our [Zulip chatroom](https://hail.zulipchat.com); - visit the [Development Forum](https://dev.hail.is) for longer-form discussions; <!--- - read [this post]() (coming soon!) for tips on submitting a successful Pull Request to our repository --->. Hail uses a continuous deployment approach to software development, which means we frequently add new features. We update users about changes to Hail via the [Discussion Forum](https://discuss.hail.is). We recommend creating an account on the Discussion Forum so that you can subscribe to these updates as well. ### Maintainer. Hail is maintained by a team in the [Neale lab](https://nealelab.is/) at",MatchSource.DOCS,README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/README.md
https://github.com/hail-is/hail/tree/0.2.133/README.md:668,Security,expose,exposed,668,"# Hail. [![Zulip](https://img.shields.io/badge/zulip-join_chat-brightgreen.svg)](https://hail.zulipchat.com?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge) [![DOI](https://zenodo.org/badge/45069467.svg)](https://zenodo.org/badge/latestdoi/45069467) [![PyPI version](https://badge.fury.io/py/hail.svg)](https://badge.fury.io/py/hail). [Hail](https://hail.is) is an open-source, general-purpose, Python-based data analysis tool with additional data types and methods for working with genomic data. Hail is built to scale and has first-class support for multi-dimensional structured data, like the genomic data in a genome-wide association study (GWAS). Hail is exposed as a Python library, using primitives for distributed queries and linear algebra implemented in Scala, [Spark](https://spark.apache.org/docs/latest/index.html), and increasingly C++. See the [documentation](https://hail.is/docs/0.2/) for more info on using Hail. ### Community. Hail has been widely adopted in academia and industry, including as the analysis platform for the [genome aggregation database](https://gnomad.broadinstitute.org) and [UK Biobank rapid GWAS](https://www.nealelab.is/uk-biobank). Learn more about [Hail-powered science](https://hail.is/references.html). ### Contribute. If you'd like to discuss or contribute to the development of methods or infrastructure, please:. - see the [For Software Developers](https://hail.is/docs/0.2/getting_started_developing.html) section of the installation guide for info on compiling Hail; - chat with us about development in our [Zulip chatroom](https://hail.zulipchat.com); - visit the [Development Forum](https://dev.hail.is) for longer-form discussions; <!--- - read [this post]() (coming soon!) for tips on submitting a successful Pull Request to our repository --->. Hail uses a continuous deployment approach to software development, which means we frequently add new features. We update users about changes to Hail via the [Discussion Forum](https://discuss.",MatchSource.DOCS,README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/README.md
https://github.com/hail-is/hail/tree/0.2.133/README.md:1491,Usability,guid,guide,1491,"e and has first-class support for multi-dimensional structured data, like the genomic data in a genome-wide association study (GWAS). Hail is exposed as a Python library, using primitives for distributed queries and linear algebra implemented in Scala, [Spark](https://spark.apache.org/docs/latest/index.html), and increasingly C++. See the [documentation](https://hail.is/docs/0.2/) for more info on using Hail. ### Community. Hail has been widely adopted in academia and industry, including as the analysis platform for the [genome aggregation database](https://gnomad.broadinstitute.org) and [UK Biobank rapid GWAS](https://www.nealelab.is/uk-biobank). Learn more about [Hail-powered science](https://hail.is/references.html). ### Contribute. If you'd like to discuss or contribute to the development of methods or infrastructure, please:. - see the [For Software Developers](https://hail.is/docs/0.2/getting_started_developing.html) section of the installation guide for info on compiling Hail; - chat with us about development in our [Zulip chatroom](https://hail.zulipchat.com); - visit the [Development Forum](https://dev.hail.is) for longer-form discussions; <!--- - read [this post]() (coming soon!) for tips on submitting a successful Pull Request to our repository --->. Hail uses a continuous deployment approach to software development, which means we frequently add new features. We update users about changes to Hail via the [Discussion Forum](https://discuss.hail.is). We recommend creating an account on the Discussion Forum so that you can subscribe to these updates as well. ### Maintainer. Hail is maintained by a team in the [Neale lab](https://nealelab.is/) at the [Stanley Center for Psychiatric Research](https://www.broadinstitute.org/stanley) of the [Broad Institute of MIT and Harvard](https://www.broadinstitute.org) and the [Analytic and Translational Genetics Unit](https://www.atgu.mgh.harvard.edu/) of [Massachusetts General Hospital](https://www.massgeneral.org/). Co",MatchSource.DOCS,README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/README.md
https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md:759,Integrability,inject,injection,759,"### Change Description. Fixes #<issue_number>. ### Security Assessment. - [ ] This change has a high security impact; - [ ] Required: and the impact has been assessed and approved by appsec; - [ ] This change has a medium security impact; - [ ] This change has a low security impact; - [ ] This change has no security impact. Description of the security impact and necessary mitigations:. - For none/low impact: a quick one/two sentence justification of the rating.; - Example: ""Docs only"", ""Low-level refactoring of non-security code"", etc.; - For medium/high impact: provide a description of the impact and the mitigations in place.; - Example: ""New UI text field added in analogy to existing elements, with input strings escaped and validated against code injection"". (Reviewers: please confirm the security impact before approving)",MatchSource.DOCS,.github/pull_request_template.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md
https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md:502,Modifiability,refactor,refactoring,502,"### Change Description. Fixes #<issue_number>. ### Security Assessment. - [ ] This change has a high security impact; - [ ] Required: and the impact has been assessed and approved by appsec; - [ ] This change has a medium security impact; - [ ] This change has a low security impact; - [ ] This change has no security impact. Description of the security impact and necessary mitigations:. - For none/low impact: a quick one/two sentence justification of the rating.; - Example: ""Docs only"", ""Low-level refactoring of non-security code"", etc.; - For medium/high impact: provide a description of the impact and the mitigations in place.; - Example: ""New UI text field added in analogy to existing elements, with input strings escaped and validated against code injection"". (Reviewers: please confirm the security impact before approving)",MatchSource.DOCS,.github/pull_request_template.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md
https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md:101,Security,secur,security,101,"### Change Description. Fixes #<issue_number>. ### Security Assessment. - [ ] This change has a high security impact; - [ ] Required: and the impact has been assessed and approved by appsec; - [ ] This change has a medium security impact; - [ ] This change has a low security impact; - [ ] This change has no security impact. Description of the security impact and necessary mitigations:. - For none/low impact: a quick one/two sentence justification of the rating.; - Example: ""Docs only"", ""Low-level refactoring of non-security code"", etc.; - For medium/high impact: provide a description of the impact and the mitigations in place.; - Example: ""New UI text field added in analogy to existing elements, with input strings escaped and validated against code injection"". (Reviewers: please confirm the security impact before approving)",MatchSource.DOCS,.github/pull_request_template.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md
https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md:222,Security,secur,security,222,"### Change Description. Fixes #<issue_number>. ### Security Assessment. - [ ] This change has a high security impact; - [ ] Required: and the impact has been assessed and approved by appsec; - [ ] This change has a medium security impact; - [ ] This change has a low security impact; - [ ] This change has no security impact. Description of the security impact and necessary mitigations:. - For none/low impact: a quick one/two sentence justification of the rating.; - Example: ""Docs only"", ""Low-level refactoring of non-security code"", etc.; - For medium/high impact: provide a description of the impact and the mitigations in place.; - Example: ""New UI text field added in analogy to existing elements, with input strings escaped and validated against code injection"". (Reviewers: please confirm the security impact before approving)",MatchSource.DOCS,.github/pull_request_template.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md
https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md:267,Security,secur,security,267,"### Change Description. Fixes #<issue_number>. ### Security Assessment. - [ ] This change has a high security impact; - [ ] Required: and the impact has been assessed and approved by appsec; - [ ] This change has a medium security impact; - [ ] This change has a low security impact; - [ ] This change has no security impact. Description of the security impact and necessary mitigations:. - For none/low impact: a quick one/two sentence justification of the rating.; - Example: ""Docs only"", ""Low-level refactoring of non-security code"", etc.; - For medium/high impact: provide a description of the impact and the mitigations in place.; - Example: ""New UI text field added in analogy to existing elements, with input strings escaped and validated against code injection"". (Reviewers: please confirm the security impact before approving)",MatchSource.DOCS,.github/pull_request_template.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md
https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md:309,Security,secur,security,309,"### Change Description. Fixes #<issue_number>. ### Security Assessment. - [ ] This change has a high security impact; - [ ] Required: and the impact has been assessed and approved by appsec; - [ ] This change has a medium security impact; - [ ] This change has a low security impact; - [ ] This change has no security impact. Description of the security impact and necessary mitigations:. - For none/low impact: a quick one/two sentence justification of the rating.; - Example: ""Docs only"", ""Low-level refactoring of non-security code"", etc.; - For medium/high impact: provide a description of the impact and the mitigations in place.; - Example: ""New UI text field added in analogy to existing elements, with input strings escaped and validated against code injection"". (Reviewers: please confirm the security impact before approving)",MatchSource.DOCS,.github/pull_request_template.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md
https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md:345,Security,secur,security,345,"### Change Description. Fixes #<issue_number>. ### Security Assessment. - [ ] This change has a high security impact; - [ ] Required: and the impact has been assessed and approved by appsec; - [ ] This change has a medium security impact; - [ ] This change has a low security impact; - [ ] This change has no security impact. Description of the security impact and necessary mitigations:. - For none/low impact: a quick one/two sentence justification of the rating.; - Example: ""Docs only"", ""Low-level refactoring of non-security code"", etc.; - For medium/high impact: provide a description of the impact and the mitigations in place.; - Example: ""New UI text field added in analogy to existing elements, with input strings escaped and validated against code injection"". (Reviewers: please confirm the security impact before approving)",MatchSource.DOCS,.github/pull_request_template.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md
https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md:521,Security,secur,security,521,"### Change Description. Fixes #<issue_number>. ### Security Assessment. - [ ] This change has a high security impact; - [ ] Required: and the impact has been assessed and approved by appsec; - [ ] This change has a medium security impact; - [ ] This change has a low security impact; - [ ] This change has no security impact. Description of the security impact and necessary mitigations:. - For none/low impact: a quick one/two sentence justification of the rating.; - Example: ""Docs only"", ""Low-level refactoring of non-security code"", etc.; - For medium/high impact: provide a description of the impact and the mitigations in place.; - Example: ""New UI text field added in analogy to existing elements, with input strings escaped and validated against code injection"". (Reviewers: please confirm the security impact before approving)",MatchSource.DOCS,.github/pull_request_template.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md
https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md:736,Security,validat,validated,736,"### Change Description. Fixes #<issue_number>. ### Security Assessment. - [ ] This change has a high security impact; - [ ] Required: and the impact has been assessed and approved by appsec; - [ ] This change has a medium security impact; - [ ] This change has a low security impact; - [ ] This change has no security impact. Description of the security impact and necessary mitigations:. - For none/low impact: a quick one/two sentence justification of the rating.; - Example: ""Docs only"", ""Low-level refactoring of non-security code"", etc.; - For medium/high impact: provide a description of the impact and the mitigations in place.; - Example: ""New UI text field added in analogy to existing elements, with input strings escaped and validated against code injection"". (Reviewers: please confirm the security impact before approving)",MatchSource.DOCS,.github/pull_request_template.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md
https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md:759,Security,inject,injection,759,"### Change Description. Fixes #<issue_number>. ### Security Assessment. - [ ] This change has a high security impact; - [ ] Required: and the impact has been assessed and approved by appsec; - [ ] This change has a medium security impact; - [ ] This change has a low security impact; - [ ] This change has no security impact. Description of the security impact and necessary mitigations:. - For none/low impact: a quick one/two sentence justification of the rating.; - Example: ""Docs only"", ""Low-level refactoring of non-security code"", etc.; - For medium/high impact: provide a description of the impact and the mitigations in place.; - Example: ""New UI text field added in analogy to existing elements, with input strings escaped and validated against code injection"". (Reviewers: please confirm the security impact before approving)",MatchSource.DOCS,.github/pull_request_template.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md
https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md:802,Security,secur,security,802,"### Change Description. Fixes #<issue_number>. ### Security Assessment. - [ ] This change has a high security impact; - [ ] Required: and the impact has been assessed and approved by appsec; - [ ] This change has a medium security impact; - [ ] This change has a low security impact; - [ ] This change has no security impact. Description of the security impact and necessary mitigations:. - For none/low impact: a quick one/two sentence justification of the rating.; - Example: ""Docs only"", ""Low-level refactoring of non-security code"", etc.; - For medium/high impact: provide a description of the impact and the mitigations in place.; - Example: ""New UI text field added in analogy to existing elements, with input strings escaped and validated against code injection"". (Reviewers: please confirm the security impact before approving)",MatchSource.DOCS,.github/pull_request_template.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/.github/pull_request_template.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/algolia.md:2903,Deployability,configurat,configuration,2903,"for this product is absurdly hard to find. For our current application it is:; https://crawler.algolia.com/admin/crawlers?sort=status&order=ASC&limit=20&appId=SWB3TKBY4S . Click through to the ""hail_is"" index and go to the ""Editor"". Ignore most of the information here and; look at the `helpers.docsearch` method invocation. This is how we explain to DocSearch how to; convert our web pages into records for the index. As an aside: you might think this is silly we; should generate records from our structured documentation not from the HTML we generate. I; agree. Alas, I am writing this and you are reading this. ```; return helpers; .docsearch({; recordProps: {; lvl0: {; selectors: ""h1"",; },; lvl1: [; ""dl.method > dt > .descname"",; ""dl.function > dt > .descname"",; ""h2"",; ],; lvl2: ""h3"",; lvl3: ""h4"",; content: [; ""dl.method > dd"",; ""dl.function > dd"",; // get how-to guides without breaking everything else:; ""section > section > section > dl:not(.class)"",; ],; pageRank: ""1"",; },; indexHeadings: true,; }); ```. For each HTML tag, DocSearch checks if it (and its ""parent"" tags) match the recordProps. For; example, this matches:. ```; <h1>Hello!</h1>; ```. It becomes a lvl0 hierarchy record. All sibling tags which appear after this `h1` are considered; children of it. For example, the following page would generate two records, one at `lvl0` and one at; `lvl1`:. ```; <h1>Hello!</h1>; <h2>Good bye!</h2>; ```. Terminal records are only generated when the ""content"" CSS selector matches. I manually iterated on this, checking one how-to guide page, one class reference page, and one VDS; reference page, until I found a set of CSS selectors that appeared to me to parse the page into a; reasonable hierarchy. Once you're happy with the configuration, ""Save"" it. You can now eagerly request a crawl of the; website. Once the crawl is complete, as long as the new index doesn't have many fewer records, it; will immediately become active. Every search on hail.is will now use your new index.; ",MatchSource.DOCS,dev-docs/algolia.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/algolia.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/algolia.md:2903,Modifiability,config,configuration,2903,"for this product is absurdly hard to find. For our current application it is:; https://crawler.algolia.com/admin/crawlers?sort=status&order=ASC&limit=20&appId=SWB3TKBY4S . Click through to the ""hail_is"" index and go to the ""Editor"". Ignore most of the information here and; look at the `helpers.docsearch` method invocation. This is how we explain to DocSearch how to; convert our web pages into records for the index. As an aside: you might think this is silly we; should generate records from our structured documentation not from the HTML we generate. I; agree. Alas, I am writing this and you are reading this. ```; return helpers; .docsearch({; recordProps: {; lvl0: {; selectors: ""h1"",; },; lvl1: [; ""dl.method > dt > .descname"",; ""dl.function > dt > .descname"",; ""h2"",; ],; lvl2: ""h3"",; lvl3: ""h4"",; content: [; ""dl.method > dd"",; ""dl.function > dd"",; // get how-to guides without breaking everything else:; ""section > section > section > dl:not(.class)"",; ],; pageRank: ""1"",; },; indexHeadings: true,; }); ```. For each HTML tag, DocSearch checks if it (and its ""parent"" tags) match the recordProps. For; example, this matches:. ```; <h1>Hello!</h1>; ```. It becomes a lvl0 hierarchy record. All sibling tags which appear after this `h1` are considered; children of it. For example, the following page would generate two records, one at `lvl0` and one at; `lvl1`:. ```; <h1>Hello!</h1>; <h2>Good bye!</h2>; ```. Terminal records are only generated when the ""content"" CSS selector matches. I manually iterated on this, checking one how-to guide page, one class reference page, and one VDS; reference page, until I found a set of CSS selectors that appeared to me to parse the page into a; reasonable hierarchy. Once you're happy with the configuration, ""Save"" it. You can now eagerly request a crawl of the; website. Once the crawl is complete, as long as the new index doesn't have many fewer records, it; will immediately become active. Every search on hail.is will now use your new index.; ",MatchSource.DOCS,dev-docs/algolia.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/algolia.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/algolia.md:2031,Usability,guid,guides,2031," are automatically created by a ""crawler"" called ""DocSearch"". This is also a hosted; product. The URL for this product is absurdly hard to find. For our current application it is:; https://crawler.algolia.com/admin/crawlers?sort=status&order=ASC&limit=20&appId=SWB3TKBY4S . Click through to the ""hail_is"" index and go to the ""Editor"". Ignore most of the information here and; look at the `helpers.docsearch` method invocation. This is how we explain to DocSearch how to; convert our web pages into records for the index. As an aside: you might think this is silly we; should generate records from our structured documentation not from the HTML we generate. I; agree. Alas, I am writing this and you are reading this. ```; return helpers; .docsearch({; recordProps: {; lvl0: {; selectors: ""h1"",; },; lvl1: [; ""dl.method > dt > .descname"",; ""dl.function > dt > .descname"",; ""h2"",; ],; lvl2: ""h3"",; lvl3: ""h4"",; content: [; ""dl.method > dd"",; ""dl.function > dd"",; // get how-to guides without breaking everything else:; ""section > section > section > dl:not(.class)"",; ],; pageRank: ""1"",; },; indexHeadings: true,; }); ```. For each HTML tag, DocSearch checks if it (and its ""parent"" tags) match the recordProps. For; example, this matches:. ```; <h1>Hello!</h1>; ```. It becomes a lvl0 hierarchy record. All sibling tags which appear after this `h1` are considered; children of it. For example, the following page would generate two records, one at `lvl0` and one at; `lvl1`:. ```; <h1>Hello!</h1>; <h2>Good bye!</h2>; ```. Terminal records are only generated when the ""content"" CSS selector matches. I manually iterated on this, checking one how-to guide page, one class reference page, and one VDS; reference page, until I found a set of CSS selectors that appeared to me to parse the page into a; reasonable hierarchy. Once you're happy with the configuration, ""Save"" it. You can now eagerly request a crawl of the; website. Once the crawl is complete, as long as the new index doesn't have many few",MatchSource.DOCS,dev-docs/algolia.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/algolia.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/algolia.md:2704,Usability,guid,guide,2704,"for this product is absurdly hard to find. For our current application it is:; https://crawler.algolia.com/admin/crawlers?sort=status&order=ASC&limit=20&appId=SWB3TKBY4S . Click through to the ""hail_is"" index and go to the ""Editor"". Ignore most of the information here and; look at the `helpers.docsearch` method invocation. This is how we explain to DocSearch how to; convert our web pages into records for the index. As an aside: you might think this is silly we; should generate records from our structured documentation not from the HTML we generate. I; agree. Alas, I am writing this and you are reading this. ```; return helpers; .docsearch({; recordProps: {; lvl0: {; selectors: ""h1"",; },; lvl1: [; ""dl.method > dt > .descname"",; ""dl.function > dt > .descname"",; ""h2"",; ],; lvl2: ""h3"",; lvl3: ""h4"",; content: [; ""dl.method > dd"",; ""dl.function > dd"",; // get how-to guides without breaking everything else:; ""section > section > section > dl:not(.class)"",; ],; pageRank: ""1"",; },; indexHeadings: true,; }); ```. For each HTML tag, DocSearch checks if it (and its ""parent"" tags) match the recordProps. For; example, this matches:. ```; <h1>Hello!</h1>; ```. It becomes a lvl0 hierarchy record. All sibling tags which appear after this `h1` are considered; children of it. For example, the following page would generate two records, one at `lvl0` and one at; `lvl1`:. ```; <h1>Hello!</h1>; <h2>Good bye!</h2>; ```. Terminal records are only generated when the ""content"" CSS selector matches. I manually iterated on this, checking one how-to guide page, one class reference page, and one VDS; reference page, until I found a set of CSS selectors that appeared to me to parse the page into a; reasonable hierarchy. Once you're happy with the configuration, ""Save"" it. You can now eagerly request a crawl of the; website. Once the crawl is complete, as long as the new index doesn't have many fewer records, it; will immediately become active. Every search on hail.is will now use your new index.; ",MatchSource.DOCS,dev-docs/algolia.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/algolia.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:907,Availability,down,down,907,"# Hail Development Process. The lifecycle of a new contribution to the Hail code base consists of the; following steps: designing the feature, implementing changes, creating a PR,; reviewing a PR, approving and merging the PR, deploying the changes, and then; making periodic releases for users. ## Design. New features can either be bug fixes that users run into, small feature improvements, or larger,; more complicated features. For larger projects, write an; [RFC](https://github.com/hail-is/hail-rfcs), get a review, and merge it into the hail-rfcs; repository before working on a PR to the main Hail repository. We use this process as a chance to; refine the design as well as educate the rest of the team on proposed changes. It helps to have; multiple eyes thinking about what the implications of the changes are to the rest of the system. In; addition, we use this time to think about how to break down the feature into smaller, more; manageable chunks. Ideally, branches should contain up to 200 lines of changes to make the process; easier on the reviewer. It may not always be possible to break up a feature into smaller components. ## Implementation. ### Environment / Tooling; Before you can write code, there are some setup steps that will allow you to; develop effectively. Hail currently supports Python version 3.9 or greater. Install the python dependencies of every Hail sub-project (e.g. ci, batch, hail/python/hailtop):. ```bash; make install-dev-requirements; ```. Install the pre-commit hooks:. ```bash; pre-commit install --install-hooks; ```. This creates git hooks that run certain linting checks, pyright on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:7539,Availability,down,download-secret,7539,"ase for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, so the gsa-key secrets for these must be copied across from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ download-secret <username>-gsa-key; $ mv secret.json key.json; $ kubectl create secret generic <username>-gsa-key --namespace=<username> --from-file=key.json; ```. To submit jobs to your dev namespace, you need to configure your local hail; installation to point to the dev namespace. You can do this by running. ```bash; hailctl dev config set default_namespace <my_namespace>; hailctl auth login; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands. NOTE: The dev deploy command above sets `HAIL_DEFAULT_NAMESPACE=default`. That's; because `hailctl dev deploy` submits a request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed;",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:11418,Availability,down,down,11418,"ing on a Batch Worker, you will need to `make`; deploy and then delete existing workers in your namespace. ## PR. Once you have a branch that you are happy with, then you create a Pull Request; on the GitHub UI. For an overview of our practices around git and pull requests,; see [this doc](git-practices.md). Set a reviewer in the ""Assignees"" box. Do *not* use the ""Reviewers"" box. Our CI system specifically; uses the ""Assignees"" box to list a developer's pending reviews at https://ci.hail.is/me. If you are; an outside contributor and cannot request reviews, you can have CI automatically assign a; reviewer. By writing `#assign services` or `#assign compiler` in the PR body, CI will randomly; select a collaborator on the relevant team and assign them for you. You can also give the PR a set of labels. The important ones are “WIP” to make; sure the pull request doesn’t get merged accidentally until you are ready,; “migration” to warn everyone that the changes will shut down the Batch; deployment if it requires a database migration, “bug” for bug fixes, “breaking; change” for any user breaking changes for Hail Query, and “prio:high” to make; this PR the first one in line to merge. There’s also “stacked PR” to indicate; that the changes in the PR are dependent on the changes in another PR. You; should reference that PR in your commit message with “Stacked on #9883”. Most; PRs will not have any labels. For the PR title, start the title with the name of the service(s) the changes; impact. For example, if it’s a Benchmark change, then you’d write; `[benchmark]`. If it’s a Hail Query change, then it would be `[query]`. We also want; the title to be descriptive enough to know what the change is without being too; verbose. An example is “`[batch]` Added read_only option for gcsfuse”. For the PR commit message, we want the message to be descriptive of the complete; set of changes that occurred, especially if it’s a complicated set of; changes. If it’s a smaller, obvious change lik",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:16089,Availability,error,error,16089,"n you can re-request a review; in the ""Reviewers"" box. If your review is requested on a PR submitted by an outside contributor, you should; ""assign"" yourself or the appropriate team member to the PR. The assignee is; responsible for ensuring that the PR does not go stale and is eventually; merged or closed. ![](dismiss-review.png). ## Merge / Deploy. Once a PR has been approved, our continuous integration service (CI) will squash; merge the commits in the PR into the main branch of the repository. This will; then trigger a deploy batch. The deploy batch will first deploy all of the new; Docker images, redeploy the running services in the default namespace with the; latest changes, and rerun all of the tests with the new version of main; incorporating your changes. If this batch fails, a Zulip message will be sent to the entire team linking to the UI for the; failing batch. We should never ignore this message. We should figure out what component broke. If; it’s a transient error, then we need to harden our retry strategy. The `build.yaml` file describes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch deploy; ```. ## Release. Changes to the services are immediately deployed to the production clust",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:16461,Availability,avail,available,16461,". ## Merge / Deploy. Once a PR has been approved, our continuous integration service (CI) will squash; merge the commits in the PR into the main branch of the repository. This will; then trigger a deploy batch. The deploy batch will first deploy all of the new; Docker images, redeploy the running services in the default namespace with the; latest changes, and rerun all of the tests with the new version of main; incorporating your changes. If this batch fails, a Zulip message will be sent to the entire team linking to the UI for the; failing batch. We should never ignore this message. We should figure out what component broke. If; it’s a transient error, then we need to harden our retry strategy. The `build.yaml` file describes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch deploy; ```. ## Release. Changes to the services are immediately deployed to the production cluster. Changes to the Hail; Python library are disseminated via PyPI. When a main commit changes the Hail ""Python version"", CI; will run the release steps. Changes to the Python version should be done by a PR whose title is; ""[release] 0.2.XXX"", which changes the Python version in both `hail/version.mk` and `hail/build.sc`,; and whic",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:16540,Availability,down,down,16540,"ts in the PR into the main branch of the repository. This will; then trigger a deploy batch. The deploy batch will first deploy all of the new; Docker images, redeploy the running services in the default namespace with the; latest changes, and rerun all of the tests with the new version of main; incorporating your changes. If this batch fails, a Zulip message will be sent to the entire team linking to the UI for the; failing batch. We should never ignore this message. We should figure out what component broke. If; it’s a transient error, then we need to harden our retry strategy. The `build.yaml` file describes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch deploy; ```. ## Release. Changes to the services are immediately deployed to the production cluster. Changes to the Hail; Python library are disseminated via PyPI. When a main commit changes the Hail ""Python version"", CI; will run the release steps. Changes to the Python version should be done by a PR whose title is; ""[release] 0.2.XXX"", which changes the Python version in both `hail/version.mk` and `hail/build.sc`,; and which updates the Batch and Query change logs: `hail/python/hail/docs/change_log.md`; `hail/python/hailtop/batch/docs/chan",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:227,Deployability,deploy,deploying,227,"# Hail Development Process. The lifecycle of a new contribution to the Hail code base consists of the; following steps: designing the feature, implementing changes, creating a PR,; reviewing a PR, approving and merging the PR, deploying the changes, and then; making periodic releases for users. ## Design. New features can either be bug fixes that users run into, small feature improvements, or larger,; more complicated features. For larger projects, write an; [RFC](https://github.com/hail-is/hail-rfcs), get a review, and merge it into the hail-rfcs; repository before working on a PR to the main Hail repository. We use this process as a chance to; refine the design as well as educate the rest of the team on proposed changes. It helps to have; multiple eyes thinking about what the implications of the changes are to the rest of the system. In; addition, we use this time to think about how to break down the feature into smaller, more; manageable chunks. Ideally, branches should contain up to 200 lines of changes to make the process; easier on the reviewer. It may not always be possible to break up a feature into smaller components. ## Implementation. ### Environment / Tooling; Before you can write code, there are some setup steps that will allow you to; develop effectively. Hail currently supports Python version 3.9 or greater. Install the python dependencies of every Hail sub-project (e.g. ci, batch, hail/python/hailtop):. ```bash; make install-dev-requirements; ```. Install the pre-commit hooks:. ```bash; pre-commit install --install-hooks; ```. This creates git hooks that run certain linting checks, pyright on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:276,Deployability,release,releases,276,"# Hail Development Process. The lifecycle of a new contribution to the Hail code base consists of the; following steps: designing the feature, implementing changes, creating a PR,; reviewing a PR, approving and merging the PR, deploying the changes, and then; making periodic releases for users. ## Design. New features can either be bug fixes that users run into, small feature improvements, or larger,; more complicated features. For larger projects, write an; [RFC](https://github.com/hail-is/hail-rfcs), get a review, and merge it into the hail-rfcs; repository before working on a PR to the main Hail repository. We use this process as a chance to; refine the design as well as educate the rest of the team on proposed changes. It helps to have; multiple eyes thinking about what the implications of the changes are to the rest of the system. In; addition, we use this time to think about how to break down the feature into smaller, more; manageable chunks. Ideally, branches should contain up to 200 lines of changes to make the process; easier on the reviewer. It may not always be possible to break up a feature into smaller components. ## Implementation. ### Environment / Tooling; Before you can write code, there are some setup steps that will allow you to; develop effectively. Hail currently supports Python version 3.9 or greater. Install the python dependencies of every Hail sub-project (e.g. ci, batch, hail/python/hailtop):. ```bash; make install-dev-requirements; ```. Install the pre-commit hooks:. ```bash; pre-commit install --install-hooks; ```. This creates git hooks that run certain linting checks, pyright on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:1457,Deployability,install,install-dev-requirements,1457,"RFC](https://github.com/hail-is/hail-rfcs), get a review, and merge it into the hail-rfcs; repository before working on a PR to the main Hail repository. We use this process as a chance to; refine the design as well as educate the rest of the team on proposed changes. It helps to have; multiple eyes thinking about what the implications of the changes are to the rest of the system. In; addition, we use this time to think about how to break down the feature into smaller, more; manageable chunks. Ideally, branches should contain up to 200 lines of changes to make the process; easier on the reviewer. It may not always be possible to break up a feature into smaller components. ## Implementation. ### Environment / Tooling; Before you can write code, there are some setup steps that will allow you to; develop effectively. Hail currently supports Python version 3.9 or greater. Install the python dependencies of every Hail sub-project (e.g. ci, batch, hail/python/hailtop):. ```bash; make install-dev-requirements; ```. Install the pre-commit hooks:. ```bash; pre-commit install --install-hooks; ```. This creates git hooks that run certain linting checks, pyright on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.git-blame-ignore-revs`. To configure `git blame` to ignore these commits, run. ```bash; git config blame.ignoreRevsFile $HAIL/.git-blame-ignore-revs; ```. #### Services. Install and configure tools necessary for working on the Hail Services:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:1539,Deployability,install,install,1539," hail-rfcs; repository before working on a PR to the main Hail repository. We use this process as a chance to; refine the design as well as educate the rest of the team on proposed changes. It helps to have; multiple eyes thinking about what the implications of the changes are to the rest of the system. In; addition, we use this time to think about how to break down the feature into smaller, more; manageable chunks. Ideally, branches should contain up to 200 lines of changes to make the process; easier on the reviewer. It may not always be possible to break up a feature into smaller components. ## Implementation. ### Environment / Tooling; Before you can write code, there are some setup steps that will allow you to; develop effectively. Hail currently supports Python version 3.9 or greater. Install the python dependencies of every Hail sub-project (e.g. ci, batch, hail/python/hailtop):. ```bash; make install-dev-requirements; ```. Install the pre-commit hooks:. ```bash; pre-commit install --install-hooks; ```. This creates git hooks that run certain linting checks, pyright on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.git-blame-ignore-revs`. To configure `git blame` to ignore these commits, run. ```bash; git config blame.ignoreRevsFile $HAIL/.git-blame-ignore-revs; ```. #### Services. Install and configure tools necessary for working on the Hail Services:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`gcloud`](https://cloud.google.com/sdk/docs/install); 4. Configure gcloud and D",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:1549,Deployability,install,install-hooks,1549," hail-rfcs; repository before working on a PR to the main Hail repository. We use this process as a chance to; refine the design as well as educate the rest of the team on proposed changes. It helps to have; multiple eyes thinking about what the implications of the changes are to the rest of the system. In; addition, we use this time to think about how to break down the feature into smaller, more; manageable chunks. Ideally, branches should contain up to 200 lines of changes to make the process; easier on the reviewer. It may not always be possible to break up a feature into smaller components. ## Implementation. ### Environment / Tooling; Before you can write code, there are some setup steps that will allow you to; develop effectively. Hail currently supports Python version 3.9 or greater. Install the python dependencies of every Hail sub-project (e.g. ci, batch, hail/python/hailtop):. ```bash; make install-dev-requirements; ```. Install the pre-commit hooks:. ```bash; pre-commit install --install-hooks; ```. This creates git hooks that run certain linting checks, pyright on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.git-blame-ignore-revs`. To configure `git blame` to ignore these commits, run. ```bash; git config blame.ignoreRevsFile $HAIL/.git-blame-ignore-revs; ```. #### Services. Install and configure tools necessary for working on the Hail Services:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`gcloud`](https://cloud.google.com/sdk/docs/install); 4. Configure gcloud and D",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:2347,Deployability,install,install-kubectl,2347,"thon dependencies of every Hail sub-project (e.g. ci, batch, hail/python/hailtop):. ```bash; make install-dev-requirements; ```. Install the pre-commit hooks:. ```bash; pre-commit install --install-hooks; ```. This creates git hooks that run certain linting checks, pyright on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.git-blame-ignore-revs`. To configure `git blame` to ignore these commits, run. ```bash; git config blame.ignoreRevsFile $HAIL/.git-blame-ignore-revs; ```. #### Services. Install and configure tools necessary for working on the Hail Services:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`gcloud`](https://cloud.google.com/sdk/docs/install); 4. Configure gcloud and Docker for Hail:; ```bash; gcloud auth login; gcloud config set project hail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you ca",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:2381,Deployability,install,installed,2381,"thon dependencies of every Hail sub-project (e.g. ci, batch, hail/python/hailtop):. ```bash; make install-dev-requirements; ```. Install the pre-commit hooks:. ```bash; pre-commit install --install-hooks; ```. This creates git hooks that run certain linting checks, pyright on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.git-blame-ignore-revs`. To configure `git blame` to ignore these commits, run. ```bash; git config blame.ignoreRevsFile $HAIL/.git-blame-ignore-revs; ```. #### Services. Install and configure tools necessary for working on the Hail Services:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`gcloud`](https://cloud.google.com/sdk/docs/install); 4. Configure gcloud and Docker for Hail:; ```bash; gcloud auth login; gcloud config set project hail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you ca",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:2401,Deployability,install,installation,2401,"il/python/hailtop):. ```bash; make install-dev-requirements; ```. Install the pre-commit hooks:. ```bash; pre-commit install --install-hooks; ```. This creates git hooks that run certain linting checks, pyright on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.git-blame-ignore-revs`. To configure `git blame` to ignore these commits, run. ```bash; git config blame.ignoreRevsFile $HAIL/.git-blame-ignore-revs; ```. #### Services. Install and configure tools necessary for working on the Hail Services:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`gcloud`](https://cloud.google.com/sdk/docs/install); 4. Configure gcloud and Docker for Hail:; ```bash; gcloud auth login; gcloud config set project hail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build ha",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:2508,Deployability,install,install,2508,"mit hooks:. ```bash; pre-commit install --install-hooks; ```. This creates git hooks that run certain linting checks, pyright on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.git-blame-ignore-revs`. To configure `git blame` to ignore these commits, run. ```bash; git config blame.ignoreRevsFile $HAIL/.git-blame-ignore-revs; ```. #### Services. Install and configure tools necessary for working on the Hail Services:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`gcloud`](https://cloud.google.com/sdk/docs/install); 4. Configure gcloud and Docker for Hail:; ```bash; gcloud auth login; gcloud config set project hail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repositor",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:3143,Deployability,install,install,3143,"evs; ```. #### Services. Install and configure tools necessary for working on the Hail Services:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`gcloud`](https://cloud.google.com/sdk/docs/install); 4. Configure gcloud and Docker for Hail:; ```bash; gcloud auth login; gcloud config set project hail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):.",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:3538,Deployability,install,install-editable,3538," and Docker for Hail:; ```bash; gcloud auth login; gcloud config set project hail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of d",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:5553,Deployability,pipeline,pipeline,5553,"lesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/upload-remote-test-resources`. Run the Python query tests using the Batch backend and a production instance of Hail Batch; (https://batch.hail.is or https://batch.azure.hail.is depending on your dev config):. ```bash; make -C hail pytest-qob NAMESPACE=default; ```. Run the Python query documentation tests:. ```bash; make -C hail doctest-query; ```. #### Services. Production, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:5900,Deployability,deploy,deploy,5900,"tion instance of Hail Batch; (https://batch.hail.is or https://batch.azure.hail.is depending on your dev config):. ```bash; make -C hail pytest-qob NAMESPACE=default; ```. Run the Python query documentation tests:. ```bash; make -C hail doctest-query; ```. #### Services. Production, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:5937,Deployability,deploy,deploy,5937,"zure.hail.is depending on your dev config):. ```bash; make -C hail pytest-qob NAMESPACE=default; ```. Run the Python query documentation tests:. ```bash; make -C hail doctest-query; ```. #### Services. Production, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:6000,Deployability,deploy,deploy,6000,"MESPACE=default; ```. Run the Python query documentation tests:. ```bash; make -C hail doctest-query; ```. #### Services. Production, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default ha",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:6081,Deployability,deploy,deploy,6081,"duction, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy complete",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:6109,Deployability,deploy,deploys,6109,"duction, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy complete",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:6445,Deployability,deploy,deployment,6445,"ces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:6474,Deployability,deploy,deploy,6474,". Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, so the gsa-key secrets for these must be copied across from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ dow",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:6575,Deployability,deploy,deploying,6575,". Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, so the gsa-key secrets for these must be copied across from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ dow",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:6674,Deployability,deploy,deploys,6674,"long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, so the gsa-key secrets for these must be copied across from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ download-secret <username>-gsa-key; $ mv secret.json key.json; $ kubectl create secret generic <username>-gsa-key --name",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:6918,Deployability,deploy,deploy,6918,"en run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, so the gsa-key secrets for these must be copied across from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ download-secret <username>-gsa-key; $ mv secret.json key.json; $ kubectl create secret generic <username>-gsa-key --namespace=<username> --from-file=key.json; ```. To submit jobs to your dev namespace, you need to configure your local hail; installation to point to the dev namespace. You can do this by running. ```bash; hailctl dev",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:6941,Deployability,deploy,deploy,6941,"s. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, so the gsa-key secrets for these must be copied across from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ download-secret <username>-gsa-key; $ mv secret.json key.json; $ kubectl create secret generic <username>-gsa-key --namespace=<username> --from-file=key.json; ```. To submit jobs to your dev namespace, you need to configure your local hail; installation to point to the dev namespace. You can do this by running. ```bash; hailctl dev config set default_namespace <my_namespace>; hailctl auth login; ```; Th",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:7018,Deployability,deploy,deploy,7018,":<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, so the gsa-key secrets for these must be copied across from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ download-secret <username>-gsa-key; $ mv secret.json key.json; $ kubectl create secret generic <username>-gsa-key --namespace=<username> --from-file=key.json; ```. To submit jobs to your dev namespace, you need to configure your local hail; installation to point to the dev namespace. You can do this by running. ```bash; hailctl dev config set default_namespace <my_namespace>; hailctl auth login; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can al",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:7118,Deployability,deploy,deploy,7118," your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, so the gsa-key secrets for these must be copied across from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ download-secret <username>-gsa-key; $ mv secret.json key.json; $ kubectl create secret generic <username>-gsa-key --namespace=<username> --from-file=key.json; ```. To submit jobs to your dev namespace, you need to configure your local hail; installation to point to the dev namespace. You can do this by running. ```bash; hailctl dev config set default_namespace <my_namespace>; hailctl auth login; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:7780,Deployability,install,installation,7780,"the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, so the gsa-key secrets for these must be copied across from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ download-secret <username>-gsa-key; $ mv secret.json key.json; $ kubectl create secret generic <username>-gsa-key --namespace=<username> --from-file=key.json; ```. To submit jobs to your dev namespace, you need to configure your local hail; installation to point to the dev namespace. You can do this by running. ```bash; hailctl dev config set default_namespace <my_namespace>; hailctl auth login; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands. NOTE: The dev deploy command above sets `HAIL_DEFAULT_NAMESPACE=default`. That's; because `hailctl dev deploy` submits a request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ``",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:8175,Deployability,deploy,deploy,8175," https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, so the gsa-key secrets for these must be copied across from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ download-secret <username>-gsa-key; $ mv secret.json key.json; $ kubectl create secret generic <username>-gsa-key --namespace=<username> --from-file=key.json; ```. To submit jobs to your dev namespace, you need to configure your local hail; installation to point to the dev namespace. You can do this by running. ```bash; hailctl dev config set default_namespace <my_namespace>; hailctl auth login; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands. NOTE: The dev deploy command above sets `HAIL_DEFAULT_NAMESPACE=default`. That's; because `hailctl dev deploy` submits a request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy; 2. make deploy; 3. sync.py. These are listed in order of broadest scope to narrowest scope, so it is never; wrong to do something higher on the list, but it might be a slower feedback loop. A dev deploy is necessary under the following conditions:; - A dev n",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:8264,Deployability,deploy,deploy,8264,"t have permission to create pet service accounts in the; `hail-vdc` project, so the gsa-key secrets for these must be copied across from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ download-secret <username>-gsa-key; $ mv secret.json key.json; $ kubectl create secret generic <username>-gsa-key --namespace=<username> --from-file=key.json; ```. To submit jobs to your dev namespace, you need to configure your local hail; installation to point to the dev namespace. You can do this by running. ```bash; hailctl dev config set default_namespace <my_namespace>; hailctl auth login; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands. NOTE: The dev deploy command above sets `HAIL_DEFAULT_NAMESPACE=default`. That's; because `hailctl dev deploy` submits a request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy; 2. make deploy; 3. sync.py. These are listed in order of broadest scope to narrowest scope, so it is never; wrong to do something higher on the list, but it might be a slower feedback loop. A dev deploy is necessary under the following conditions:; - A dev namespace has not been used before; - You have added a migration that must be run against a service'",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:8405,Deployability,deploy,deployed,8405,"s from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ download-secret <username>-gsa-key; $ mv secret.json key.json; $ kubectl create secret generic <username>-gsa-key --namespace=<username> --from-file=key.json; ```. To submit jobs to your dev namespace, you need to configure your local hail; installation to point to the dev namespace. You can do this by running. ```bash; hailctl dev config set default_namespace <my_namespace>; hailctl auth login; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands. NOTE: The dev deploy command above sets `HAIL_DEFAULT_NAMESPACE=default`. That's; because `hailctl dev deploy` submits a request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy; 2. make deploy; 3. sync.py. These are listed in order of broadest scope to narrowest scope, so it is never; wrong to do something higher on the list, but it might be a slower feedback loop. A dev deploy is necessary under the following conditions:; - A dev namespace has not been used before; - You have added a migration that must be run against a service's database; - Not all necessary services (like Auth) are deployed in the namespace and you; do not want to `make` deploy them indi",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:8461,Deployability,deploy,deployed,8461,"s from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ download-secret <username>-gsa-key; $ mv secret.json key.json; $ kubectl create secret generic <username>-gsa-key --namespace=<username> --from-file=key.json; ```. To submit jobs to your dev namespace, you need to configure your local hail; installation to point to the dev namespace. You can do this by running. ```bash; hailctl dev config set default_namespace <my_namespace>; hailctl auth login; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands. NOTE: The dev deploy command above sets `HAIL_DEFAULT_NAMESPACE=default`. That's; because `hailctl dev deploy` submits a request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy; 2. make deploy; 3. sync.py. These are listed in order of broadest scope to narrowest scope, so it is never; wrong to do something higher on the list, but it might be a slower feedback loop. A dev deploy is necessary under the following conditions:; - A dev namespace has not been used before; - You have added a migration that must be run against a service's database; - Not all necessary services (like Auth) are deployed in the namespace and you; do not want to `make` deploy them indi",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:8857,Deployability,deploy,deploy,8857,"ing. ```bash; hailctl dev config set default_namespace <my_namespace>; hailctl auth login; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands. NOTE: The dev deploy command above sets `HAIL_DEFAULT_NAMESPACE=default`. That's; because `hailctl dev deploy` submits a request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy; 2. make deploy; 3. sync.py. These are listed in order of broadest scope to narrowest scope, so it is never; wrong to do something higher on the list, but it might be a slower feedback loop. A dev deploy is necessary under the following conditions:; - A dev namespace has not been used before; - You have added a migration that must be run against a service's database; - Not all necessary services (like Auth) are deployed in the namespace and you; do not want to `make` deploy them individually. If you just want to redeploy a single service, you can do so with the following:. ```bash; make -C batch deploy NAMESPACE=<my_namespace>; ```. If you only want to make a Python code change to an already-running service; in Kubernetes, you can run `devbin/sync.py` like the following:. ```bash; python3 devbin/sync.py \; --namespace <my_namespace> \; --app batch --app batch-driver \; --path batch/batch /usr/loc",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:8889,Deployability,deploy,deploy,8889,"espace <my_namespace>; hailctl auth login; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands. NOTE: The dev deploy command above sets `HAIL_DEFAULT_NAMESPACE=default`. That's; because `hailctl dev deploy` submits a request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy; 2. make deploy; 3. sync.py. These are listed in order of broadest scope to narrowest scope, so it is never; wrong to do something higher on the list, but it might be a slower feedback loop. A dev deploy is necessary under the following conditions:; - A dev namespace has not been used before; - You have added a migration that must be run against a service's database; - Not all necessary services (like Auth) are deployed in the namespace and you; do not want to `make` deploy them individually. If you just want to redeploy a single service, you can do so with the following:. ```bash; make -C batch deploy NAMESPACE=<my_namespace>; ```. If you only want to make a Python code change to an already-running service; in Kubernetes, you can run `devbin/sync.py` like the following:. ```bash; python3 devbin/sync.py \; --namespace <my_namespace> \; --app batch --app batch-driver \; --path batch/batch /usr/local/lib/python3.9/dist-packages/ \; --path gear/g",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:8931,Deployability,deploy,deploy,8931,"gin; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands. NOTE: The dev deploy command above sets `HAIL_DEFAULT_NAMESPACE=default`. That's; because `hailctl dev deploy` submits a request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy; 2. make deploy; 3. sync.py. These are listed in order of broadest scope to narrowest scope, so it is never; wrong to do something higher on the list, but it might be a slower feedback loop. A dev deploy is necessary under the following conditions:; - A dev namespace has not been used before; - You have added a migration that must be run against a service's database; - Not all necessary services (like Auth) are deployed in the namespace and you; do not want to `make` deploy them individually. If you just want to redeploy a single service, you can do so with the following:. ```bash; make -C batch deploy NAMESPACE=<my_namespace>; ```. If you only want to make a Python code change to an already-running service; in Kubernetes, you can run `devbin/sync.py` like the following:. ```bash; python3 devbin/sync.py \; --namespace <my_namespace> \; --app batch --app batch-driver \; --path batch/batch /usr/local/lib/python3.9/dist-packages/ \; --path gear/gear /usr/local/lib/python3.9/dist-packa",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:8947,Deployability,deploy,deploy,8947,"any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands. NOTE: The dev deploy command above sets `HAIL_DEFAULT_NAMESPACE=default`. That's; because `hailctl dev deploy` submits a request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy; 2. make deploy; 3. sync.py. These are listed in order of broadest scope to narrowest scope, so it is never; wrong to do something higher on the list, but it might be a slower feedback loop. A dev deploy is necessary under the following conditions:; - A dev namespace has not been used before; - You have added a migration that must be run against a service's database; - Not all necessary services (like Auth) are deployed in the namespace and you; do not want to `make` deploy them individually. If you just want to redeploy a single service, you can do so with the following:. ```bash; make -C batch deploy NAMESPACE=<my_namespace>; ```. If you only want to make a Python code change to an already-running service; in Kubernetes, you can run `devbin/sync.py` like the following:. ```bash; python3 devbin/sync.py \; --namespace <my_namespace> \; --app batch --app batch-driver \; --path batch/batch /usr/local/lib/python3.9/dist-packages/ \; --path gear/gear /usr/local/lib/python3.9/dist-packages/ \; --path ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:9135,Deployability,deploy,deploy,9135," request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy; 2. make deploy; 3. sync.py. These are listed in order of broadest scope to narrowest scope, so it is never; wrong to do something higher on the list, but it might be a slower feedback loop. A dev deploy is necessary under the following conditions:; - A dev namespace has not been used before; - You have added a migration that must be run against a service's database; - Not all necessary services (like Auth) are deployed in the namespace and you; do not want to `make` deploy them individually. If you just want to redeploy a single service, you can do so with the following:. ```bash; make -C batch deploy NAMESPACE=<my_namespace>; ```. If you only want to make a Python code change to an already-running service; in Kubernetes, you can run `devbin/sync.py` like the following:. ```bash; python3 devbin/sync.py \; --namespace <my_namespace> \; --app batch --app batch-driver \; --path batch/batch /usr/local/lib/python3.9/dist-packages/ \; --path gear/gear /usr/local/lib/python3.9/dist-packages/ \; --path web_common/web_common /usr/local/lib/python3.9/dist-packages/ \; --path hail/python/hailtop /usr/local/lib/python3.9/dist-packages/; ```. This will create a long-running process that watches the files in the `batch`; and `gear` modules. When changes to those files are saved it will upload those; changes to currently running pods in",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:9353,Deployability,deploy,deployed,9353," request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy; 2. make deploy; 3. sync.py. These are listed in order of broadest scope to narrowest scope, so it is never; wrong to do something higher on the list, but it might be a slower feedback loop. A dev deploy is necessary under the following conditions:; - A dev namespace has not been used before; - You have added a migration that must be run against a service's database; - Not all necessary services (like Auth) are deployed in the namespace and you; do not want to `make` deploy them individually. If you just want to redeploy a single service, you can do so with the following:. ```bash; make -C batch deploy NAMESPACE=<my_namespace>; ```. If you only want to make a Python code change to an already-running service; in Kubernetes, you can run `devbin/sync.py` like the following:. ```bash; python3 devbin/sync.py \; --namespace <my_namespace> \; --app batch --app batch-driver \; --path batch/batch /usr/local/lib/python3.9/dist-packages/ \; --path gear/gear /usr/local/lib/python3.9/dist-packages/ \; --path web_common/web_common /usr/local/lib/python3.9/dist-packages/ \; --path hail/python/hailtop /usr/local/lib/python3.9/dist-packages/; ```. This will create a long-running process that watches the files in the `batch`; and `gear` modules. When changes to those files are saved it will upload those; changes to currently running pods in",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:9410,Deployability,deploy,deploy,9410," request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy; 2. make deploy; 3. sync.py. These are listed in order of broadest scope to narrowest scope, so it is never; wrong to do something higher on the list, but it might be a slower feedback loop. A dev deploy is necessary under the following conditions:; - A dev namespace has not been used before; - You have added a migration that must be run against a service's database; - Not all necessary services (like Auth) are deployed in the namespace and you; do not want to `make` deploy them individually. If you just want to redeploy a single service, you can do so with the following:. ```bash; make -C batch deploy NAMESPACE=<my_namespace>; ```. If you only want to make a Python code change to an already-running service; in Kubernetes, you can run `devbin/sync.py` like the following:. ```bash; python3 devbin/sync.py \; --namespace <my_namespace> \; --app batch --app batch-driver \; --path batch/batch /usr/local/lib/python3.9/dist-packages/ \; --path gear/gear /usr/local/lib/python3.9/dist-packages/ \; --path web_common/web_common /usr/local/lib/python3.9/dist-packages/ \; --path hail/python/hailtop /usr/local/lib/python3.9/dist-packages/; ```. This will create a long-running process that watches the files in the `batch`; and `gear` modules. When changes to those files are saved it will upload those; changes to currently running pods in",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:9541,Deployability,deploy,deploy,9541,"ur namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy; 2. make deploy; 3. sync.py. These are listed in order of broadest scope to narrowest scope, so it is never; wrong to do something higher on the list, but it might be a slower feedback loop. A dev deploy is necessary under the following conditions:; - A dev namespace has not been used before; - You have added a migration that must be run against a service's database; - Not all necessary services (like Auth) are deployed in the namespace and you; do not want to `make` deploy them individually. If you just want to redeploy a single service, you can do so with the following:. ```bash; make -C batch deploy NAMESPACE=<my_namespace>; ```. If you only want to make a Python code change to an already-running service; in Kubernetes, you can run `devbin/sync.py` like the following:. ```bash; python3 devbin/sync.py \; --namespace <my_namespace> \; --app batch --app batch-driver \; --path batch/batch /usr/local/lib/python3.9/dist-packages/ \; --path gear/gear /usr/local/lib/python3.9/dist-packages/ \; --path web_common/web_common /usr/local/lib/python3.9/dist-packages/ \; --path hail/python/hailtop /usr/local/lib/python3.9/dist-packages/; ```. This will create a long-running process that watches the files in the `batch`; and `gear` modules. When changes to those files are saved it will upload those; changes to currently running pods in the namespace and restart those pods. Note; that Batch Workers are not running as pods in Kubernetes, and are immutable. So; if you want to update code running on a Batch Worker, you will need to `make`; deploy and then delete existing workers in your namespace. ##",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:10423,Deployability,update,update,10423,"service, you can do so with the following:. ```bash; make -C batch deploy NAMESPACE=<my_namespace>; ```. If you only want to make a Python code change to an already-running service; in Kubernetes, you can run `devbin/sync.py` like the following:. ```bash; python3 devbin/sync.py \; --namespace <my_namespace> \; --app batch --app batch-driver \; --path batch/batch /usr/local/lib/python3.9/dist-packages/ \; --path gear/gear /usr/local/lib/python3.9/dist-packages/ \; --path web_common/web_common /usr/local/lib/python3.9/dist-packages/ \; --path hail/python/hailtop /usr/local/lib/python3.9/dist-packages/; ```. This will create a long-running process that watches the files in the `batch`; and `gear` modules. When changes to those files are saved it will upload those; changes to currently running pods in the namespace and restart those pods. Note; that Batch Workers are not running as pods in Kubernetes, and are immutable. So; if you want to update code running on a Batch Worker, you will need to `make`; deploy and then delete existing workers in your namespace. ## PR. Once you have a branch that you are happy with, then you create a Pull Request; on the GitHub UI. For an overview of our practices around git and pull requests,; see [this doc](git-practices.md). Set a reviewer in the ""Assignees"" box. Do *not* use the ""Reviewers"" box. Our CI system specifically; uses the ""Assignees"" box to list a developer's pending reviews at https://ci.hail.is/me. If you are; an outside contributor and cannot request reviews, you can have CI automatically assign a; reviewer. By writing `#assign services` or `#assign compiler` in the PR body, CI will randomly; select a collaborator on the relevant team and assign them for you. You can also give the PR a set of labels. The important ones are “WIP” to make; sure the pull request doesn’t get merged accidentally until you are ready,; “migration” to warn everyone that the changes will shut down the Batch; deployment if it requires a database mig",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:10487,Deployability,deploy,deploy,10487,"service, you can do so with the following:. ```bash; make -C batch deploy NAMESPACE=<my_namespace>; ```. If you only want to make a Python code change to an already-running service; in Kubernetes, you can run `devbin/sync.py` like the following:. ```bash; python3 devbin/sync.py \; --namespace <my_namespace> \; --app batch --app batch-driver \; --path batch/batch /usr/local/lib/python3.9/dist-packages/ \; --path gear/gear /usr/local/lib/python3.9/dist-packages/ \; --path web_common/web_common /usr/local/lib/python3.9/dist-packages/ \; --path hail/python/hailtop /usr/local/lib/python3.9/dist-packages/; ```. This will create a long-running process that watches the files in the `batch`; and `gear` modules. When changes to those files are saved it will upload those; changes to currently running pods in the namespace and restart those pods. Note; that Batch Workers are not running as pods in Kubernetes, and are immutable. So; if you want to update code running on a Batch Worker, you will need to `make`; deploy and then delete existing workers in your namespace. ## PR. Once you have a branch that you are happy with, then you create a Pull Request; on the GitHub UI. For an overview of our practices around git and pull requests,; see [this doc](git-practices.md). Set a reviewer in the ""Assignees"" box. Do *not* use the ""Reviewers"" box. Our CI system specifically; uses the ""Assignees"" box to list a developer's pending reviews at https://ci.hail.is/me. If you are; an outside contributor and cannot request reviews, you can have CI automatically assign a; reviewer. By writing `#assign services` or `#assign compiler` in the PR body, CI will randomly; select a collaborator on the relevant team and assign them for you. You can also give the PR a set of labels. The important ones are “WIP” to make; sure the pull request doesn’t get merged accidentally until you are ready,; “migration” to warn everyone that the changes will shut down the Batch; deployment if it requires a database mig",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:11434,Deployability,deploy,deployment,11434,"ing on a Batch Worker, you will need to `make`; deploy and then delete existing workers in your namespace. ## PR. Once you have a branch that you are happy with, then you create a Pull Request; on the GitHub UI. For an overview of our practices around git and pull requests,; see [this doc](git-practices.md). Set a reviewer in the ""Assignees"" box. Do *not* use the ""Reviewers"" box. Our CI system specifically; uses the ""Assignees"" box to list a developer's pending reviews at https://ci.hail.is/me. If you are; an outside contributor and cannot request reviews, you can have CI automatically assign a; reviewer. By writing `#assign services` or `#assign compiler` in the PR body, CI will randomly; select a collaborator on the relevant team and assign them for you. You can also give the PR a set of labels. The important ones are “WIP” to make; sure the pull request doesn’t get merged accidentally until you are ready,; “migration” to warn everyone that the changes will shut down the Batch; deployment if it requires a database migration, “bug” for bug fixes, “breaking; change” for any user breaking changes for Hail Query, and “prio:high” to make; this PR the first one in line to merge. There’s also “stacked PR” to indicate; that the changes in the PR are dependent on the changes in another PR. You; should reference that PR in your commit message with “Stacked on #9883”. Most; PRs will not have any labels. For the PR title, start the title with the name of the service(s) the changes; impact. For example, if it’s a Benchmark change, then you’d write; `[benchmark]`. If it’s a Hail Query change, then it would be `[query]`. We also want; the title to be descriptive enough to know what the change is without being too; verbose. An example is “`[batch]` Added read_only option for gcsfuse”. For the PR commit message, we want the message to be descriptive of the complete; set of changes that occurred, especially if it’s a complicated set of; changes. If it’s a smaller, obvious change lik",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:13231,Deployability,continuous,continuous,13231,"it message, we want the message to be descriptive of the complete; set of changes that occurred, especially if it’s a complicated set of; changes. If it’s a smaller, obvious change like a one-liner, then it’s okay to; omit the commit message. For UI changes, it’s helpful to paste a screenshot of; the changes. It’s also a good idea to comment on how you tested the changes and; whether there are any implications of your changes. If the PR fixes a bug that; is a GitHub issue, then you can say “Fixes #8900” to make sure the issue gets; automatically closed by GitHub when your PR is merged. You can also tag a; specific member of the team in the message to get their attention with “@user”. If the changes are user-facing, then add a line in your commit message that; starts with “CHANGELOG: description.”. This should be one line with one sentence; that ends in a period. Once you are done with writing up all the details of the Pull Request, you can; then submit it. Our continuous integration (CI) system watches for new PRs. When; it sees a new PR, it creates a new batch that will test everything defined in; the build.yaml file in the root of the repository. This will create a temporary; namespace in kubernetes for your PR and deploy all the services into it. It will; also run all the tests such as for query, batch, and ci after merging it with; the latest version of main. You can view the progress of the build and the; logs for your PR at [ci.hail.is](https://ci.hail.is). ## Review. Once the PR has been created, it is the responsibility of the reviewer(s) to; review the PR. Our goal as a team is to give comments within 24 hours. To review; someone else’s changes, click on “Files changed”. This will show the diff; between the old code and the new proposed changes. You can make comments on; specific lines of the code. Feel free to ask questions here, especially if you; don’t understand something! It’s a good idea to think critically about the; changes. There should also be tes",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:13242,Deployability,integrat,integration,13242,"it message, we want the message to be descriptive of the complete; set of changes that occurred, especially if it’s a complicated set of; changes. If it’s a smaller, obvious change like a one-liner, then it’s okay to; omit the commit message. For UI changes, it’s helpful to paste a screenshot of; the changes. It’s also a good idea to comment on how you tested the changes and; whether there are any implications of your changes. If the PR fixes a bug that; is a GitHub issue, then you can say “Fixes #8900” to make sure the issue gets; automatically closed by GitHub when your PR is merged. You can also tag a; specific member of the team in the message to get their attention with “@user”. If the changes are user-facing, then add a line in your commit message that; starts with “CHANGELOG: description.”. This should be one line with one sentence; that ends in a period. Once you are done with writing up all the details of the Pull Request, you can; then submit it. Our continuous integration (CI) system watches for new PRs. When; it sees a new PR, it creates a new batch that will test everything defined in; the build.yaml file in the root of the repository. This will create a temporary; namespace in kubernetes for your PR and deploy all the services into it. It will; also run all the tests such as for query, batch, and ci after merging it with; the latest version of main. You can view the progress of the build and the; logs for your PR at [ci.hail.is](https://ci.hail.is). ## Review. Once the PR has been created, it is the responsibility of the reviewer(s) to; review the PR. Our goal as a team is to give comments within 24 hours. To review; someone else’s changes, click on “Files changed”. This will show the diff; between the old code and the new proposed changes. You can make comments on; specific lines of the code. Feel free to ask questions here, especially if you; don’t understand something! It’s a good idea to think critically about the; changes. There should also be tes",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:13493,Deployability,deploy,deploy,13493," omit the commit message. For UI changes, it’s helpful to paste a screenshot of; the changes. It’s also a good idea to comment on how you tested the changes and; whether there are any implications of your changes. If the PR fixes a bug that; is a GitHub issue, then you can say “Fixes #8900” to make sure the issue gets; automatically closed by GitHub when your PR is merged. You can also tag a; specific member of the team in the message to get their attention with “@user”. If the changes are user-facing, then add a line in your commit message that; starts with “CHANGELOG: description.”. This should be one line with one sentence; that ends in a period. Once you are done with writing up all the details of the Pull Request, you can; then submit it. Our continuous integration (CI) system watches for new PRs. When; it sees a new PR, it creates a new batch that will test everything defined in; the build.yaml file in the root of the repository. This will create a temporary; namespace in kubernetes for your PR and deploy all the services into it. It will; also run all the tests such as for query, batch, and ci after merging it with; the latest version of main. You can view the progress of the build and the; logs for your PR at [ci.hail.is](https://ci.hail.is). ## Review. Once the PR has been created, it is the responsibility of the reviewer(s) to; review the PR. Our goal as a team is to give comments within 24 hours. To review; someone else’s changes, click on “Files changed”. This will show the diff; between the old code and the new proposed changes. You can make comments on; specific lines of the code. Feel free to ask questions here, especially if you; don’t understand something! It’s a good idea to think critically about the; changes. There should also be tests either added or existing to make sure the; code changes do not break any existing functionality and actually implement what; was intended. For example, a change to test whether Batch doesn’t crash when a; user gives",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:15488,Deployability,continuous,continuous,15488,"kay to spend a; lot of time reviewing PRs! This is a critical part of our development process to; avoid bugs and unintentional breaking changes. If there are items for the; developer to address, then submit your review with “Request Changes”. Otherwise,; once you are happy with the changes and all comments have been addressed, you; can “Approve” the PR. If you are the person whose code is being reviewed and your PR is in the Request; Changes state, then you’ll need to address their comments by pushing new commit; changes or answering questions. Once you are done, then you can re-request a review; in the ""Reviewers"" box. If your review is requested on a PR submitted by an outside contributor, you should; ""assign"" yourself or the appropriate team member to the PR. The assignee is; responsible for ensuring that the PR does not go stale and is eventually; merged or closed. ![](dismiss-review.png). ## Merge / Deploy. Once a PR has been approved, our continuous integration service (CI) will squash; merge the commits in the PR into the main branch of the repository. This will; then trigger a deploy batch. The deploy batch will first deploy all of the new; Docker images, redeploy the running services in the default namespace with the; latest changes, and rerun all of the tests with the new version of main; incorporating your changes. If this batch fails, a Zulip message will be sent to the entire team linking to the UI for the; failing batch. We should never ignore this message. We should figure out what component broke. If; it’s a transient error, then we need to harden our retry strategy. The `build.yaml` file describes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:15499,Deployability,integrat,integration,15499,"kay to spend a; lot of time reviewing PRs! This is a critical part of our development process to; avoid bugs and unintentional breaking changes. If there are items for the; developer to address, then submit your review with “Request Changes”. Otherwise,; once you are happy with the changes and all comments have been addressed, you; can “Approve” the PR. If you are the person whose code is being reviewed and your PR is in the Request; Changes state, then you’ll need to address their comments by pushing new commit; changes or answering questions. Once you are done, then you can re-request a review; in the ""Reviewers"" box. If your review is requested on a PR submitted by an outside contributor, you should; ""assign"" yourself or the appropriate team member to the PR. The assignee is; responsible for ensuring that the PR does not go stale and is eventually; merged or closed. ![](dismiss-review.png). ## Merge / Deploy. Once a PR has been approved, our continuous integration service (CI) will squash; merge the commits in the PR into the main branch of the repository. This will; then trigger a deploy batch. The deploy batch will first deploy all of the new; Docker images, redeploy the running services in the default namespace with the; latest changes, and rerun all of the tests with the new version of main; incorporating your changes. If this batch fails, a Zulip message will be sent to the entire team linking to the UI for the; failing batch. We should never ignore this message. We should figure out what component broke. If; it’s a transient error, then we need to harden our retry strategy. The `build.yaml` file describes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:15631,Deployability,deploy,deploy,15631,"o; avoid bugs and unintentional breaking changes. If there are items for the; developer to address, then submit your review with “Request Changes”. Otherwise,; once you are happy with the changes and all comments have been addressed, you; can “Approve” the PR. If you are the person whose code is being reviewed and your PR is in the Request; Changes state, then you’ll need to address their comments by pushing new commit; changes or answering questions. Once you are done, then you can re-request a review; in the ""Reviewers"" box. If your review is requested on a PR submitted by an outside contributor, you should; ""assign"" yourself or the appropriate team member to the PR. The assignee is; responsible for ensuring that the PR does not go stale and is eventually; merged or closed. ![](dismiss-review.png). ## Merge / Deploy. Once a PR has been approved, our continuous integration service (CI) will squash; merge the commits in the PR into the main branch of the repository. This will; then trigger a deploy batch. The deploy batch will first deploy all of the new; Docker images, redeploy the running services in the default namespace with the; latest changes, and rerun all of the tests with the new version of main; incorporating your changes. If this batch fails, a Zulip message will be sent to the entire team linking to the UI for the; failing batch. We should never ignore this message. We should figure out what component broke. If; it’s a transient error, then we need to harden our retry strategy. The `build.yaml` file describes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:15649,Deployability,deploy,deploy,15649,"t Changes”. Otherwise,; once you are happy with the changes and all comments have been addressed, you; can “Approve” the PR. If you are the person whose code is being reviewed and your PR is in the Request; Changes state, then you’ll need to address their comments by pushing new commit; changes or answering questions. Once you are done, then you can re-request a review; in the ""Reviewers"" box. If your review is requested on a PR submitted by an outside contributor, you should; ""assign"" yourself or the appropriate team member to the PR. The assignee is; responsible for ensuring that the PR does not go stale and is eventually; merged or closed. ![](dismiss-review.png). ## Merge / Deploy. Once a PR has been approved, our continuous integration service (CI) will squash; merge the commits in the PR into the main branch of the repository. This will; then trigger a deploy batch. The deploy batch will first deploy all of the new; Docker images, redeploy the running services in the default namespace with the; latest changes, and rerun all of the tests with the new version of main; incorporating your changes. If this batch fails, a Zulip message will be sent to the entire team linking to the UI for the; failing batch. We should never ignore this message. We should figure out what component broke. If; it’s a transient error, then we need to harden our retry strategy. The `build.yaml` file describes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_m",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:15673,Deployability,deploy,deploy,15673,"t Changes”. Otherwise,; once you are happy with the changes and all comments have been addressed, you; can “Approve” the PR. If you are the person whose code is being reviewed and your PR is in the Request; Changes state, then you’ll need to address their comments by pushing new commit; changes or answering questions. Once you are done, then you can re-request a review; in the ""Reviewers"" box. If your review is requested on a PR submitted by an outside contributor, you should; ""assign"" yourself or the appropriate team member to the PR. The assignee is; responsible for ensuring that the PR does not go stale and is eventually; merged or closed. ![](dismiss-review.png). ## Merge / Deploy. Once a PR has been approved, our continuous integration service (CI) will squash; merge the commits in the PR into the main branch of the repository. This will; then trigger a deploy batch. The deploy batch will first deploy all of the new; Docker images, redeploy the running services in the default namespace with the; latest changes, and rerun all of the tests with the new version of main; incorporating your changes. If this batch fails, a Zulip message will be sent to the entire team linking to the UI for the; failing batch. We should never ignore this message. We should figure out what component broke. If; it’s a transient error, then we need to harden our retry strategy. The `build.yaml` file describes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_m",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:16188,Deployability,deploy,deploy,16188,"ed on a PR submitted by an outside contributor, you should; ""assign"" yourself or the appropriate team member to the PR. The assignee is; responsible for ensuring that the PR does not go stale and is eventually; merged or closed. ![](dismiss-review.png). ## Merge / Deploy. Once a PR has been approved, our continuous integration service (CI) will squash; merge the commits in the PR into the main branch of the repository. This will; then trigger a deploy batch. The deploy batch will first deploy all of the new; Docker images, redeploy the running services in the default namespace with the; latest changes, and rerun all of the tests with the new version of main; incorporating your changes. If this batch fails, a Zulip message will be sent to the entire team linking to the UI for the; failing batch. We should never ignore this message. We should figure out what component broke. If; it’s a transient error, then we need to harden our retry strategy. The `build.yaml` file describes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch deploy; ```. ## Release. Changes to the services are immediately deployed to the production cluster. Changes to the Hail; Python library are disseminated via PyPI. When a main c",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:16962,Deployability,deploy,deploy,16962," linking to the UI for the; failing batch. We should never ignore this message. We should figure out what component broke. If; it’s a transient error, then we need to harden our retry strategy. The `build.yaml` file describes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch deploy; ```. ## Release. Changes to the services are immediately deployed to the production cluster. Changes to the Hail; Python library are disseminated via PyPI. When a main commit changes the Hail ""Python version"", CI; will run the release steps. Changes to the Python version should be done by a PR whose title is; ""[release] 0.2.XXX"", which changes the Python version in both `hail/version.mk` and `hail/build.sc`,; and which updates the Batch and Query change logs: `hail/python/hail/docs/change_log.md`; `hail/python/hailtop/batch/docs/change_log.rst`. The change logs should comprise all the; `CHANGELOG:` lines from commits between the last release and the new release, for example:. ```; git log 0.2.127..HEAD; ```. CI will create a git tag, a GitHub release, a PyPI package version, update the public Hail docs. We; aim to release once per month. Note that our wheels are limited to 200 MiB and the sum total size of; all publis",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:17006,Deployability,deploy,deploy,17006,"ver ignore this message. We should figure out what component broke. If; it’s a transient error, then we need to harden our retry strategy. The `build.yaml` file describes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch deploy; ```. ## Release. Changes to the services are immediately deployed to the production cluster. Changes to the Hail; Python library are disseminated via PyPI. When a main commit changes the Hail ""Python version"", CI; will run the release steps. Changes to the Python version should be done by a PR whose title is; ""[release] 0.2.XXX"", which changes the Python version in both `hail/version.mk` and `hail/build.sc`,; and which updates the Batch and Query change logs: `hail/python/hail/docs/change_log.md`; `hail/python/hailtop/batch/docs/change_log.rst`. The change logs should comprise all the; `CHANGELOG:` lines from commits between the last release and the new release, for example:. ```; git log 0.2.127..HEAD; ```. CI will create a git tag, a GitHub release, a PyPI package version, update the public Hail docs. We; aim to release once per month. Note that our wheels are limited to 200 MiB and the sum total size of; all published PyPI wheels must be less than 20 GiB. Check [the cu",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:17071,Deployability,deploy,deployed,17071,"If; it’s a transient error, then we need to harden our retry strategy. The `build.yaml` file describes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch deploy; ```. ## Release. Changes to the services are immediately deployed to the production cluster. Changes to the Hail; Python library are disseminated via PyPI. When a main commit changes the Hail ""Python version"", CI; will run the release steps. Changes to the Python version should be done by a PR whose title is; ""[release] 0.2.XXX"", which changes the Python version in both `hail/version.mk` and `hail/build.sc`,; and which updates the Batch and Query change logs: `hail/python/hail/docs/change_log.md`; `hail/python/hailtop/batch/docs/change_log.rst`. The change logs should comprise all the; `CHANGELOG:` lines from commits between the last release and the new release, for example:. ```; git log 0.2.127..HEAD; ```. CI will create a git tag, a GitHub release, a PyPI package version, update the public Hail docs. We; aim to release once per month. Note that our wheels are limited to 200 MiB and the sum total size of; all published PyPI wheels must be less than 20 GiB. Check [the current limits at; PyPI](https://pypi.org/manage/project/hail/settings",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:17241,Deployability,release,release,17241,"cribes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch deploy; ```. ## Release. Changes to the services are immediately deployed to the production cluster. Changes to the Hail; Python library are disseminated via PyPI. When a main commit changes the Hail ""Python version"", CI; will run the release steps. Changes to the Python version should be done by a PR whose title is; ""[release] 0.2.XXX"", which changes the Python version in both `hail/version.mk` and `hail/build.sc`,; and which updates the Batch and Query change logs: `hail/python/hail/docs/change_log.md`; `hail/python/hailtop/batch/docs/change_log.rst`. The change logs should comprise all the; `CHANGELOG:` lines from commits between the last release and the new release, for example:. ```; git log 0.2.127..HEAD; ```. CI will create a git tag, a GitHub release, a PyPI package version, update the public Hail docs. We; aim to release once per month. Note that our wheels are limited to 200 MiB and the sum total size of; all published PyPI wheels must be less than 20 GiB. Check [the current limits at; PyPI](https://pypi.org/manage/project/hail/settings/). We had to [request a size; increase](https://github.com/pypi/support/issues/2857) in 2023.; ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:17327,Deployability,release,release,17327,"cribes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch deploy; ```. ## Release. Changes to the services are immediately deployed to the production cluster. Changes to the Hail; Python library are disseminated via PyPI. When a main commit changes the Hail ""Python version"", CI; will run the release steps. Changes to the Python version should be done by a PR whose title is; ""[release] 0.2.XXX"", which changes the Python version in both `hail/version.mk` and `hail/build.sc`,; and which updates the Batch and Query change logs: `hail/python/hail/docs/change_log.md`; `hail/python/hailtop/batch/docs/change_log.rst`. The change logs should comprise all the; `CHANGELOG:` lines from commits between the last release and the new release, for example:. ```; git log 0.2.127..HEAD; ```. CI will create a git tag, a GitHub release, a PyPI package version, update the public Hail docs. We; aim to release once per month. Note that our wheels are limited to 200 MiB and the sum total size of; all published PyPI wheels must be less than 20 GiB. Check [the current limits at; PyPI](https://pypi.org/manage/project/hail/settings/). We had to [request a size; increase](https://github.com/pypi/support/issues/2857) in 2023.; ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:17437,Deployability,update,updates,17437,"cribes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch deploy; ```. ## Release. Changes to the services are immediately deployed to the production cluster. Changes to the Hail; Python library are disseminated via PyPI. When a main commit changes the Hail ""Python version"", CI; will run the release steps. Changes to the Python version should be done by a PR whose title is; ""[release] 0.2.XXX"", which changes the Python version in both `hail/version.mk` and `hail/build.sc`,; and which updates the Batch and Query change logs: `hail/python/hail/docs/change_log.md`; `hail/python/hailtop/batch/docs/change_log.rst`. The change logs should comprise all the; `CHANGELOG:` lines from commits between the last release and the new release, for example:. ```; git log 0.2.127..HEAD; ```. CI will create a git tag, a GitHub release, a PyPI package version, update the public Hail docs. We; aim to release once per month. Note that our wheels are limited to 200 MiB and the sum total size of; all published PyPI wheels must be less than 20 GiB. Check [the current limits at; PyPI](https://pypi.org/manage/project/hail/settings/). We had to [request a size; increase](https://github.com/pypi/support/issues/2857) in 2023.; ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:17656,Deployability,release,release,17656,"cribes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch deploy; ```. ## Release. Changes to the services are immediately deployed to the production cluster. Changes to the Hail; Python library are disseminated via PyPI. When a main commit changes the Hail ""Python version"", CI; will run the release steps. Changes to the Python version should be done by a PR whose title is; ""[release] 0.2.XXX"", which changes the Python version in both `hail/version.mk` and `hail/build.sc`,; and which updates the Batch and Query change logs: `hail/python/hail/docs/change_log.md`; `hail/python/hailtop/batch/docs/change_log.rst`. The change logs should comprise all the; `CHANGELOG:` lines from commits between the last release and the new release, for example:. ```; git log 0.2.127..HEAD; ```. CI will create a git tag, a GitHub release, a PyPI package version, update the public Hail docs. We; aim to release once per month. Note that our wheels are limited to 200 MiB and the sum total size of; all published PyPI wheels must be less than 20 GiB. Check [the current limits at; PyPI](https://pypi.org/manage/project/hail/settings/). We had to [request a size; increase](https://github.com/pypi/support/issues/2857) in 2023.; ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:17676,Deployability,release,release,17676,"cribes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch deploy; ```. ## Release. Changes to the services are immediately deployed to the production cluster. Changes to the Hail; Python library are disseminated via PyPI. When a main commit changes the Hail ""Python version"", CI; will run the release steps. Changes to the Python version should be done by a PR whose title is; ""[release] 0.2.XXX"", which changes the Python version in both `hail/version.mk` and `hail/build.sc`,; and which updates the Batch and Query change logs: `hail/python/hail/docs/change_log.md`; `hail/python/hailtop/batch/docs/change_log.rst`. The change logs should comprise all the; `CHANGELOG:` lines from commits between the last release and the new release, for example:. ```; git log 0.2.127..HEAD; ```. CI will create a git tag, a GitHub release, a PyPI package version, update the public Hail docs. We; aim to release once per month. Note that our wheels are limited to 200 MiB and the sum total size of; all published PyPI wheels must be less than 20 GiB. Check [the current limits at; PyPI](https://pypi.org/manage/project/hail/settings/). We had to [request a size; increase](https://github.com/pypi/support/issues/2857) in 2023.; ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:17767,Deployability,release,release,17767,"cribes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch deploy; ```. ## Release. Changes to the services are immediately deployed to the production cluster. Changes to the Hail; Python library are disseminated via PyPI. When a main commit changes the Hail ""Python version"", CI; will run the release steps. Changes to the Python version should be done by a PR whose title is; ""[release] 0.2.XXX"", which changes the Python version in both `hail/version.mk` and `hail/build.sc`,; and which updates the Batch and Query change logs: `hail/python/hail/docs/change_log.md`; `hail/python/hailtop/batch/docs/change_log.rst`. The change logs should comprise all the; `CHANGELOG:` lines from commits between the last release and the new release, for example:. ```; git log 0.2.127..HEAD; ```. CI will create a git tag, a GitHub release, a PyPI package version, update the public Hail docs. We; aim to release once per month. Note that our wheels are limited to 200 MiB and the sum total size of; all published PyPI wheels must be less than 20 GiB. Check [the current limits at; PyPI](https://pypi.org/manage/project/hail/settings/). We had to [request a size; increase](https://github.com/pypi/support/issues/2857) in 2023.; ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:17800,Deployability,update,update,17800,"cribes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch deploy; ```. ## Release. Changes to the services are immediately deployed to the production cluster. Changes to the Hail; Python library are disseminated via PyPI. When a main commit changes the Hail ""Python version"", CI; will run the release steps. Changes to the Python version should be done by a PR whose title is; ""[release] 0.2.XXX"", which changes the Python version in both `hail/version.mk` and `hail/build.sc`,; and which updates the Batch and Query change logs: `hail/python/hail/docs/change_log.md`; `hail/python/hailtop/batch/docs/change_log.rst`. The change logs should comprise all the; `CHANGELOG:` lines from commits between the last release and the new release, for example:. ```; git log 0.2.127..HEAD; ```. CI will create a git tag, a GitHub release, a PyPI package version, update the public Hail docs. We; aim to release once per month. Note that our wheels are limited to 200 MiB and the sum total size of; all published PyPI wheels must be less than 20 GiB. Check [the current limits at; PyPI](https://pypi.org/manage/project/hail/settings/). We had to [request a size; increase](https://github.com/pypi/support/issues/2857) in 2023.; ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:17840,Deployability,release,release,17840,"cribes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch deploy; ```. ## Release. Changes to the services are immediately deployed to the production cluster. Changes to the Hail; Python library are disseminated via PyPI. When a main commit changes the Hail ""Python version"", CI; will run the release steps. Changes to the Python version should be done by a PR whose title is; ""[release] 0.2.XXX"", which changes the Python version in both `hail/version.mk` and `hail/build.sc`,; and which updates the Batch and Query change logs: `hail/python/hail/docs/change_log.md`; `hail/python/hailtop/batch/docs/change_log.rst`. The change logs should comprise all the; `CHANGELOG:` lines from commits between the last release and the new release, for example:. ```; git log 0.2.127..HEAD; ```. CI will create a git tag, a GitHub release, a PyPI package version, update the public Hail docs. We; aim to release once per month. Note that our wheels are limited to 200 MiB and the sum total size of; all published PyPI wheels must be less than 20 GiB. Check [the current limits at; PyPI](https://pypi.org/manage/project/hail/settings/). We had to [request a size; increase](https://github.com/pypi/support/issues/2857) in 2023.; ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:6540,Energy Efficiency,monitor,monitor,6540,". Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, so the gsa-key secrets for these must be copied across from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ dow",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:1364,Integrability,depend,dependencies,1364,"ure improvements, or larger,; more complicated features. For larger projects, write an; [RFC](https://github.com/hail-is/hail-rfcs), get a review, and merge it into the hail-rfcs; repository before working on a PR to the main Hail repository. We use this process as a chance to; refine the design as well as educate the rest of the team on proposed changes. It helps to have; multiple eyes thinking about what the implications of the changes are to the rest of the system. In; addition, we use this time to think about how to break down the feature into smaller, more; manageable chunks. Ideally, branches should contain up to 200 lines of changes to make the process; easier on the reviewer. It may not always be possible to break up a feature into smaller components. ## Implementation. ### Environment / Tooling; Before you can write code, there are some setup steps that will allow you to; develop effectively. Hail currently supports Python version 3.9 or greater. Install the python dependencies of every Hail sub-project (e.g. ci, batch, hail/python/hailtop):. ```bash; make install-dev-requirements; ```. Install the pre-commit hooks:. ```bash; pre-commit install --install-hooks; ```. This creates git hooks that run certain linting checks, pyright on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.git-blame-ignore-revs`. To configure `git blame` to ignore these commits, run. ```bash; git config blame.ignoreRevsFile $HAIL/.git-blame-ignore-revs; ```. #### Services. Install and configure tools necessary for working on the Hail Services:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not al",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:3231,Integrability,depend,depending,3231,"vices:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`gcloud`](https://cloud.google.com/sdk/docs/install); 4. Configure gcloud and Docker for Hail:; ```bash; gcloud auth login; gcloud config set project hail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query t",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:3741,Integrability,depend,depend,3741,"es to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/up",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:4666,Integrability,depend,depend,4666,"ke -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/upload-remote-test-resources`. Run the Python query tests using the Batch backend and a production instance of Hail Batch; (https://batch.hail.is or https://batch.azure.hail.is depending on your dev config):. ```bash; make -C hail pytest-qob NAMESPACE=default; ```. Run the Python query documentation tests:. ```bash; make -C hail doctest-query; ```. #### Services. Production, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:4940,Integrability,depend,depending,4940,"Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/upload-remote-test-resources`. Run the Python query tests using the Batch backend and a production instance of Hail Batch; (https://batch.hail.is or https://batch.azure.hail.is depending on your dev config):. ```bash; make -C hail pytest-qob NAMESPACE=default; ```. Run the Python query documentation tests:. ```bash; make -C hail doctest-query; ```. #### Services. Production, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bas",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:6314,Integrability,depend,dependent,6314,"ces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:11703,Integrability,depend,dependent,11703,"und git and pull requests,; see [this doc](git-practices.md). Set a reviewer in the ""Assignees"" box. Do *not* use the ""Reviewers"" box. Our CI system specifically; uses the ""Assignees"" box to list a developer's pending reviews at https://ci.hail.is/me. If you are; an outside contributor and cannot request reviews, you can have CI automatically assign a; reviewer. By writing `#assign services` or `#assign compiler` in the PR body, CI will randomly; select a collaborator on the relevant team and assign them for you. You can also give the PR a set of labels. The important ones are “WIP” to make; sure the pull request doesn’t get merged accidentally until you are ready,; “migration” to warn everyone that the changes will shut down the Batch; deployment if it requires a database migration, “bug” for bug fixes, “breaking; change” for any user breaking changes for Hail Query, and “prio:high” to make; this PR the first one in line to merge. There’s also “stacked PR” to indicate; that the changes in the PR are dependent on the changes in another PR. You; should reference that PR in your commit message with “Stacked on #9883”. Most; PRs will not have any labels. For the PR title, start the title with the name of the service(s) the changes; impact. For example, if it’s a Benchmark change, then you’d write; `[benchmark]`. If it’s a Hail Query change, then it would be `[query]`. We also want; the title to be descriptive enough to know what the change is without being too; verbose. An example is “`[batch]` Added read_only option for gcsfuse”. For the PR commit message, we want the message to be descriptive of the complete; set of changes that occurred, especially if it’s a complicated set of; changes. If it’s a smaller, obvious change like a one-liner, then it’s okay to; omit the commit message. For UI changes, it’s helpful to paste a screenshot of; the changes. It’s also a good idea to comment on how you tested the changes and; whether there are any implications of your changes. ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:11788,Integrability,message,message,11788,""" box. Do *not* use the ""Reviewers"" box. Our CI system specifically; uses the ""Assignees"" box to list a developer's pending reviews at https://ci.hail.is/me. If you are; an outside contributor and cannot request reviews, you can have CI automatically assign a; reviewer. By writing `#assign services` or `#assign compiler` in the PR body, CI will randomly; select a collaborator on the relevant team and assign them for you. You can also give the PR a set of labels. The important ones are “WIP” to make; sure the pull request doesn’t get merged accidentally until you are ready,; “migration” to warn everyone that the changes will shut down the Batch; deployment if it requires a database migration, “bug” for bug fixes, “breaking; change” for any user breaking changes for Hail Query, and “prio:high” to make; this PR the first one in line to merge. There’s also “stacked PR” to indicate; that the changes in the PR are dependent on the changes in another PR. You; should reference that PR in your commit message with “Stacked on #9883”. Most; PRs will not have any labels. For the PR title, start the title with the name of the service(s) the changes; impact. For example, if it’s a Benchmark change, then you’d write; `[benchmark]`. If it’s a Hail Query change, then it would be `[query]`. We also want; the title to be descriptive enough to know what the change is without being too; verbose. An example is “`[batch]` Added read_only option for gcsfuse”. For the PR commit message, we want the message to be descriptive of the complete; set of changes that occurred, especially if it’s a complicated set of; changes. If it’s a smaller, obvious change like a one-liner, then it’s okay to; omit the commit message. For UI changes, it’s helpful to paste a screenshot of; the changes. It’s also a good idea to comment on how you tested the changes and; whether there are any implications of your changes. If the PR fixes a bug that; is a GitHub issue, then you can say “Fixes #8900” to make sure the",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:12259,Integrability,message,message,12259,"erged accidentally until you are ready,; “migration” to warn everyone that the changes will shut down the Batch; deployment if it requires a database migration, “bug” for bug fixes, “breaking; change” for any user breaking changes for Hail Query, and “prio:high” to make; this PR the first one in line to merge. There’s also “stacked PR” to indicate; that the changes in the PR are dependent on the changes in another PR. You; should reference that PR in your commit message with “Stacked on #9883”. Most; PRs will not have any labels. For the PR title, start the title with the name of the service(s) the changes; impact. For example, if it’s a Benchmark change, then you’d write; `[benchmark]`. If it’s a Hail Query change, then it would be `[query]`. We also want; the title to be descriptive enough to know what the change is without being too; verbose. An example is “`[batch]` Added read_only option for gcsfuse”. For the PR commit message, we want the message to be descriptive of the complete; set of changes that occurred, especially if it’s a complicated set of; changes. If it’s a smaller, obvious change like a one-liner, then it’s okay to; omit the commit message. For UI changes, it’s helpful to paste a screenshot of; the changes. It’s also a good idea to comment on how you tested the changes and; whether there are any implications of your changes. If the PR fixes a bug that; is a GitHub issue, then you can say “Fixes #8900” to make sure the issue gets; automatically closed by GitHub when your PR is merged. You can also tag a; specific member of the team in the message to get their attention with “@user”. If the changes are user-facing, then add a line in your commit message that; starts with “CHANGELOG: description.”. This should be one line with one sentence; that ends in a period. Once you are done with writing up all the details of the Pull Request, you can; then submit it. Our continuous integration (CI) system watches for new PRs. When; it sees a new PR, it creates",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:12280,Integrability,message,message,12280,"erged accidentally until you are ready,; “migration” to warn everyone that the changes will shut down the Batch; deployment if it requires a database migration, “bug” for bug fixes, “breaking; change” for any user breaking changes for Hail Query, and “prio:high” to make; this PR the first one in line to merge. There’s also “stacked PR” to indicate; that the changes in the PR are dependent on the changes in another PR. You; should reference that PR in your commit message with “Stacked on #9883”. Most; PRs will not have any labels. For the PR title, start the title with the name of the service(s) the changes; impact. For example, if it’s a Benchmark change, then you’d write; `[benchmark]`. If it’s a Hail Query change, then it would be `[query]`. We also want; the title to be descriptive enough to know what the change is without being too; verbose. An example is “`[batch]` Added read_only option for gcsfuse”. For the PR commit message, we want the message to be descriptive of the complete; set of changes that occurred, especially if it’s a complicated set of; changes. If it’s a smaller, obvious change like a one-liner, then it’s okay to; omit the commit message. For UI changes, it’s helpful to paste a screenshot of; the changes. It’s also a good idea to comment on how you tested the changes and; whether there are any implications of your changes. If the PR fixes a bug that; is a GitHub issue, then you can say “Fixes #8900” to make sure the issue gets; automatically closed by GitHub when your PR is merged. You can also tag a; specific member of the team in the message to get their attention with “@user”. If the changes are user-facing, then add a line in your commit message that; starts with “CHANGELOG: description.”. This should be one line with one sentence; that ends in a period. Once you are done with writing up all the details of the Pull Request, you can; then submit it. Our continuous integration (CI) system watches for new PRs. When; it sees a new PR, it creates",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:12490,Integrability,message,message,12490," requires a database migration, “bug” for bug fixes, “breaking; change” for any user breaking changes for Hail Query, and “prio:high” to make; this PR the first one in line to merge. There’s also “stacked PR” to indicate; that the changes in the PR are dependent on the changes in another PR. You; should reference that PR in your commit message with “Stacked on #9883”. Most; PRs will not have any labels. For the PR title, start the title with the name of the service(s) the changes; impact. For example, if it’s a Benchmark change, then you’d write; `[benchmark]`. If it’s a Hail Query change, then it would be `[query]`. We also want; the title to be descriptive enough to know what the change is without being too; verbose. An example is “`[batch]` Added read_only option for gcsfuse”. For the PR commit message, we want the message to be descriptive of the complete; set of changes that occurred, especially if it’s a complicated set of; changes. If it’s a smaller, obvious change like a one-liner, then it’s okay to; omit the commit message. For UI changes, it’s helpful to paste a screenshot of; the changes. It’s also a good idea to comment on how you tested the changes and; whether there are any implications of your changes. If the PR fixes a bug that; is a GitHub issue, then you can say “Fixes #8900” to make sure the issue gets; automatically closed by GitHub when your PR is merged. You can also tag a; specific member of the team in the message to get their attention with “@user”. If the changes are user-facing, then add a line in your commit message that; starts with “CHANGELOG: description.”. This should be one line with one sentence; that ends in a period. Once you are done with writing up all the details of the Pull Request, you can; then submit it. Our continuous integration (CI) system watches for new PRs. When; it sees a new PR, it creates a new batch that will test everything defined in; the build.yaml file in the root of the repository. This will create a temporar",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:12904,Integrability,message,message,12904,"e name of the service(s) the changes; impact. For example, if it’s a Benchmark change, then you’d write; `[benchmark]`. If it’s a Hail Query change, then it would be `[query]`. We also want; the title to be descriptive enough to know what the change is without being too; verbose. An example is “`[batch]` Added read_only option for gcsfuse”. For the PR commit message, we want the message to be descriptive of the complete; set of changes that occurred, especially if it’s a complicated set of; changes. If it’s a smaller, obvious change like a one-liner, then it’s okay to; omit the commit message. For UI changes, it’s helpful to paste a screenshot of; the changes. It’s also a good idea to comment on how you tested the changes and; whether there are any implications of your changes. If the PR fixes a bug that; is a GitHub issue, then you can say “Fixes #8900” to make sure the issue gets; automatically closed by GitHub when your PR is merged. You can also tag a; specific member of the team in the message to get their attention with “@user”. If the changes are user-facing, then add a line in your commit message that; starts with “CHANGELOG: description.”. This should be one line with one sentence; that ends in a period. Once you are done with writing up all the details of the Pull Request, you can; then submit it. Our continuous integration (CI) system watches for new PRs. When; it sees a new PR, it creates a new batch that will test everything defined in; the build.yaml file in the root of the repository. This will create a temporary; namespace in kubernetes for your PR and deploy all the services into it. It will; also run all the tests such as for query, batch, and ci after merging it with; the latest version of main. You can view the progress of the build and the; logs for your PR at [ci.hail.is](https://ci.hail.is). ## Review. Once the PR has been created, it is the responsibility of the reviewer(s) to; review the PR. Our goal as a team is to give comments within 24 h",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:13012,Integrability,message,message,13012,"benchmark]`. If it’s a Hail Query change, then it would be `[query]`. We also want; the title to be descriptive enough to know what the change is without being too; verbose. An example is “`[batch]` Added read_only option for gcsfuse”. For the PR commit message, we want the message to be descriptive of the complete; set of changes that occurred, especially if it’s a complicated set of; changes. If it’s a smaller, obvious change like a one-liner, then it’s okay to; omit the commit message. For UI changes, it’s helpful to paste a screenshot of; the changes. It’s also a good idea to comment on how you tested the changes and; whether there are any implications of your changes. If the PR fixes a bug that; is a GitHub issue, then you can say “Fixes #8900” to make sure the issue gets; automatically closed by GitHub when your PR is merged. You can also tag a; specific member of the team in the message to get their attention with “@user”. If the changes are user-facing, then add a line in your commit message that; starts with “CHANGELOG: description.”. This should be one line with one sentence; that ends in a period. Once you are done with writing up all the details of the Pull Request, you can; then submit it. Our continuous integration (CI) system watches for new PRs. When; it sees a new PR, it creates a new batch that will test everything defined in; the build.yaml file in the root of the repository. This will create a temporary; namespace in kubernetes for your PR and deploy all the services into it. It will; also run all the tests such as for query, batch, and ci after merging it with; the latest version of main. You can view the progress of the build and the; logs for your PR at [ci.hail.is](https://ci.hail.is). ## Review. Once the PR has been created, it is the responsibility of the reviewer(s) to; review the PR. Our goal as a team is to give comments within 24 hours. To review; someone else’s changes, click on “Files changed”. This will show the diff; between the old",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:13242,Integrability,integrat,integration,13242,"it message, we want the message to be descriptive of the complete; set of changes that occurred, especially if it’s a complicated set of; changes. If it’s a smaller, obvious change like a one-liner, then it’s okay to; omit the commit message. For UI changes, it’s helpful to paste a screenshot of; the changes. It’s also a good idea to comment on how you tested the changes and; whether there are any implications of your changes. If the PR fixes a bug that; is a GitHub issue, then you can say “Fixes #8900” to make sure the issue gets; automatically closed by GitHub when your PR is merged. You can also tag a; specific member of the team in the message to get their attention with “@user”. If the changes are user-facing, then add a line in your commit message that; starts with “CHANGELOG: description.”. This should be one line with one sentence; that ends in a period. Once you are done with writing up all the details of the Pull Request, you can; then submit it. Our continuous integration (CI) system watches for new PRs. When; it sees a new PR, it creates a new batch that will test everything defined in; the build.yaml file in the root of the repository. This will create a temporary; namespace in kubernetes for your PR and deploy all the services into it. It will; also run all the tests such as for query, batch, and ci after merging it with; the latest version of main. You can view the progress of the build and the; logs for your PR at [ci.hail.is](https://ci.hail.is). ## Review. Once the PR has been created, it is the responsibility of the reviewer(s) to; review the PR. Our goal as a team is to give comments within 24 hours. To review; someone else’s changes, click on “Files changed”. This will show the diff; between the old code and the new proposed changes. You can make comments on; specific lines of the code. Feel free to ask questions here, especially if you; don’t understand something! It’s a good idea to think critically about the; changes. There should also be tes",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:15499,Integrability,integrat,integration,15499,"kay to spend a; lot of time reviewing PRs! This is a critical part of our development process to; avoid bugs and unintentional breaking changes. If there are items for the; developer to address, then submit your review with “Request Changes”. Otherwise,; once you are happy with the changes and all comments have been addressed, you; can “Approve” the PR. If you are the person whose code is being reviewed and your PR is in the Request; Changes state, then you’ll need to address their comments by pushing new commit; changes or answering questions. Once you are done, then you can re-request a review; in the ""Reviewers"" box. If your review is requested on a PR submitted by an outside contributor, you should; ""assign"" yourself or the appropriate team member to the PR. The assignee is; responsible for ensuring that the PR does not go stale and is eventually; merged or closed. ![](dismiss-review.png). ## Merge / Deploy. Once a PR has been approved, our continuous integration service (CI) will squash; merge the commits in the PR into the main branch of the repository. This will; then trigger a deploy batch. The deploy batch will first deploy all of the new; Docker images, redeploy the running services in the default namespace with the; latest changes, and rerun all of the tests with the new version of main; incorporating your changes. If this batch fails, a Zulip message will be sent to the entire team linking to the UI for the; failing batch. We should never ignore this message. We should figure out what component broke. If; it’s a transient error, then we need to harden our retry strategy. The `build.yaml` file describes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:15906,Integrability,message,message,15906,"ewed and your PR is in the Request; Changes state, then you’ll need to address their comments by pushing new commit; changes or answering questions. Once you are done, then you can re-request a review; in the ""Reviewers"" box. If your review is requested on a PR submitted by an outside contributor, you should; ""assign"" yourself or the appropriate team member to the PR. The assignee is; responsible for ensuring that the PR does not go stale and is eventually; merged or closed. ![](dismiss-review.png). ## Merge / Deploy. Once a PR has been approved, our continuous integration service (CI) will squash; merge the commits in the PR into the main branch of the repository. This will; then trigger a deploy batch. The deploy batch will first deploy all of the new; Docker images, redeploy the running services in the default namespace with the; latest changes, and rerun all of the tests with the new version of main; incorporating your changes. If this batch fails, a Zulip message will be sent to the entire team linking to the UI for the; failing batch. We should never ignore this message. We should figure out what component broke. If; it’s a transient error, then we need to harden our retry strategy. The `build.yaml` file describes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is c",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:16016,Integrability,message,message,16016,"ress their comments by pushing new commit; changes or answering questions. Once you are done, then you can re-request a review; in the ""Reviewers"" box. If your review is requested on a PR submitted by an outside contributor, you should; ""assign"" yourself or the appropriate team member to the PR. The assignee is; responsible for ensuring that the PR does not go stale and is eventually; merged or closed. ![](dismiss-review.png). ## Merge / Deploy. Once a PR has been approved, our continuous integration service (CI) will squash; merge the commits in the PR into the main branch of the repository. This will; then trigger a deploy batch. The deploy batch will first deploy all of the new; Docker images, redeploy the running services in the default namespace with the; latest changes, and rerun all of the tests with the new version of main; incorporating your changes. If this batch fails, a Zulip message will be sent to the entire team linking to the UI for the; failing batch. We should never ignore this message. We should figure out what component broke. If; it’s a transient error, then we need to harden our retry strategy. The `build.yaml` file describes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:1868,Modifiability,refactor,refactoring,1868,"think about how to break down the feature into smaller, more; manageable chunks. Ideally, branches should contain up to 200 lines of changes to make the process; easier on the reviewer. It may not always be possible to break up a feature into smaller components. ## Implementation. ### Environment / Tooling; Before you can write code, there are some setup steps that will allow you to; develop effectively. Hail currently supports Python version 3.9 or greater. Install the python dependencies of every Hail sub-project (e.g. ci, batch, hail/python/hailtop):. ```bash; make install-dev-requirements; ```. Install the pre-commit hooks:. ```bash; pre-commit install --install-hooks; ```. This creates git hooks that run certain linting checks, pyright on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.git-blame-ignore-revs`. To configure `git blame` to ignore these commits, run. ```bash; git config blame.ignoreRevsFile $HAIL/.git-blame-ignore-revs; ```. #### Services. Install and configure tools necessary for working on the Hail Services:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`gcloud`](https://cloud.google.com/sdk/docs/install); 4. Configure gcloud and Docker for Hail:; ```bash; gcloud auth login; gcloud config set project hail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell comma",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:2028,Modifiability,config,configure,2028,"on the reviewer. It may not always be possible to break up a feature into smaller components. ## Implementation. ### Environment / Tooling; Before you can write code, there are some setup steps that will allow you to; develop effectively. Hail currently supports Python version 3.9 or greater. Install the python dependencies of every Hail sub-project (e.g. ci, batch, hail/python/hailtop):. ```bash; make install-dev-requirements; ```. Install the pre-commit hooks:. ```bash; pre-commit install --install-hooks; ```. This creates git hooks that run certain linting checks, pyright on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.git-blame-ignore-revs`. To configure `git blame` to ignore these commits, run. ```bash; git config blame.ignoreRevsFile $HAIL/.git-blame-ignore-revs; ```. #### Services. Install and configure tools necessary for working on the Hail Services:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`gcloud`](https://cloud.google.com/sdk/docs/install); 4. Configure gcloud and Docker for Hail:; ```bash; gcloud auth login; gcloud config set project hail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:2093,Modifiability,config,config,2093,"sible to break up a feature into smaller components. ## Implementation. ### Environment / Tooling; Before you can write code, there are some setup steps that will allow you to; develop effectively. Hail currently supports Python version 3.9 or greater. Install the python dependencies of every Hail sub-project (e.g. ci, batch, hail/python/hailtop):. ```bash; make install-dev-requirements; ```. Install the pre-commit hooks:. ```bash; pre-commit install --install-hooks; ```. This creates git hooks that run certain linting checks, pyright on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.git-blame-ignore-revs`. To configure `git blame` to ignore these commits, run. ```bash; git config blame.ignoreRevsFile $HAIL/.git-blame-ignore-revs; ```. #### Services. Install and configure tools necessary for working on the Hail Services:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`gcloud`](https://cloud.google.com/sdk/docs/install); 4. Configure gcloud and Docker for Hail:; ```bash; gcloud auth login; gcloud config set project hail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; so",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:2183,Modifiability,config,configure,2183,"write code, there are some setup steps that will allow you to; develop effectively. Hail currently supports Python version 3.9 or greater. Install the python dependencies of every Hail sub-project (e.g. ci, batch, hail/python/hailtop):. ```bash; make install-dev-requirements; ```. Install the pre-commit hooks:. ```bash; pre-commit install --install-hooks; ```. This creates git hooks that run certain linting checks, pyright on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.git-blame-ignore-revs`. To configure `git blame` to ignore these commits, run. ```bash; git config blame.ignoreRevsFile $HAIL/.git-blame-ignore-revs; ```. #### Services. Install and configure tools necessary for working on the Hail Services:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`gcloud`](https://cloud.google.com/sdk/docs/install); 4. Configure gcloud and Docker for Hail:; ```bash; gcloud auth login; gcloud config set project hail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different s",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:2595,Modifiability,config,config,2595,"right on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.git-blame-ignore-revs`. To configure `git blame` to ignore these commits, run. ```bash; git config blame.ignoreRevsFile $HAIL/.git-blame-ignore-revs; ```. #### Services. Install and configure tools necessary for working on the Hail Services:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`gcloud`](https://cloud.google.com/sdk/docs/install); 4. Configure gcloud and Docker for Hail:; ```bash; gcloud auth login; gcloud config set project hail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-tes",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:2707,Modifiability,config,configure-docker,2707,"right on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.git-blame-ignore-revs`. To configure `git blame` to ignore these commits, run. ```bash; git config blame.ignoreRevsFile $HAIL/.git-blame-ignore-revs; ```. #### Services. Install and configure tools necessary for working on the Hail Services:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`gcloud`](https://cloud.google.com/sdk/docs/install); 4. Configure gcloud and Docker for Hail:; ```bash; gcloud auth login; gcloud config set project hail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-tes",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:2799,Modifiability,config,configure,2799,"rce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.git-blame-ignore-revs`. To configure `git blame` to ignore these commits, run. ```bash; git config blame.ignoreRevsFile $HAIL/.git-blame-ignore-revs; ```. #### Services. Install and configure tools necessary for working on the Hail Services:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`gcloud`](https://cloud.google.com/sdk/docs/install); 4. Configure gcloud and Docker for Hail:; ```bash; gcloud auth login; gcloud config set project hail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:4962,Modifiability,config,config,4962,"Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/upload-remote-test-resources`. Run the Python query tests using the Batch backend and a production instance of Hail Batch; (https://batch.hail.is or https://batch.azure.hail.is depending on your dev config):. ```bash; make -C hail pytest-qob NAMESPACE=default; ```. Run the Python query documentation tests:. ```bash; make -C hail doctest-query; ```. #### Services. Production, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bas",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:7753,Modifiability,config,configure,7753,"the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, so the gsa-key secrets for these must be copied across from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ download-secret <username>-gsa-key; $ mv secret.json key.json; $ kubectl create secret generic <username>-gsa-key --namespace=<username> --from-file=key.json; ```. To submit jobs to your dev namespace, you need to configure your local hail; installation to point to the dev namespace. You can do this by running. ```bash; hailctl dev config set default_namespace <my_namespace>; hailctl auth login; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands. NOTE: The dev deploy command above sets `HAIL_DEFAULT_NAMESPACE=default`. That's; because `hailctl dev deploy` submits a request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ``",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:7873,Modifiability,config,config,7873,"dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, so the gsa-key secrets for these must be copied across from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ download-secret <username>-gsa-key; $ mv secret.json key.json; $ kubectl create secret generic <username>-gsa-key --namespace=<username> --from-file=key.json; ```. To submit jobs to your dev namespace, you need to configure your local hail; installation to point to the dev namespace. You can do this by running. ```bash; hailctl dev config set default_namespace <my_namespace>; hailctl auth login; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands. NOTE: The dev deploy command above sets `HAIL_DEFAULT_NAMESPACE=default`. That's; because `hailctl dev deploy` submits a request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:8082,Modifiability,variab,variable,8082,"opers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, so the gsa-key secrets for these must be copied across from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ download-secret <username>-gsa-key; $ mv secret.json key.json; $ kubectl create secret generic <username>-gsa-key --namespace=<username> --from-file=key.json; ```. To submit jobs to your dev namespace, you need to configure your local hail; installation to point to the dev namespace. You can do this by running. ```bash; hailctl dev config set default_namespace <my_namespace>; hailctl auth login; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands. NOTE: The dev deploy command above sets `HAIL_DEFAULT_NAMESPACE=default`. That's; because `hailctl dev deploy` submits a request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy; 2. make deploy; 3. sync.py. These are listed in order of broadest scope to narrowest scope, so it is never; wrong to do something higher on the list, but",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:14627,Safety,avoid,avoid,14627,"merging it with; the latest version of main. You can view the progress of the build and the; logs for your PR at [ci.hail.is](https://ci.hail.is). ## Review. Once the PR has been created, it is the responsibility of the reviewer(s) to; review the PR. Our goal as a team is to give comments within 24 hours. To review; someone else’s changes, click on “Files changed”. This will show the diff; between the old code and the new proposed changes. You can make comments on; specific lines of the code. Feel free to ask questions here, especially if you; don’t understand something! It’s a good idea to think critically about the; changes. There should also be tests either added or existing to make sure the; code changes do not break any existing functionality and actually implement what; was intended. For example, a change to test whether Batch doesn’t crash when a; user gives a bad input should have a test with bad inputs. It’s okay to spend a; lot of time reviewing PRs! This is a critical part of our development process to; avoid bugs and unintentional breaking changes. If there are items for the; developer to address, then submit your review with “Request Changes”. Otherwise,; once you are happy with the changes and all comments have been addressed, you; can “Approve” the PR. If you are the person whose code is being reviewed and your PR is in the Request; Changes state, then you’ll need to address their comments by pushing new commit; changes or answering questions. Once you are done, then you can re-request a review; in the ""Reviewers"" box. If your review is requested on a PR submitted by an outside contributor, you should; ""assign"" yourself or the appropriate team member to the PR. The assignee is; responsible for ensuring that the PR does not go stale and is eventually; merged or closed. ![](dismiss-review.png). ## Merge / Deploy. Once a PR has been approved, our continuous integration service (CI) will squash; merge the commits in the PR into the main branch of the repos",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:7158,Security,access,access,7158," your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, so the gsa-key secrets for these must be copied across from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ download-secret <username>-gsa-key; $ mv secret.json key.json; $ kubectl create secret generic <username>-gsa-key --namespace=<username> --from-file=key.json; ```. To submit jobs to your dev namespace, you need to configure your local hail; installation to point to the dev namespace. You can do this by running. ```bash; hailctl dev config set default_namespace <my_namespace>; hailctl auth login; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:2396,Testability,test,test,2396,"il/python/hailtop):. ```bash; make install-dev-requirements; ```. Install the pre-commit hooks:. ```bash; pre-commit install --install-hooks; ```. This creates git hooks that run certain linting checks, pyright on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.git-blame-ignore-revs`. To configure `git blame` to ignore these commits, run. ```bash; git config blame.ignoreRevsFile $HAIL/.git-blame-ignore-revs; ```. #### Services. Install and configure tools necessary for working on the Hail Services:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`gcloud`](https://cloud.google.com/sdk/docs/install); 4. Configure gcloud and Docker for Hail:; ```bash; gcloud auth login; gcloud config set project hail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build ha",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:2581,Testability,log,login,2581,"right on some sub-projects, and; auto-formatting on changed files every commit. For example, services code uses the [Black python; formatter](https://black.readthedocs.io/en/stable/) to enforce PEP8 compliance. Sometimes large formatting or refactoring commits can muddle the git history; for a file. If your change is one of these, follow up by adding the commit SHA to; `.git-blame-ignore-revs`. To configure `git blame` to ignore these commits, run. ```bash; git config blame.ignoreRevsFile $HAIL/.git-blame-ignore-revs; ```. #### Services. Install and configure tools necessary for working on the Hail Services:. 1. Install [Docker](https://docker.com); 2. Install [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/), if not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`gcloud`](https://cloud.google.com/sdk/docs/install); 4. Configure gcloud and Docker for Hail:; ```bash; gcloud auth login; gcloud config set project hail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-tes",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:3379,Testability,test,tests,3379,"not already installed. (To test installation, run `kubectl` in a terminal window); 3. Install [`gcloud`](https://cloud.google.com/sdk/docs/install); 4. Configure gcloud and Docker for Hail:; ```bash; gcloud auth login; gcloud config set project hail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/py",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:3590,Testability,test,tests,3590,"th login; gcloud config set project hail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your lapto",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:3624,Testability,test,test,3624,"ail-vdc; gcloud container clusters get-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hai",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:3661,Testability,test,tests,3661,"et-credentials vdc --zone=us-central1-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, n",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:3698,Testability,test,test,3698,"-a; gcloud auth -q configure-docker us-docker.pkg.dev; ```. 5. Add these lines to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resou",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:3735,Testability,test,tests,3735,"es to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/up",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:3758,Testability,test,test,3758,"es to `~/.zshrc` or `~/.bashrc` to configure your shell and environment for; Hail. `functions.sh` contains shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/up",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:3857,Testability,test,test,3857,"ns shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/upload-remote-test-resources`. Run the Python query tests using the Batch backend and a production instanc",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:3900,Testability,test,test-resources,3900,"ns shell commands for working with the Kubernetes cluster. ```; export HAIL=/path/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/upload-remote-test-resources`. Run the Python query tests using the Batch backend and a production instanc",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:3938,Testability,test,tests,3938,"/to/hail-repository; # BuildKit, a fast docker backend; export DOCKER_BUILDKIT=1; # Shell utilities for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/upload-remote-test-resources`. Run the Python query tests using the Batch backend and a production instance of Hail Batch; (https://batch.hail.is or https://batch.azure.hail.is depending ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:4040,Testability,test,tests,4040,"for managing the Hail kubernetes cluster; source $HAIL/devbin/functions.sh; ```. 6. Run `brew install fswatch`. ### Testing / Debugging; There are different strategies for debugging depending on whether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/upload-remote-test-resources`. Run the Python query tests using the Batch backend and a production instance of Hail Batch; (https://batch.hail.is or https://batch.azure.hail.is depending on your dev config):. ```bash; make -C hail pytest-qob NAMESPACE=default; ```. Run the Python query",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:4236,Testability,test,tests,4236,"ether you are; working on a compiler project or a services project. #### Compiler. For a compiler project, you can build and run the tests locally on your; computer. To build hail for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/upload-remote-test-resources`. Run the Python query tests using the Batch backend and a production instance of Hail Batch; (https://batch.hail.is or https://batch.azure.hail.is depending on your dev config):. ```bash; make -C hail pytest-qob NAMESPACE=default; ```. Run the Python query documentation tests:. ```bash; make -C hail doctest-query; ```. #### Services. Production, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:4432,Testability,test,test,4432,"il for development purposes, you should run the following; command from the repository root. ```bash; make -C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/upload-remote-test-resources`. Run the Python query tests using the Batch backend and a production instance of Hail Batch; (https://batch.hail.is or https://batch.azure.hail.is depending on your dev config):. ```bash; make -C hail pytest-qob NAMESPACE=default; ```. Run the Python query documentation tests:. ```bash; make -C hail doctest-query; ```. #### Services. Production, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespac",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:4507,Testability,test,tests,4507,"C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/upload-remote-test-resources`. Run the Python query tests using the Batch backend and a production instance of Hail Batch; (https://batch.hail.is or https://batch.azure.hail.is depending on your dev config):. ```bash; make -C hail pytest-qob NAMESPACE=default; ```. Run the Python query documentation tests:. ```bash; make -C hail doctest-query; ```. #### Services. Production, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used t",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:4519,Testability,test,test,4519,"C hail install-editable; ```. Run the Scala query compiler tests:. ```bash; make -C hail jvm-test; ```. Run the Scala file system tests:. ```bash; make -C hail fs-jvm-test; ```. Note that the file system tests depend on remote test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/upload-remote-test-resources`. Run the Python query tests using the Batch backend and a production instance of Hail Batch; (https://batch.hail.is or https://batch.azure.hail.is depending on your dev config):. ```bash; make -C hail pytest-qob NAMESPACE=default; ```. Run the Python query documentation tests:. ```bash; make -C hail doctest-query; ```. #### Services. Production, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used t",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:4777,Testability,test,test-resources,4777,"te test resources which are automatically cleaned up; after 1 day. To force re-upload of these remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/upload-remote-test-resources`. Run the Python query tests using the Batch backend and a production instance of Hail Batch; (https://batch.hail.is or https://batch.azure.hail.is depending on your dev config):. ```bash; make -C hail pytest-qob NAMESPACE=default; ```. Run the Python query documentation tests:. ```bash; make -C hail doctest-query; ```. #### Services. Production, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's n",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:4815,Testability,test,tests,4815,"se remote test resources delete; `hail/upload-remote-test-resources`. Run the Python query tests using the Spark backend in local mode:. ```bash; make -C hail pytest; ```. Run the Python query tests [matching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/upload-remote-test-resources`. Run the Python query tests using the Batch backend and a production instance of Hail Batch; (https://batch.hail.is or https://batch.azure.hail.is depending on your dev config):. ```bash; make -C hail pytest-qob NAMESPACE=default; ```. Run the Python query documentation tests:. ```bash; make -C hail doctest-query; ```. #### Services. Production, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push you",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:5064,Testability,test,tests,5064,"tching a pattern](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags):. ```bash; make -C hail pytest PYTEST_ARGS='-k service_backend'; ```. Run the Python query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/upload-remote-test-resources`. Run the Python query tests using the Batch backend and a production instance of Hail Batch; (https://batch.hail.is or https://batch.azure.hail.is depending on your dev config):. ```bash; make -C hail pytest-qob NAMESPACE=default; ```. Run the Python query documentation tests:. ```bash; make -C hail doctest-query; ```. #### Services. Production, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:5141,Testability,test,test,5141,"query tests [in a particular; file](https://docs.pytest.org/en/7.1.x/reference/reference.html#command-line-flags) (relative to the; `hail/python` directory):. ```bash; make -C hail pytest PYTEST_TARGET=test/hail/table/test_table.py; ```. Run the Python copy tool and sync tool tests which test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/upload-remote-test-resources`. Run the Python query tests using the Batch backend and a production instance of Hail Batch; (https://batch.hail.is or https://batch.azure.hail.is depending on your dev config):. ```bash; make -C hail pytest-qob NAMESPACE=default; ```. Run the Python query documentation tests:. ```bash; make -C hail doctest-query; ```. #### Services. Production, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:5533,Testability,test,test,5533,"h test transfers of data between GCS, S3, ABS, and; your laptop's filesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/upload-remote-test-resources`. Run the Python query tests using the Batch backend and a production instance of Hail Batch; (https://batch.hail.is or https://batch.azure.hail.is depending on your dev config):. ```bash; make -C hail pytest-qob NAMESPACE=default; ```. Run the Python query documentation tests:. ```bash; make -C hail doctest-query; ```. #### Services. Production, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link t",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:5615,Testability,test,tests,5615,"lesystem:. ```bash; make -C hail pytest-inter-cloud; ```. Again, note that these depend on remote resources that are auto-cleaned up. See the above comment; about deleting `hail/upload-remote-test-resources`. Run the Python query tests using the Batch backend and a production instance of Hail Batch; (https://batch.hail.is or https://batch.azure.hail.is depending on your dev config):. ```bash; make -C hail pytest-qob NAMESPACE=default; ```. Run the Python query documentation tests:. ```bash; make -C hail doctest-query; ```. #### Services. Production, test and development versions of Hail Batch share one Kubernetes; cluster, but are able to coexist without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:6217,Testability,test,test,6217," without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch.",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:6241,Testability,test,tests,6241," without interference because they are isolated; in different [namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch.",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:6334,Testability,test,testing,6334,"ces-walkthrough/).; The different kinds of namespaces are:. - The `default` or production namespace.; - Test namespaces. These are ephemeral namespaces used to test PRs. Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:6597,Testability,log,logs,6597,". Every CI; pipeline for a PR creates a namespace and deletes it when the tests finish.; - Dev namespaces. These are long-lived namespaces for developing new features.; Each developer has their own namespace. It's name is the same as the developer's; hail username. For a services project, you can push your branch to GitHub and then run what we; call a “dev deploy”. The command to invoke a dev deploy is. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s <step1>,<step2>,...; ```. Dev deploy creates a batch that deploys the build steps specified by the `-s` in; your Kubernetes dev namespace. For example, if we want to test; whether the Batch tests still pass, we would specify -s test_batch. This will; run all the dependent steps for testing Batch such as creating credentials,; a live Auth service, a MySQL database for Batch, and a live Batch deployment. Submitting a dev deploy with hailctl will give you the link to a UI; where you can monitor the progress of everything deploying and get the logs for; any steps that fail. You can also see a recent history of your dev deploys at; [ci.hail.is/me](https://ci.hail.is/me). The first time a namespace is used, the Auth service in that namespace won't; have any users. In order submit jobs to the namespace, make sure to include; the `add_developers` step to the dev deploy. So a first dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, so the gsa-key secrets for these must be copied across from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ dow",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:7931,Testability,log,login,7931,"dev deploy might look like:. ```bash; HAIL_DEFAULT_NAMESPACE=default hailctl dev deploy -b <github_user_name>/hail:<branch_name> -s deploy_batch,add_developers; ```. After this dev deploy completes, you should be able to access your namespace; by navigating to https://internal.hail.is/<username>/batch. The service accounts used in developer namespaces do not have permission to create pet service accounts in the; `hail-vdc` project, so the gsa-key secrets for these must be copied across from the default namespace in order; to run real jobs on the developer namespaces' batch services:; ```bash; $ download-secret <username>-gsa-key; $ mv secret.json key.json; $ kubectl create secret generic <username>-gsa-key --namespace=<username> --from-file=key.json; ```. To submit jobs to your dev namespace, you need to configure your local hail; installation to point to the dev namespace. You can do this by running. ```bash; hailctl dev config set default_namespace <my_namespace>; hailctl auth login; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands. NOTE: The dev deploy command above sets `HAIL_DEFAULT_NAMESPACE=default`. That's; because `hailctl dev deploy` submits a request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:8521,Testability,log,logs,8521,"$ download-secret <username>-gsa-key; $ mv secret.json key.json; $ kubectl create secret generic <username>-gsa-key --namespace=<username> --from-file=key.json; ```. To submit jobs to your dev namespace, you need to configure your local hail; installation to point to the dev namespace. You can do this by running. ```bash; hailctl dev config set default_namespace <my_namespace>; hailctl auth login; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands. NOTE: The dev deploy command above sets `HAIL_DEFAULT_NAMESPACE=default`. That's; because `hailctl dev deploy` submits a request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy; 2. make deploy; 3. sync.py. These are listed in order of broadest scope to narrowest scope, so it is never; wrong to do something higher on the list, but it might be a slower feedback loop. A dev deploy is necessary under the following conditions:; - A dev namespace has not been used before; - You have added a migration that must be run against a service's database; - Not all necessary services (like Auth) are deployed in the namespace and you; do not want to `make` deploy them individually. If you just want to redeploy a single service, you can do so with the following:. ```bash; make -C ba",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:8616,Testability,log,logs,8616,"et generic <username>-gsa-key --namespace=<username> --from-file=key.json; ```. To submit jobs to your dev namespace, you need to configure your local hail; installation to point to the dev namespace. You can do this by running. ```bash; hailctl dev config set default_namespace <my_namespace>; hailctl auth login; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands. NOTE: The dev deploy command above sets `HAIL_DEFAULT_NAMESPACE=default`. That's; because `hailctl dev deploy` submits a request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy; 2. make deploy; 3. sync.py. These are listed in order of broadest scope to narrowest scope, so it is never; wrong to do something higher on the list, but it might be a slower feedback loop. A dev deploy is necessary under the following conditions:; - A dev namespace has not been used before; - You have added a migration that must be run against a service's database; - Not all necessary services (like Auth) are deployed in the namespace and you; do not want to `make` deploy them individually. If you just want to redeploy a single service, you can do so with the following:. ```bash; make -C batch deploy NAMESPACE=<my_namespace>; ```. If you only want to make a Python code change",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:8723,Testability,log,log,8723,"jobs to your dev namespace, you need to configure your local hail; installation to point to the dev namespace. You can do this by running. ```bash; hailctl dev config set default_namespace <my_namespace>; hailctl auth login; ```; Then, any Hail Batch or Query-on-Batch script will submit to your dev namespace. You can also use the `HAIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands. NOTE: The dev deploy command above sets `HAIL_DEFAULT_NAMESPACE=default`. That's; because `hailctl dev deploy` submits a request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy; 2. make deploy; 3. sync.py. These are listed in order of broadest scope to narrowest scope, so it is never; wrong to do something higher on the list, but it might be a slower feedback loop. A dev deploy is necessary under the following conditions:; - A dev namespace has not been used before; - You have added a migration that must be run against a service's database; - Not all necessary services (like Auth) are deployed in the namespace and you; do not want to `make` deploy them individually. If you just want to redeploy a single service, you can do so with the following:. ```bash; make -C batch deploy NAMESPACE=<my_namespace>; ```. If you only want to make a Python code change to an already-running service; in Kubernetes, you can run `devbin/sync.py` like the follo",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:12005,Testability,benchmark,benchmark,12005,"nnot request reviews, you can have CI automatically assign a; reviewer. By writing `#assign services` or `#assign compiler` in the PR body, CI will randomly; select a collaborator on the relevant team and assign them for you. You can also give the PR a set of labels. The important ones are “WIP” to make; sure the pull request doesn’t get merged accidentally until you are ready,; “migration” to warn everyone that the changes will shut down the Batch; deployment if it requires a database migration, “bug” for bug fixes, “breaking; change” for any user breaking changes for Hail Query, and “prio:high” to make; this PR the first one in line to merge. There’s also “stacked PR” to indicate; that the changes in the PR are dependent on the changes in another PR. You; should reference that PR in your commit message with “Stacked on #9883”. Most; PRs will not have any labels. For the PR title, start the title with the name of the service(s) the changes; impact. For example, if it’s a Benchmark change, then you’d write; `[benchmark]`. If it’s a Hail Query change, then it would be `[query]`. We also want; the title to be descriptive enough to know what the change is without being too; verbose. An example is “`[batch]` Added read_only option for gcsfuse”. For the PR commit message, we want the message to be descriptive of the complete; set of changes that occurred, especially if it’s a complicated set of; changes. If it’s a smaller, obvious change like a one-liner, then it’s okay to; omit the commit message. For UI changes, it’s helpful to paste a screenshot of; the changes. It’s also a good idea to comment on how you tested the changes and; whether there are any implications of your changes. If the PR fixes a bug that; is a GitHub issue, then you can say “Fixes #8900” to make sure the issue gets; automatically closed by GitHub when your PR is merged. You can also tag a; specific member of the team in the message to get their attention with “@user”. If the changes are user-facing,",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:12611,Testability,test,tested,12611,"merge. There’s also “stacked PR” to indicate; that the changes in the PR are dependent on the changes in another PR. You; should reference that PR in your commit message with “Stacked on #9883”. Most; PRs will not have any labels. For the PR title, start the title with the name of the service(s) the changes; impact. For example, if it’s a Benchmark change, then you’d write; `[benchmark]`. If it’s a Hail Query change, then it would be `[query]`. We also want; the title to be descriptive enough to know what the change is without being too; verbose. An example is “`[batch]` Added read_only option for gcsfuse”. For the PR commit message, we want the message to be descriptive of the complete; set of changes that occurred, especially if it’s a complicated set of; changes. If it’s a smaller, obvious change like a one-liner, then it’s okay to; omit the commit message. For UI changes, it’s helpful to paste a screenshot of; the changes. It’s also a good idea to comment on how you tested the changes and; whether there are any implications of your changes. If the PR fixes a bug that; is a GitHub issue, then you can say “Fixes #8900” to make sure the issue gets; automatically closed by GitHub when your PR is merged. You can also tag a; specific member of the team in the message to get their attention with “@user”. If the changes are user-facing, then add a line in your commit message that; starts with “CHANGELOG: description.”. This should be one line with one sentence; that ends in a period. Once you are done with writing up all the details of the Pull Request, you can; then submit it. Our continuous integration (CI) system watches for new PRs. When; it sees a new PR, it creates a new batch that will test everything defined in; the build.yaml file in the root of the repository. This will create a temporary; namespace in kubernetes for your PR and deploy all the services into it. It will; also run all the tests such as for query, batch, and ci after merging it with; the latest v",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:13344,Testability,test,test,13344,"ges that occurred, especially if it’s a complicated set of; changes. If it’s a smaller, obvious change like a one-liner, then it’s okay to; omit the commit message. For UI changes, it’s helpful to paste a screenshot of; the changes. It’s also a good idea to comment on how you tested the changes and; whether there are any implications of your changes. If the PR fixes a bug that; is a GitHub issue, then you can say “Fixes #8900” to make sure the issue gets; automatically closed by GitHub when your PR is merged. You can also tag a; specific member of the team in the message to get their attention with “@user”. If the changes are user-facing, then add a line in your commit message that; starts with “CHANGELOG: description.”. This should be one line with one sentence; that ends in a period. Once you are done with writing up all the details of the Pull Request, you can; then submit it. Our continuous integration (CI) system watches for new PRs. When; it sees a new PR, it creates a new batch that will test everything defined in; the build.yaml file in the root of the repository. This will create a temporary; namespace in kubernetes for your PR and deploy all the services into it. It will; also run all the tests such as for query, batch, and ci after merging it with; the latest version of main. You can view the progress of the build and the; logs for your PR at [ci.hail.is](https://ci.hail.is). ## Review. Once the PR has been created, it is the responsibility of the reviewer(s) to; review the PR. Our goal as a team is to give comments within 24 hours. To review; someone else’s changes, click on “Files changed”. This will show the diff; between the old code and the new proposed changes. You can make comments on; specific lines of the code. Feel free to ask questions here, especially if you; don’t understand something! It’s a good idea to think critically about the; changes. There should also be tests either added or existing to make sure the; code changes do not break any ex",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:13552,Testability,test,tests,13552," idea to comment on how you tested the changes and; whether there are any implications of your changes. If the PR fixes a bug that; is a GitHub issue, then you can say “Fixes #8900” to make sure the issue gets; automatically closed by GitHub when your PR is merged. You can also tag a; specific member of the team in the message to get their attention with “@user”. If the changes are user-facing, then add a line in your commit message that; starts with “CHANGELOG: description.”. This should be one line with one sentence; that ends in a period. Once you are done with writing up all the details of the Pull Request, you can; then submit it. Our continuous integration (CI) system watches for new PRs. When; it sees a new PR, it creates a new batch that will test everything defined in; the build.yaml file in the root of the repository. This will create a temporary; namespace in kubernetes for your PR and deploy all the services into it. It will; also run all the tests such as for query, batch, and ci after merging it with; the latest version of main. You can view the progress of the build and the; logs for your PR at [ci.hail.is](https://ci.hail.is). ## Review. Once the PR has been created, it is the responsibility of the reviewer(s) to; review the PR. Our goal as a team is to give comments within 24 hours. To review; someone else’s changes, click on “Files changed”. This will show the diff; between the old code and the new proposed changes. You can make comments on; specific lines of the code. Feel free to ask questions here, especially if you; don’t understand something! It’s a good idea to think critically about the; changes. There should also be tests either added or existing to make sure the; code changes do not break any existing functionality and actually implement what; was intended. For example, a change to test whether Batch doesn’t crash when a; user gives a bad input should have a test with bad inputs. It’s okay to spend a; lot of time reviewing PRs! This is a c",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:13690,Testability,log,logs,13690," changes. If the PR fixes a bug that; is a GitHub issue, then you can say “Fixes #8900” to make sure the issue gets; automatically closed by GitHub when your PR is merged. You can also tag a; specific member of the team in the message to get their attention with “@user”. If the changes are user-facing, then add a line in your commit message that; starts with “CHANGELOG: description.”. This should be one line with one sentence; that ends in a period. Once you are done with writing up all the details of the Pull Request, you can; then submit it. Our continuous integration (CI) system watches for new PRs. When; it sees a new PR, it creates a new batch that will test everything defined in; the build.yaml file in the root of the repository. This will create a temporary; namespace in kubernetes for your PR and deploy all the services into it. It will; also run all the tests such as for query, batch, and ci after merging it with; the latest version of main. You can view the progress of the build and the; logs for your PR at [ci.hail.is](https://ci.hail.is). ## Review. Once the PR has been created, it is the responsibility of the reviewer(s) to; review the PR. Our goal as a team is to give comments within 24 hours. To review; someone else’s changes, click on “Files changed”. This will show the diff; between the old code and the new proposed changes. You can make comments on; specific lines of the code. Feel free to ask questions here, especially if you; don’t understand something! It’s a good idea to think critically about the; changes. There should also be tests either added or existing to make sure the; code changes do not break any existing functionality and actually implement what; was intended. For example, a change to test whether Batch doesn’t crash when a; user gives a bad input should have a test with bad inputs. It’s okay to spend a; lot of time reviewing PRs! This is a critical part of our development process to; avoid bugs and unintentional breaking changes. If t",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:14253,Testability,test,tests,14253,"creates a new batch that will test everything defined in; the build.yaml file in the root of the repository. This will create a temporary; namespace in kubernetes for your PR and deploy all the services into it. It will; also run all the tests such as for query, batch, and ci after merging it with; the latest version of main. You can view the progress of the build and the; logs for your PR at [ci.hail.is](https://ci.hail.is). ## Review. Once the PR has been created, it is the responsibility of the reviewer(s) to; review the PR. Our goal as a team is to give comments within 24 hours. To review; someone else’s changes, click on “Files changed”. This will show the diff; between the old code and the new proposed changes. You can make comments on; specific lines of the code. Feel free to ask questions here, especially if you; don’t understand something! It’s a good idea to think critically about the; changes. There should also be tests either added or existing to make sure the; code changes do not break any existing functionality and actually implement what; was intended. For example, a change to test whether Batch doesn’t crash when a; user gives a bad input should have a test with bad inputs. It’s okay to spend a; lot of time reviewing PRs! This is a critical part of our development process to; avoid bugs and unintentional breaking changes. If there are items for the; developer to address, then submit your review with “Request Changes”. Otherwise,; once you are happy with the changes and all comments have been addressed, you; can “Approve” the PR. If you are the person whose code is being reviewed and your PR is in the Request; Changes state, then you’ll need to address their comments by pushing new commit; changes or answering questions. Once you are done, then you can re-request a review; in the ""Reviewers"" box. If your review is requested on a PR submitted by an outside contributor, you should; ""assign"" yourself or the appropriate team member to the PR. The assignee",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:14423,Testability,test,test,14423,"ace in kubernetes for your PR and deploy all the services into it. It will; also run all the tests such as for query, batch, and ci after merging it with; the latest version of main. You can view the progress of the build and the; logs for your PR at [ci.hail.is](https://ci.hail.is). ## Review. Once the PR has been created, it is the responsibility of the reviewer(s) to; review the PR. Our goal as a team is to give comments within 24 hours. To review; someone else’s changes, click on “Files changed”. This will show the diff; between the old code and the new proposed changes. You can make comments on; specific lines of the code. Feel free to ask questions here, especially if you; don’t understand something! It’s a good idea to think critically about the; changes. There should also be tests either added or existing to make sure the; code changes do not break any existing functionality and actually implement what; was intended. For example, a change to test whether Batch doesn’t crash when a; user gives a bad input should have a test with bad inputs. It’s okay to spend a; lot of time reviewing PRs! This is a critical part of our development process to; avoid bugs and unintentional breaking changes. If there are items for the; developer to address, then submit your review with “Request Changes”. Otherwise,; once you are happy with the changes and all comments have been addressed, you; can “Approve” the PR. If you are the person whose code is being reviewed and your PR is in the Request; Changes state, then you’ll need to address their comments by pushing new commit; changes or answering questions. Once you are done, then you can re-request a review; in the ""Reviewers"" box. If your review is requested on a PR submitted by an outside contributor, you should; ""assign"" yourself or the appropriate team member to the PR. The assignee is; responsible for ensuring that the PR does not go stale and is eventually; merged or closed. ![](dismiss-review.png). ## Merge / Deploy. Once ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:14501,Testability,test,test,14501,"ace in kubernetes for your PR and deploy all the services into it. It will; also run all the tests such as for query, batch, and ci after merging it with; the latest version of main. You can view the progress of the build and the; logs for your PR at [ci.hail.is](https://ci.hail.is). ## Review. Once the PR has been created, it is the responsibility of the reviewer(s) to; review the PR. Our goal as a team is to give comments within 24 hours. To review; someone else’s changes, click on “Files changed”. This will show the diff; between the old code and the new proposed changes. You can make comments on; specific lines of the code. Feel free to ask questions here, especially if you; don’t understand something! It’s a good idea to think critically about the; changes. There should also be tests either added or existing to make sure the; code changes do not break any existing functionality and actually implement what; was intended. For example, a change to test whether Batch doesn’t crash when a; user gives a bad input should have a test with bad inputs. It’s okay to spend a; lot of time reviewing PRs! This is a critical part of our development process to; avoid bugs and unintentional breaking changes. If there are items for the; developer to address, then submit your review with “Request Changes”. Otherwise,; once you are happy with the changes and all comments have been addressed, you; can “Approve” the PR. If you are the person whose code is being reviewed and your PR is in the Request; Changes state, then you’ll need to address their comments by pushing new commit; changes or answering questions. Once you are done, then you can re-request a review; in the ""Reviewers"" box. If your review is requested on a PR submitted by an outside contributor, you should; ""assign"" yourself or the appropriate team member to the PR. The assignee is; responsible for ensuring that the PR does not go stale and is eventually; merged or closed. ![](dismiss-review.png). ## Merge / Deploy. Once ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:15813,Testability,test,tests,15813,"t Changes”. Otherwise,; once you are happy with the changes and all comments have been addressed, you; can “Approve” the PR. If you are the person whose code is being reviewed and your PR is in the Request; Changes state, then you’ll need to address their comments by pushing new commit; changes or answering questions. Once you are done, then you can re-request a review; in the ""Reviewers"" box. If your review is requested on a PR submitted by an outside contributor, you should; ""assign"" yourself or the appropriate team member to the PR. The assignee is; responsible for ensuring that the PR does not go stale and is eventually; merged or closed. ![](dismiss-review.png). ## Merge / Deploy. Once a PR has been approved, our continuous integration service (CI) will squash; merge the commits in the PR into the main branch of the repository. This will; then trigger a deploy batch. The deploy batch will first deploy all of the new; Docker images, redeploy the running services in the default namespace with the; latest changes, and rerun all of the tests with the new version of main; incorporating your changes. If this batch fails, a Zulip message will be sent to the entire team linking to the UI for the; failing batch. We should never ignore this message. We should figure out what component broke. If; it’s a transient error, then we need to harden our retry strategy. The `build.yaml` file describes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_m",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:17472,Testability,log,logs,17472,"cribes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch deploy; ```. ## Release. Changes to the services are immediately deployed to the production cluster. Changes to the Hail; Python library are disseminated via PyPI. When a main commit changes the Hail ""Python version"", CI; will run the release steps. Changes to the Python version should be done by a PR whose title is; ""[release] 0.2.XXX"", which changes the Python version in both `hail/version.mk` and `hail/build.sc`,; and which updates the Batch and Query change logs: `hail/python/hail/docs/change_log.md`; `hail/python/hailtop/batch/docs/change_log.rst`. The change logs should comprise all the; `CHANGELOG:` lines from commits between the last release and the new release, for example:. ```; git log 0.2.127..HEAD; ```. CI will create a git tag, a GitHub release, a PyPI package version, update the public Hail docs. We; aim to release once per month. Note that our wheels are limited to 200 MiB and the sum total size of; all published PyPI wheels must be less than 20 GiB. Check [the current limits at; PyPI](https://pypi.org/manage/project/hail/settings/). We had to [request a size; increase](https://github.com/pypi/support/issues/2857) in 2023.; ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:17577,Testability,log,logs,17577,"cribes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch deploy; ```. ## Release. Changes to the services are immediately deployed to the production cluster. Changes to the Hail; Python library are disseminated via PyPI. When a main commit changes the Hail ""Python version"", CI; will run the release steps. Changes to the Python version should be done by a PR whose title is; ""[release] 0.2.XXX"", which changes the Python version in both `hail/version.mk` and `hail/build.sc`,; and which updates the Batch and Query change logs: `hail/python/hail/docs/change_log.md`; `hail/python/hailtop/batch/docs/change_log.rst`. The change logs should comprise all the; `CHANGELOG:` lines from commits between the last release and the new release, for example:. ```; git log 0.2.127..HEAD; ```. CI will create a git tag, a GitHub release, a PyPI package version, update the public Hail docs. We; aim to release once per month. Note that our wheels are limited to 200 MiB and the sum total size of; all published PyPI wheels must be less than 20 GiB. Check [the current limits at; PyPI](https://pypi.org/manage/project/hail/settings/). We had to [request a size; increase](https://github.com/pypi/support/issues/2857) in 2023.; ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:17708,Testability,log,log,17708,"cribes how to build and deploy each commit of Hail. Steps with kind; `createDatabase2` create or migrate databases. A database is a collection of tables. A migration can; be ""offline,"" the default, or ""online"". For the last couple years, we have not used offline; migrations because Batch must be available 24/7 for the scientists. When CI runs an offline migration, it shuts down the services listed in ""shutdowns"" before running; the migration. You can check which migrations have complete by connecting to the admin-pod:. ```bash; kssh admin-pod; ```. Starting mysql and looking at the `NAME_migration_version` and `NAME_migrations` tables:. ```bash; mysql; use NAME; select * from NAME_migrations; select * from NAME_migration_version; ```. When the migration is complete **you must manually** deploy the service:. ```bash; make -c batch deploy; ```. ## Release. Changes to the services are immediately deployed to the production cluster. Changes to the Hail; Python library are disseminated via PyPI. When a main commit changes the Hail ""Python version"", CI; will run the release steps. Changes to the Python version should be done by a PR whose title is; ""[release] 0.2.XXX"", which changes the Python version in both `hail/version.mk` and `hail/build.sc`,; and which updates the Batch and Query change logs: `hail/python/hail/docs/change_log.md`; `hail/python/hailtop/batch/docs/change_log.rst`. The change logs should comprise all the; `CHANGELOG:` lines from commits between the last release and the new release, for example:. ```; git log 0.2.127..HEAD; ```. CI will create a git tag, a GitHub release, a PyPI package version, update the public Hail docs. We; aim to release once per month. Note that our wheels are limited to 200 MiB and the sum total size of; all published PyPI wheels must be less than 20 GiB. Check [the current limits at; PyPI](https://pypi.org/manage/project/hail/settings/). We had to [request a size; increase](https://github.com/pypi/support/issues/2857) in 2023.; ",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md:9114,Usability,feedback,feedback,9114,"AIL_DEFAULT_NAMESPACE` environment variable to alter; the namespace for individual terminal sessions or commands. NOTE: The dev deploy command above sets `HAIL_DEFAULT_NAMESPACE=default`. That's; because `hailctl dev deploy` submits a request to the production CI service that; lives in the `default` namespace. This does *not* mean that your branch will be deployed; to the `default` namespace, it will always be deployed into your dev namespace. To examine the Kubernetes logs for the service in your namespace, use something like. ```bash; kubectl -n <my_namespace> logs -l app=batch-driver --tail=999999 | less; ```. To check the MySQL database in your namespace, you can log in to the database; pod like so:. ```; kubectl -n <my_namespace> exec -it db-0 /bin/bash; $ mysql; ```. ##### Alternatives to dev deploy. There are three ways to deploy code into a dev namespace:; 1. dev deploy; 2. make deploy; 3. sync.py. These are listed in order of broadest scope to narrowest scope, so it is never; wrong to do something higher on the list, but it might be a slower feedback loop. A dev deploy is necessary under the following conditions:; - A dev namespace has not been used before; - You have added a migration that must be run against a service's database; - Not all necessary services (like Auth) are deployed in the namespace and you; do not want to `make` deploy them individually. If you just want to redeploy a single service, you can do so with the following:. ```bash; make -C batch deploy NAMESPACE=<my_namespace>; ```. If you only want to make a Python code change to an already-running service; in Kubernetes, you can run `devbin/sync.py` like the following:. ```bash; python3 devbin/sync.py \; --namespace <my_namespace> \; --app batch --app batch-driver \; --path batch/batch /usr/local/lib/python3.9/dist-packages/ \; --path gear/gear /usr/local/lib/python3.9/dist-packages/ \; --path web_common/web_common /usr/local/lib/python3.9/dist-packages/ \; --path hail/python/hailtop /usr/l",MatchSource.DOCS,dev-docs/development-process.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/development-process.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/git-practices.md:3167,Availability,error,error,3167,"ave developed along the; way. To rebase your feature branch on the latest upstream main, run. ```bash; git fetch upstream; git rebase upstream/main <feature_branch>; ```. The `-i` here is optional but extremely informative, and can help you understand; what the rebase is doing. You can leave all your commits as `pick`, and keep; all your commits while perfoming the rebase, or you can change some commits to; `squash` to collapse your changes into a smaller number of commits. When opening; your change for review, it can be helpful for the reviewers if you squash your; branch into a small number of self-contained commits. For example, if your change; requires upgrading a dependency, it is helpful to put the dependency upgrades; into a separate commmit from the code changes. ## Making a PR. When a feature branch is ready for PR, push the branch to your fork:. ```bash; git push origin <feature_branch>; ```. If you have already done this before but have since rebased, you may get; an error because the history on your fork is no longer a prefix of your local; branch's history. Force push your branch to overwrite the history on GitHub; with that of your local branch. ```bash; git push --force-with-lease origin <feature_branch>; ```. You can then make a PR on GitHub from the branch on your fork to `hail-is/hail:main`. ## While a feature is in PR. If a reviewer requests changes on a PR, you can make those changes on; your feature branch and `git push origin <feature_branch>` to reflect those; changes in your PR. However, once the review process has begun it is best not to `rebase` the branch; any further. Doing so rewrites the commit history of the PR and causes GitHub to lose; when and where review comments were made. It also removes the PR reviewer's ability; to use the GitHub feature ""see changes since last review"", which can be very; helpful for long PRs and review processes. If an existing PR runs into merge conflicts, you can instead merge main *into* your; feature branc",MatchSource.DOCS,dev-docs/git-practices.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/git-practices.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/git-practices.md:2899,Deployability,upgrade,upgrades,2899,"n upstream `main` conflict with the code you; are working on. To cope with this, we recommend [rebasing](https://git-scm.com/docs/git-rebase#_description); regularly and often. Rebasing will incoporate the new changes from main into your; branch and allow you to resolve any conflicts that might have developed along the; way. To rebase your feature branch on the latest upstream main, run. ```bash; git fetch upstream; git rebase upstream/main <feature_branch>; ```. The `-i` here is optional but extremely informative, and can help you understand; what the rebase is doing. You can leave all your commits as `pick`, and keep; all your commits while perfoming the rebase, or you can change some commits to; `squash` to collapse your changes into a smaller number of commits. When opening; your change for review, it can be helpful for the reviewers if you squash your; branch into a small number of self-contained commits. For example, if your change; requires upgrading a dependency, it is helpful to put the dependency upgrades; into a separate commmit from the code changes. ## Making a PR. When a feature branch is ready for PR, push the branch to your fork:. ```bash; git push origin <feature_branch>; ```. If you have already done this before but have since rebased, you may get; an error because the history on your fork is no longer a prefix of your local; branch's history. Force push your branch to overwrite the history on GitHub; with that of your local branch. ```bash; git push --force-with-lease origin <feature_branch>; ```. You can then make a PR on GitHub from the branch on your fork to `hail-is/hail:main`. ## While a feature is in PR. If a reviewer requests changes on a PR, you can make those changes on; your feature branch and `git push origin <feature_branch>` to reflect those; changes in your PR. However, once the review process has begun it is best not to `rebase` the branch; any further. Doing so rewrites the commit history of the PR and causes GitHub to lose; when an",MatchSource.DOCS,dev-docs/git-practices.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/git-practices.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/git-practices.md:2851,Integrability,depend,dependency,2851,"n upstream `main` conflict with the code you; are working on. To cope with this, we recommend [rebasing](https://git-scm.com/docs/git-rebase#_description); regularly and often. Rebasing will incoporate the new changes from main into your; branch and allow you to resolve any conflicts that might have developed along the; way. To rebase your feature branch on the latest upstream main, run. ```bash; git fetch upstream; git rebase upstream/main <feature_branch>; ```. The `-i` here is optional but extremely informative, and can help you understand; what the rebase is doing. You can leave all your commits as `pick`, and keep; all your commits while perfoming the rebase, or you can change some commits to; `squash` to collapse your changes into a smaller number of commits. When opening; your change for review, it can be helpful for the reviewers if you squash your; branch into a small number of self-contained commits. For example, if your change; requires upgrading a dependency, it is helpful to put the dependency upgrades; into a separate commmit from the code changes. ## Making a PR. When a feature branch is ready for PR, push the branch to your fork:. ```bash; git push origin <feature_branch>; ```. If you have already done this before but have since rebased, you may get; an error because the history on your fork is no longer a prefix of your local; branch's history. Force push your branch to overwrite the history on GitHub; with that of your local branch. ```bash; git push --force-with-lease origin <feature_branch>; ```. You can then make a PR on GitHub from the branch on your fork to `hail-is/hail:main`. ## While a feature is in PR. If a reviewer requests changes on a PR, you can make those changes on; your feature branch and `git push origin <feature_branch>` to reflect those; changes in your PR. However, once the review process has begun it is best not to `rebase` the branch; any further. Doing so rewrites the commit history of the PR and causes GitHub to lose; when an",MatchSource.DOCS,dev-docs/git-practices.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/git-practices.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/git-practices.md:2888,Integrability,depend,dependency,2888,"n upstream `main` conflict with the code you; are working on. To cope with this, we recommend [rebasing](https://git-scm.com/docs/git-rebase#_description); regularly and often. Rebasing will incoporate the new changes from main into your; branch and allow you to resolve any conflicts that might have developed along the; way. To rebase your feature branch on the latest upstream main, run. ```bash; git fetch upstream; git rebase upstream/main <feature_branch>; ```. The `-i` here is optional but extremely informative, and can help you understand; what the rebase is doing. You can leave all your commits as `pick`, and keep; all your commits while perfoming the rebase, or you can change some commits to; `squash` to collapse your changes into a smaller number of commits. When opening; your change for review, it can be helpful for the reviewers if you squash your; branch into a small number of self-contained commits. For example, if your change; requires upgrading a dependency, it is helpful to put the dependency upgrades; into a separate commmit from the code changes. ## Making a PR. When a feature branch is ready for PR, push the branch to your fork:. ```bash; git push origin <feature_branch>; ```. If you have already done this before but have since rebased, you may get; an error because the history on your fork is no longer a prefix of your local; branch's history. Force push your branch to overwrite the history on GitHub; with that of your local branch. ```bash; git push --force-with-lease origin <feature_branch>; ```. You can then make a PR on GitHub from the branch on your fork to `hail-is/hail:main`. ## While a feature is in PR. If a reviewer requests changes on a PR, you can make those changes on; your feature branch and `git push origin <feature_branch>` to reflect those; changes in your PR. However, once the review process has begun it is best not to `rebase` the branch; any further. Doing so rewrites the commit history of the PR and causes GitHub to lose; when an",MatchSource.DOCS,dev-docs/git-practices.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/git-practices.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/git-practices.md:763,Modifiability,config,configured,763,"# Forks and PR workflows. Changes to the Hail codebase are made through pull requests directly to the; `main` branch of the `hail-is/hail` repository. Here are the steps to; take when contributing for the first time and when making a pull request. ## First time actions. In order to keep the `hail-is/hail` repo clean, we ask that contributors develop; features in branches on a personal fork. The first step then is to [fork; the repository](https://github.com/hail-is/hail/fork). Once you've done so, clone the repository from your fork. This will set up a; local copy of your fork with a single remote called `origin`. In this document,; we will refer to `origin` as your developer fork, and `upstream` as `hail-is/hail`.; You can check which origins you have configured by running `git remote -v`. While feature branches will live on your fork, those branches are still going to want; to be based on the latest changes in `hail-is/hail:main`. So we will add the; upstream repository as another remote so we can pull in those changes. ```bash; git remote add upstream https://github.com/hail-is/hail.git; ```. If you run `git remote -v` again, you should see something like the following:. ```; origin	https://github.com/<your-github-username>/hail.git (fetch); origin	https://github.com/<your-github-username>/hail.git (push); upstream	https://github.com/hail-is/hail.git (fetch); upstream	https://github.com/hail-is/hail.git (push); ```. When starting a new feature branch, retrieve the latest changes from; upstream and checkout a branch based on those changes:. ```bash; git fetch upstream; git checkout -b <feature_branch> upstream/main; ```. ## While developing a feature. `hail-is/hail:main` moves quickly, and it is likely that it will have progressed; significantly while you work on a feature. This is not in itself a problem,; but can cause headaches if changes in upstream `main` conflict with the code you; are working on. To cope with this, we recommend [rebasing](https://git-scm.com",MatchSource.DOCS,dev-docs/git-practices.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/git-practices.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/git-practices.md:3806,Modifiability,rewrite,rewrites,3806,"ample, if your change; requires upgrading a dependency, it is helpful to put the dependency upgrades; into a separate commmit from the code changes. ## Making a PR. When a feature branch is ready for PR, push the branch to your fork:. ```bash; git push origin <feature_branch>; ```. If you have already done this before but have since rebased, you may get; an error because the history on your fork is no longer a prefix of your local; branch's history. Force push your branch to overwrite the history on GitHub; with that of your local branch. ```bash; git push --force-with-lease origin <feature_branch>; ```. You can then make a PR on GitHub from the branch on your fork to `hail-is/hail:main`. ## While a feature is in PR. If a reviewer requests changes on a PR, you can make those changes on; your feature branch and `git push origin <feature_branch>` to reflect those; changes in your PR. However, once the review process has begun it is best not to `rebase` the branch; any further. Doing so rewrites the commit history of the PR and causes GitHub to lose; when and where review comments were made. It also removes the PR reviewer's ability; to use the GitHub feature ""see changes since last review"", which can be very; helpful for long PRs and review processes. If an existing PR runs into merge conflicts, you can instead merge main *into* your; feature branch. ```bash; git fetch upstream; git checkout <feature_branch>; git merge upstream/main. ... resolve any conflicts and `git add` any resolved files ... git commit; git push origin <feature_branch>; ```. Instead of rewriting the history and losing the state of the review, this will; add a single merge commit that can be ignored by the reviewer. Another reason not to force push once a change is in PR is if another collaborator; adds changes to the branch. If someone else has made a change to your PR, pull; those changes into your local branch before adding new changes by running. ```bash; git pull origin <feature_branch>; ```; ",MatchSource.DOCS,dev-docs/git-practices.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/git-practices.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/google-cloud-cookbook.md:846,Availability,echo,echo,846,"# Google Cloud Cookbook. Executable snippets for certain operational tasks. These snippets will likely bitrot and otherwise; be unreliable. The reader is recommended to test the snippet before use. ## Delete All Detached Disks. There were approximately 240,000 disks accidentally left unattached in a project. This script was; used to delete them. We only delete 990 at a time because there is a ""queries per second"" quota that; limits how many queries we can make every 100 seconds. We empirically determined that deleting 990; instances at a time did not exceed the quota. ```; for i in $(seq 0 280); do; gcloud compute disks list \; --filter='LAST_ATTACH_TIMESTAMP < LAST_DETATCH_TIMESTAMP and name ~ ""^batch-disk"" and STATUS=Ready' \; --limit=990 \; | tail -n +2 \; > /tmp/disks; for zone in $(cat /tmp/disks | awk '{print $2}' | uniq ); do; echo $zone; awk '$2==""'$zone'"" {print $1}' /tmp/disks \; | grep -Ee '^batch-disk' \; | xargs /bin/bash -c 'yes | gcloud compute disks delete '--zone=$zone' $*' % &; done; wait; done; ```; ",MatchSource.DOCS,dev-docs/google-cloud-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/google-cloud-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/google-cloud-cookbook.md:169,Testability,test,test,169,"# Google Cloud Cookbook. Executable snippets for certain operational tasks. These snippets will likely bitrot and otherwise; be unreliable. The reader is recommended to test the snippet before use. ## Delete All Detached Disks. There were approximately 240,000 disks accidentally left unattached in a project. This script was; used to delete them. We only delete 990 at a time because there is a ""queries per second"" quota that; limits how many queries we can make every 100 seconds. We empirically determined that deleting 990; instances at a time did not exceed the quota. ```; for i in $(seq 0 280); do; gcloud compute disks list \; --filter='LAST_ATTACH_TIMESTAMP < LAST_DETATCH_TIMESTAMP and name ~ ""^batch-disk"" and STATUS=Ready' \; --limit=990 \; | tail -n +2 \; > /tmp/disks; for zone in $(cat /tmp/disks | awk '{print $2}' | uniq ); do; echo $zone; awk '$2==""'$zone'"" {print $1}' /tmp/disks \; | grep -Ee '^batch-disk' \; | xargs /bin/bash -c 'yes | gcloud compute disks delete '--zone=$zone' $*' % &; done; wait; done; ```; ",MatchSource.DOCS,dev-docs/google-cloud-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/google-cloud-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/gvs.md:124,Performance,perform,performing,124,"# Genomic Variant Store. The Genomic Variant Store (GVS) is a BigQuery based system for storing human genome sequences and; performing joint-calling. It roughly resembles the ""Scalable Variant Call Representation""; implemented in terms of BigQuery SQL tables. It was developed by DSP. It inspired the ""split""; representation (one matrix table for reference data and one for variant data) of the Hail Variant; Dataset. Our main interaction with GVS is when the produce large callsets. They export their data in Avro; format and then use Hail to import and combine that data into a VDS. They have one variant data; table and one reference data table per 4,000 samples so when they export we get a folder per group; of 4,000 samples. Each folder contains a partitioned dataset ordered by genomic locus; however, they; encode their locus in a 64-bit integer by shifting the contig number into the high bits. We import each folder of Avro files (using the general purpose JVM Avro reader, which is rather; slow) and convert from its point-wise representation (one record per locus per sample) into a wide; representation (one record per locus with an array of possibly missing sample data). This is called; ""sample group import"". After import, we massage this data into VDS format and write each one as an; intermediate VDS. Finally, we run the VDS Combiner on these intermediate VDSes, of which there may; be tens or a couple hundred, to produce single final VDS. We then import some variant filtration; annotations from them and add these annotations to the variant data matrix table. We have proposed using Hail instead of BigQuery for the Azure implementation of GVS:; https://docs.google.com/document/d/1OluN0dEIIKtI2KksFDIC_ZFA4aiCXasZ1OwzagGqtio/edit#heading=h.8ghvgsh8r2db; ",MatchSource.DOCS,dev-docs/gvs.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/gvs.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:9042,Availability,robust,robustly,9042,"e and returned to the user. ### Hail Batch. Hail Batch is a system for executing arbitrary Linux programs. Each invocation of a program is; called a ""job"". Zero or more jobs comprise a Batch. Moreover, jobs may depend on the files written; by other jobs in the same Batch. The job dependencies are allowed to form any [directed, acyclic; graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph). Users create batches and jobs using a Python library: `hailtop.batch`. The [Batch Service](https://batch.hail.is) schedules jobs on an; [elastically](https://en.wikipedia.org/wiki/Elasticity_(cloud_computing)) sized group of virtual; machines. The virtual machines are often called ""batch workers"". The software that manages a single; virtual machine is also called ""the batch worker"". ### Hail FS. Hail FS is an filesystem-like interface which supports both local file systems and three of the; major cloud object stores: S3, ABS, and GCS. Upon this foundation, we have built tools for robustly; copying terabytes of data from one cloud to another. These APIs are also used pervasively by Hail Batch and Hail Query-on-Batch to read and write data to; the cloud object stores. ### Hail Variant Dataset. The Hail Variant Dataset (VDS) and the [Scalable Variant Call Format; (SVCR)](https://www.biorxiv.org/content/10.1101/2024.01.09.574205v1) are a format and general; representation for sparsely storing a collection of genome sequences aligned to a reference. The VDS; is mergeable and losslessly preserves the information of a GVCF. The VDS Combiner is a robust,; horizontally-scalable-in-variants, and tree-scalable-in-samples tool for creating a VDS from one or; more GVCFs or VDSes. The VDS Combiner has been used with as many as 955,000 exomes, VDS is a; realization of the SVCR in terms of two Hail matrix tables. SVCR comprises two key sparse; representations. First, allele-indexed arrays (such as the depth per allele, AD) elide entries for; unobserved alleles. Second, homozygous refe",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:9612,Availability,robust,robust,9612,"sized group of virtual; machines. The virtual machines are often called ""batch workers"". The software that manages a single; virtual machine is also called ""the batch worker"". ### Hail FS. Hail FS is an filesystem-like interface which supports both local file systems and three of the; major cloud object stores: S3, ABS, and GCS. Upon this foundation, we have built tools for robustly; copying terabytes of data from one cloud to another. These APIs are also used pervasively by Hail Batch and Hail Query-on-Batch to read and write data to; the cloud object stores. ### Hail Variant Dataset. The Hail Variant Dataset (VDS) and the [Scalable Variant Call Format; (SVCR)](https://www.biorxiv.org/content/10.1101/2024.01.09.574205v1) are a format and general; representation for sparsely storing a collection of genome sequences aligned to a reference. The VDS; is mergeable and losslessly preserves the information of a GVCF. The VDS Combiner is a robust,; horizontally-scalable-in-variants, and tree-scalable-in-samples tool for creating a VDS from one or; more GVCFs or VDSes. The VDS Combiner has been used with as many as 955,000 exomes, VDS is a; realization of the SVCR in terms of two Hail matrix tables. SVCR comprises two key sparse; representations. First, allele-indexed arrays (such as the depth per allele, AD) elide entries for; unobserved alleles. Second, homozygous reference calls are run-length encoded. ## Infrastructure and Technology. The Hail team does not maintain any physical computer infrastructure. We maintain some virtual infrastructure, almost exclusively within the Google Cloud Platform (GCP). These include:; - a [Kubernetes](https://kubernetes.io) (k8s) cluster called `vdc` (Virtual Data Center); - many Google Cloud Storage ([an object store](https://en.wikipedia.org/wiki/Object_storage)) buckets; - one Cloud SQL instance with a production database, ephemeral pull-request-test databases, and a; database per developer; - logs for basically anything can be found i",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:5112,Deployability,deploy,deploying,5112," a user needs an Apache Spark; cluster. Most Hail users use Google Dataproc Spark clusters. The essential feature of a human genetic dataset is a two-dimensional matrix of human; genotypes. Every genotype has the property ""number of alternate alleles"". This property allows a; matrix of genotypes to be represented as a numeric matrix. Geneticists use linear algebraic; techniques on this numeric matrix to understand the relationship between human disease and human; genetics. In November of 2016, the Hail command-line interface was eliminated and a Python interface was; introduced. During this time, Hail was not versioned. Users had to build and use Hail directly from; the source code repository. In March of 2017, Hail team began work on a compiler. In October of 2018, the Hail Python interface was modified, in a backwards-incompatible way. This; new Python interface was named ""Hail 0.2"". The old Python interface was retroactively named ""Hail; 0.1"". Hail team began deploying a Python package named `hail` to [PyPI](https://pypi.org). The Hail; Python package version complies with [Semantic Versioning](https://semver.org). Meanwhile, in September of 2018, Hail team began work on a system called ""Batch"". Batch runs; programs in parallel on a cluster of virtual machines. Also in September, Hail team began work on a; system called ""CI"" (Continuous Integration). CI runs the tests for every pull request (PR) into the; `main` branch of [`hail-is/hail`](https://github.com/hail-is/hail). CI automatically merges into; main any pull request that both passes the tests and has at least one ""approving"" review and no; ""changes requested"" reviews. CI uses Hail Batch to run the tests. Around this time, the Hail team organized itself into two sub-teams: ""compilers"" team and ""services""; team. The compilers team is responsible for the Hail Python library, the compiler, and the; associated runtime. The compilers team code is entirely contained in the `hail` directory of the; `hail-is/hail` ",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:7639,Deployability,pipeline,pipeline,7639,"Hail Query Python library using Hail Batch to run an analysis across; many computer cores instead of using Apache Spark. ## Hail Products, Briefly. Hail team maintains four core produces. ### Hail Query. Hail Query is a Python library for the analysis of large datasets. In Hail Query, a dataset is; represented as Table or a Matrix Table. Hail Tables are similar to SQL tables, Pandas Dataframes, Excel spreadsheets, and CSV files. Hail Matrix Tables do not have analogues in most other systems. Perhaps the only analogue is; [SciDB](https://dbdb.io/db/scidb) and its descendants: [TileDB](https://tiledb.com) and; [GenomicsDB](https://github.com/GenomicsDB/GenomicsDB)). A Hail Matrix Table can represent dense,; two-dimensional, homogeneous data. For example, datasets of Human genetic sequences, dense [numeric; matrices](https://en.wikipedia.org/wiki/Matrix_(mathematics)), and [astronomical; surveys](https://en.wikipedia.org/wiki/Astronomical_survey). Users use the Hail Query Python library to write a ""pipeline"" to analyze their data. The Python; library sends this pipeline to a compiler written in Scala. The compiler converts the pipeline into; an Apache Spark job or a Hail Batch job. A pipeline typically reads a dataset, processes it, and; either writes a new dataset or aggregates (e.g. computes the mean of a field). If a pipeline; aggregates, the resulting aggregated value is converted to a Python value and returned to the user. ### Hail Batch. Hail Batch is a system for executing arbitrary Linux programs. Each invocation of a program is; called a ""job"". Zero or more jobs comprise a Batch. Moreover, jobs may depend on the files written; by other jobs in the same Batch. The job dependencies are allowed to form any [directed, acyclic; graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph). Users create batches and jobs using a Python library: `hailtop.batch`. The [Batch Service](https://batch.hail.is) schedules jobs on an; [elastically](https://en.wikipedia.org",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:7703,Deployability,pipeline,pipeline,7703,"uter cores instead of using Apache Spark. ## Hail Products, Briefly. Hail team maintains four core produces. ### Hail Query. Hail Query is a Python library for the analysis of large datasets. In Hail Query, a dataset is; represented as Table or a Matrix Table. Hail Tables are similar to SQL tables, Pandas Dataframes, Excel spreadsheets, and CSV files. Hail Matrix Tables do not have analogues in most other systems. Perhaps the only analogue is; [SciDB](https://dbdb.io/db/scidb) and its descendants: [TileDB](https://tiledb.com) and; [GenomicsDB](https://github.com/GenomicsDB/GenomicsDB)). A Hail Matrix Table can represent dense,; two-dimensional, homogeneous data. For example, datasets of Human genetic sequences, dense [numeric; matrices](https://en.wikipedia.org/wiki/Matrix_(mathematics)), and [astronomical; surveys](https://en.wikipedia.org/wiki/Astronomical_survey). Users use the Hail Query Python library to write a ""pipeline"" to analyze their data. The Python; library sends this pipeline to a compiler written in Scala. The compiler converts the pipeline into; an Apache Spark job or a Hail Batch job. A pipeline typically reads a dataset, processes it, and; either writes a new dataset or aggregates (e.g. computes the mean of a field). If a pipeline; aggregates, the resulting aggregated value is converted to a Python value and returned to the user. ### Hail Batch. Hail Batch is a system for executing arbitrary Linux programs. Each invocation of a program is; called a ""job"". Zero or more jobs comprise a Batch. Moreover, jobs may depend on the files written; by other jobs in the same Batch. The job dependencies are allowed to form any [directed, acyclic; graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph). Users create batches and jobs using a Python library: `hailtop.batch`. The [Batch Service](https://batch.hail.is) schedules jobs on an; [elastically](https://en.wikipedia.org/wiki/Elasticity_(cloud_computing)) sized group of virtual; machines. The virt",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:7770,Deployability,pipeline,pipeline,7770,"m maintains four core produces. ### Hail Query. Hail Query is a Python library for the analysis of large datasets. In Hail Query, a dataset is; represented as Table or a Matrix Table. Hail Tables are similar to SQL tables, Pandas Dataframes, Excel spreadsheets, and CSV files. Hail Matrix Tables do not have analogues in most other systems. Perhaps the only analogue is; [SciDB](https://dbdb.io/db/scidb) and its descendants: [TileDB](https://tiledb.com) and; [GenomicsDB](https://github.com/GenomicsDB/GenomicsDB)). A Hail Matrix Table can represent dense,; two-dimensional, homogeneous data. For example, datasets of Human genetic sequences, dense [numeric; matrices](https://en.wikipedia.org/wiki/Matrix_(mathematics)), and [astronomical; surveys](https://en.wikipedia.org/wiki/Astronomical_survey). Users use the Hail Query Python library to write a ""pipeline"" to analyze their data. The Python; library sends this pipeline to a compiler written in Scala. The compiler converts the pipeline into; an Apache Spark job or a Hail Batch job. A pipeline typically reads a dataset, processes it, and; either writes a new dataset or aggregates (e.g. computes the mean of a field). If a pipeline; aggregates, the resulting aggregated value is converted to a Python value and returned to the user. ### Hail Batch. Hail Batch is a system for executing arbitrary Linux programs. Each invocation of a program is; called a ""job"". Zero or more jobs comprise a Batch. Moreover, jobs may depend on the files written; by other jobs in the same Batch. The job dependencies are allowed to form any [directed, acyclic; graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph). Users create batches and jobs using a Python library: `hailtop.batch`. The [Batch Service](https://batch.hail.is) schedules jobs on an; [elastically](https://en.wikipedia.org/wiki/Elasticity_(cloud_computing)) sized group of virtual; machines. The virtual machines are often called ""batch workers"". The software that manages a si",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:7828,Deployability,pipeline,pipeline,7828,"sis of large datasets. In Hail Query, a dataset is; represented as Table or a Matrix Table. Hail Tables are similar to SQL tables, Pandas Dataframes, Excel spreadsheets, and CSV files. Hail Matrix Tables do not have analogues in most other systems. Perhaps the only analogue is; [SciDB](https://dbdb.io/db/scidb) and its descendants: [TileDB](https://tiledb.com) and; [GenomicsDB](https://github.com/GenomicsDB/GenomicsDB)). A Hail Matrix Table can represent dense,; two-dimensional, homogeneous data. For example, datasets of Human genetic sequences, dense [numeric; matrices](https://en.wikipedia.org/wiki/Matrix_(mathematics)), and [astronomical; surveys](https://en.wikipedia.org/wiki/Astronomical_survey). Users use the Hail Query Python library to write a ""pipeline"" to analyze their data. The Python; library sends this pipeline to a compiler written in Scala. The compiler converts the pipeline into; an Apache Spark job or a Hail Batch job. A pipeline typically reads a dataset, processes it, and; either writes a new dataset or aggregates (e.g. computes the mean of a field). If a pipeline; aggregates, the resulting aggregated value is converted to a Python value and returned to the user. ### Hail Batch. Hail Batch is a system for executing arbitrary Linux programs. Each invocation of a program is; called a ""job"". Zero or more jobs comprise a Batch. Moreover, jobs may depend on the files written; by other jobs in the same Batch. The job dependencies are allowed to form any [directed, acyclic; graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph). Users create batches and jobs using a Python library: `hailtop.batch`. The [Batch Service](https://batch.hail.is) schedules jobs on an; [elastically](https://en.wikipedia.org/wiki/Elasticity_(cloud_computing)) sized group of virtual; machines. The virtual machines are often called ""batch workers"". The software that manages a single; virtual machine is also called ""the batch worker"". ### Hail FS. Hail FS is an filesyste",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:7967,Deployability,pipeline,pipeline,7967,"frames, Excel spreadsheets, and CSV files. Hail Matrix Tables do not have analogues in most other systems. Perhaps the only analogue is; [SciDB](https://dbdb.io/db/scidb) and its descendants: [TileDB](https://tiledb.com) and; [GenomicsDB](https://github.com/GenomicsDB/GenomicsDB)). A Hail Matrix Table can represent dense,; two-dimensional, homogeneous data. For example, datasets of Human genetic sequences, dense [numeric; matrices](https://en.wikipedia.org/wiki/Matrix_(mathematics)), and [astronomical; surveys](https://en.wikipedia.org/wiki/Astronomical_survey). Users use the Hail Query Python library to write a ""pipeline"" to analyze their data. The Python; library sends this pipeline to a compiler written in Scala. The compiler converts the pipeline into; an Apache Spark job or a Hail Batch job. A pipeline typically reads a dataset, processes it, and; either writes a new dataset or aggregates (e.g. computes the mean of a field). If a pipeline; aggregates, the resulting aggregated value is converted to a Python value and returned to the user. ### Hail Batch. Hail Batch is a system for executing arbitrary Linux programs. Each invocation of a program is; called a ""job"". Zero or more jobs comprise a Batch. Moreover, jobs may depend on the files written; by other jobs in the same Batch. The job dependencies are allowed to form any [directed, acyclic; graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph). Users create batches and jobs using a Python library: `hailtop.batch`. The [Batch Service](https://batch.hail.is) schedules jobs on an; [elastically](https://en.wikipedia.org/wiki/Elasticity_(cloud_computing)) sized group of virtual; machines. The virtual machines are often called ""batch workers"". The software that manages a single; virtual machine is also called ""the batch worker"". ### Hail FS. Hail FS is an filesystem-like interface which supports both local file systems and three of the; major cloud object stores: S3, ABS, and GCS. Upon this foundation, w",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:11868,Deployability,deploy,deployment,11868," Hail Query compiler and run-time; - the JVM is the legacy target of the Hail Query compiler; - C++ is the aspirational language of high-performance services and the Hail Query compiler and run-time; - LLVM is the aspirational target of the Hail Query compiler; - Docker is our container image and run-time system; - MySQL is our SQL database of choice. ### Services Technology. We almost exclusively write services in Python 3.9. We use a number of Python packages:; - [`asyncio`](https://docs.python.org/3.9/library/asyncio.html) for concurrency which is built on; [coroutines](https://en.wikipedia.org/wiki/Coroutine) not threads; - [`aiohttp`](https://docs.aiohttp.org/en/stable/) for serving HTTPS requests (most services speak; HTTPS); - [`jinja2`](https://jinja.palletsprojects.com/en/2.11.x/) for ""templating"" which simply means; programmatically generating text files. A service is realized as:. - a Docker image containing the executable code for the service; - a Kubernetes deployment (which defines the command to run, how much CPU is needed, what; environment variables are set, etc.); - a Kubernetes service (which defines which ports are accessible); - possibly a database within our Cloud SQL instance. We call a complete, working set of all services and databases a ""namespace"". Every namespace; corresponds to a Kubernetes namespace. All namespaces share one CloudSQL instance, but only have; access to their databases. The default namespace contains our ""production"" services and is accessible to the outside world at; https://hail.is, https://batch.hail.is, etc. Every developer and every pull request test job also has a namespace. Developer namespaces are; accessible at https://internal.hail.is/DEVELOPER_USERNAME/SERVICE/ . Unlike the default namespace,; every other namespace has exactly one database (containing all tables from each service's database). All incoming traffic passes through either gateway or internal-gateway which route requests to; the appropriate namespac",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:8569,Energy Efficiency,schedul,schedules,8569,"rs use the Hail Query Python library to write a ""pipeline"" to analyze their data. The Python; library sends this pipeline to a compiler written in Scala. The compiler converts the pipeline into; an Apache Spark job or a Hail Batch job. A pipeline typically reads a dataset, processes it, and; either writes a new dataset or aggregates (e.g. computes the mean of a field). If a pipeline; aggregates, the resulting aggregated value is converted to a Python value and returned to the user. ### Hail Batch. Hail Batch is a system for executing arbitrary Linux programs. Each invocation of a program is; called a ""job"". Zero or more jobs comprise a Batch. Moreover, jobs may depend on the files written; by other jobs in the same Batch. The job dependencies are allowed to form any [directed, acyclic; graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph). Users create batches and jobs using a Python library: `hailtop.batch`. The [Batch Service](https://batch.hail.is) schedules jobs on an; [elastically](https://en.wikipedia.org/wiki/Elasticity_(cloud_computing)) sized group of virtual; machines. The virtual machines are often called ""batch workers"". The software that manages a single; virtual machine is also called ""the batch worker"". ### Hail FS. Hail FS is an filesystem-like interface which supports both local file systems and three of the; major cloud object stores: S3, ABS, and GCS. Upon this foundation, we have built tools for robustly; copying terabytes of data from one cloud to another. These APIs are also used pervasively by Hail Batch and Hail Query-on-Batch to read and write data to; the cloud object stores. ### Hail Variant Dataset. The Hail Variant Dataset (VDS) and the [Scalable Variant Call Format; (SVCR)](https://www.biorxiv.org/content/10.1101/2024.01.09.574205v1) are a format and general; representation for sparsely storing a collection of genome sequences aligned to a reference. The VDS; is mergeable and losslessly preserves the information of a GVCF. T",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:1751,Integrability,message,messages,1751,"different field of study: genomics. > [The] Broad Institute of MIT and Harvard was launched in 2004 to improve human health by using; > genomics to advance our understanding of the biology and treatment of human disease, and to help; > lay the groundwork for a new generation of therapies. Genetics and genomics may seem similar to a software engineer's ear; however, genetics is the study; of heredity whereas genomics is the study of the genome.[^1] The [history of; genetics](https://en.wikipedia.org/wiki/History_of_genetics) is deeply intertwined with statistics; which perhaps explains some of the distinction from genomics whose history lies more firmly in; biology. The history of genetics is also deeply intertwined with; [eugenics](https://en.wikipedia.org/wiki/History_of_eugenics) and; [racism](https://en.wikipedia.org/wiki/Scientific_racism). Sadly, this continues today (see: [James; Watson](https://en.wikipedia.org/wiki/James_Watson)). The team Zulip channel and private messages; are both valid forums for discussing these issues. The Neale Lab manifests the Broad's credo by studying the relationship between human disease and; human genetics. This is sometimes called ""genetic epidemiology"". One common; statistical-epidemiological study design is the case-control study. A case-control study involves; two groups of people. The ""case"" group has been diagnosed with the disease. The ""control"" group has; not. We collect genetic material from both groups and search for a correlation between the material; and the groups. There is at least one successful example of genetic studies leading to the development of a drug:; the discovery of PCSK9. In 2013, the New York Times [reported on the; discovery](https://www.nytimes.com/2013/07/10/health/rare-mutation-prompts-race-for-cholesterol-drug.html); of an association between mutations in the PCSK9 gene and high levels of LDL cholesterol. By 2017,; [further; studies](https://www.nytimes.com/2017/03/17/health/cholesterol-drugs-rep",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:4656,Integrability,interface,interface,4656,"ysis on a single computer was; prohibitively time-consuming. Moreover, partitioning the dataset and analyzing each partition; separately necessitated increasingly complex software engineering. We started the Hail project to; re-enable simple and interactive (i.e. fast) analysis of these large datasets. Hail was a command-line program that used Apache Spark to run analysis on partitioned genetic; datasets simultaneously using hundreds of computer cores. To use Hail, a user needs an Apache Spark; cluster. Most Hail users use Google Dataproc Spark clusters. The essential feature of a human genetic dataset is a two-dimensional matrix of human; genotypes. Every genotype has the property ""number of alternate alleles"". This property allows a; matrix of genotypes to be represented as a numeric matrix. Geneticists use linear algebraic; techniques on this numeric matrix to understand the relationship between human disease and human; genetics. In November of 2016, the Hail command-line interface was eliminated and a Python interface was; introduced. During this time, Hail was not versioned. Users had to build and use Hail directly from; the source code repository. In March of 2017, Hail team began work on a compiler. In October of 2018, the Hail Python interface was modified, in a backwards-incompatible way. This; new Python interface was named ""Hail 0.2"". The old Python interface was retroactively named ""Hail; 0.1"". Hail team began deploying a Python package named `hail` to [PyPI](https://pypi.org). The Hail; Python package version complies with [Semantic Versioning](https://semver.org). Meanwhile, in September of 2018, Hail team began work on a system called ""Batch"". Batch runs; programs in parallel on a cluster of virtual machines. Also in September, Hail team began work on a; system called ""CI"" (Continuous Integration). CI runs the tests for every pull request (PR) into the; `main` branch of [`hail-is/hail`](https://github.com/hail-is/hail). CI automatically merges into; m",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:4694,Integrability,interface,interface,4694,"ysis on a single computer was; prohibitively time-consuming. Moreover, partitioning the dataset and analyzing each partition; separately necessitated increasingly complex software engineering. We started the Hail project to; re-enable simple and interactive (i.e. fast) analysis of these large datasets. Hail was a command-line program that used Apache Spark to run analysis on partitioned genetic; datasets simultaneously using hundreds of computer cores. To use Hail, a user needs an Apache Spark; cluster. Most Hail users use Google Dataproc Spark clusters. The essential feature of a human genetic dataset is a two-dimensional matrix of human; genotypes. Every genotype has the property ""number of alternate alleles"". This property allows a; matrix of genotypes to be represented as a numeric matrix. Geneticists use linear algebraic; techniques on this numeric matrix to understand the relationship between human disease and human; genetics. In November of 2016, the Hail command-line interface was eliminated and a Python interface was; introduced. During this time, Hail was not versioned. Users had to build and use Hail directly from; the source code repository. In March of 2017, Hail team began work on a compiler. In October of 2018, the Hail Python interface was modified, in a backwards-incompatible way. This; new Python interface was named ""Hail 0.2"". The old Python interface was retroactively named ""Hail; 0.1"". Hail team began deploying a Python package named `hail` to [PyPI](https://pypi.org). The Hail; Python package version complies with [Semantic Versioning](https://semver.org). Meanwhile, in September of 2018, Hail team began work on a system called ""Batch"". Batch runs; programs in parallel on a cluster of virtual machines. Also in September, Hail team began work on a; system called ""CI"" (Continuous Integration). CI runs the tests for every pull request (PR) into the; `main` branch of [`hail-is/hail`](https://github.com/hail-is/hail). CI automatically merges into; m",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:4928,Integrability,interface,interface,4928,"nalysis of these large datasets. Hail was a command-line program that used Apache Spark to run analysis on partitioned genetic; datasets simultaneously using hundreds of computer cores. To use Hail, a user needs an Apache Spark; cluster. Most Hail users use Google Dataproc Spark clusters. The essential feature of a human genetic dataset is a two-dimensional matrix of human; genotypes. Every genotype has the property ""number of alternate alleles"". This property allows a; matrix of genotypes to be represented as a numeric matrix. Geneticists use linear algebraic; techniques on this numeric matrix to understand the relationship between human disease and human; genetics. In November of 2016, the Hail command-line interface was eliminated and a Python interface was; introduced. During this time, Hail was not versioned. Users had to build and use Hail directly from; the source code repository. In March of 2017, Hail team began work on a compiler. In October of 2018, the Hail Python interface was modified, in a backwards-incompatible way. This; new Python interface was named ""Hail 0.2"". The old Python interface was retroactively named ""Hail; 0.1"". Hail team began deploying a Python package named `hail` to [PyPI](https://pypi.org). The Hail; Python package version complies with [Semantic Versioning](https://semver.org). Meanwhile, in September of 2018, Hail team began work on a system called ""Batch"". Batch runs; programs in parallel on a cluster of virtual machines. Also in September, Hail team began work on a; system called ""CI"" (Continuous Integration). CI runs the tests for every pull request (PR) into the; `main` branch of [`hail-is/hail`](https://github.com/hail-is/hail). CI automatically merges into; main any pull request that both passes the tests and has at least one ""approving"" review and no; ""changes requested"" reviews. CI uses Hail Batch to run the tests. Around this time, the Hail team organized itself into two sub-teams: ""compilers"" team and ""services""; team. Th",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:5002,Integrability,interface,interface,5002,"used Apache Spark to run analysis on partitioned genetic; datasets simultaneously using hundreds of computer cores. To use Hail, a user needs an Apache Spark; cluster. Most Hail users use Google Dataproc Spark clusters. The essential feature of a human genetic dataset is a two-dimensional matrix of human; genotypes. Every genotype has the property ""number of alternate alleles"". This property allows a; matrix of genotypes to be represented as a numeric matrix. Geneticists use linear algebraic; techniques on this numeric matrix to understand the relationship between human disease and human; genetics. In November of 2016, the Hail command-line interface was eliminated and a Python interface was; introduced. During this time, Hail was not versioned. Users had to build and use Hail directly from; the source code repository. In March of 2017, Hail team began work on a compiler. In October of 2018, the Hail Python interface was modified, in a backwards-incompatible way. This; new Python interface was named ""Hail 0.2"". The old Python interface was retroactively named ""Hail; 0.1"". Hail team began deploying a Python package named `hail` to [PyPI](https://pypi.org). The Hail; Python package version complies with [Semantic Versioning](https://semver.org). Meanwhile, in September of 2018, Hail team began work on a system called ""Batch"". Batch runs; programs in parallel on a cluster of virtual machines. Also in September, Hail team began work on a; system called ""CI"" (Continuous Integration). CI runs the tests for every pull request (PR) into the; `main` branch of [`hail-is/hail`](https://github.com/hail-is/hail). CI automatically merges into; main any pull request that both passes the tests and has at least one ""approving"" review and no; ""changes requested"" reviews. CI uses Hail Batch to run the tests. Around this time, the Hail team organized itself into two sub-teams: ""compilers"" team and ""services""; team. The compilers team is responsible for the Hail Python library, the comp",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:5049,Integrability,interface,interface,5049,"c; datasets simultaneously using hundreds of computer cores. To use Hail, a user needs an Apache Spark; cluster. Most Hail users use Google Dataproc Spark clusters. The essential feature of a human genetic dataset is a two-dimensional matrix of human; genotypes. Every genotype has the property ""number of alternate alleles"". This property allows a; matrix of genotypes to be represented as a numeric matrix. Geneticists use linear algebraic; techniques on this numeric matrix to understand the relationship between human disease and human; genetics. In November of 2016, the Hail command-line interface was eliminated and a Python interface was; introduced. During this time, Hail was not versioned. Users had to build and use Hail directly from; the source code repository. In March of 2017, Hail team began work on a compiler. In October of 2018, the Hail Python interface was modified, in a backwards-incompatible way. This; new Python interface was named ""Hail 0.2"". The old Python interface was retroactively named ""Hail; 0.1"". Hail team began deploying a Python package named `hail` to [PyPI](https://pypi.org). The Hail; Python package version complies with [Semantic Versioning](https://semver.org). Meanwhile, in September of 2018, Hail team began work on a system called ""Batch"". Batch runs; programs in parallel on a cluster of virtual machines. Also in September, Hail team began work on a; system called ""CI"" (Continuous Integration). CI runs the tests for every pull request (PR) into the; `main` branch of [`hail-is/hail`](https://github.com/hail-is/hail). CI automatically merges into; main any pull request that both passes the tests and has at least one ""approving"" review and no; ""changes requested"" reviews. CI uses Hail Batch to run the tests. Around this time, the Hail team organized itself into two sub-teams: ""compilers"" team and ""services""; team. The compilers team is responsible for the Hail Python library, the compiler, and the; associated runtime. The compilers team co",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:8260,Integrability,depend,depend,8260,"micsDB/GenomicsDB)). A Hail Matrix Table can represent dense,; two-dimensional, homogeneous data. For example, datasets of Human genetic sequences, dense [numeric; matrices](https://en.wikipedia.org/wiki/Matrix_(mathematics)), and [astronomical; surveys](https://en.wikipedia.org/wiki/Astronomical_survey). Users use the Hail Query Python library to write a ""pipeline"" to analyze their data. The Python; library sends this pipeline to a compiler written in Scala. The compiler converts the pipeline into; an Apache Spark job or a Hail Batch job. A pipeline typically reads a dataset, processes it, and; either writes a new dataset or aggregates (e.g. computes the mean of a field). If a pipeline; aggregates, the resulting aggregated value is converted to a Python value and returned to the user. ### Hail Batch. Hail Batch is a system for executing arbitrary Linux programs. Each invocation of a program is; called a ""job"". Zero or more jobs comprise a Batch. Moreover, jobs may depend on the files written; by other jobs in the same Batch. The job dependencies are allowed to form any [directed, acyclic; graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph). Users create batches and jobs using a Python library: `hailtop.batch`. The [Batch Service](https://batch.hail.is) schedules jobs on an; [elastically](https://en.wikipedia.org/wiki/Elasticity_(cloud_computing)) sized group of virtual; machines. The virtual machines are often called ""batch workers"". The software that manages a single; virtual machine is also called ""the batch worker"". ### Hail FS. Hail FS is an filesystem-like interface which supports both local file systems and three of the; major cloud object stores: S3, ABS, and GCS. Upon this foundation, we have built tools for robustly; copying terabytes of data from one cloud to another. These APIs are also used pervasively by Hail Batch and Hail Query-on-Batch to read and write data to; the cloud object stores. ### Hail Variant Dataset. The Hail Variant Datase",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:8330,Integrability,depend,dependencies,8330,"neous data. For example, datasets of Human genetic sequences, dense [numeric; matrices](https://en.wikipedia.org/wiki/Matrix_(mathematics)), and [astronomical; surveys](https://en.wikipedia.org/wiki/Astronomical_survey). Users use the Hail Query Python library to write a ""pipeline"" to analyze their data. The Python; library sends this pipeline to a compiler written in Scala. The compiler converts the pipeline into; an Apache Spark job or a Hail Batch job. A pipeline typically reads a dataset, processes it, and; either writes a new dataset or aggregates (e.g. computes the mean of a field). If a pipeline; aggregates, the resulting aggregated value is converted to a Python value and returned to the user. ### Hail Batch. Hail Batch is a system for executing arbitrary Linux programs. Each invocation of a program is; called a ""job"". Zero or more jobs comprise a Batch. Moreover, jobs may depend on the files written; by other jobs in the same Batch. The job dependencies are allowed to form any [directed, acyclic; graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph). Users create batches and jobs using a Python library: `hailtop.batch`. The [Batch Service](https://batch.hail.is) schedules jobs on an; [elastically](https://en.wikipedia.org/wiki/Elasticity_(cloud_computing)) sized group of virtual; machines. The virtual machines are often called ""batch workers"". The software that manages a single; virtual machine is also called ""the batch worker"". ### Hail FS. Hail FS is an filesystem-like interface which supports both local file systems and three of the; major cloud object stores: S3, ABS, and GCS. Upon this foundation, we have built tools for robustly; copying terabytes of data from one cloud to another. These APIs are also used pervasively by Hail Batch and Hail Query-on-Batch to read and write data to; the cloud object stores. ### Hail Variant Dataset. The Hail Variant Dataset (VDS) and the [Scalable Variant Call Format; (SVCR)](https://www.biorxiv.org/conten",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:8884,Integrability,interface,interface,8884," (e.g. computes the mean of a field). If a pipeline; aggregates, the resulting aggregated value is converted to a Python value and returned to the user. ### Hail Batch. Hail Batch is a system for executing arbitrary Linux programs. Each invocation of a program is; called a ""job"". Zero or more jobs comprise a Batch. Moreover, jobs may depend on the files written; by other jobs in the same Batch. The job dependencies are allowed to form any [directed, acyclic; graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph). Users create batches and jobs using a Python library: `hailtop.batch`. The [Batch Service](https://batch.hail.is) schedules jobs on an; [elastically](https://en.wikipedia.org/wiki/Elasticity_(cloud_computing)) sized group of virtual; machines. The virtual machines are often called ""batch workers"". The software that manages a single; virtual machine is also called ""the batch worker"". ### Hail FS. Hail FS is an filesystem-like interface which supports both local file systems and three of the; major cloud object stores: S3, ABS, and GCS. Upon this foundation, we have built tools for robustly; copying terabytes of data from one cloud to another. These APIs are also used pervasively by Hail Batch and Hail Query-on-Batch to read and write data to; the cloud object stores. ### Hail Variant Dataset. The Hail Variant Dataset (VDS) and the [Scalable Variant Call Format; (SVCR)](https://www.biorxiv.org/content/10.1101/2024.01.09.574205v1) are a format and general; representation for sparsely storing a collection of genome sequences aligned to a reference. The VDS; is mergeable and losslessly preserves the information of a GVCF. The VDS Combiner is a robust,; horizontally-scalable-in-variants, and tree-scalable-in-samples tool for creating a VDS from one or; more GVCFs or VDSes. The VDS Combiner has been used with as many as 955,000 exomes, VDS is a; realization of the SVCR in terms of two Hail matrix tables. SVCR comprises two key sparse; representations. ",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:12840,Integrability,rout,route,12840,"LVM is the aspirational target of the Hail Query compiler; - Docker is our container image and run-time system; - MySQL is our SQL database of choice. ### Services Technology. We almost exclusively write services in Python 3.9. We use a number of Python packages:; - [`asyncio`](https://docs.python.org/3.9/library/asyncio.html) for concurrency which is built on; [coroutines](https://en.wikipedia.org/wiki/Coroutine) not threads; - [`aiohttp`](https://docs.aiohttp.org/en/stable/) for serving HTTPS requests (most services speak; HTTPS); - [`jinja2`](https://jinja.palletsprojects.com/en/2.11.x/) for ""templating"" which simply means; programmatically generating text files. A service is realized as:. - a Docker image containing the executable code for the service; - a Kubernetes deployment (which defines the command to run, how much CPU is needed, what; environment variables are set, etc.); - a Kubernetes service (which defines which ports are accessible); - possibly a database within our Cloud SQL instance. We call a complete, working set of all services and databases a ""namespace"". Every namespace; corresponds to a Kubernetes namespace. All namespaces share one CloudSQL instance, but only have; access to their databases. The default namespace contains our ""production"" services and is accessible to the outside world at; https://hail.is, https://batch.hail.is, etc. Every developer and every pull request test job also has a namespace. Developer namespaces are; accessible at https://internal.hail.is/DEVELOPER_USERNAME/SERVICE/ . Unlike the default namespace,; every other namespace has exactly one database (containing all tables from each service's database). All incoming traffic passes through either gateway or internal-gateway which route requests to; the appropriate namespace and service. Traffic from the Internet enters the cluster through gateway,; while traffic from Batch workers enters through internal-gateway. [^1]: https://www.who.int/genomics/geneticsVSgenomics/en/; ",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:11956,Modifiability,variab,variables,11956," Hail Query compiler and run-time; - the JVM is the legacy target of the Hail Query compiler; - C++ is the aspirational language of high-performance services and the Hail Query compiler and run-time; - LLVM is the aspirational target of the Hail Query compiler; - Docker is our container image and run-time system; - MySQL is our SQL database of choice. ### Services Technology. We almost exclusively write services in Python 3.9. We use a number of Python packages:; - [`asyncio`](https://docs.python.org/3.9/library/asyncio.html) for concurrency which is built on; [coroutines](https://en.wikipedia.org/wiki/Coroutine) not threads; - [`aiohttp`](https://docs.aiohttp.org/en/stable/) for serving HTTPS requests (most services speak; HTTPS); - [`jinja2`](https://jinja.palletsprojects.com/en/2.11.x/) for ""templating"" which simply means; programmatically generating text files. A service is realized as:. - a Docker image containing the executable code for the service; - a Kubernetes deployment (which defines the command to run, how much CPU is needed, what; environment variables are set, etc.); - a Kubernetes service (which defines which ports are accessible); - possibly a database within our Cloud SQL instance. We call a complete, working set of all services and databases a ""namespace"". Every namespace; corresponds to a Kubernetes namespace. All namespaces share one CloudSQL instance, but only have; access to their databases. The default namespace contains our ""production"" services and is accessible to the outside world at; https://hail.is, https://batch.hail.is, etc. Every developer and every pull request test job also has a namespace. Developer namespaces are; accessible at https://internal.hail.is/DEVELOPER_USERNAME/SERVICE/ . Unlike the default namespace,; every other namespace has exactly one database (containing all tables from each service's database). All incoming traffic passes through either gateway or internal-gateway which route requests to; the appropriate namespac",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:9634,Performance,scalab,scalable-in-variants,9634,"sized group of virtual; machines. The virtual machines are often called ""batch workers"". The software that manages a single; virtual machine is also called ""the batch worker"". ### Hail FS. Hail FS is an filesystem-like interface which supports both local file systems and three of the; major cloud object stores: S3, ABS, and GCS. Upon this foundation, we have built tools for robustly; copying terabytes of data from one cloud to another. These APIs are also used pervasively by Hail Batch and Hail Query-on-Batch to read and write data to; the cloud object stores. ### Hail Variant Dataset. The Hail Variant Dataset (VDS) and the [Scalable Variant Call Format; (SVCR)](https://www.biorxiv.org/content/10.1101/2024.01.09.574205v1) are a format and general; representation for sparsely storing a collection of genome sequences aligned to a reference. The VDS; is mergeable and losslessly preserves the information of a GVCF. The VDS Combiner is a robust,; horizontally-scalable-in-variants, and tree-scalable-in-samples tool for creating a VDS from one or; more GVCFs or VDSes. The VDS Combiner has been used with as many as 955,000 exomes, VDS is a; realization of the SVCR in terms of two Hail matrix tables. SVCR comprises two key sparse; representations. First, allele-indexed arrays (such as the depth per allele, AD) elide entries for; unobserved alleles. Second, homozygous reference calls are run-length encoded. ## Infrastructure and Technology. The Hail team does not maintain any physical computer infrastructure. We maintain some virtual infrastructure, almost exclusively within the Google Cloud Platform (GCP). These include:; - a [Kubernetes](https://kubernetes.io) (k8s) cluster called `vdc` (Virtual Data Center); - many Google Cloud Storage ([an object store](https://en.wikipedia.org/wiki/Object_storage)) buckets; - one Cloud SQL instance with a production database, ephemeral pull-request-test databases, and a; database per developer; - logs for basically anything can be found i",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:9665,Performance,scalab,scalable-in-samples,9665,"sized group of virtual; machines. The virtual machines are often called ""batch workers"". The software that manages a single; virtual machine is also called ""the batch worker"". ### Hail FS. Hail FS is an filesystem-like interface which supports both local file systems and three of the; major cloud object stores: S3, ABS, and GCS. Upon this foundation, we have built tools for robustly; copying terabytes of data from one cloud to another. These APIs are also used pervasively by Hail Batch and Hail Query-on-Batch to read and write data to; the cloud object stores. ### Hail Variant Dataset. The Hail Variant Dataset (VDS) and the [Scalable Variant Call Format; (SVCR)](https://www.biorxiv.org/content/10.1101/2024.01.09.574205v1) are a format and general; representation for sparsely storing a collection of genome sequences aligned to a reference. The VDS; is mergeable and losslessly preserves the information of a GVCF. The VDS Combiner is a robust,; horizontally-scalable-in-variants, and tree-scalable-in-samples tool for creating a VDS from one or; more GVCFs or VDSes. The VDS Combiner has been used with as many as 955,000 exomes, VDS is a; realization of the SVCR in terms of two Hail matrix tables. SVCR comprises two key sparse; representations. First, allele-indexed arrays (such as the depth per allele, AD) elide entries for; unobserved alleles. Second, homozygous reference calls are run-length encoded. ## Infrastructure and Technology. The Hail team does not maintain any physical computer infrastructure. We maintain some virtual infrastructure, almost exclusively within the Google Cloud Platform (GCP). These include:; - a [Kubernetes](https://kubernetes.io) (k8s) cluster called `vdc` (Virtual Data Center); - many Google Cloud Storage ([an object store](https://en.wikipedia.org/wiki/Object_storage)) buckets; - one Cloud SQL instance with a production database, ephemeral pull-request-test databases, and a; database per developer; - logs for basically anything can be found i",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:11020,Performance,perform,performance,11020,"llele, AD) elide entries for; unobserved alleles. Second, homozygous reference calls are run-length encoded. ## Infrastructure and Technology. The Hail team does not maintain any physical computer infrastructure. We maintain some virtual infrastructure, almost exclusively within the Google Cloud Platform (GCP). These include:; - a [Kubernetes](https://kubernetes.io) (k8s) cluster called `vdc` (Virtual Data Center); - many Google Cloud Storage ([an object store](https://en.wikipedia.org/wiki/Object_storage)) buckets; - one Cloud SQL instance with a production database, ephemeral pull-request-test databases, and a; database per developer; - logs for basically anything can be found in [GCP Logs](https://console.cloud.google.com/logs). We use a number of technologies:; - Python is the language of choice for web applications, services, and anything user-facing; - Scala is the legacy language of the Hail Query compiler and run-time; - the JVM is the legacy target of the Hail Query compiler; - C++ is the aspirational language of high-performance services and the Hail Query compiler and run-time; - LLVM is the aspirational target of the Hail Query compiler; - Docker is our container image and run-time system; - MySQL is our SQL database of choice. ### Services Technology. We almost exclusively write services in Python 3.9. We use a number of Python packages:; - [`asyncio`](https://docs.python.org/3.9/library/asyncio.html) for concurrency which is built on; [coroutines](https://en.wikipedia.org/wiki/Coroutine) not threads; - [`aiohttp`](https://docs.aiohttp.org/en/stable/) for serving HTTPS requests (most services speak; HTTPS); - [`jinja2`](https://jinja.palletsprojects.com/en/2.11.x/) for ""templating"" which simply means; programmatically generating text files. A service is realized as:. - a Docker image containing the executable code for the service; - a Kubernetes deployment (which defines the command to run, how much CPU is needed, what; environment variables are set, et",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:11419,Performance,concurren,concurrency,11419,"](https://en.wikipedia.org/wiki/Object_storage)) buckets; - one Cloud SQL instance with a production database, ephemeral pull-request-test databases, and a; database per developer; - logs for basically anything can be found in [GCP Logs](https://console.cloud.google.com/logs). We use a number of technologies:; - Python is the language of choice for web applications, services, and anything user-facing; - Scala is the legacy language of the Hail Query compiler and run-time; - the JVM is the legacy target of the Hail Query compiler; - C++ is the aspirational language of high-performance services and the Hail Query compiler and run-time; - LLVM is the aspirational target of the Hail Query compiler; - Docker is our container image and run-time system; - MySQL is our SQL database of choice. ### Services Technology. We almost exclusively write services in Python 3.9. We use a number of Python packages:; - [`asyncio`](https://docs.python.org/3.9/library/asyncio.html) for concurrency which is built on; [coroutines](https://en.wikipedia.org/wiki/Coroutine) not threads; - [`aiohttp`](https://docs.aiohttp.org/en/stable/) for serving HTTPS requests (most services speak; HTTPS); - [`jinja2`](https://jinja.palletsprojects.com/en/2.11.x/) for ""templating"" which simply means; programmatically generating text files. A service is realized as:. - a Docker image containing the executable code for the service; - a Kubernetes deployment (which defines the command to run, how much CPU is needed, what; environment variables are set, etc.); - a Kubernetes service (which defines which ports are accessible); - possibly a database within our Cloud SQL instance. We call a complete, working set of all services and databases a ""namespace"". Every namespace; corresponds to a Kubernetes namespace. All namespaces share one CloudSQL instance, but only have; access to their databases. The default namespace contains our ""production"" services and is accessible to the outside world at; https://hail.is, http",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:6477,Security,access,access,6477,"a; system called ""CI"" (Continuous Integration). CI runs the tests for every pull request (PR) into the; `main` branch of [`hail-is/hail`](https://github.com/hail-is/hail). CI automatically merges into; main any pull request that both passes the tests and has at least one ""approving"" review and no; ""changes requested"" reviews. CI uses Hail Batch to run the tests. Around this time, the Hail team organized itself into two sub-teams: ""compilers"" team and ""services""; team. The compilers team is responsible for the Hail Python library, the compiler, and the; associated runtime. The compilers team code is entirely contained in the `hail` directory of the; `hail-is/hail` repository. The services team is responsible for Batch, CI, and the software; infrastructure that supports those services. Each service has its own directory in the hail; repository. More information about the structure of the repository can be found in; [`hail-overview.md`](hail-overview.md). Beginning in early 2020, beta users were given access to Hail Batch. In April of 2020, the Hail team began referring to the Hail Python library as ""Hail Query"". ""Hail; Query-on-Batch"" refers to the Hail Query Python library using Hail Batch to run an analysis across; many computer cores instead of using Apache Spark. ## Hail Products, Briefly. Hail team maintains four core produces. ### Hail Query. Hail Query is a Python library for the analysis of large datasets. In Hail Query, a dataset is; represented as Table or a Matrix Table. Hail Tables are similar to SQL tables, Pandas Dataframes, Excel spreadsheets, and CSV files. Hail Matrix Tables do not have analogues in most other systems. Perhaps the only analogue is; [SciDB](https://dbdb.io/db/scidb) and its descendants: [TileDB](https://tiledb.com) and; [GenomicsDB](https://github.com/GenomicsDB/GenomicsDB)). A Hail Matrix Table can represent dense,; two-dimensional, homogeneous data. For example, datasets of Human genetic sequences, dense [numeric; matrices](https://en",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:12036,Security,access,accessible,12036," and the Hail Query compiler and run-time; - LLVM is the aspirational target of the Hail Query compiler; - Docker is our container image and run-time system; - MySQL is our SQL database of choice. ### Services Technology. We almost exclusively write services in Python 3.9. We use a number of Python packages:; - [`asyncio`](https://docs.python.org/3.9/library/asyncio.html) for concurrency which is built on; [coroutines](https://en.wikipedia.org/wiki/Coroutine) not threads; - [`aiohttp`](https://docs.aiohttp.org/en/stable/) for serving HTTPS requests (most services speak; HTTPS); - [`jinja2`](https://jinja.palletsprojects.com/en/2.11.x/) for ""templating"" which simply means; programmatically generating text files. A service is realized as:. - a Docker image containing the executable code for the service; - a Kubernetes deployment (which defines the command to run, how much CPU is needed, what; environment variables are set, etc.); - a Kubernetes service (which defines which ports are accessible); - possibly a database within our Cloud SQL instance. We call a complete, working set of all services and databases a ""namespace"". Every namespace; corresponds to a Kubernetes namespace. All namespaces share one CloudSQL instance, but only have; access to their databases. The default namespace contains our ""production"" services and is accessible to the outside world at; https://hail.is, https://batch.hail.is, etc. Every developer and every pull request test job also has a namespace. Developer namespaces are; accessible at https://internal.hail.is/DEVELOPER_USERNAME/SERVICE/ . Unlike the default namespace,; every other namespace has exactly one database (containing all tables from each service's database). All incoming traffic passes through either gateway or internal-gateway which route requests to; the appropriate namespace and service. Traffic from the Internet enters the cluster through gateway,; while traffic from Batch workers enters through internal-gateway. [^1]: https:/",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:12294,Security,access,access,12294,"LVM is the aspirational target of the Hail Query compiler; - Docker is our container image and run-time system; - MySQL is our SQL database of choice. ### Services Technology. We almost exclusively write services in Python 3.9. We use a number of Python packages:; - [`asyncio`](https://docs.python.org/3.9/library/asyncio.html) for concurrency which is built on; [coroutines](https://en.wikipedia.org/wiki/Coroutine) not threads; - [`aiohttp`](https://docs.aiohttp.org/en/stable/) for serving HTTPS requests (most services speak; HTTPS); - [`jinja2`](https://jinja.palletsprojects.com/en/2.11.x/) for ""templating"" which simply means; programmatically generating text files. A service is realized as:. - a Docker image containing the executable code for the service; - a Kubernetes deployment (which defines the command to run, how much CPU is needed, what; environment variables are set, etc.); - a Kubernetes service (which defines which ports are accessible); - possibly a database within our Cloud SQL instance. We call a complete, working set of all services and databases a ""namespace"". Every namespace; corresponds to a Kubernetes namespace. All namespaces share one CloudSQL instance, but only have; access to their databases. The default namespace contains our ""production"" services and is accessible to the outside world at; https://hail.is, https://batch.hail.is, etc. Every developer and every pull request test job also has a namespace. Developer namespaces are; accessible at https://internal.hail.is/DEVELOPER_USERNAME/SERVICE/ . Unlike the default namespace,; every other namespace has exactly one database (containing all tables from each service's database). All incoming traffic passes through either gateway or internal-gateway which route requests to; the appropriate namespace and service. Traffic from the Internet enters the cluster through gateway,; while traffic from Batch workers enters through internal-gateway. [^1]: https://www.who.int/genomics/geneticsVSgenomics/en/; ",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:12385,Security,access,accessible,12385,"LVM is the aspirational target of the Hail Query compiler; - Docker is our container image and run-time system; - MySQL is our SQL database of choice. ### Services Technology. We almost exclusively write services in Python 3.9. We use a number of Python packages:; - [`asyncio`](https://docs.python.org/3.9/library/asyncio.html) for concurrency which is built on; [coroutines](https://en.wikipedia.org/wiki/Coroutine) not threads; - [`aiohttp`](https://docs.aiohttp.org/en/stable/) for serving HTTPS requests (most services speak; HTTPS); - [`jinja2`](https://jinja.palletsprojects.com/en/2.11.x/) for ""templating"" which simply means; programmatically generating text files. A service is realized as:. - a Docker image containing the executable code for the service; - a Kubernetes deployment (which defines the command to run, how much CPU is needed, what; environment variables are set, etc.); - a Kubernetes service (which defines which ports are accessible); - possibly a database within our Cloud SQL instance. We call a complete, working set of all services and databases a ""namespace"". Every namespace; corresponds to a Kubernetes namespace. All namespaces share one CloudSQL instance, but only have; access to their databases. The default namespace contains our ""production"" services and is accessible to the outside world at; https://hail.is, https://batch.hail.is, etc. Every developer and every pull request test job also has a namespace. Developer namespaces are; accessible at https://internal.hail.is/DEVELOPER_USERNAME/SERVICE/ . Unlike the default namespace,; every other namespace has exactly one database (containing all tables from each service's database). All incoming traffic passes through either gateway or internal-gateway which route requests to; the appropriate namespace and service. Traffic from the Internet enters the cluster through gateway,; while traffic from Batch workers enters through internal-gateway. [^1]: https://www.who.int/genomics/geneticsVSgenomics/en/; ",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:12562,Security,access,accessible,12562,"LVM is the aspirational target of the Hail Query compiler; - Docker is our container image and run-time system; - MySQL is our SQL database of choice. ### Services Technology. We almost exclusively write services in Python 3.9. We use a number of Python packages:; - [`asyncio`](https://docs.python.org/3.9/library/asyncio.html) for concurrency which is built on; [coroutines](https://en.wikipedia.org/wiki/Coroutine) not threads; - [`aiohttp`](https://docs.aiohttp.org/en/stable/) for serving HTTPS requests (most services speak; HTTPS); - [`jinja2`](https://jinja.palletsprojects.com/en/2.11.x/) for ""templating"" which simply means; programmatically generating text files. A service is realized as:. - a Docker image containing the executable code for the service; - a Kubernetes deployment (which defines the command to run, how much CPU is needed, what; environment variables are set, etc.); - a Kubernetes service (which defines which ports are accessible); - possibly a database within our Cloud SQL instance. We call a complete, working set of all services and databases a ""namespace"". Every namespace; corresponds to a Kubernetes namespace. All namespaces share one CloudSQL instance, but only have; access to their databases. The default namespace contains our ""production"" services and is accessible to the outside world at; https://hail.is, https://batch.hail.is, etc. Every developer and every pull request test job also has a namespace. Developer namespaces are; accessible at https://internal.hail.is/DEVELOPER_USERNAME/SERVICE/ . Unlike the default namespace,; every other namespace has exactly one database (containing all tables from each service's database). All incoming traffic passes through either gateway or internal-gateway which route requests to; the appropriate namespace and service. Traffic from the Internet enters the cluster through gateway,; while traffic from Batch workers enters through internal-gateway. [^1]: https://www.who.int/genomics/geneticsVSgenomics/en/; ",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:5523,Testability,test,tests,5523,"onship between human disease and human; genetics. In November of 2016, the Hail command-line interface was eliminated and a Python interface was; introduced. During this time, Hail was not versioned. Users had to build and use Hail directly from; the source code repository. In March of 2017, Hail team began work on a compiler. In October of 2018, the Hail Python interface was modified, in a backwards-incompatible way. This; new Python interface was named ""Hail 0.2"". The old Python interface was retroactively named ""Hail; 0.1"". Hail team began deploying a Python package named `hail` to [PyPI](https://pypi.org). The Hail; Python package version complies with [Semantic Versioning](https://semver.org). Meanwhile, in September of 2018, Hail team began work on a system called ""Batch"". Batch runs; programs in parallel on a cluster of virtual machines. Also in September, Hail team began work on a; system called ""CI"" (Continuous Integration). CI runs the tests for every pull request (PR) into the; `main` branch of [`hail-is/hail`](https://github.com/hail-is/hail). CI automatically merges into; main any pull request that both passes the tests and has at least one ""approving"" review and no; ""changes requested"" reviews. CI uses Hail Batch to run the tests. Around this time, the Hail team organized itself into two sub-teams: ""compilers"" team and ""services""; team. The compilers team is responsible for the Hail Python library, the compiler, and the; associated runtime. The compilers team code is entirely contained in the `hail` directory of the; `hail-is/hail` repository. The services team is responsible for Batch, CI, and the software; infrastructure that supports those services. Each service has its own directory in the hail; repository. More information about the structure of the repository can be found in; [`hail-overview.md`](hail-overview.md). Beginning in early 2020, beta users were given access to Hail Batch. In April of 2020, the Hail team began referring to the Hail Pyth",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:5708,Testability,test,tests,5708,"roduced. During this time, Hail was not versioned. Users had to build and use Hail directly from; the source code repository. In March of 2017, Hail team began work on a compiler. In October of 2018, the Hail Python interface was modified, in a backwards-incompatible way. This; new Python interface was named ""Hail 0.2"". The old Python interface was retroactively named ""Hail; 0.1"". Hail team began deploying a Python package named `hail` to [PyPI](https://pypi.org). The Hail; Python package version complies with [Semantic Versioning](https://semver.org). Meanwhile, in September of 2018, Hail team began work on a system called ""Batch"". Batch runs; programs in parallel on a cluster of virtual machines. Also in September, Hail team began work on a; system called ""CI"" (Continuous Integration). CI runs the tests for every pull request (PR) into the; `main` branch of [`hail-is/hail`](https://github.com/hail-is/hail). CI automatically merges into; main any pull request that both passes the tests and has at least one ""approving"" review and no; ""changes requested"" reviews. CI uses Hail Batch to run the tests. Around this time, the Hail team organized itself into two sub-teams: ""compilers"" team and ""services""; team. The compilers team is responsible for the Hail Python library, the compiler, and the; associated runtime. The compilers team code is entirely contained in the `hail` directory of the; `hail-is/hail` repository. The services team is responsible for Batch, CI, and the software; infrastructure that supports those services. Each service has its own directory in the hail; repository. More information about the structure of the repository can be found in; [`hail-overview.md`](hail-overview.md). Beginning in early 2020, beta users were given access to Hail Batch. In April of 2020, the Hail team began referring to the Hail Python library as ""Hail Query"". ""Hail; Query-on-Batch"" refers to the Hail Query Python library using Hail Batch to run an analysis across; many computer ",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:5821,Testability,test,tests,5821,"; the source code repository. In March of 2017, Hail team began work on a compiler. In October of 2018, the Hail Python interface was modified, in a backwards-incompatible way. This; new Python interface was named ""Hail 0.2"". The old Python interface was retroactively named ""Hail; 0.1"". Hail team began deploying a Python package named `hail` to [PyPI](https://pypi.org). The Hail; Python package version complies with [Semantic Versioning](https://semver.org). Meanwhile, in September of 2018, Hail team began work on a system called ""Batch"". Batch runs; programs in parallel on a cluster of virtual machines. Also in September, Hail team began work on a; system called ""CI"" (Continuous Integration). CI runs the tests for every pull request (PR) into the; `main` branch of [`hail-is/hail`](https://github.com/hail-is/hail). CI automatically merges into; main any pull request that both passes the tests and has at least one ""approving"" review and no; ""changes requested"" reviews. CI uses Hail Batch to run the tests. Around this time, the Hail team organized itself into two sub-teams: ""compilers"" team and ""services""; team. The compilers team is responsible for the Hail Python library, the compiler, and the; associated runtime. The compilers team code is entirely contained in the `hail` directory of the; `hail-is/hail` repository. The services team is responsible for Batch, CI, and the software; infrastructure that supports those services. Each service has its own directory in the hail; repository. More information about the structure of the repository can be found in; [`hail-overview.md`](hail-overview.md). Beginning in early 2020, beta users were given access to Hail Batch. In April of 2020, the Hail team began referring to the Hail Python library as ""Hail Query"". ""Hail; Query-on-Batch"" refers to the Hail Query Python library using Hail Batch to run an analysis across; many computer cores instead of using Apache Spark. ## Hail Products, Briefly. Hail team maintains four core pro",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:10575,Testability,test,test,10575,"of a GVCF. The VDS Combiner is a robust,; horizontally-scalable-in-variants, and tree-scalable-in-samples tool for creating a VDS from one or; more GVCFs or VDSes. The VDS Combiner has been used with as many as 955,000 exomes, VDS is a; realization of the SVCR in terms of two Hail matrix tables. SVCR comprises two key sparse; representations. First, allele-indexed arrays (such as the depth per allele, AD) elide entries for; unobserved alleles. Second, homozygous reference calls are run-length encoded. ## Infrastructure and Technology. The Hail team does not maintain any physical computer infrastructure. We maintain some virtual infrastructure, almost exclusively within the Google Cloud Platform (GCP). These include:; - a [Kubernetes](https://kubernetes.io) (k8s) cluster called `vdc` (Virtual Data Center); - many Google Cloud Storage ([an object store](https://en.wikipedia.org/wiki/Object_storage)) buckets; - one Cloud SQL instance with a production database, ephemeral pull-request-test databases, and a; database per developer; - logs for basically anything can be found in [GCP Logs](https://console.cloud.google.com/logs). We use a number of technologies:; - Python is the language of choice for web applications, services, and anything user-facing; - Scala is the legacy language of the Hail Query compiler and run-time; - the JVM is the legacy target of the Hail Query compiler; - C++ is the aspirational language of high-performance services and the Hail Query compiler and run-time; - LLVM is the aspirational target of the Hail Query compiler; - Docker is our container image and run-time system; - MySQL is our SQL database of choice. ### Services Technology. We almost exclusively write services in Python 3.9. We use a number of Python packages:; - [`asyncio`](https://docs.python.org/3.9/library/asyncio.html) for concurrency which is built on; [coroutines](https://en.wikipedia.org/wiki/Coroutine) not threads; - [`aiohttp`](https://docs.aiohttp.org/en/stable/) for serving ",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:10624,Testability,log,logs,10624,"of a GVCF. The VDS Combiner is a robust,; horizontally-scalable-in-variants, and tree-scalable-in-samples tool for creating a VDS from one or; more GVCFs or VDSes. The VDS Combiner has been used with as many as 955,000 exomes, VDS is a; realization of the SVCR in terms of two Hail matrix tables. SVCR comprises two key sparse; representations. First, allele-indexed arrays (such as the depth per allele, AD) elide entries for; unobserved alleles. Second, homozygous reference calls are run-length encoded. ## Infrastructure and Technology. The Hail team does not maintain any physical computer infrastructure. We maintain some virtual infrastructure, almost exclusively within the Google Cloud Platform (GCP). These include:; - a [Kubernetes](https://kubernetes.io) (k8s) cluster called `vdc` (Virtual Data Center); - many Google Cloud Storage ([an object store](https://en.wikipedia.org/wiki/Object_storage)) buckets; - one Cloud SQL instance with a production database, ephemeral pull-request-test databases, and a; database per developer; - logs for basically anything can be found in [GCP Logs](https://console.cloud.google.com/logs). We use a number of technologies:; - Python is the language of choice for web applications, services, and anything user-facing; - Scala is the legacy language of the Hail Query compiler and run-time; - the JVM is the legacy target of the Hail Query compiler; - C++ is the aspirational language of high-performance services and the Hail Query compiler and run-time; - LLVM is the aspirational target of the Hail Query compiler; - Docker is our container image and run-time system; - MySQL is our SQL database of choice. ### Services Technology. We almost exclusively write services in Python 3.9. We use a number of Python packages:; - [`asyncio`](https://docs.python.org/3.9/library/asyncio.html) for concurrency which is built on; [coroutines](https://en.wikipedia.org/wiki/Coroutine) not threads; - [`aiohttp`](https://docs.aiohttp.org/en/stable/) for serving ",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:10712,Testability,log,logs,10712," one or; more GVCFs or VDSes. The VDS Combiner has been used with as many as 955,000 exomes, VDS is a; realization of the SVCR in terms of two Hail matrix tables. SVCR comprises two key sparse; representations. First, allele-indexed arrays (such as the depth per allele, AD) elide entries for; unobserved alleles. Second, homozygous reference calls are run-length encoded. ## Infrastructure and Technology. The Hail team does not maintain any physical computer infrastructure. We maintain some virtual infrastructure, almost exclusively within the Google Cloud Platform (GCP). These include:; - a [Kubernetes](https://kubernetes.io) (k8s) cluster called `vdc` (Virtual Data Center); - many Google Cloud Storage ([an object store](https://en.wikipedia.org/wiki/Object_storage)) buckets; - one Cloud SQL instance with a production database, ephemeral pull-request-test databases, and a; database per developer; - logs for basically anything can be found in [GCP Logs](https://console.cloud.google.com/logs). We use a number of technologies:; - Python is the language of choice for web applications, services, and anything user-facing; - Scala is the legacy language of the Hail Query compiler and run-time; - the JVM is the legacy target of the Hail Query compiler; - C++ is the aspirational language of high-performance services and the Hail Query compiler and run-time; - LLVM is the aspirational target of the Hail Query compiler; - Docker is our container image and run-time system; - MySQL is our SQL database of choice. ### Services Technology. We almost exclusively write services in Python 3.9. We use a number of Python packages:; - [`asyncio`](https://docs.python.org/3.9/library/asyncio.html) for concurrency which is built on; [coroutines](https://en.wikipedia.org/wiki/Coroutine) not threads; - [`aiohttp`](https://docs.aiohttp.org/en/stable/) for serving HTTPS requests (most services speak; HTTPS); - [`jinja2`](https://jinja.palletsprojects.com/en/2.11.x/) for ""templating"" which simply",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:12505,Testability,test,test,12505,"LVM is the aspirational target of the Hail Query compiler; - Docker is our container image and run-time system; - MySQL is our SQL database of choice. ### Services Technology. We almost exclusively write services in Python 3.9. We use a number of Python packages:; - [`asyncio`](https://docs.python.org/3.9/library/asyncio.html) for concurrency which is built on; [coroutines](https://en.wikipedia.org/wiki/Coroutine) not threads; - [`aiohttp`](https://docs.aiohttp.org/en/stable/) for serving HTTPS requests (most services speak; HTTPS); - [`jinja2`](https://jinja.palletsprojects.com/en/2.11.x/) for ""templating"" which simply means; programmatically generating text files. A service is realized as:. - a Docker image containing the executable code for the service; - a Kubernetes deployment (which defines the command to run, how much CPU is needed, what; environment variables are set, etc.); - a Kubernetes service (which defines which ports are accessible); - possibly a database within our Cloud SQL instance. We call a complete, working set of all services and databases a ""namespace"". Every namespace; corresponds to a Kubernetes namespace. All namespaces share one CloudSQL instance, but only have; access to their databases. The default namespace contains our ""production"" services and is accessible to the outside world at; https://hail.is, https://batch.hail.is, etc. Every developer and every pull request test job also has a namespace. Developer namespaces are; accessible at https://internal.hail.is/DEVELOPER_USERNAME/SERVICE/ . Unlike the default namespace,; every other namespace has exactly one database (containing all tables from each service's database). All incoming traffic passes through either gateway or internal-gateway which route requests to; the appropriate namespace and service. Traffic from the Internet enters the cluster through gateway,; while traffic from Batch workers enters through internal-gateway. [^1]: https://www.who.int/genomics/geneticsVSgenomics/en/; ",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:3359,Usability,simpl,simple,3359,":; the discovery of PCSK9. In 2013, the New York Times [reported on the; discovery](https://www.nytimes.com/2013/07/10/health/rare-mutation-prompts-race-for-cholesterol-drug.html); of an association between mutations in the PCSK9 gene and high levels of LDL cholesterol. By 2017,; [further; studies](https://www.nytimes.com/2017/03/17/health/cholesterol-drugs-repatha-amgen-pcsk9-inhibitors.html); demonstrated *remarkable* reduction in LDL cholesterol levels. Unfortunately, as late as 2020, these; drugs [remain extraordinarily; expensive](https://www.nytimes.com/2018/10/02/health/pcsk9-cholesterol-prices.html). Hail is also the computational foundation of the [Genome Aggregation; Database](https://gnomad.broadinstitute.org), gnomAD. gnomAD collects a large and diverse set of; human sequences (gnomAD v4 comprised 955,000 individuals) in order to characterize the statistical; distributions of properties of the human genome. Some properties are quite simple, such as the; frequency of a given allele, and some are quite complex such as the; [LOEUF](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7334197/). This information is used every day; by researchers *and* clinicians. ## A Brief History. Around 2015, human genetic datasets had grown so large that analysis on a single computer was; prohibitively time-consuming. Moreover, partitioning the dataset and analyzing each partition; separately necessitated increasingly complex software engineering. We started the Hail project to; re-enable simple and interactive (i.e. fast) analysis of these large datasets. Hail was a command-line program that used Apache Spark to run analysis on partitioned genetic; datasets simultaneously using hundreds of computer cores. To use Hail, a user needs an Apache Spark; cluster. Most Hail users use Google Dataproc Spark clusters. The essential feature of a human genetic dataset is a two-dimensional matrix of human; genotypes. Every genotype has the property ""number of alternate alleles"". This property",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:3901,Usability,simpl,simple,3901," these; drugs [remain extraordinarily; expensive](https://www.nytimes.com/2018/10/02/health/pcsk9-cholesterol-prices.html). Hail is also the computational foundation of the [Genome Aggregation; Database](https://gnomad.broadinstitute.org), gnomAD. gnomAD collects a large and diverse set of; human sequences (gnomAD v4 comprised 955,000 individuals) in order to characterize the statistical; distributions of properties of the human genome. Some properties are quite simple, such as the; frequency of a given allele, and some are quite complex such as the; [LOEUF](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7334197/). This information is used every day; by researchers *and* clinicians. ## A Brief History. Around 2015, human genetic datasets had grown so large that analysis on a single computer was; prohibitively time-consuming. Moreover, partitioning the dataset and analyzing each partition; separately necessitated increasingly complex software engineering. We started the Hail project to; re-enable simple and interactive (i.e. fast) analysis of these large datasets. Hail was a command-line program that used Apache Spark to run analysis on partitioned genetic; datasets simultaneously using hundreds of computer cores. To use Hail, a user needs an Apache Spark; cluster. Most Hail users use Google Dataproc Spark clusters. The essential feature of a human genetic dataset is a two-dimensional matrix of human; genotypes. Every genotype has the property ""number of alternate alleles"". This property allows a; matrix of genotypes to be represented as a numeric matrix. Geneticists use linear algebraic; techniques on this numeric matrix to understand the relationship between human disease and human; genetics. In November of 2016, the Hail command-line interface was eliminated and a Python interface was; introduced. During this time, Hail was not versioned. Users had to build and use Hail directly from; the source code repository. In March of 2017, Hail team began work on a compiler. I",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md:11707,Usability,simpl,simply,11707,"e use a number of technologies:; - Python is the language of choice for web applications, services, and anything user-facing; - Scala is the legacy language of the Hail Query compiler and run-time; - the JVM is the legacy target of the Hail Query compiler; - C++ is the aspirational language of high-performance services and the Hail Query compiler and run-time; - LLVM is the aspirational target of the Hail Query compiler; - Docker is our container image and run-time system; - MySQL is our SQL database of choice. ### Services Technology. We almost exclusively write services in Python 3.9. We use a number of Python packages:; - [`asyncio`](https://docs.python.org/3.9/library/asyncio.html) for concurrency which is built on; [coroutines](https://en.wikipedia.org/wiki/Coroutine) not threads; - [`aiohttp`](https://docs.aiohttp.org/en/stable/) for serving HTTPS requests (most services speak; HTTPS); - [`jinja2`](https://jinja.palletsprojects.com/en/2.11.x/) for ""templating"" which simply means; programmatically generating text files. A service is realized as:. - a Docker image containing the executable code for the service; - a Kubernetes deployment (which defines the command to run, how much CPU is needed, what; environment variables are set, etc.); - a Kubernetes service (which defines which ports are accessible); - possibly a database within our Cloud SQL instance. We call a complete, working set of all services and databases a ""namespace"". Every namespace; corresponds to a Kubernetes namespace. All namespaces share one CloudSQL instance, but only have; access to their databases. The default namespace contains our ""production"" services and is accessible to the outside world at; https://hail.is, https://batch.hail.is, etc. Every developer and every pull request test job also has a namespace. Developer namespaces are; accessible at https://internal.hail.is/DEVELOPER_USERNAME/SERVICE/ . Unlike the default namespace,; every other namespace has exactly one database (containing",MatchSource.DOCS,dev-docs/hail-for-new-engineers.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-for-new-engineers.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:2173,Availability,error,error,2173,"ation,; interacting with the Hail developer tools, and managing the Hail Batch; service. `hailctl` is implemented in the `hailtop.hailctl` submodule. Users can run Hail on their own system, or on the Hail service that is; operated by the Hail team. The Hail service is implemented by a; collection of microservices that are deployed on Kubernetes (K8s) and; Google Cloud Platform (GCP). We refer to K8s and GCP as our Virtual; Datacenter (VDC). ## Hail Query. The `hail` module is implemented in $HAIL/hail/python/hail, this is the Hail Query Python interface.; The Hail Query interface is lazy: executing a pipeline builds an intermediate representation; representing the query. The IR is implemented in the `hail.ir` submodule. When a query is ready to; be executed, it is sent to a backend, implemented in `hail.backend`. There are three backends:; SparkBackend, LocalBackend and ServiceBackend. At the time of writing, the SparkBackend is complete; and the ServiceBackend is mostly complete (it is missing a PCA implementation with clear error; bounds and some block matrix operations do not scale). The LocalBackend has languished. The Spark backend works as follows. The IR is serialized and sent to; a JVM child process via [py4j](https://www.py4j.org/). The entrypoint; for this is the Scala class is.hail.backend.spark.SparkBackend. The; SparkBackend parses the IR, runs a query optimizer on it, generates; custom JVM bytecode for the query which is called from a Spark; computational graph, which it then submits to Spark for execution.; The result is then returned via py4j back to Python and the user. The ServiceBackend is structured differently. The IR is again; serialized, but is written to cloud storage and a job is submitted; to Hail Batch to execute the aforementioned IR. The local backend is similar to the Spark backend, except the; execution is performed on the local machine in the JVM instead of; being submitted to Spark. ## Hail Batch. A batch is a graph of jobs with depen",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:5200,Availability,avail,available,5200," * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime values: as JVM; objects or as a pointer to explicitly managed memory off the Java; heap called a region value. Annotation also sometimes refer to just; the JVM object representation. Explicitly managed off-(Java-)heap; values are also referred to as ""unsafe"". * is.hail.asm4s: The Hail Query optimizer generates JVM bytecode to; implement queries. asm4s is a high-level Scala interface for; generating JVM bytecode. * is.hail.lir: lir is a low-level intermediate representation (IR) for; JVM bytecode. The high-level asm4s interface is implemented in; t",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:7335,Availability,resilien,resilient,7335,"ypes are usual-visible types like int,; array and string. Physical types are implementations of virtual; types. For example, arrays might be stored densely or sparsely.; Encoded types specify how to (de)serialize types for reading and; writing. There also higher-level types for Tables, MatrixTables and; BlockMatrices. * is.hail.expr: expr is a large, disorganized package that contains; the Hail Query IR, query optimizer and IR code generator. * is.hail.io.fs: fs contains an abstract filesystem interface, FS, and; implementations for Google Storage and Hadoop. We need to implement; one for local UNIX filesystems. * is.hail.io: This includes support for reading and writing a variety; of file formats. * is.hail.services: This package is related to the implementation of; services. This includes a Java/Scala client for Hail Batch, the; shuffle service client and server, and support for TLS,; authentication, deployment configuration, etc. * is.hail.rvd: The fundamental abstraction in Spark is the resilient; distributed dataset (RDD). When Hail generates Spark pipelines to; implement queries, it generates an RDD of off-(Java-)heap region; values. An RVD is a Region Value Dataset, an abstraction for a RDD; of region values. * is.hail.backend: each Python backend (Spark, local, service) has a; corresponding backend in Scala. ## Microservice Overview. The Hail service is implemented as a collection of microservices (for; the list, see below). The services are implemented in Python (mostly); and Java/Scala (or a mix, using py4j). The code for the services can; be found at the top-level, e.g. the batch and batch-driver; implementation can be found in $HAIL/batch. For publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using Python asyncio,; [aiohttp](https://docs.aiohttp.org/en/stable/) and Jinja2 for HTML; templating. Some services rely on 3rd party serv",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:8053,Availability,avail,available,8053,".services: This package is related to the implementation of; services. This includes a Java/Scala client for Hail Batch, the; shuffle service client and server, and support for TLS,; authentication, deployment configuration, etc. * is.hail.rvd: The fundamental abstraction in Spark is the resilient; distributed dataset (RDD). When Hail generates Spark pipelines to; implement queries, it generates an RDD of off-(Java-)heap region; values. An RVD is a Region Value Dataset, an abstraction for a RDD; of region values. * is.hail.backend: each Python backend (Spark, local, service) has a; corresponding backend in Scala. ## Microservice Overview. The Hail service is implemented as a collection of microservices (for; the list, see below). The services are implemented in Python (mostly); and Java/Scala (or a mix, using py4j). The code for the services can; be found at the top-level, e.g. the batch and batch-driver; implementation can be found in $HAIL/batch. For publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using Python asyncio,; [aiohttp](https://docs.aiohttp.org/en/stable/) and Jinja2 for HTML; templating. Some services rely on 3rd party services. Those include:. * ci depends on GitHub. * batch, ci and auth depend on K8s. * batch depends on K8s and GCP. * website depends (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:8121,Availability,avail,available,8121,"ncludes a Java/Scala client for Hail Batch, the; shuffle service client and server, and support for TLS,; authentication, deployment configuration, etc. * is.hail.rvd: The fundamental abstraction in Spark is the resilient; distributed dataset (RDD). When Hail generates Spark pipelines to; implement queries, it generates an RDD of off-(Java-)heap region; values. An RVD is a Region Value Dataset, an abstraction for a RDD; of region values. * is.hail.backend: each Python backend (Spark, local, service) has a; corresponding backend in Scala. ## Microservice Overview. The Hail service is implemented as a collection of microservices (for; the list, see below). The services are implemented in Python (mostly); and Java/Scala (or a mix, using py4j). The code for the services can; be found at the top-level, e.g. the batch and batch-driver; implementation can be found in $HAIL/batch. For publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using Python asyncio,; [aiohttp](https://docs.aiohttp.org/en/stable/) and Jinja2 for HTML; templating. Some services rely on 3rd party services. Those include:. * ci depends on GitHub. * batch, ci and auth depend on K8s. * batch depends on K8s and GCP. * website depends (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the servi",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:636,Deployability,deploy,deployed,636,"# Hail. This document gives an overview of the Hail architecture and source; code. ## Background. Hail source code is stored in a monolithic repository (monorepo) on; GitHub at hail-is/hail. $HAIL will denote the repository root below.; Hail is open source and developed in the open. Hail is written in Python, Java/Scala (both JVM langauges), and C/C++. Hail has two user-facing components: Hail Query and Hail Batch. Both; provide Python interfaces. Hail Query is for distributed, out-of-core; manipulation and analysis of tabular and genomic data. Hail Batch is; for the execution of graphs containers. The Hail client libraries are deployed in the Python Package Index; (PyPI) hail package. The Hail package exposes two Python modules:; `hail` and `hailtop`. `hail` provides the Hail Query interface.; `hailtop.batch` provides the batch interface. `hailtop` contains; various other infrastructure submodules (see below). The hail package also contains the `hailctl` command line utility that; provides a number of features, including managing Google Dataproc (a; hosted Spark service), Hail service authorization, Hail configuration,; interacting with the Hail developer tools, and managing the Hail Batch; service. `hailctl` is implemented in the `hailtop.hailctl` submodule. Users can run Hail on their own system, or on the Hail service that is; operated by the Hail team. The Hail service is implemented by a; collection of microservices that are deployed on Kubernetes (K8s) and; Google Cloud Platform (GCP). We refer to K8s and GCP as our Virtual; Datacenter (VDC). ## Hail Query. The `hail` module is implemented in $HAIL/hail/python/hail, this is the Hail Query Python interface.; The Hail Query interface is lazy: executing a pipeline builds an intermediate representation; representing the query. The IR is implemented in the `hail.ir` submodule. When a query is ready to; be executed, it is sent to a backend, implemented in `hail.backend`. There are three backends:; SparkBackend, Loca",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:1123,Deployability,configurat,configuration,1123,"urce; code. ## Background. Hail source code is stored in a monolithic repository (monorepo) on; GitHub at hail-is/hail. $HAIL will denote the repository root below.; Hail is open source and developed in the open. Hail is written in Python, Java/Scala (both JVM langauges), and C/C++. Hail has two user-facing components: Hail Query and Hail Batch. Both; provide Python interfaces. Hail Query is for distributed, out-of-core; manipulation and analysis of tabular and genomic data. Hail Batch is; for the execution of graphs containers. The Hail client libraries are deployed in the Python Package Index; (PyPI) hail package. The Hail package exposes two Python modules:; `hail` and `hailtop`. `hail` provides the Hail Query interface.; `hailtop.batch` provides the batch interface. `hailtop` contains; various other infrastructure submodules (see below). The hail package also contains the `hailctl` command line utility that; provides a number of features, including managing Google Dataproc (a; hosted Spark service), Hail service authorization, Hail configuration,; interacting with the Hail developer tools, and managing the Hail Batch; service. `hailctl` is implemented in the `hailtop.hailctl` submodule. Users can run Hail on their own system, or on the Hail service that is; operated by the Hail team. The Hail service is implemented by a; collection of microservices that are deployed on Kubernetes (K8s) and; Google Cloud Platform (GCP). We refer to K8s and GCP as our Virtual; Datacenter (VDC). ## Hail Query. The `hail` module is implemented in $HAIL/hail/python/hail, this is the Hail Query Python interface.; The Hail Query interface is lazy: executing a pipeline builds an intermediate representation; representing the query. The IR is implemented in the `hail.ir` submodule. When a query is ready to; be executed, it is sent to a backend, implemented in `hail.backend`. There are three backends:; SparkBackend, LocalBackend and ServiceBackend. At the time of writing, the SparkBackend i",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:1455,Deployability,deploy,deployed,1455,"es. Hail Query is for distributed, out-of-core; manipulation and analysis of tabular and genomic data. Hail Batch is; for the execution of graphs containers. The Hail client libraries are deployed in the Python Package Index; (PyPI) hail package. The Hail package exposes two Python modules:; `hail` and `hailtop`. `hail` provides the Hail Query interface.; `hailtop.batch` provides the batch interface. `hailtop` contains; various other infrastructure submodules (see below). The hail package also contains the `hailctl` command line utility that; provides a number of features, including managing Google Dataproc (a; hosted Spark service), Hail service authorization, Hail configuration,; interacting with the Hail developer tools, and managing the Hail Batch; service. `hailctl` is implemented in the `hailtop.hailctl` submodule. Users can run Hail on their own system, or on the Hail service that is; operated by the Hail team. The Hail service is implemented by a; collection of microservices that are deployed on Kubernetes (K8s) and; Google Cloud Platform (GCP). We refer to K8s and GCP as our Virtual; Datacenter (VDC). ## Hail Query. The `hail` module is implemented in $HAIL/hail/python/hail, this is the Hail Query Python interface.; The Hail Query interface is lazy: executing a pipeline builds an intermediate representation; representing the query. The IR is implemented in the `hail.ir` submodule. When a query is ready to; be executed, it is sent to a backend, implemented in `hail.backend`. There are three backends:; SparkBackend, LocalBackend and ServiceBackend. At the time of writing, the SparkBackend is complete; and the ServiceBackend is mostly complete (it is missing a PCA implementation with clear error; bounds and some block matrix operations do not scale). The LocalBackend has languished. The Spark backend works as follows. The IR is serialized and sent to; a JVM child process via [py4j](https://www.py4j.org/). The entrypoint; for this is the Scala class is.hail.bac",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:1739,Deployability,pipeline,pipeline,1739,"d `hailtop`. `hail` provides the Hail Query interface.; `hailtop.batch` provides the batch interface. `hailtop` contains; various other infrastructure submodules (see below). The hail package also contains the `hailctl` command line utility that; provides a number of features, including managing Google Dataproc (a; hosted Spark service), Hail service authorization, Hail configuration,; interacting with the Hail developer tools, and managing the Hail Batch; service. `hailctl` is implemented in the `hailtop.hailctl` submodule. Users can run Hail on their own system, or on the Hail service that is; operated by the Hail team. The Hail service is implemented by a; collection of microservices that are deployed on Kubernetes (K8s) and; Google Cloud Platform (GCP). We refer to K8s and GCP as our Virtual; Datacenter (VDC). ## Hail Query. The `hail` module is implemented in $HAIL/hail/python/hail, this is the Hail Query Python interface.; The Hail Query interface is lazy: executing a pipeline builds an intermediate representation; representing the query. The IR is implemented in the `hail.ir` submodule. When a query is ready to; be executed, it is sent to a backend, implemented in `hail.backend`. There are three backends:; SparkBackend, LocalBackend and ServiceBackend. At the time of writing, the SparkBackend is complete; and the ServiceBackend is mostly complete (it is missing a PCA implementation with clear error; bounds and some block matrix operations do not scale). The LocalBackend has languished. The Spark backend works as follows. The IR is serialized and sent to; a JVM child process via [py4j](https://www.py4j.org/). The entrypoint; for this is the Scala class is.hail.backend.spark.SparkBackend. The; SparkBackend parses the IR, runs a query optimizer on it, generates; custom JVM bytecode for the query which is called from a Spark; computational graph, which it then submits to Spark for execution.; The result is then returned via py4j back to Python and the user. The Se",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:3157,Deployability,configurat,configuration,3157,"x operations do not scale). The LocalBackend has languished. The Spark backend works as follows. The IR is serialized and sent to; a JVM child process via [py4j](https://www.py4j.org/). The entrypoint; for this is the Scala class is.hail.backend.spark.SparkBackend. The; SparkBackend parses the IR, runs a query optimizer on it, generates; custom JVM bytecode for the query which is called from a Spark; computational graph, which it then submits to Spark for execution.; The result is then returned via py4j back to Python and the user. The ServiceBackend is structured differently. The IR is again; serialized, but is written to cloud storage and a job is submitted; to Hail Batch to execute the aforementioned IR. The local backend is similar to the Spark backend, except the; execution is performed on the local machine in the JVM instead of; being submitted to Spark. ## Hail Batch. A batch is a graph of jobs with dependencies. A job includes; configuration for a container to execute, including image, inputs,; outputs, command line, environment variables, etc. The Hail Batch; Python interface provides and high-level interface for constructing; batches of jobs. A batch can then be submitted to a backend for; execution. There are two backends: LocalBackend and ServiceBackend. The local backend executes the batch on the local machine. The service backend serializes the batch as JSON and submits it to the; batch microservice at https://batch.hail.is/. The Batch service then; executes the graph on a fleet of GCP virtual machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below ",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:4586,Deployability,deploy,deployment,4586,"l backend executes the batch on the local machine. The service backend serializes the batch as JSON and submits it to the; batch microservice at https://batch.hail.is/. The Batch service then; executes the graph on a fleet of GCP virtual machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime va",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:4597,Deployability,configurat,configuration,4597,"l backend executes the batch on the local machine. The service backend serializes the batch as JSON and submits it to the; batch microservice at https://batch.hail.is/. The Batch service then; executes the graph on a fleet of GCP virtual machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime va",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:4749,Deployability,configurat,configuration,4749,"al machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime values: as JVM; objects or as a pointer to explicitly managed memory off the Java; heap called a region value. Annotation also sometimes refer to just; the JVM object representation. Explicitly managed off-(Java-)heap; values are also r",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:4767,Deployability,deploy,deployment,4767,"al machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime values: as JVM; objects or as a pointer to explicitly managed memory off the Java; heap called a region value. Annotation also sometimes refer to just; the JVM object representation. Explicitly managed off-(Java-)heap; values are also r",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:4850,Deployability,configurat,configuration,4850,"IL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime values: as JVM; objects or as a pointer to explicitly managed memory off the Java; heap called a region value. Annotation also sometimes refer to just; the JVM object representation. Explicitly managed off-(Java-)heap; values are also referred to as ""unsafe"". * is.hail.asm4s: The Hail Query optimizer generates JVM bytecode to; implement queries. asm4s ",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:4910,Deployability,deploy,deployment,4910," * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime values: as JVM; objects or as a pointer to explicitly managed memory off the Java; heap called a region value. Annotation also sometimes refer to just; the JVM object representation. Explicitly managed off-(Java-)heap; values are also referred to as ""unsafe"". * is.hail.asm4s: The Hail Query optimizer generates JVM bytecode to; implement queries. asm4s is a high-level Scala interface for; generating JVM bytecode. * is.hail.lir: lir is a low-level intermediate representation (IR) for; JVM bytecode. The high-level asm4s interface is implemented in; t",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:4921,Deployability,configurat,configuration,4921," * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime values: as JVM; objects or as a pointer to explicitly managed memory off the Java; heap called a region value. Annotation also sometimes refer to just; the JVM object representation. Explicitly managed off-(Java-)heap; values are also referred to as ""unsafe"". * is.hail.asm4s: The Hail Query optimizer generates JVM bytecode to; implement queries. asm4s is a high-level Scala interface for; generating JVM bytecode. * is.hail.lir: lir is a low-level intermediate representation (IR) for; JVM bytecode. The high-level asm4s interface is implemented in; t",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:5185,Deployability,configurat,configuration,5185," * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime values: as JVM; objects or as a pointer to explicitly managed memory off the Java; heap called a region value. Annotation also sometimes refer to just; the JVM object representation. Explicitly managed off-(Java-)heap; values are also referred to as ""unsafe"". * is.hail.asm4s: The Hail Query optimizer generates JVM bytecode to; implement queries. asm4s is a high-level Scala interface for; generating JVM bytecode. * is.hail.lir: lir is a low-level intermediate representation (IR) for; JVM bytecode. The high-level asm4s interface is implemented in; t",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:7245,Deployability,deploy,deployment,7245,"ry has several different notions of types.; For values, there are three kinds of type: virtual, physical and; encoded types. Virtual types are usual-visible types like int,; array and string. Physical types are implementations of virtual; types. For example, arrays might be stored densely or sparsely.; Encoded types specify how to (de)serialize types for reading and; writing. There also higher-level types for Tables, MatrixTables and; BlockMatrices. * is.hail.expr: expr is a large, disorganized package that contains; the Hail Query IR, query optimizer and IR code generator. * is.hail.io.fs: fs contains an abstract filesystem interface, FS, and; implementations for Google Storage and Hadoop. We need to implement; one for local UNIX filesystems. * is.hail.io: This includes support for reading and writing a variety; of file formats. * is.hail.services: This package is related to the implementation of; services. This includes a Java/Scala client for Hail Batch, the; shuffle service client and server, and support for TLS,; authentication, deployment configuration, etc. * is.hail.rvd: The fundamental abstraction in Spark is the resilient; distributed dataset (RDD). When Hail generates Spark pipelines to; implement queries, it generates an RDD of off-(Java-)heap region; values. An RVD is a Region Value Dataset, an abstraction for a RDD; of region values. * is.hail.backend: each Python backend (Spark, local, service) has a; corresponding backend in Scala. ## Microservice Overview. The Hail service is implemented as a collection of microservices (for; the list, see below). The services are implemented in Python (mostly); and Java/Scala (or a mix, using py4j). The code for the services can; be found at the top-level, e.g. the batch and batch-driver; implementation can be found in $HAIL/batch. For publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using ",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:7256,Deployability,configurat,configuration,7256,"ry has several different notions of types.; For values, there are three kinds of type: virtual, physical and; encoded types. Virtual types are usual-visible types like int,; array and string. Physical types are implementations of virtual; types. For example, arrays might be stored densely or sparsely.; Encoded types specify how to (de)serialize types for reading and; writing. There also higher-level types for Tables, MatrixTables and; BlockMatrices. * is.hail.expr: expr is a large, disorganized package that contains; the Hail Query IR, query optimizer and IR code generator. * is.hail.io.fs: fs contains an abstract filesystem interface, FS, and; implementations for Google Storage and Hadoop. We need to implement; one for local UNIX filesystems. * is.hail.io: This includes support for reading and writing a variety; of file formats. * is.hail.services: This package is related to the implementation of; services. This includes a Java/Scala client for Hail Batch, the; shuffle service client and server, and support for TLS,; authentication, deployment configuration, etc. * is.hail.rvd: The fundamental abstraction in Spark is the resilient; distributed dataset (RDD). When Hail generates Spark pipelines to; implement queries, it generates an RDD of off-(Java-)heap region; values. An RVD is a Region Value Dataset, an abstraction for a RDD; of region values. * is.hail.backend: each Python backend (Spark, local, service) has a; corresponding backend in Scala. ## Microservice Overview. The Hail service is implemented as a collection of microservices (for; the list, see below). The services are implemented in Python (mostly); and Java/Scala (or a mix, using py4j). The code for the services can; be found at the top-level, e.g. the batch and batch-driver; implementation can be found in $HAIL/batch. For publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using ",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:7399,Deployability,pipeline,pipelines,7399,"ual; types. For example, arrays might be stored densely or sparsely.; Encoded types specify how to (de)serialize types for reading and; writing. There also higher-level types for Tables, MatrixTables and; BlockMatrices. * is.hail.expr: expr is a large, disorganized package that contains; the Hail Query IR, query optimizer and IR code generator. * is.hail.io.fs: fs contains an abstract filesystem interface, FS, and; implementations for Google Storage and Hadoop. We need to implement; one for local UNIX filesystems. * is.hail.io: This includes support for reading and writing a variety; of file formats. * is.hail.services: This package is related to the implementation of; services. This includes a Java/Scala client for Hail Batch, the; shuffle service client and server, and support for TLS,; authentication, deployment configuration, etc. * is.hail.rvd: The fundamental abstraction in Spark is the resilient; distributed dataset (RDD). When Hail generates Spark pipelines to; implement queries, it generates an RDD of off-(Java-)heap region; values. An RVD is a Region Value Dataset, an abstraction for a RDD; of region values. * is.hail.backend: each Python backend (Spark, local, service) has a; corresponding backend in Scala. ## Microservice Overview. The Hail service is implemented as a collection of microservices (for; the list, see below). The services are implemented in Python (mostly); and Java/Scala (or a mix, using py4j). The code for the services can; be found at the top-level, e.g. the batch and batch-driver; implementation can be found in $HAIL/batch. For publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using Python asyncio,; [aiohttp](https://docs.aiohttp.org/en/stable/) and Jinja2 for HTML; templating. Some services rely on 3rd party services. Those include:. * ci depends on GitHub. * batch, ci and auth depend on K8s. * batch depends on",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:8848,Deployability,deploy,deployment,8848,"ala (or a mix, using py4j). The code for the services can; be found at the top-level, e.g. the batch and batch-driver; implementation can be found in $HAIL/batch. For publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using Python asyncio,; [aiohttp](https://docs.aiohttp.org/en/stable/) and Jinja2 for HTML; templating. Some services rely on 3rd party services. Those include:. * ci depends on GitHub. * batch, ci and auth depend on K8s. * batch depends on K8s and GCP. * website depends (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've implemented our own continuous integration and continuous; deployed (CI/CD) system. It also hosts a developer status board; at https://ci.hail.is/me. * gateway: gateway is an nginx reverse proxy that terminates TLS; connections and forwards requests to services in K8s. It is; possible to run multiple copies of the Hail services in K8s, each; set in a separate K8s namespace. gateway forwards requests to the; K8s service",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:8859,Deployability,configurat,configuration,8859,"ala (or a mix, using py4j). The code for the services can; be found at the top-level, e.g. the batch and batch-driver; implementation can be found in $HAIL/batch. For publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using Python asyncio,; [aiohttp](https://docs.aiohttp.org/en/stable/) and Jinja2 for HTML; templating. Some services rely on 3rd party services. Those include:. * ci depends on GitHub. * batch, ci and auth depend on K8s. * batch depends on K8s and GCP. * website depends (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've implemented our own continuous integration and continuous; deployed (CI/CD) system. It also hosts a developer status board; at https://ci.hail.is/me. * gateway: gateway is an nginx reverse proxy that terminates TLS; connections and forwards requests to services in K8s. It is; possible to run multiple copies of the Hail services in K8s, each; set in a separate K8s namespace. gateway forwards requests to the; K8s service",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:9445,Deployability,continuous,continuous,9445," (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've implemented our own continuous integration and continuous; deployed (CI/CD) system. It also hosts a developer status board; at https://ci.hail.is/me. * gateway: gateway is an nginx reverse proxy that terminates TLS; connections and forwards requests to services in K8s. It is; possible to run multiple copies of the Hail services in K8s, each; set in a separate K8s namespace. gateway forwards requests to the; K8s service in the appropriate namespace. * internal-gateway: batch workers run on GCE VMs, not in K8s. The; internal-gateway is an nginx reverse proxy that terminates; connections from the Google Virtual Private Cloud (VPC) network and; connections to the services in K8s. * website: site implements the main Hail website https://hail.is/; including the landing page and Hail Query and Hail Batch; documentation. There are two types of services: managed and unmanaged.; CI handles deployment of managed services, while unmanaged services; are deployed by hand using their respective Makefiles. The; unmanaged services are g",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:9456,Deployability,integrat,integration,9456," (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've implemented our own continuous integration and continuous; deployed (CI/CD) system. It also hosts a developer status board; at https://ci.hail.is/me. * gateway: gateway is an nginx reverse proxy that terminates TLS; connections and forwards requests to services in K8s. It is; possible to run multiple copies of the Hail services in K8s, each; set in a separate K8s namespace. gateway forwards requests to the; K8s service in the appropriate namespace. * internal-gateway: batch workers run on GCE VMs, not in K8s. The; internal-gateway is an nginx reverse proxy that terminates; connections from the Google Virtual Private Cloud (VPC) network and; connections to the services in K8s. * website: site implements the main Hail website https://hail.is/; including the landing page and Hail Query and Hail Batch; documentation. There are two types of services: managed and unmanaged.; CI handles deployment of managed services, while unmanaged services; are deployed by hand using their respective Makefiles. The; unmanaged services are g",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:9472,Deployability,continuous,continuous,9472," (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've implemented our own continuous integration and continuous; deployed (CI/CD) system. It also hosts a developer status board; at https://ci.hail.is/me. * gateway: gateway is an nginx reverse proxy that terminates TLS; connections and forwards requests to services in K8s. It is; possible to run multiple copies of the Hail services in K8s, each; set in a separate K8s namespace. gateway forwards requests to the; K8s service in the appropriate namespace. * internal-gateway: batch workers run on GCE VMs, not in K8s. The; internal-gateway is an nginx reverse proxy that terminates; connections from the Google Virtual Private Cloud (VPC) network and; connections to the services in K8s. * website: site implements the main Hail website https://hail.is/; including the landing page and Hail Query and Hail Batch; documentation. There are two types of services: managed and unmanaged.; CI handles deployment of managed services, while unmanaged services; are deployed by hand using their respective Makefiles. The; unmanaged services are g",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:9484,Deployability,deploy,deployed,9484," (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've implemented our own continuous integration and continuous; deployed (CI/CD) system. It also hosts a developer status board; at https://ci.hail.is/me. * gateway: gateway is an nginx reverse proxy that terminates TLS; connections and forwards requests to services in K8s. It is; possible to run multiple copies of the Hail services in K8s, each; set in a separate K8s namespace. gateway forwards requests to the; K8s service in the appropriate namespace. * internal-gateway: batch workers run on GCE VMs, not in K8s. The; internal-gateway is an nginx reverse proxy that terminates; connections from the Google Virtual Private Cloud (VPC) network and; connections to the services in K8s. * website: site implements the main Hail website https://hail.is/; including the landing page and Hail Query and Hail Batch; documentation. There are two types of services: managed and unmanaged.; CI handles deployment of managed services, while unmanaged services; are deployed by hand using their respective Makefiles. The; unmanaged services are g",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:10318,Deployability,deploy,deployment,10318,"search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've implemented our own continuous integration and continuous; deployed (CI/CD) system. It also hosts a developer status board; at https://ci.hail.is/me. * gateway: gateway is an nginx reverse proxy that terminates TLS; connections and forwards requests to services in K8s. It is; possible to run multiple copies of the Hail services in K8s, each; set in a separate K8s namespace. gateway forwards requests to the; K8s service in the appropriate namespace. * internal-gateway: batch workers run on GCE VMs, not in K8s. The; internal-gateway is an nginx reverse proxy that terminates; connections from the Google Virtual Private Cloud (VPC) network and; connections to the services in K8s. * website: site implements the main Hail website https://hail.is/; including the landing page and Hail Query and Hail Batch; documentation. There are two types of services: managed and unmanaged.; CI handles deployment of managed services, while unmanaged services; are deployed by hand using their respective Makefiles. The; unmanaged services are gateway and internal-gateway.; ",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:10380,Deployability,deploy,deployed,10380,"search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've implemented our own continuous integration and continuous; deployed (CI/CD) system. It also hosts a developer status board; at https://ci.hail.is/me. * gateway: gateway is an nginx reverse proxy that terminates TLS; connections and forwards requests to services in K8s. It is; possible to run multiple copies of the Hail services in K8s, each; set in a separate K8s namespace. gateway forwards requests to the; K8s service in the appropriate namespace. * internal-gateway: batch workers run on GCE VMs, not in K8s. The; internal-gateway is an nginx reverse proxy that terminates; connections from the Google Virtual Private Cloud (VPC) network and; connections to the services in K8s. * website: site implements the main Hail website https://hail.is/; including the landing page and Hail Query and Hail Batch; documentation. There are two types of services: managed and unmanaged.; CI handles deployment of managed services, while unmanaged services; are deployed by hand using their respective Makefiles. The; unmanaged services are gateway and internal-gateway.; ",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:440,Integrability,interface,interfaces,440,"# Hail. This document gives an overview of the Hail architecture and source; code. ## Background. Hail source code is stored in a monolithic repository (monorepo) on; GitHub at hail-is/hail. $HAIL will denote the repository root below.; Hail is open source and developed in the open. Hail is written in Python, Java/Scala (both JVM langauges), and C/C++. Hail has two user-facing components: Hail Query and Hail Batch. Both; provide Python interfaces. Hail Query is for distributed, out-of-core; manipulation and analysis of tabular and genomic data. Hail Batch is; for the execution of graphs containers. The Hail client libraries are deployed in the Python Package Index; (PyPI) hail package. The Hail package exposes two Python modules:; `hail` and `hailtop`. `hail` provides the Hail Query interface.; `hailtop.batch` provides the batch interface. `hailtop` contains; various other infrastructure submodules (see below). The hail package also contains the `hailctl` command line utility that; provides a number of features, including managing Google Dataproc (a; hosted Spark service), Hail service authorization, Hail configuration,; interacting with the Hail developer tools, and managing the Hail Batch; service. `hailctl` is implemented in the `hailtop.hailctl` submodule. Users can run Hail on their own system, or on the Hail service that is; operated by the Hail team. The Hail service is implemented by a; collection of microservices that are deployed on Kubernetes (K8s) and; Google Cloud Platform (GCP). We refer to K8s and GCP as our Virtual; Datacenter (VDC). ## Hail Query. The `hail` module is implemented in $HAIL/hail/python/hail, this is the Hail Query Python interface.; The Hail Query interface is lazy: executing a pipeline builds an intermediate representation; representing the query. The IR is implemented in the `hail.ir` submodule. When a query is ready to; be executed, it is sent to a backend, implemented in `hail.backend`. There are three backends:; SparkBackend, Loca",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:794,Integrability,interface,interface,794,"# Hail. This document gives an overview of the Hail architecture and source; code. ## Background. Hail source code is stored in a monolithic repository (monorepo) on; GitHub at hail-is/hail. $HAIL will denote the repository root below.; Hail is open source and developed in the open. Hail is written in Python, Java/Scala (both JVM langauges), and C/C++. Hail has two user-facing components: Hail Query and Hail Batch. Both; provide Python interfaces. Hail Query is for distributed, out-of-core; manipulation and analysis of tabular and genomic data. Hail Batch is; for the execution of graphs containers. The Hail client libraries are deployed in the Python Package Index; (PyPI) hail package. The Hail package exposes two Python modules:; `hail` and `hailtop`. `hail` provides the Hail Query interface.; `hailtop.batch` provides the batch interface. `hailtop` contains; various other infrastructure submodules (see below). The hail package also contains the `hailctl` command line utility that; provides a number of features, including managing Google Dataproc (a; hosted Spark service), Hail service authorization, Hail configuration,; interacting with the Hail developer tools, and managing the Hail Batch; service. `hailctl` is implemented in the `hailtop.hailctl` submodule. Users can run Hail on their own system, or on the Hail service that is; operated by the Hail team. The Hail service is implemented by a; collection of microservices that are deployed on Kubernetes (K8s) and; Google Cloud Platform (GCP). We refer to K8s and GCP as our Virtual; Datacenter (VDC). ## Hail Query. The `hail` module is implemented in $HAIL/hail/python/hail, this is the Hail Query Python interface.; The Hail Query interface is lazy: executing a pipeline builds an intermediate representation; representing the query. The IR is implemented in the `hail.ir` submodule. When a query is ready to; be executed, it is sent to a backend, implemented in `hail.backend`. There are three backends:; SparkBackend, Loca",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:841,Integrability,interface,interface,841,"# Hail. This document gives an overview of the Hail architecture and source; code. ## Background. Hail source code is stored in a monolithic repository (monorepo) on; GitHub at hail-is/hail. $HAIL will denote the repository root below.; Hail is open source and developed in the open. Hail is written in Python, Java/Scala (both JVM langauges), and C/C++. Hail has two user-facing components: Hail Query and Hail Batch. Both; provide Python interfaces. Hail Query is for distributed, out-of-core; manipulation and analysis of tabular and genomic data. Hail Batch is; for the execution of graphs containers. The Hail client libraries are deployed in the Python Package Index; (PyPI) hail package. The Hail package exposes two Python modules:; `hail` and `hailtop`. `hail` provides the Hail Query interface.; `hailtop.batch` provides the batch interface. `hailtop` contains; various other infrastructure submodules (see below). The hail package also contains the `hailctl` command line utility that; provides a number of features, including managing Google Dataproc (a; hosted Spark service), Hail service authorization, Hail configuration,; interacting with the Hail developer tools, and managing the Hail Batch; service. `hailctl` is implemented in the `hailtop.hailctl` submodule. Users can run Hail on their own system, or on the Hail service that is; operated by the Hail team. The Hail service is implemented by a; collection of microservices that are deployed on Kubernetes (K8s) and; Google Cloud Platform (GCP). We refer to K8s and GCP as our Virtual; Datacenter (VDC). ## Hail Query. The `hail` module is implemented in $HAIL/hail/python/hail, this is the Hail Query Python interface.; The Hail Query interface is lazy: executing a pipeline builds an intermediate representation; representing the query. The IR is implemented in the `hail.ir` submodule. When a query is ready to; be executed, it is sent to a backend, implemented in `hail.backend`. There are three backends:; SparkBackend, Loca",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:1681,Integrability,interface,interface,1681,"oyed in the Python Package Index; (PyPI) hail package. The Hail package exposes two Python modules:; `hail` and `hailtop`. `hail` provides the Hail Query interface.; `hailtop.batch` provides the batch interface. `hailtop` contains; various other infrastructure submodules (see below). The hail package also contains the `hailctl` command line utility that; provides a number of features, including managing Google Dataproc (a; hosted Spark service), Hail service authorization, Hail configuration,; interacting with the Hail developer tools, and managing the Hail Batch; service. `hailctl` is implemented in the `hailtop.hailctl` submodule. Users can run Hail on their own system, or on the Hail service that is; operated by the Hail team. The Hail service is implemented by a; collection of microservices that are deployed on Kubernetes (K8s) and; Google Cloud Platform (GCP). We refer to K8s and GCP as our Virtual; Datacenter (VDC). ## Hail Query. The `hail` module is implemented in $HAIL/hail/python/hail, this is the Hail Query Python interface.; The Hail Query interface is lazy: executing a pipeline builds an intermediate representation; representing the query. The IR is implemented in the `hail.ir` submodule. When a query is ready to; be executed, it is sent to a backend, implemented in `hail.backend`. There are three backends:; SparkBackend, LocalBackend and ServiceBackend. At the time of writing, the SparkBackend is complete; and the ServiceBackend is mostly complete (it is missing a PCA implementation with clear error; bounds and some block matrix operations do not scale). The LocalBackend has languished. The Spark backend works as follows. The IR is serialized and sent to; a JVM child process via [py4j](https://www.py4j.org/). The entrypoint; for this is the Scala class is.hail.backend.spark.SparkBackend. The; SparkBackend parses the IR, runs a query optimizer on it, generates; custom JVM bytecode for the query which is called from a Spark; computational graph, which it ",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:1708,Integrability,interface,interface,1708,"d `hailtop`. `hail` provides the Hail Query interface.; `hailtop.batch` provides the batch interface. `hailtop` contains; various other infrastructure submodules (see below). The hail package also contains the `hailctl` command line utility that; provides a number of features, including managing Google Dataproc (a; hosted Spark service), Hail service authorization, Hail configuration,; interacting with the Hail developer tools, and managing the Hail Batch; service. `hailctl` is implemented in the `hailtop.hailctl` submodule. Users can run Hail on their own system, or on the Hail service that is; operated by the Hail team. The Hail service is implemented by a; collection of microservices that are deployed on Kubernetes (K8s) and; Google Cloud Platform (GCP). We refer to K8s and GCP as our Virtual; Datacenter (VDC). ## Hail Query. The `hail` module is implemented in $HAIL/hail/python/hail, this is the Hail Query Python interface.; The Hail Query interface is lazy: executing a pipeline builds an intermediate representation; representing the query. The IR is implemented in the `hail.ir` submodule. When a query is ready to; be executed, it is sent to a backend, implemented in `hail.backend`. There are three backends:; SparkBackend, LocalBackend and ServiceBackend. At the time of writing, the SparkBackend is complete; and the ServiceBackend is mostly complete (it is missing a PCA implementation with clear error; bounds and some block matrix operations do not scale). The LocalBackend has languished. The Spark backend works as follows. The IR is serialized and sent to; a JVM child process via [py4j](https://www.py4j.org/). The entrypoint; for this is the Scala class is.hail.backend.spark.SparkBackend. The; SparkBackend parses the IR, runs a query optimizer on it, generates; custom JVM bytecode for the query which is called from a Spark; computational graph, which it then submits to Spark for execution.; The result is then returned via py4j back to Python and the user. The Se",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:3127,Integrability,depend,dependencies,3127,"complete (it is missing a PCA implementation with clear error; bounds and some block matrix operations do not scale). The LocalBackend has languished. The Spark backend works as follows. The IR is serialized and sent to; a JVM child process via [py4j](https://www.py4j.org/). The entrypoint; for this is the Scala class is.hail.backend.spark.SparkBackend. The; SparkBackend parses the IR, runs a query optimizer on it, generates; custom JVM bytecode for the query which is called from a Spark; computational graph, which it then submits to Spark for execution.; The result is then returned via py4j back to Python and the user. The ServiceBackend is structured differently. The IR is again; serialized, but is written to cloud storage and a job is submitted; to Hail Batch to execute the aforementioned IR. The local backend is similar to the Spark backend, except the; execution is performed on the local machine in the JVM instead of; being submitted to Spark. ## Hail Batch. A batch is a graph of jobs with dependencies. A job includes; configuration for a container to execute, including image, inputs,; outputs, command line, environment variables, etc. The Hail Batch; Python interface provides and high-level interface for constructing; batches of jobs. A batch can then be submitted to a backend for; execution. There are two backends: LocalBackend and ServiceBackend. The local backend executes the batch on the local machine. The service backend serializes the batch as JSON and submits it to the; batch microservice at https://batch.hail.is/. The Batch service then; executes the graph on a fleet of GCP virtual machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:3299,Integrability,interface,interface,3299,"nd sent to; a JVM child process via [py4j](https://www.py4j.org/). The entrypoint; for this is the Scala class is.hail.backend.spark.SparkBackend. The; SparkBackend parses the IR, runs a query optimizer on it, generates; custom JVM bytecode for the query which is called from a Spark; computational graph, which it then submits to Spark for execution.; The result is then returned via py4j back to Python and the user. The ServiceBackend is structured differently. The IR is again; serialized, but is written to cloud storage and a job is submitted; to Hail Batch to execute the aforementioned IR. The local backend is similar to the Spark backend, except the; execution is performed on the local machine in the JVM instead of; being submitted to Spark. ## Hail Batch. A batch is a graph of jobs with dependencies. A job includes; configuration for a container to execute, including image, inputs,; outputs, command line, environment variables, etc. The Hail Batch; Python interface provides and high-level interface for constructing; batches of jobs. A batch can then be submitted to a backend for; execution. There are two backends: LocalBackend and ServiceBackend. The local backend executes the batch on the local machine. The service backend serializes the batch as JSON and submits it to the; batch microservice at https://batch.hail.is/. The Batch service then; executes the graph on a fleet of GCP virtual machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/websit",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:3333,Integrability,interface,interface,3333,"nd sent to; a JVM child process via [py4j](https://www.py4j.org/). The entrypoint; for this is the Scala class is.hail.backend.spark.SparkBackend. The; SparkBackend parses the IR, runs a query optimizer on it, generates; custom JVM bytecode for the query which is called from a Spark; computational graph, which it then submits to Spark for execution.; The result is then returned via py4j back to Python and the user. The ServiceBackend is structured differently. The IR is again; serialized, but is written to cloud storage and a job is submitted; to Hail Batch to execute the aforementioned IR. The local backend is similar to the Spark backend, except the; execution is performed on the local machine in the JVM instead of; being submitted to Spark. ## Hail Batch. A batch is a graph of jobs with dependencies. A job includes; configuration for a container to execute, including image, inputs,; outputs, command line, environment variables, etc. The Hail Batch; Python interface provides and high-level interface for constructing; batches of jobs. A batch can then be submitted to a backend for; execution. There are two backends: LocalBackend and ServiceBackend. The local backend executes the batch on the local machine. The service backend serializes the batch as JSON and submits it to the; batch microservice at https://batch.hail.is/. The Batch service then; executes the graph on a fleet of GCP virtual machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/websit",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:3897,Integrability,interface,interface,3897,"he aforementioned IR. The local backend is similar to the Spark backend, except the; execution is performed on the local machine in the JVM instead of; being submitted to Spark. ## Hail Batch. A batch is a graph of jobs with dependencies. A job includes; configuration for a container to execute, including image, inputs,; outputs, command line, environment variables, etc. The Hail Batch; Python interface provides and high-level interface for constructing; batches of jobs. A batch can then be submitted to a backend for; execution. There are two backends: LocalBackend and ServiceBackend. The local backend executes the batch on the local machine. The service backend serializes the batch as JSON and submits it to the; batch microservice at https://batch.hail.is/. The Batch service then; executes the graph on a fleet of GCP virtual machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, te",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:4103,Integrability,interface,interface,4103,". ## Hail Batch. A batch is a graph of jobs with dependencies. A job includes; configuration for a container to execute, including image, inputs,; outputs, command line, environment variables, etc. The Hail Batch; Python interface provides and high-level interface for constructing; batches of jobs. A batch can then be submitted to a backend for; execution. There are two backends: LocalBackend and ServiceBackend. The local backend executes the batch on the local machine. The service backend serializes the batch as JSON and submits it to the; batch microservice at https://batch.hail.is/. The Batch service then; executes the graph on a fleet of GCP virtual machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/ha",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:5877,Integrability,interface,interface,5877,"l: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime values: as JVM; objects or as a pointer to explicitly managed memory off the Java; heap called a region value. Annotation also sometimes refer to just; the JVM object representation. Explicitly managed off-(Java-)heap; values are also referred to as ""unsafe"". * is.hail.asm4s: The Hail Query optimizer generates JVM bytecode to; implement queries. asm4s is a high-level Scala interface for; generating JVM bytecode. * is.hail.lir: lir is a low-level intermediate representation (IR) for; JVM bytecode. The high-level asm4s interface is implemented in; terms of lir. lir can generate raw JVM bytecode that can be loaded; into the interpreter and invoked via reflection. * is.hail.types: Hail Query has several different notions of types.; For values, there are three kinds of type: virtual, physical and; encoded types. Virtual types are usual-visible types like int,; array and string. Physical types are implementations of virtual; types. For example, arrays might be stored densely or sparsely.; Encoded types specify how to (de)serialize types for reading and; writing. There also higher-level types for Tables, MatrixTables and; BlockMatrices. * is.hail.expr: expr is a large, disorganized package that contains; the Hail Query IR, query optimizer and IR code generator. * is.hail.io.fs: fs contains an abstract filesystem interface, FS, and; implementations for Google Storage",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:6024,Integrability,interface,interface,6024,"d by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime values: as JVM; objects or as a pointer to explicitly managed memory off the Java; heap called a region value. Annotation also sometimes refer to just; the JVM object representation. Explicitly managed off-(Java-)heap; values are also referred to as ""unsafe"". * is.hail.asm4s: The Hail Query optimizer generates JVM bytecode to; implement queries. asm4s is a high-level Scala interface for; generating JVM bytecode. * is.hail.lir: lir is a low-level intermediate representation (IR) for; JVM bytecode. The high-level asm4s interface is implemented in; terms of lir. lir can generate raw JVM bytecode that can be loaded; into the interpreter and invoked via reflection. * is.hail.types: Hail Query has several different notions of types.; For values, there are three kinds of type: virtual, physical and; encoded types. Virtual types are usual-visible types like int,; array and string. Physical types are implementations of virtual; types. For example, arrays might be stored densely or sparsely.; Encoded types specify how to (de)serialize types for reading and; writing. There also higher-level types for Tables, MatrixTables and; BlockMatrices. * is.hail.expr: expr is a large, disorganized package that contains; the Hail Query IR, query optimizer and IR code generator. * is.hail.io.fs: fs contains an abstract filesystem interface, FS, and; implementations for Google Storage and Hadoop. We need to implement; one for local UNIX filesystems. * is.hail.io: This includes support for reading and writing a variety; of file format",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:6828,Integrability,interface,interface,6828,"ueries. asm4s is a high-level Scala interface for; generating JVM bytecode. * is.hail.lir: lir is a low-level intermediate representation (IR) for; JVM bytecode. The high-level asm4s interface is implemented in; terms of lir. lir can generate raw JVM bytecode that can be loaded; into the interpreter and invoked via reflection. * is.hail.types: Hail Query has several different notions of types.; For values, there are three kinds of type: virtual, physical and; encoded types. Virtual types are usual-visible types like int,; array and string. Physical types are implementations of virtual; types. For example, arrays might be stored densely or sparsely.; Encoded types specify how to (de)serialize types for reading and; writing. There also higher-level types for Tables, MatrixTables and; BlockMatrices. * is.hail.expr: expr is a large, disorganized package that contains; the Hail Query IR, query optimizer and IR code generator. * is.hail.io.fs: fs contains an abstract filesystem interface, FS, and; implementations for Google Storage and Hadoop. We need to implement; one for local UNIX filesystems. * is.hail.io: This includes support for reading and writing a variety; of file formats. * is.hail.services: This package is related to the implementation of; services. This includes a Java/Scala client for Hail Batch, the; shuffle service client and server, and support for TLS,; authentication, deployment configuration, etc. * is.hail.rvd: The fundamental abstraction in Spark is the resilient; distributed dataset (RDD). When Hail generates Spark pipelines to; implement queries, it generates an RDD of off-(Java-)heap region; values. An RVD is a Region Value Dataset, an abstraction for a RDD; of region values. * is.hail.backend: each Python backend (Spark, local, service) has a; corresponding backend in Scala. ## Microservice Overview. The Hail service is implemented as a collection of microservices (for; the list, see below). The services are implemented in Python (mostly); and Jav",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:8356,Integrability,depend,depends,8356,"set (RDD). When Hail generates Spark pipelines to; implement queries, it generates an RDD of off-(Java-)heap region; values. An RVD is a Region Value Dataset, an abstraction for a RDD; of region values. * is.hail.backend: each Python backend (Spark, local, service) has a; corresponding backend in Scala. ## Microservice Overview. The Hail service is implemented as a collection of microservices (for; the list, see below). The services are implemented in Python (mostly); and Java/Scala (or a mix, using py4j). The code for the services can; be found at the top-level, e.g. the batch and batch-driver; implementation can be found in $HAIL/batch. For publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using Python asyncio,; [aiohttp](https://docs.aiohttp.org/en/stable/) and Jinja2 for HTML; templating. Some services rely on 3rd party services. Those include:. * ci depends on GitHub. * batch, ci and auth depend on K8s. * batch depends on K8s and GCP. * website depends (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management.",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:8396,Integrability,depend,depend,8396," Spark pipelines to; implement queries, it generates an RDD of off-(Java-)heap region; values. An RVD is a Region Value Dataset, an abstraction for a RDD; of region values. * is.hail.backend: each Python backend (Spark, local, service) has a; corresponding backend in Scala. ## Microservice Overview. The Hail service is implemented as a collection of microservices (for; the list, see below). The services are implemented in Python (mostly); and Java/Scala (or a mix, using py4j). The code for the services can; be found at the top-level, e.g. the batch and batch-driver; implementation can be found in $HAIL/batch. For publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using Python asyncio,; [aiohttp](https://docs.aiohttp.org/en/stable/) and Jinja2 for HTML; templating. Some services rely on 3rd party services. Those include:. * ci depends on GitHub. * batch, ci and auth depend on K8s. * batch depends on K8s and GCP. * website depends (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:8419,Integrability,depend,depends,8419,"ries, it generates an RDD of off-(Java-)heap region; values. An RVD is a Region Value Dataset, an abstraction for a RDD; of region values. * is.hail.backend: each Python backend (Spark, local, service) has a; corresponding backend in Scala. ## Microservice Overview. The Hail service is implemented as a collection of microservices (for; the list, see below). The services are implemented in Python (mostly); and Java/Scala (or a mix, using py4j). The code for the services can; be found at the top-level, e.g. the batch and batch-driver; implementation can be found in $HAIL/batch. For publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using Python asyncio,; [aiohttp](https://docs.aiohttp.org/en/stable/) and Jinja2 for HTML; templating. Some services rely on 3rd party services. Those include:. * ci depends on GitHub. * batch, ci and auth depend on K8s. * batch depends on K8s and GCP. * website depends (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've i",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:8453,Integrability,depend,depends,8453,"p region; values. An RVD is a Region Value Dataset, an abstraction for a RDD; of region values. * is.hail.backend: each Python backend (Spark, local, service) has a; corresponding backend in Scala. ## Microservice Overview. The Hail service is implemented as a collection of microservices (for; the list, see below). The services are implemented in Python (mostly); and Java/Scala (or a mix, using py4j). The code for the services can; be found at the top-level, e.g. the batch and batch-driver; implementation can be found in $HAIL/batch. For publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using Python asyncio,; [aiohttp](https://docs.aiohttp.org/en/stable/) and Jinja2 for HTML; templating. Some services rely on 3rd party services. Those include:. * ci depends on GitHub. * batch, ci and auth depend on K8s. * batch depends on K8s and GCP. * website depends (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've implemented our own continuous integration an",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:9024,Integrability,interface,interface,9024,"publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using Python asyncio,; [aiohttp](https://docs.aiohttp.org/en/stable/) and Jinja2 for HTML; templating. Some services rely on 3rd party services. Those include:. * ci depends on GitHub. * batch, ci and auth depend on K8s. * batch depends on K8s and GCP. * website depends (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've implemented our own continuous integration and continuous; deployed (CI/CD) system. It also hosts a developer status board; at https://ci.hail.is/me. * gateway: gateway is an nginx reverse proxy that terminates TLS; connections and forwards requests to services in K8s. It is; possible to run multiple copies of the Hail services in K8s, each; set in a separate K8s namespace. gateway forwards requests to the; K8s service in the appropriate namespace. * internal-gateway: batch workers run on GCE VMs, not in K8s. The; internal-gateway is an nginx reverse proxy that terminates; connectio",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:9456,Integrability,integrat,integration,9456," (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've implemented our own continuous integration and continuous; deployed (CI/CD) system. It also hosts a developer status board; at https://ci.hail.is/me. * gateway: gateway is an nginx reverse proxy that terminates TLS; connections and forwards requests to services in K8s. It is; possible to run multiple copies of the Hail services in K8s, each; set in a separate K8s namespace. gateway forwards requests to the; K8s service in the appropriate namespace. * internal-gateway: batch workers run on GCE VMs, not in K8s. The; internal-gateway is an nginx reverse proxy that terminates; connections from the Google Virtual Private Cloud (VPC) network and; connections to the services in K8s. * website: site implements the main Hail website https://hail.is/; including the landing page and Hail Query and Hail Batch; documentation. There are two types of services: managed and unmanaged.; CI handles deployment of managed services, while unmanaged services; are deployed by hand using their respective Makefiles. The; unmanaged services are g",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:1123,Modifiability,config,configuration,1123,"urce; code. ## Background. Hail source code is stored in a monolithic repository (monorepo) on; GitHub at hail-is/hail. $HAIL will denote the repository root below.; Hail is open source and developed in the open. Hail is written in Python, Java/Scala (both JVM langauges), and C/C++. Hail has two user-facing components: Hail Query and Hail Batch. Both; provide Python interfaces. Hail Query is for distributed, out-of-core; manipulation and analysis of tabular and genomic data. Hail Batch is; for the execution of graphs containers. The Hail client libraries are deployed in the Python Package Index; (PyPI) hail package. The Hail package exposes two Python modules:; `hail` and `hailtop`. `hail` provides the Hail Query interface.; `hailtop.batch` provides the batch interface. `hailtop` contains; various other infrastructure submodules (see below). The hail package also contains the `hailctl` command line utility that; provides a number of features, including managing Google Dataproc (a; hosted Spark service), Hail service authorization, Hail configuration,; interacting with the Hail developer tools, and managing the Hail Batch; service. `hailctl` is implemented in the `hailtop.hailctl` submodule. Users can run Hail on their own system, or on the Hail service that is; operated by the Hail team. The Hail service is implemented by a; collection of microservices that are deployed on Kubernetes (K8s) and; Google Cloud Platform (GCP). We refer to K8s and GCP as our Virtual; Datacenter (VDC). ## Hail Query. The `hail` module is implemented in $HAIL/hail/python/hail, this is the Hail Query Python interface.; The Hail Query interface is lazy: executing a pipeline builds an intermediate representation; representing the query. The IR is implemented in the `hail.ir` submodule. When a query is ready to; be executed, it is sent to a backend, implemented in `hail.backend`. There are three backends:; SparkBackend, LocalBackend and ServiceBackend. At the time of writing, the SparkBackend i",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:3157,Modifiability,config,configuration,3157,"x operations do not scale). The LocalBackend has languished. The Spark backend works as follows. The IR is serialized and sent to; a JVM child process via [py4j](https://www.py4j.org/). The entrypoint; for this is the Scala class is.hail.backend.spark.SparkBackend. The; SparkBackend parses the IR, runs a query optimizer on it, generates; custom JVM bytecode for the query which is called from a Spark; computational graph, which it then submits to Spark for execution.; The result is then returned via py4j back to Python and the user. The ServiceBackend is structured differently. The IR is again; serialized, but is written to cloud storage and a job is submitted; to Hail Batch to execute the aforementioned IR. The local backend is similar to the Spark backend, except the; execution is performed on the local machine in the JVM instead of; being submitted to Spark. ## Hail Batch. A batch is a graph of jobs with dependencies. A job includes; configuration for a container to execute, including image, inputs,; outputs, command line, environment variables, etc. The Hail Batch; Python interface provides and high-level interface for constructing; batches of jobs. A batch can then be submitted to a backend for; execution. There are two backends: LocalBackend and ServiceBackend. The local backend executes the batch on the local machine. The service backend serializes the batch as JSON and submits it to the; batch microservice at https://batch.hail.is/. The Batch service then; executes the graph on a fleet of GCP virtual machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below ",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:3260,Modifiability,variab,variables,3260,"x operations do not scale). The LocalBackend has languished. The Spark backend works as follows. The IR is serialized and sent to; a JVM child process via [py4j](https://www.py4j.org/). The entrypoint; for this is the Scala class is.hail.backend.spark.SparkBackend. The; SparkBackend parses the IR, runs a query optimizer on it, generates; custom JVM bytecode for the query which is called from a Spark; computational graph, which it then submits to Spark for execution.; The result is then returned via py4j back to Python and the user. The ServiceBackend is structured differently. The IR is again; serialized, but is written to cloud storage and a job is submitted; to Hail Batch to execute the aforementioned IR. The local backend is similar to the Spark backend, except the; execution is performed on the local machine in the JVM instead of; being submitted to Spark. ## Hail Batch. A batch is a graph of jobs with dependencies. A job includes; configuration for a container to execute, including image, inputs,; outputs, command line, environment variables, etc. The Hail Batch; Python interface provides and high-level interface for constructing; batches of jobs. A batch can then be submitted to a backend for; execution. There are two backends: LocalBackend and ServiceBackend. The local backend executes the batch on the local machine. The service backend serializes the batch as JSON and submits it to the; batch microservice at https://batch.hail.is/. The Batch service then; executes the graph on a fleet of GCP virtual machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below ",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:4569,Modifiability,config,config,4569,"l backend executes the batch on the local machine. The service backend serializes the batch as JSON and submits it to the; batch microservice at https://batch.hail.is/. The Batch service then; executes the graph on a fleet of GCP virtual machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime va",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:4597,Modifiability,config,configuration,4597,"l backend executes the batch on the local machine. The service backend serializes the batch as JSON and submits it to the; batch microservice at https://batch.hail.is/. The Batch service then; executes the graph on a fleet of GCP virtual machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime va",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:4749,Modifiability,config,configuration,4749,"al machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime values: as JVM; objects or as a pointer to explicitly managed memory off the Java; heap called a region value. Annotation also sometimes refer to just; the JVM object representation. Explicitly managed off-(Java-)heap; values are also r",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:4850,Modifiability,config,configuration,4850,"IL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime values: as JVM; objects or as a pointer to explicitly managed memory off the Java; heap called a region value. Annotation also sometimes refer to just; the JVM object representation. Explicitly managed off-(Java-)heap; values are also referred to as ""unsafe"". * is.hail.asm4s: The Hail Query optimizer generates JVM bytecode to; implement queries. asm4s ",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:4921,Modifiability,config,configuration,4921," * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime values: as JVM; objects or as a pointer to explicitly managed memory off the Java; heap called a region value. Annotation also sometimes refer to just; the JVM object representation. Explicitly managed off-(Java-)heap; values are also referred to as ""unsafe"". * is.hail.asm4s: The Hail Query optimizer generates JVM bytecode to; implement queries. asm4s is a high-level Scala interface for; generating JVM bytecode. * is.hail.lir: lir is a low-level intermediate representation (IR) for; JVM bytecode. The high-level asm4s interface is implemented in; t",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:5185,Modifiability,config,configuration,5185," * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime values: as JVM; objects or as a pointer to explicitly managed memory off the Java; heap called a region value. Annotation also sometimes refer to just; the JVM object representation. Explicitly managed off-(Java-)heap; values are also referred to as ""unsafe"". * is.hail.asm4s: The Hail Query optimizer generates JVM bytecode to; implement queries. asm4s is a high-level Scala interface for; generating JVM bytecode. * is.hail.lir: lir is a low-level intermediate representation (IR) for; JVM bytecode. The high-level asm4s interface is implemented in; t",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:7256,Modifiability,config,configuration,7256,"ry has several different notions of types.; For values, there are three kinds of type: virtual, physical and; encoded types. Virtual types are usual-visible types like int,; array and string. Physical types are implementations of virtual; types. For example, arrays might be stored densely or sparsely.; Encoded types specify how to (de)serialize types for reading and; writing. There also higher-level types for Tables, MatrixTables and; BlockMatrices. * is.hail.expr: expr is a large, disorganized package that contains; the Hail Query IR, query optimizer and IR code generator. * is.hail.io.fs: fs contains an abstract filesystem interface, FS, and; implementations for Google Storage and Hadoop. We need to implement; one for local UNIX filesystems. * is.hail.io: This includes support for reading and writing a variety; of file formats. * is.hail.services: This package is related to the implementation of; services. This includes a Java/Scala client for Hail Batch, the; shuffle service client and server, and support for TLS,; authentication, deployment configuration, etc. * is.hail.rvd: The fundamental abstraction in Spark is the resilient; distributed dataset (RDD). When Hail generates Spark pipelines to; implement queries, it generates an RDD of off-(Java-)heap region; values. An RVD is a Region Value Dataset, an abstraction for a RDD; of region values. * is.hail.backend: each Python backend (Spark, local, service) has a; corresponding backend in Scala. ## Microservice Overview. The Hail service is implemented as a collection of microservices (for; the list, see below). The services are implemented in Python (mostly); and Java/Scala (or a mix, using py4j). The code for the services can; be found at the top-level, e.g. the batch and batch-driver; implementation can be found in $HAIL/batch. For publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using ",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:8820,Modifiability,config,config,8820,"ala (or a mix, using py4j). The code for the services can; be found at the top-level, e.g. the batch and batch-driver; implementation can be found in $HAIL/batch. For publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using Python asyncio,; [aiohttp](https://docs.aiohttp.org/en/stable/) and Jinja2 for HTML; templating. Some services rely on 3rd party services. Those include:. * ci depends on GitHub. * batch, ci and auth depend on K8s. * batch depends on K8s and GCP. * website depends (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've implemented our own continuous integration and continuous; deployed (CI/CD) system. It also hosts a developer status board; at https://ci.hail.is/me. * gateway: gateway is an nginx reverse proxy that terminates TLS; connections and forwards requests to services in K8s. It is; possible to run multiple copies of the Hail services in K8s, each; set in a separate K8s namespace. gateway forwards requests to the; K8s service",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:8859,Modifiability,config,configuration,8859,"ala (or a mix, using py4j). The code for the services can; be found at the top-level, e.g. the batch and batch-driver; implementation can be found in $HAIL/batch. For publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using Python asyncio,; [aiohttp](https://docs.aiohttp.org/en/stable/) and Jinja2 for HTML; templating. Some services rely on 3rd party services. Those include:. * ci depends on GitHub. * batch, ci and auth depend on K8s. * batch depends on K8s and GCP. * website depends (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've implemented our own continuous integration and continuous; deployed (CI/CD) system. It also hosts a developer status board; at https://ci.hail.is/me. * gateway: gateway is an nginx reverse proxy that terminates TLS; connections and forwards requests to services in K8s. It is; possible to run multiple copies of the Hail services in K8s, each; set in a separate K8s namespace. gateway forwards requests to the; K8s service",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:2519,Performance,optimiz,optimizer,2519,". ## Hail Query. The `hail` module is implemented in $HAIL/hail/python/hail, this is the Hail Query Python interface.; The Hail Query interface is lazy: executing a pipeline builds an intermediate representation; representing the query. The IR is implemented in the `hail.ir` submodule. When a query is ready to; be executed, it is sent to a backend, implemented in `hail.backend`. There are three backends:; SparkBackend, LocalBackend and ServiceBackend. At the time of writing, the SparkBackend is complete; and the ServiceBackend is mostly complete (it is missing a PCA implementation with clear error; bounds and some block matrix operations do not scale). The LocalBackend has languished. The Spark backend works as follows. The IR is serialized and sent to; a JVM child process via [py4j](https://www.py4j.org/). The entrypoint; for this is the Scala class is.hail.backend.spark.SparkBackend. The; SparkBackend parses the IR, runs a query optimizer on it, generates; custom JVM bytecode for the query which is called from a Spark; computational graph, which it then submits to Spark for execution.; The result is then returned via py4j back to Python and the user. The ServiceBackend is structured differently. The IR is again; serialized, but is written to cloud storage and a job is submitted; to Hail Batch to execute the aforementioned IR. The local backend is similar to the Spark backend, except the; execution is performed on the local machine in the JVM instead of; being submitted to Spark. ## Hail Batch. A batch is a graph of jobs with dependencies. A job includes; configuration for a container to execute, including image, inputs,; outputs, command line, environment variables, etc. The Hail Batch; Python interface provides and high-level interface for constructing; batches of jobs. A batch can then be submitted to a backend for; execution. There are two backends: LocalBackend and ServiceBackend. The local backend executes the batch on the local machine. The service backend se",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:3000,Performance,perform,performed,3000,"lBackend and ServiceBackend. At the time of writing, the SparkBackend is complete; and the ServiceBackend is mostly complete (it is missing a PCA implementation with clear error; bounds and some block matrix operations do not scale). The LocalBackend has languished. The Spark backend works as follows. The IR is serialized and sent to; a JVM child process via [py4j](https://www.py4j.org/). The entrypoint; for this is the Scala class is.hail.backend.spark.SparkBackend. The; SparkBackend parses the IR, runs a query optimizer on it, generates; custom JVM bytecode for the query which is called from a Spark; computational graph, which it then submits to Spark for execution.; The result is then returned via py4j back to Python and the user. The ServiceBackend is structured differently. The IR is again; serialized, but is written to cloud storage and a job is submitted; to Hail Batch to execute the aforementioned IR. The local backend is similar to the Spark backend, except the; execution is performed on the local machine in the JVM instead of; being submitted to Spark. ## Hail Batch. A batch is a graph of jobs with dependencies. A job includes; configuration for a container to execute, including image, inputs,; outputs, command line, environment variables, etc. The Hail Batch; Python interface provides and high-level interface for constructing; batches of jobs. A batch can then be submitted to a backend for; execution. There are two backends: LocalBackend and ServiceBackend. The local backend executes the batch on the local machine. The service backend serializes the batch as JSON and submits it to the; batch microservice at https://batch.hail.is/. The Batch service then; executes the graph on a fleet of GCP virtual machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch servi",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:5793,Performance,optimiz,optimizer,5793,"ML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime values: as JVM; objects or as a pointer to explicitly managed memory off the Java; heap called a region value. Annotation also sometimes refer to just; the JVM object representation. Explicitly managed off-(Java-)heap; values are also referred to as ""unsafe"". * is.hail.asm4s: The Hail Query optimizer generates JVM bytecode to; implement queries. asm4s is a high-level Scala interface for; generating JVM bytecode. * is.hail.lir: lir is a low-level intermediate representation (IR) for; JVM bytecode. The high-level asm4s interface is implemented in; terms of lir. lir can generate raw JVM bytecode that can be loaded; into the interpreter and invoked via reflection. * is.hail.types: Hail Query has several different notions of types.; For values, there are three kinds of type: virtual, physical and; encoded types. Virtual types are usual-visible types like int,; array and string. Physical types are implementations of virtual; types. For example, arrays might be stored densely or sparsely.; Encoded types specify how to (de)serialize types for reading and; writing. There also higher-level types for Tables, MatrixTables and; BlockMatrices. * is.hail.expr: expr is a large, disorganized package that contains; the Hail Query IR, query optimizer and IR code generator. * is.hail.io.fs: fs contains an ab",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:6113,Performance,load,loaded,6113,"ommand-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime values: as JVM; objects or as a pointer to explicitly managed memory off the Java; heap called a region value. Annotation also sometimes refer to just; the JVM object representation. Explicitly managed off-(Java-)heap; values are also referred to as ""unsafe"". * is.hail.asm4s: The Hail Query optimizer generates JVM bytecode to; implement queries. asm4s is a high-level Scala interface for; generating JVM bytecode. * is.hail.lir: lir is a low-level intermediate representation (IR) for; JVM bytecode. The high-level asm4s interface is implemented in; terms of lir. lir can generate raw JVM bytecode that can be loaded; into the interpreter and invoked via reflection. * is.hail.types: Hail Query has several different notions of types.; For values, there are three kinds of type: virtual, physical and; encoded types. Virtual types are usual-visible types like int,; array and string. Physical types are implementations of virtual; types. For example, arrays might be stored densely or sparsely.; Encoded types specify how to (de)serialize types for reading and; writing. There also higher-level types for Tables, MatrixTables and; BlockMatrices. * is.hail.expr: expr is a large, disorganized package that contains; the Hail Query IR, query optimizer and IR code generator. * is.hail.io.fs: fs contains an abstract filesystem interface, FS, and; implementations for Google Storage and Hadoop. We need to implement; one for local UNIX filesystems. * is.hail.io: This includes support for reading and writing a variety; of file formats. * is.hail.services: This package is related to the implementation of; services. T",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:6743,Performance,optimiz,optimizer,6743,"p; values are also referred to as ""unsafe"". * is.hail.asm4s: The Hail Query optimizer generates JVM bytecode to; implement queries. asm4s is a high-level Scala interface for; generating JVM bytecode. * is.hail.lir: lir is a low-level intermediate representation (IR) for; JVM bytecode. The high-level asm4s interface is implemented in; terms of lir. lir can generate raw JVM bytecode that can be loaded; into the interpreter and invoked via reflection. * is.hail.types: Hail Query has several different notions of types.; For values, there are three kinds of type: virtual, physical and; encoded types. Virtual types are usual-visible types like int,; array and string. Physical types are implementations of virtual; types. For example, arrays might be stored densely or sparsely.; Encoded types specify how to (de)serialize types for reading and; writing. There also higher-level types for Tables, MatrixTables and; BlockMatrices. * is.hail.expr: expr is a large, disorganized package that contains; the Hail Query IR, query optimizer and IR code generator. * is.hail.io.fs: fs contains an abstract filesystem interface, FS, and; implementations for Google Storage and Hadoop. We need to implement; one for local UNIX filesystems. * is.hail.io: This includes support for reading and writing a variety; of file formats. * is.hail.services: This package is related to the implementation of; services. This includes a Java/Scala client for Hail Batch, the; shuffle service client and server, and support for TLS,; authentication, deployment configuration, etc. * is.hail.rvd: The fundamental abstraction in Spark is the resilient; distributed dataset (RDD). When Hail generates Spark pipelines to; implement queries, it generates an RDD of off-(Java-)heap region; values. An RVD is a Region Value Dataset, an abstraction for a RDD; of region values. * is.hail.backend: each Python backend (Spark, local, service) has a; corresponding backend in Scala. ## Microservice Overview. The Hail service is impl",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:5752,Safety,unsafe,unsafe,5752,"rious; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime values: as JVM; objects or as a pointer to explicitly managed memory off the Java; heap called a region value. Annotation also sometimes refer to just; the JVM object representation. Explicitly managed off-(Java-)heap; values are also referred to as ""unsafe"". * is.hail.asm4s: The Hail Query optimizer generates JVM bytecode to; implement queries. asm4s is a high-level Scala interface for; generating JVM bytecode. * is.hail.lir: lir is a low-level intermediate representation (IR) for; JVM bytecode. The high-level asm4s interface is implemented in; terms of lir. lir can generate raw JVM bytecode that can be loaded; into the interpreter and invoked via reflection. * is.hail.types: Hail Query has several different notions of types.; For values, there are three kinds of type: virtual, physical and; encoded types. Virtual types are usual-visible types like int,; array and string. Physical types are implementations of virtual; types. For example, arrays might be stored densely or sparsely.; Encoded types specify how to (de)serialize types for reading and; writing. There also higher-level types for Tables, MatrixTables and; BlockMatrices. * is.hail.expr: expr is a large, disorganized package that contains; the ",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:712,Security,expose,exposes,712,"# Hail. This document gives an overview of the Hail architecture and source; code. ## Background. Hail source code is stored in a monolithic repository (monorepo) on; GitHub at hail-is/hail. $HAIL will denote the repository root below.; Hail is open source and developed in the open. Hail is written in Python, Java/Scala (both JVM langauges), and C/C++. Hail has two user-facing components: Hail Query and Hail Batch. Both; provide Python interfaces. Hail Query is for distributed, out-of-core; manipulation and analysis of tabular and genomic data. Hail Batch is; for the execution of graphs containers. The Hail client libraries are deployed in the Python Package Index; (PyPI) hail package. The Hail package exposes two Python modules:; `hail` and `hailtop`. `hail` provides the Hail Query interface.; `hailtop.batch` provides the batch interface. `hailtop` contains; various other infrastructure submodules (see below). The hail package also contains the `hailctl` command line utility that; provides a number of features, including managing Google Dataproc (a; hosted Spark service), Hail service authorization, Hail configuration,; interacting with the Hail developer tools, and managing the Hail Batch; service. `hailctl` is implemented in the `hailtop.hailctl` submodule. Users can run Hail on their own system, or on the Hail service that is; operated by the Hail team. The Hail service is implemented by a; collection of microservices that are deployed on Kubernetes (K8s) and; Google Cloud Platform (GCP). We refer to K8s and GCP as our Virtual; Datacenter (VDC). ## Hail Query. The `hail` module is implemented in $HAIL/hail/python/hail, this is the Hail Query Python interface.; The Hail Query interface is lazy: executing a pipeline builds an intermediate representation; representing the query. The IR is implemented in the `hail.ir` submodule. When a query is ready to; be executed, it is sent to a backend, implemented in `hail.backend`. There are three backends:; SparkBackend, Loca",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:1103,Security,authoriz,authorization,1103,"urce; code. ## Background. Hail source code is stored in a monolithic repository (monorepo) on; GitHub at hail-is/hail. $HAIL will denote the repository root below.; Hail is open source and developed in the open. Hail is written in Python, Java/Scala (both JVM langauges), and C/C++. Hail has two user-facing components: Hail Query and Hail Batch. Both; provide Python interfaces. Hail Query is for distributed, out-of-core; manipulation and analysis of tabular and genomic data. Hail Batch is; for the execution of graphs containers. The Hail client libraries are deployed in the Python Package Index; (PyPI) hail package. The Hail package exposes two Python modules:; `hail` and `hailtop`. `hail` provides the Hail Query interface.; `hailtop.batch` provides the batch interface. `hailtop` contains; various other infrastructure submodules (see below). The hail package also contains the `hailctl` command line utility that; provides a number of features, including managing Google Dataproc (a; hosted Spark service), Hail service authorization, Hail configuration,; interacting with the Hail developer tools, and managing the Hail Batch; service. `hailctl` is implemented in the `hailtop.hailctl` submodule. Users can run Hail on their own system, or on the Hail service that is; operated by the Hail team. The Hail service is implemented by a; collection of microservices that are deployed on Kubernetes (K8s) and; Google Cloud Platform (GCP). We refer to K8s and GCP as our Virtual; Datacenter (VDC). ## Hail Query. The `hail` module is implemented in $HAIL/hail/python/hail, this is the Hail Query Python interface.; The Hail Query interface is lazy: executing a pipeline builds an intermediate representation; representing the query. The IR is implemented in the `hail.ir` submodule. When a query is ready to; be executed, it is sent to a backend, implemented in `hail.backend`. There are three backends:; SparkBackend, LocalBackend and ServiceBackend. At the time of writing, the SparkBackend i",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:4518,Security,authoriz,authorization,4518,"l backend executes the batch on the local machine. The service backend serializes the batch as JSON and submits it to the; batch microservice at https://batch.hail.is/. The Batch service then; executes the graph on a fleet of GCP virtual machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime va",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:7229,Security,authenticat,authentication,7229,"ry has several different notions of types.; For values, there are three kinds of type: virtual, physical and; encoded types. Virtual types are usual-visible types like int,; array and string. Physical types are implementations of virtual; types. For example, arrays might be stored densely or sparsely.; Encoded types specify how to (de)serialize types for reading and; writing. There also higher-level types for Tables, MatrixTables and; BlockMatrices. * is.hail.expr: expr is a large, disorganized package that contains; the Hail Query IR, query optimizer and IR code generator. * is.hail.io.fs: fs contains an abstract filesystem interface, FS, and; implementations for Google Storage and Hadoop. We need to implement; one for local UNIX filesystems. * is.hail.io: This includes support for reading and writing a variety; of file formats. * is.hail.services: This package is related to the implementation of; services. This includes a Java/Scala client for Hail Batch, the; shuffle service client and server, and support for TLS,; authentication, deployment configuration, etc. * is.hail.rvd: The fundamental abstraction in Spark is the resilient; distributed dataset (RDD). When Hail generates Spark pipelines to; implement queries, it generates an RDD of off-(Java-)heap region; values. An RVD is a Region Value Dataset, an abstraction for a RDD; of region values. * is.hail.backend: each Python backend (Spark, local, service) has a; corresponding backend in Scala. ## Microservice Overview. The Hail service is implemented as a collection of microservices (for; the list, see below). The services are implemented in Python (mostly); and Java/Scala (or a mix, using py4j). The code for the services can; be found at the top-level, e.g. the batch and batch-driver; implementation can be found in $HAIL/batch. For publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using ",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:8022,Security,access,accessible,8022,".services: This package is related to the implementation of; services. This includes a Java/Scala client for Hail Batch, the; shuffle service client and server, and support for TLS,; authentication, deployment configuration, etc. * is.hail.rvd: The fundamental abstraction in Spark is the resilient; distributed dataset (RDD). When Hail generates Spark pipelines to; implement queries, it generates an RDD of off-(Java-)heap region; values. An RVD is a Region Value Dataset, an abstraction for a RDD; of region values. * is.hail.backend: each Python backend (Spark, local, service) has a; corresponding backend in Scala. ## Microservice Overview. The Hail service is implemented as a collection of microservices (for; the list, see below). The services are implemented in Python (mostly); and Java/Scala (or a mix, using py4j). The code for the services can; be found at the top-level, e.g. the batch and batch-driver; implementation can be found in $HAIL/batch. For publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using Python asyncio,; [aiohttp](https://docs.aiohttp.org/en/stable/) and Jinja2 for HTML; templating. Some services rely on 3rd party services. Those include:. * ci depends on GitHub. * batch, ci and auth depend on K8s. * batch depends on K8s and GCP. * website depends (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:8907,Security,authoriz,authorization,8907,"e found at the top-level, e.g. the batch and batch-driver; implementation can be found in $HAIL/batch. For publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using Python asyncio,; [aiohttp](https://docs.aiohttp.org/en/stable/) and Jinja2 for HTML; templating. Some services rely on 3rd party services. Those include:. * ci depends on GitHub. * batch, ci and auth depend on K8s. * batch depends on K8s and GCP. * website depends (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've implemented our own continuous integration and continuous; deployed (CI/CD) system. It also hosts a developer status board; at https://ci.hail.is/me. * gateway: gateway is an nginx reverse proxy that terminates TLS; connections and forwards requests to services in K8s. It is; possible to run multiple copies of the Hail services in K8s, each; set in a separate K8s namespace. gateway forwards requests to the; K8s service in the appropriate namespace. * internal-gateway: batch wor",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:9178,Security,encrypt,encrypting,9178," using Python asyncio,; [aiohttp](https://docs.aiohttp.org/en/stable/) and Jinja2 for HTML; templating. Some services rely on 3rd party services. Those include:. * ci depends on GitHub. * batch, ci and auth depend on K8s. * batch depends on K8s and GCP. * website depends (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've implemented our own continuous integration and continuous; deployed (CI/CD) system. It also hosts a developer status board; at https://ci.hail.is/me. * gateway: gateway is an nginx reverse proxy that terminates TLS; connections and forwards requests to services in K8s. It is; possible to run multiple copies of the Hail services in K8s, each; set in a separate K8s namespace. gateway forwards requests to the; K8s service in the appropriate namespace. * internal-gateway: batch workers run on GCE VMs, not in K8s. The; internal-gateway is an nginx reverse proxy that terminates; connections from the Google Virtual Private Cloud (VPC) network and; connections to the services in K8s. * website: site implements the main Hail website https://hail.is/; including the",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:9290,Security,authoriz,authorization,9290,"ly on 3rd party services. Those include:. * ci depends on GitHub. * batch, ci and auth depend on K8s. * batch depends on K8s and GCP. * website depends (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've implemented our own continuous integration and continuous; deployed (CI/CD) system. It also hosts a developer status board; at https://ci.hail.is/me. * gateway: gateway is an nginx reverse proxy that terminates TLS; connections and forwards requests to services in K8s. It is; possible to run multiple copies of the Hail services in K8s, each; set in a separate K8s namespace. gateway forwards requests to the; K8s service in the appropriate namespace. * internal-gateway: batch workers run on GCE VMs, not in K8s. The; internal-gateway is an nginx reverse proxy that terminates; connections from the Google Virtual Private Cloud (VPC) network and; connections to the services in K8s. * website: site implements the main Hail website https://hail.is/; including the landing page and Hail Query and Hail Batch; documentation. There are two types of services: managed and unmanaged.; CI",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:9305,Security,authoriz,authorization,9305,"ly on 3rd party services. Those include:. * ci depends on GitHub. * batch, ci and auth depend on K8s. * batch depends on K8s and GCP. * website depends (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've implemented our own continuous integration and continuous; deployed (CI/CD) system. It also hosts a developer status board; at https://ci.hail.is/me. * gateway: gateway is an nginx reverse proxy that terminates TLS; connections and forwards requests to services in K8s. It is; possible to run multiple copies of the Hail services in K8s, each; set in a separate K8s namespace. gateway forwards requests to the; K8s service in the appropriate namespace. * internal-gateway: batch workers run on GCE VMs, not in K8s. The; internal-gateway is an nginx reverse proxy that terminates; connections from the Google Virtual Private Cloud (VPC) network and; connections to the services in K8s. * website: site implements the main Hail website https://hail.is/; including the landing page and Hail Query and Hail Batch; documentation. There are two types of services: managed and unmanaged.; CI",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:4013,Testability,benchmark,benchmark,4013,". ## Hail Batch. A batch is a graph of jobs with dependencies. A job includes; configuration for a container to execute, including image, inputs,; outputs, command line, environment variables, etc. The Hail Batch; Python interface provides and high-level interface for constructing; batches of jobs. A batch can then be submitted to a backend for; execution. There are two backends: LocalBackend and ServiceBackend. The local backend executes the batch on the local machine. The service backend serializes the batch as JSON and submits it to the; batch microservice at https://batch.hail.is/. The Batch service then; executes the graph on a fleet of GCP virtual machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/ha",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:4035,Testability,benchmark,benchmarking,4035,". ## Hail Batch. A batch is a graph of jobs with dependencies. A job includes; configuration for a container to execute, including image, inputs,; outputs, command line, environment variables, etc. The Hail Batch; Python interface provides and high-level interface for constructing; batches of jobs. A batch can then be submitted to a backend for; execution. There are two backends: LocalBackend and ServiceBackend. The local backend executes the batch on the local machine. The service backend serializes the batch as JSON and submits it to the; batch microservice at https://batch.hail.is/. The Batch service then; executes the graph on a fleet of GCP virtual machines called workers. ## Source Code Organization. Here are some key source code locations:. Hail Query:. * $HAIL/hail/python/hail: the Hail Query Python interface; * $HAIL/hail/src: the Hail Query Java/Scala code. Hail Batch:. * $HAIL/batch: the batch service; * $HAIL/benchmark: Hail Query benchmarking tools; * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/ha",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:4901,Testability,test,test,4901," * $HAIL/hail/python/hailtop/batch: Python Batch interface; * $HAIL/hail/python/hailtop/batch_client: low-level Batch client library. Services (see below for descriptions):. * $HAIL/auth; * $HAIL/batch; * $HAIL/ci; * $HAIL/gateway; * $HAIL/internal-gateway; * $HAIL/website. Libraries for services:. * $HAIL/gear: gear services library; * $HAIL/hail/python/hailtop/aiocloud: asyncio client libraries for AWS, GCP, and Azure; * $HAIL/hail/python/hailtop/auth: user authorization library; * $HAIL/hail/python/hailtop/config: user and deployment configuration library; * $HAIL/hail/python/hailtop/tls.py: TLS utilities for services; * $HAIL/hail/python/hailtop/utils: various; * $HAIL/tls: for TLS configuration and deployment; * $HAIL/web_common: service HTML templates. Other:. * $HAIL/blog: blog configuration; * $HAIL/build.yaml: the Hail build, test and deployment configuration; * $HAIL/datasets: ETL code for the Hail Query Datasets API; * $HAIL/docker: base docker images used by services; * $HAIL/hail/python/hailtop/hailctl: implementation of the hailctl command-line tool; * $HAIL/ukbb-rg: UKBB genetic correlation browser configuration, available at https://ukbb-rg.hail.is/. ## Hail Query Java/Scala Code Organization. This section is not complete. * is.hail.annotation: For historical reasons, a Hail Query runtime; value (e.g. an int, array, string, etc.) is called an annotation.; In the JVM, there are two representations of runtime values: as JVM; objects or as a pointer to explicitly managed memory off the Java; heap called a region value. Annotation also sometimes refer to just; the JVM object representation. Explicitly managed off-(Java-)heap; values are also referred to as ""unsafe"". * is.hail.asm4s: The Hail Query optimizer generates JVM bytecode to; implement queries. asm4s is a high-level Scala interface for; generating JVM bytecode. * is.hail.lir: lir is a low-level intermediate representation (IR) for; JVM bytecode. The high-level asm4s interface is implemented in; t",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:9035,Testability,log,logging,9035,"publicly accessible; services, they are available at https://<service>.hail.is/, e.g. the; batch service is available at https://batch.hail.is/. Python services are implemented using Python asyncio,; [aiohttp](https://docs.aiohttp.org/en/stable/) and Jinja2 for HTML; templating. Some services rely on 3rd party services. Those include:. * ci depends on GitHub. * batch, ci and auth depend on K8s. * batch depends on K8s and GCP. * website depends (client-side) on Algolia for search. Services store state in a managed MySQL Google CloudSQL instance. There is a collection of libraries to facilitate service development:. * `hailtop.aiogoogle`: asyncio client libraries for Google services.; There are currently libraries for GCE, GCR, IAM, Stackdriver Logging; and (in progress) Google Storage. * `hailtop.config`: to manage user and deployment configuration. * `hailtop.auth`: to manage user authorization tokens. * `hailtop.utils`: various. * `gear`: verifying user credentials, CSRF protection, a database; interface, logging and user session management. * `web_common`: common HTML templates for the services Web UI (except; site). * `tls`: infrastructure for encrypting internal communication between; services. ## List of Microservices. * auth and auth-driver: for user authorization, authorization token; verification and account management. * batch and batch-driver: the Hail Batch service. * ci: We've implemented our own continuous integration and continuous; deployed (CI/CD) system. It also hosts a developer status board; at https://ci.hail.is/me. * gateway: gateway is an nginx reverse proxy that terminates TLS; connections and forwards requests to services in K8s. It is; possible to run multiple copies of the Hail services in K8s, each; set in a separate K8s namespace. gateway forwards requests to the; K8s service in the appropriate namespace. * internal-gateway: batch workers run on GCE VMs, not in K8s. The; internal-gateway is an nginx reverse proxy that terminates; connectio",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md:2167,Usability,clear,clear,2167,"ation,; interacting with the Hail developer tools, and managing the Hail Batch; service. `hailctl` is implemented in the `hailtop.hailctl` submodule. Users can run Hail on their own system, or on the Hail service that is; operated by the Hail team. The Hail service is implemented by a; collection of microservices that are deployed on Kubernetes (K8s) and; Google Cloud Platform (GCP). We refer to K8s and GCP as our Virtual; Datacenter (VDC). ## Hail Query. The `hail` module is implemented in $HAIL/hail/python/hail, this is the Hail Query Python interface.; The Hail Query interface is lazy: executing a pipeline builds an intermediate representation; representing the query. The IR is implemented in the `hail.ir` submodule. When a query is ready to; be executed, it is sent to a backend, implemented in `hail.backend`. There are three backends:; SparkBackend, LocalBackend and ServiceBackend. At the time of writing, the SparkBackend is complete; and the ServiceBackend is mostly complete (it is missing a PCA implementation with clear error; bounds and some block matrix operations do not scale). The LocalBackend has languished. The Spark backend works as follows. The IR is serialized and sent to; a JVM child process via [py4j](https://www.py4j.org/). The entrypoint; for this is the Scala class is.hail.backend.spark.SparkBackend. The; SparkBackend parses the IR, runs a query optimizer on it, generates; custom JVM bytecode for the query which is called from a Spark; computational graph, which it then submits to Spark for execution.; The result is then returned via py4j back to Python and the user. The ServiceBackend is structured differently. The IR is again; serialized, but is written to cloud storage and a job is submitted; to Hail Batch to execute the aforementioned IR. The local backend is similar to the Spark backend, except the; execution is performed on the local machine in the JVM instead of; being submitted to Spark. ## Hail Batch. A batch is a graph of jobs with depen",MatchSource.DOCS,dev-docs/hail-overview.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-overview.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md:502,Availability,error,errors,502,"# Process for Production Issues. When a production issue is entered into this system, it must be given a priority. These are the priorities [1]:; 1. P1: Service is unusable or business impact is critical.; 2. P2: Service use severely impaired or business impact is severe.; 3. P3: Service use partially impaired or business impact is minor. Examples of P1 issues:; - A user cannot submit a batch.; - A user cannot access the batch.hail.is UI.; - A majority of a user's jobs are failing due to non-user errors.; - Hail team is incurring unexpected cost 100 USD/hour or 500 USD/day. [2]; - A private key has been leaked.; - An exploit granting arbitrary code execution.; - An exploit is discovered that grants access to a user's data.; - CI is not merging PRs. Examples of P2 issues:; - A user is experiencing latencies higher than 5 seconds to load a UI page.; - User-facing API endpoint latencies are 5x the expected latency (e.g. last month's average latency).; - A user is experiencing high latency for image pulling, job scheduling, or batch submission.; - Hail team is incurring unexpected cost >25 USD/hour or 100 USD/day.; - An exploit is discovered that, under special circumstances, could leak a private key.; - A bug present in the current version of the Hail PyPI library prevents use, but previous versions; still work.; - CI transiently fails one in four PRs. P3 issues are usually P1 or P2 issues that have been mitigated. The urgency of the priorities:; 1. A developer should immediately interrupt their daily work to address a P1 issue.; 2. A developer should address a P2 issue within three business days.; 3. A developer should resolve a P3 issue within twenty business days. P1 issues can be mitigated such that they are re-characterized as a P2 issue. Likewise, P2 issues; may be mitigated into P3 issues. Examples of P1 mitigations:; - A user cannot submit batches due to a bug in hailtop. If the user can revert to a previous version; of Hail, then the issue is mitigated.; - Batc",MatchSource.DOCS,dev-docs/hail-production-issues.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md:1024,Energy Efficiency,schedul,scheduling,1024,"s for Production Issues. When a production issue is entered into this system, it must be given a priority. These are the priorities [1]:; 1. P1: Service is unusable or business impact is critical.; 2. P2: Service use severely impaired or business impact is severe.; 3. P3: Service use partially impaired or business impact is minor. Examples of P1 issues:; - A user cannot submit a batch.; - A user cannot access the batch.hail.is UI.; - A majority of a user's jobs are failing due to non-user errors.; - Hail team is incurring unexpected cost 100 USD/hour or 500 USD/day. [2]; - A private key has been leaked.; - An exploit granting arbitrary code execution.; - An exploit is discovered that grants access to a user's data.; - CI is not merging PRs. Examples of P2 issues:; - A user is experiencing latencies higher than 5 seconds to load a UI page.; - User-facing API endpoint latencies are 5x the expected latency (e.g. last month's average latency).; - A user is experiencing high latency for image pulling, job scheduling, or batch submission.; - Hail team is incurring unexpected cost >25 USD/hour or 100 USD/day.; - An exploit is discovered that, under special circumstances, could leak a private key.; - A bug present in the current version of the Hail PyPI library prevents use, but previous versions; still work.; - CI transiently fails one in four PRs. P3 issues are usually P1 or P2 issues that have been mitigated. The urgency of the priorities:; 1. A developer should immediately interrupt their daily work to address a P1 issue.; 2. A developer should address a P2 issue within three business days.; 3. A developer should resolve a P3 issue within twenty business days. P1 issues can be mitigated such that they are re-characterized as a P2 issue. Likewise, P2 issues; may be mitigated into P3 issues. Examples of P1 mitigations:; - A user cannot submit batches due to a bug in hailtop. If the user can revert to a previous version; of Hail, then the issue is mitigated.; - Batch is off",MatchSource.DOCS,dev-docs/hail-production-issues.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md:3073,Energy Efficiency,charge,charge,3073,"ould leak a private key.; - A bug present in the current version of the Hail PyPI library prevents use, but previous versions; still work.; - CI transiently fails one in four PRs. P3 issues are usually P1 or P2 issues that have been mitigated. The urgency of the priorities:; 1. A developer should immediately interrupt their daily work to address a P1 issue.; 2. A developer should address a P2 issue within three business days.; 3. A developer should resolve a P3 issue within twenty business days. P1 issues can be mitigated such that they are re-characterized as a P2 issue. Likewise, P2 issues; may be mitigated into P3 issues. Examples of P1 mitigations:; - A user cannot submit batches due to a bug in hailtop. If the user can revert to a previous version; of Hail, then the issue is mitigated.; - Batch is off the rails churning through machines without doing any billable work. If freezing; Batch stops the churn, then the issue is mitigated.; - A private key is leaked. If the key is rotated and the leaking channel is temporarily shut off,; then the issue is mitigated to P2. Examples of P2 mitigations:; - Due to database memory or CPU limitations, one in four CI jobs are failing. If doubling database; memory or CPU resolves the issue, then the issue is mitigated.; - A bug is causing one in four CI jobs to fail. If a small number of tests are failing, disabling; those tests mitigates the issue.; - Batch is experiencing higher than usual latencies for all user-facing APIs. If shrinking the; cluster restores latencies to an acceptable range, then the issue is mitigated. [1] Inspired by [Google's priorities](https://cloud.google.com/support/docs/procedures#support_case_priority). [2] I arrived at these numbers by considering the cost to Hail team if a P1 issue goes unresolved; for 16 hours (e.g. 5p - 9a) or three days (a long weekend). A 1500 USD unexpected charge is not; a problem for Hail's budget, but repeated, unaddressed P1 issues would accumulate into a real; problem. ",MatchSource.DOCS,dev-docs/hail-production-issues.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md:843,Performance,load,load,843,"# Process for Production Issues. When a production issue is entered into this system, it must be given a priority. These are the priorities [1]:; 1. P1: Service is unusable or business impact is critical.; 2. P2: Service use severely impaired or business impact is severe.; 3. P3: Service use partially impaired or business impact is minor. Examples of P1 issues:; - A user cannot submit a batch.; - A user cannot access the batch.hail.is UI.; - A majority of a user's jobs are failing due to non-user errors.; - Hail team is incurring unexpected cost 100 USD/hour or 500 USD/day. [2]; - A private key has been leaked.; - An exploit granting arbitrary code execution.; - An exploit is discovered that grants access to a user's data.; - CI is not merging PRs. Examples of P2 issues:; - A user is experiencing latencies higher than 5 seconds to load a UI page.; - User-facing API endpoint latencies are 5x the expected latency (e.g. last month's average latency).; - A user is experiencing high latency for image pulling, job scheduling, or batch submission.; - Hail team is incurring unexpected cost >25 USD/hour or 100 USD/day.; - An exploit is discovered that, under special circumstances, could leak a private key.; - A bug present in the current version of the Hail PyPI library prevents use, but previous versions; still work.; - CI transiently fails one in four PRs. P3 issues are usually P1 or P2 issues that have been mitigated. The urgency of the priorities:; 1. A developer should immediately interrupt their daily work to address a P1 issue.; 2. A developer should address a P2 issue within three business days.; 3. A developer should resolve a P3 issue within twenty business days. P1 issues can be mitigated such that they are re-characterized as a P2 issue. Likewise, P2 issues; may be mitigated into P3 issues. Examples of P1 mitigations:; - A user cannot submit batches due to a bug in hailtop. If the user can revert to a previous version; of Hail, then the issue is mitigated.; - Batc",MatchSource.DOCS,dev-docs/hail-production-issues.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md:917,Performance,latency,latency,917,"# Process for Production Issues. When a production issue is entered into this system, it must be given a priority. These are the priorities [1]:; 1. P1: Service is unusable or business impact is critical.; 2. P2: Service use severely impaired or business impact is severe.; 3. P3: Service use partially impaired or business impact is minor. Examples of P1 issues:; - A user cannot submit a batch.; - A user cannot access the batch.hail.is UI.; - A majority of a user's jobs are failing due to non-user errors.; - Hail team is incurring unexpected cost 100 USD/hour or 500 USD/day. [2]; - A private key has been leaked.; - An exploit granting arbitrary code execution.; - An exploit is discovered that grants access to a user's data.; - CI is not merging PRs. Examples of P2 issues:; - A user is experiencing latencies higher than 5 seconds to load a UI page.; - User-facing API endpoint latencies are 5x the expected latency (e.g. last month's average latency).; - A user is experiencing high latency for image pulling, job scheduling, or batch submission.; - Hail team is incurring unexpected cost >25 USD/hour or 100 USD/day.; - An exploit is discovered that, under special circumstances, could leak a private key.; - A bug present in the current version of the Hail PyPI library prevents use, but previous versions; still work.; - CI transiently fails one in four PRs. P3 issues are usually P1 or P2 issues that have been mitigated. The urgency of the priorities:; 1. A developer should immediately interrupt their daily work to address a P1 issue.; 2. A developer should address a P2 issue within three business days.; 3. A developer should resolve a P3 issue within twenty business days. P1 issues can be mitigated such that they are re-characterized as a P2 issue. Likewise, P2 issues; may be mitigated into P3 issues. Examples of P1 mitigations:; - A user cannot submit batches due to a bug in hailtop. If the user can revert to a previous version; of Hail, then the issue is mitigated.; - Batc",MatchSource.DOCS,dev-docs/hail-production-issues.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md:952,Performance,latency,latency,952,"# Process for Production Issues. When a production issue is entered into this system, it must be given a priority. These are the priorities [1]:; 1. P1: Service is unusable or business impact is critical.; 2. P2: Service use severely impaired or business impact is severe.; 3. P3: Service use partially impaired or business impact is minor. Examples of P1 issues:; - A user cannot submit a batch.; - A user cannot access the batch.hail.is UI.; - A majority of a user's jobs are failing due to non-user errors.; - Hail team is incurring unexpected cost 100 USD/hour or 500 USD/day. [2]; - A private key has been leaked.; - An exploit granting arbitrary code execution.; - An exploit is discovered that grants access to a user's data.; - CI is not merging PRs. Examples of P2 issues:; - A user is experiencing latencies higher than 5 seconds to load a UI page.; - User-facing API endpoint latencies are 5x the expected latency (e.g. last month's average latency).; - A user is experiencing high latency for image pulling, job scheduling, or batch submission.; - Hail team is incurring unexpected cost >25 USD/hour or 100 USD/day.; - An exploit is discovered that, under special circumstances, could leak a private key.; - A bug present in the current version of the Hail PyPI library prevents use, but previous versions; still work.; - CI transiently fails one in four PRs. P3 issues are usually P1 or P2 issues that have been mitigated. The urgency of the priorities:; 1. A developer should immediately interrupt their daily work to address a P1 issue.; 2. A developer should address a P2 issue within three business days.; 3. A developer should resolve a P3 issue within twenty business days. P1 issues can be mitigated such that they are re-characterized as a P2 issue. Likewise, P2 issues; may be mitigated into P3 issues. Examples of P1 mitigations:; - A user cannot submit batches due to a bug in hailtop. If the user can revert to a previous version; of Hail, then the issue is mitigated.; - Batc",MatchSource.DOCS,dev-docs/hail-production-issues.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md:993,Performance,latency,latency,993,"s for Production Issues. When a production issue is entered into this system, it must be given a priority. These are the priorities [1]:; 1. P1: Service is unusable or business impact is critical.; 2. P2: Service use severely impaired or business impact is severe.; 3. P3: Service use partially impaired or business impact is minor. Examples of P1 issues:; - A user cannot submit a batch.; - A user cannot access the batch.hail.is UI.; - A majority of a user's jobs are failing due to non-user errors.; - Hail team is incurring unexpected cost 100 USD/hour or 500 USD/day. [2]; - A private key has been leaked.; - An exploit granting arbitrary code execution.; - An exploit is discovered that grants access to a user's data.; - CI is not merging PRs. Examples of P2 issues:; - A user is experiencing latencies higher than 5 seconds to load a UI page.; - User-facing API endpoint latencies are 5x the expected latency (e.g. last month's average latency).; - A user is experiencing high latency for image pulling, job scheduling, or batch submission.; - Hail team is incurring unexpected cost >25 USD/hour or 100 USD/day.; - An exploit is discovered that, under special circumstances, could leak a private key.; - A bug present in the current version of the Hail PyPI library prevents use, but previous versions; still work.; - CI transiently fails one in four PRs. P3 issues are usually P1 or P2 issues that have been mitigated. The urgency of the priorities:; 1. A developer should immediately interrupt their daily work to address a P1 issue.; 2. A developer should address a P2 issue within three business days.; 3. A developer should resolve a P3 issue within twenty business days. P1 issues can be mitigated such that they are re-characterized as a P2 issue. Likewise, P2 issues; may be mitigated into P3 issues. Examples of P1 mitigations:; - A user cannot submit batches due to a bug in hailtop. If the user can revert to a previous version; of Hail, then the issue is mitigated.; - Batch is off",MatchSource.DOCS,dev-docs/hail-production-issues.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md:414,Security,access,access,414,"# Process for Production Issues. When a production issue is entered into this system, it must be given a priority. These are the priorities [1]:; 1. P1: Service is unusable or business impact is critical.; 2. P2: Service use severely impaired or business impact is severe.; 3. P3: Service use partially impaired or business impact is minor. Examples of P1 issues:; - A user cannot submit a batch.; - A user cannot access the batch.hail.is UI.; - A majority of a user's jobs are failing due to non-user errors.; - Hail team is incurring unexpected cost 100 USD/hour or 500 USD/day. [2]; - A private key has been leaked.; - An exploit granting arbitrary code execution.; - An exploit is discovered that grants access to a user's data.; - CI is not merging PRs. Examples of P2 issues:; - A user is experiencing latencies higher than 5 seconds to load a UI page.; - User-facing API endpoint latencies are 5x the expected latency (e.g. last month's average latency).; - A user is experiencing high latency for image pulling, job scheduling, or batch submission.; - Hail team is incurring unexpected cost >25 USD/hour or 100 USD/day.; - An exploit is discovered that, under special circumstances, could leak a private key.; - A bug present in the current version of the Hail PyPI library prevents use, but previous versions; still work.; - CI transiently fails one in four PRs. P3 issues are usually P1 or P2 issues that have been mitigated. The urgency of the priorities:; 1. A developer should immediately interrupt their daily work to address a P1 issue.; 2. A developer should address a P2 issue within three business days.; 3. A developer should resolve a P3 issue within twenty business days. P1 issues can be mitigated such that they are re-characterized as a P2 issue. Likewise, P2 issues; may be mitigated into P3 issues. Examples of P1 mitigations:; - A user cannot submit batches due to a bug in hailtop. If the user can revert to a previous version; of Hail, then the issue is mitigated.; - Batc",MatchSource.DOCS,dev-docs/hail-production-issues.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md:708,Security,access,access,708,"# Process for Production Issues. When a production issue is entered into this system, it must be given a priority. These are the priorities [1]:; 1. P1: Service is unusable or business impact is critical.; 2. P2: Service use severely impaired or business impact is severe.; 3. P3: Service use partially impaired or business impact is minor. Examples of P1 issues:; - A user cannot submit a batch.; - A user cannot access the batch.hail.is UI.; - A majority of a user's jobs are failing due to non-user errors.; - Hail team is incurring unexpected cost 100 USD/hour or 500 USD/day. [2]; - A private key has been leaked.; - An exploit granting arbitrary code execution.; - An exploit is discovered that grants access to a user's data.; - CI is not merging PRs. Examples of P2 issues:; - A user is experiencing latencies higher than 5 seconds to load a UI page.; - User-facing API endpoint latencies are 5x the expected latency (e.g. last month's average latency).; - A user is experiencing high latency for image pulling, job scheduling, or batch submission.; - Hail team is incurring unexpected cost >25 USD/hour or 100 USD/day.; - An exploit is discovered that, under special circumstances, could leak a private key.; - A bug present in the current version of the Hail PyPI library prevents use, but previous versions; still work.; - CI transiently fails one in four PRs. P3 issues are usually P1 or P2 issues that have been mitigated. The urgency of the priorities:; 1. A developer should immediately interrupt their daily work to address a P1 issue.; 2. A developer should address a P2 issue within three business days.; 3. A developer should resolve a P3 issue within twenty business days. P1 issues can be mitigated such that they are re-characterized as a P2 issue. Likewise, P2 issues; may be mitigated into P3 issues. Examples of P1 mitigations:; - A user cannot submit batches due to a bug in hailtop. If the user can revert to a previous version; of Hail, then the issue is mitigated.; - Batc",MatchSource.DOCS,dev-docs/hail-production-issues.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md:2541,Testability,test,tests,2541,"ould leak a private key.; - A bug present in the current version of the Hail PyPI library prevents use, but previous versions; still work.; - CI transiently fails one in four PRs. P3 issues are usually P1 or P2 issues that have been mitigated. The urgency of the priorities:; 1. A developer should immediately interrupt their daily work to address a P1 issue.; 2. A developer should address a P2 issue within three business days.; 3. A developer should resolve a P3 issue within twenty business days. P1 issues can be mitigated such that they are re-characterized as a P2 issue. Likewise, P2 issues; may be mitigated into P3 issues. Examples of P1 mitigations:; - A user cannot submit batches due to a bug in hailtop. If the user can revert to a previous version; of Hail, then the issue is mitigated.; - Batch is off the rails churning through machines without doing any billable work. If freezing; Batch stops the churn, then the issue is mitigated.; - A private key is leaked. If the key is rotated and the leaking channel is temporarily shut off,; then the issue is mitigated to P2. Examples of P2 mitigations:; - Due to database memory or CPU limitations, one in four CI jobs are failing. If doubling database; memory or CPU resolves the issue, then the issue is mitigated.; - A bug is causing one in four CI jobs to fail. If a small number of tests are failing, disabling; those tests mitigates the issue.; - Batch is experiencing higher than usual latencies for all user-facing APIs. If shrinking the; cluster restores latencies to an acceptable range, then the issue is mitigated. [1] Inspired by [Google's priorities](https://cloud.google.com/support/docs/procedures#support_case_priority). [2] I arrived at these numbers by considering the cost to Hail team if a P1 issue goes unresolved; for 16 hours (e.g. 5p - 9a) or three days (a long weekend). A 1500 USD unexpected charge is not; a problem for Hail's budget, but repeated, unaddressed P1 issues would accumulate into a real; problem. ",MatchSource.DOCS,dev-docs/hail-production-issues.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md:2577,Testability,test,tests,2577,"ould leak a private key.; - A bug present in the current version of the Hail PyPI library prevents use, but previous versions; still work.; - CI transiently fails one in four PRs. P3 issues are usually P1 or P2 issues that have been mitigated. The urgency of the priorities:; 1. A developer should immediately interrupt their daily work to address a P1 issue.; 2. A developer should address a P2 issue within three business days.; 3. A developer should resolve a P3 issue within twenty business days. P1 issues can be mitigated such that they are re-characterized as a P2 issue. Likewise, P2 issues; may be mitigated into P3 issues. Examples of P1 mitigations:; - A user cannot submit batches due to a bug in hailtop. If the user can revert to a previous version; of Hail, then the issue is mitigated.; - Batch is off the rails churning through machines without doing any billable work. If freezing; Batch stops the churn, then the issue is mitigated.; - A private key is leaked. If the key is rotated and the leaking channel is temporarily shut off,; then the issue is mitigated to P2. Examples of P2 mitigations:; - Due to database memory or CPU limitations, one in four CI jobs are failing. If doubling database; memory or CPU resolves the issue, then the issue is mitigated.; - A bug is causing one in four CI jobs to fail. If a small number of tests are failing, disabling; those tests mitigates the issue.; - Batch is experiencing higher than usual latencies for all user-facing APIs. If shrinking the; cluster restores latencies to an acceptable range, then the issue is mitigated. [1] Inspired by [Google's priorities](https://cloud.google.com/support/docs/procedures#support_case_priority). [2] I arrived at these numbers by considering the cost to Hail team if a P1 issue goes unresolved; for 16 hours (e.g. 5p - 9a) or three days (a long weekend). A 1500 USD unexpected charge is not; a problem for Hail's budget, but repeated, unaddressed P1 issues would accumulate into a real; problem. ",MatchSource.DOCS,dev-docs/hail-production-issues.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-production-issues.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/pip-dependencies.md:11,Integrability,depend,dependencies,11,"# Updating dependencies. Pip dependencies for a particular python module should be listed in a; `requirements.txt` file, e.g. `$HAIL/hail/python/requirements.txt`.; The version should be the most permissive version range compatible with the hail; package. In each directory that contains a `requirements.txt` file there should; also be a `pinned-requirements.txt` file that contains a fully resolved; dependency tree with each pip dependency pinned to a specific version.; This allows deterministic builds in our Dockerfiles. When adding a dependency; to a `requirements.txt` file, run. ```bash; make generate-pip-lockfiles; ```. to regenerate the `pinnned-requirements.txt` files in the repository.; Note that the full dependency tree for a pip package can; differ on different operating systems. All services and docker images in CI; use the fully pinned requirements on a Linux platform, so the pinned requirements; files are only generated for Linux.; ",MatchSource.DOCS,dev-docs/pip-dependencies.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/pip-dependencies.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/pip-dependencies.md:29,Integrability,depend,dependencies,29,"# Updating dependencies. Pip dependencies for a particular python module should be listed in a; `requirements.txt` file, e.g. `$HAIL/hail/python/requirements.txt`.; The version should be the most permissive version range compatible with the hail; package. In each directory that contains a `requirements.txt` file there should; also be a `pinned-requirements.txt` file that contains a fully resolved; dependency tree with each pip dependency pinned to a specific version.; This allows deterministic builds in our Dockerfiles. When adding a dependency; to a `requirements.txt` file, run. ```bash; make generate-pip-lockfiles; ```. to regenerate the `pinnned-requirements.txt` files in the repository.; Note that the full dependency tree for a pip package can; differ on different operating systems. All services and docker images in CI; use the fully pinned requirements on a Linux platform, so the pinned requirements; files are only generated for Linux.; ",MatchSource.DOCS,dev-docs/pip-dependencies.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/pip-dependencies.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/pip-dependencies.md:401,Integrability,depend,dependency,401,"# Updating dependencies. Pip dependencies for a particular python module should be listed in a; `requirements.txt` file, e.g. `$HAIL/hail/python/requirements.txt`.; The version should be the most permissive version range compatible with the hail; package. In each directory that contains a `requirements.txt` file there should; also be a `pinned-requirements.txt` file that contains a fully resolved; dependency tree with each pip dependency pinned to a specific version.; This allows deterministic builds in our Dockerfiles. When adding a dependency; to a `requirements.txt` file, run. ```bash; make generate-pip-lockfiles; ```. to regenerate the `pinnned-requirements.txt` files in the repository.; Note that the full dependency tree for a pip package can; differ on different operating systems. All services and docker images in CI; use the fully pinned requirements on a Linux platform, so the pinned requirements; files are only generated for Linux.; ",MatchSource.DOCS,dev-docs/pip-dependencies.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/pip-dependencies.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/pip-dependencies.md:431,Integrability,depend,dependency,431,"# Updating dependencies. Pip dependencies for a particular python module should be listed in a; `requirements.txt` file, e.g. `$HAIL/hail/python/requirements.txt`.; The version should be the most permissive version range compatible with the hail; package. In each directory that contains a `requirements.txt` file there should; also be a `pinned-requirements.txt` file that contains a fully resolved; dependency tree with each pip dependency pinned to a specific version.; This allows deterministic builds in our Dockerfiles. When adding a dependency; to a `requirements.txt` file, run. ```bash; make generate-pip-lockfiles; ```. to regenerate the `pinnned-requirements.txt` files in the repository.; Note that the full dependency tree for a pip package can; differ on different operating systems. All services and docker images in CI; use the fully pinned requirements on a Linux platform, so the pinned requirements; files are only generated for Linux.; ",MatchSource.DOCS,dev-docs/pip-dependencies.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/pip-dependencies.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/pip-dependencies.md:540,Integrability,depend,dependency,540,"# Updating dependencies. Pip dependencies for a particular python module should be listed in a; `requirements.txt` file, e.g. `$HAIL/hail/python/requirements.txt`.; The version should be the most permissive version range compatible with the hail; package. In each directory that contains a `requirements.txt` file there should; also be a `pinned-requirements.txt` file that contains a fully resolved; dependency tree with each pip dependency pinned to a specific version.; This allows deterministic builds in our Dockerfiles. When adding a dependency; to a `requirements.txt` file, run. ```bash; make generate-pip-lockfiles; ```. to regenerate the `pinnned-requirements.txt` files in the repository.; Note that the full dependency tree for a pip package can; differ on different operating systems. All services and docker images in CI; use the fully pinned requirements on a Linux platform, so the pinned requirements; files are only generated for Linux.; ",MatchSource.DOCS,dev-docs/pip-dependencies.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/pip-dependencies.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/pip-dependencies.md:720,Integrability,depend,dependency,720,"# Updating dependencies. Pip dependencies for a particular python module should be listed in a; `requirements.txt` file, e.g. `$HAIL/hail/python/requirements.txt`.; The version should be the most permissive version range compatible with the hail; package. In each directory that contains a `requirements.txt` file there should; also be a `pinned-requirements.txt` file that contains a fully resolved; dependency tree with each pip dependency pinned to a specific version.; This allows deterministic builds in our Dockerfiles. When adding a dependency; to a `requirements.txt` file, run. ```bash; make generate-pip-lockfiles; ```. to regenerate the `pinnned-requirements.txt` files in the repository.; Note that the full dependency tree for a pip package can; differ on different operating systems. All services and docker images in CI; use the fully pinned requirements on a Linux platform, so the pinned requirements; files are only generated for Linux.; ",MatchSource.DOCS,dev-docs/pip-dependencies.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/pip-dependencies.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:990,Availability,down,downloading,990,"# Releasing FAQ. ## Commits were merged after a broken release. If the release process broke and new commits have merged since we modified the change log, which; commit should we release?. If a commit has been tagged, release that commit. If fixes are necessary, create a branch from the; tagged commit, add commits as necessary, modify the tag, and hand release that. ## Failure due to ""active Temporary Hold"". The release build.yaml job fails due to ""'hail-common/dataproc/0.2.XXX/vep-GRCh37.sh' is under; active Temporary Hold"". What do I do?. There are four files uploaded by Hail for use by Dataproc clusters (a wheel, two VEP scripts, and a; notebook initialization script). We place a temporary hold on these files to prevent them from being; inadvertently deleted. If all four files were successfully uploaded, you can continue the release; from this point by directly executing release.sh for the particular commit used to generate the; uploaded wheel. You can check the commit by downloading the wheel, installing it, and running. python3 -c 'import hail; print(hail.version())'. Checkout this commit locally. Then find the corresponding release batch by searching for:. sha = THE_FULL_SHA. Change directories into the Hail directory:. cd /PATH/TO/REPO/hail. You can download all the necessary files to execute release.sh by downloading them from the; hail-ci-bpk3h bucket. The necessary files are listed under ""Sources: "" in the ""Inputs"" log of the; ""release"" build step. They should look something like:. Sources:; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_version: 1 files, 21 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_pip_version: 1 files, 8 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/buil",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:1277,Availability,down,download,1277,"mmit, add commits as necessary, modify the tag, and hand release that. ## Failure due to ""active Temporary Hold"". The release build.yaml job fails due to ""'hail-common/dataproc/0.2.XXX/vep-GRCh37.sh' is under; active Temporary Hold"". What do I do?. There are four files uploaded by Hail for use by Dataproc clusters (a wheel, two VEP scripts, and a; notebook initialization script). We place a temporary hold on these files to prevent them from being; inadvertently deleted. If all four files were successfully uploaded, you can continue the release; from this point by directly executing release.sh for the particular commit used to generate the; uploaded wheel. You can check the commit by downloading the wheel, installing it, and running. python3 -c 'import hail; print(hail.version())'. Checkout this commit locally. Then find the corresponding release batch by searching for:. sha = THE_FULL_SHA. Change directories into the Hail directory:. cd /PATH/TO/REPO/hail. You can download all the necessary files to execute release.sh by downloading them from the; hail-ci-bpk3h bucket. The necessary files are listed under ""Sources: "" in the ""Inputs"" log of the; ""release"" build step. They should look something like:. Sources:; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_version: 1 files, 21 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_pip_version: 1 files, 8 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/www.tar.gz: 1 files, 43.5 MB. Download all these files except the repo (which you do not need, because you checked out the commit):. BUILD_TOKEN=9cabeeb4ba047d1722e6f8da0383ab97; mkdir $BUILD_TOKEN; 	RELEASE_ARTIFACTS_DIR=$(realpath $BUILD_TOKEN); gcloud storage ",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:1335,Availability,down,downloading,1335,"elease that. ## Failure due to ""active Temporary Hold"". The release build.yaml job fails due to ""'hail-common/dataproc/0.2.XXX/vep-GRCh37.sh' is under; active Temporary Hold"". What do I do?. There are four files uploaded by Hail for use by Dataproc clusters (a wheel, two VEP scripts, and a; notebook initialization script). We place a temporary hold on these files to prevent them from being; inadvertently deleted. If all four files were successfully uploaded, you can continue the release; from this point by directly executing release.sh for the particular commit used to generate the; uploaded wheel. You can check the commit by downloading the wheel, installing it, and running. python3 -c 'import hail; print(hail.version())'. Checkout this commit locally. Then find the corresponding release batch by searching for:. sha = THE_FULL_SHA. Change directories into the Hail directory:. cd /PATH/TO/REPO/hail. You can download all the necessary files to execute release.sh by downloading them from the; hail-ci-bpk3h bucket. The necessary files are listed under ""Sources: "" in the ""Inputs"" log of the; ""release"" build step. They should look something like:. Sources:; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_version: 1 files, 21 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_pip_version: 1 files, 8 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/www.tar.gz: 1 files, 43.5 MB. Download all these files except the repo (which you do not need, because you checked out the commit):. BUILD_TOKEN=9cabeeb4ba047d1722e6f8da0383ab97; mkdir $BUILD_TOKEN; 	RELEASE_ARTIFACTS_DIR=$(realpath $BUILD_TOKEN); gcloud storage cp -r \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_versi",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:2785,Availability,down,download-secret,2785,"i-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/www.tar.gz: 1 files, 43.5 MB. Download all these files except the repo (which you do not need, because you checked out the commit):. BUILD_TOKEN=9cabeeb4ba047d1722e6f8da0383ab97; mkdir $BUILD_TOKEN; 	RELEASE_ARTIFACTS_DIR=$(realpath $BUILD_TOKEN); gcloud storage cp -r \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_pip_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/git_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/azure-wheel \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/www.tar.gz \; $BUILD_TOKEN. Note that the `-r` is necessary because some of these things like `azure-wheel` are folders. Next we need to authenticate with DockerHub. Download the secret and authenticate skopeo with; it. `download-secret` is a function stored in `devbin/functions.sh`. download-secret docker-hub-hailgenetics; cat contents/password | skopeo login --username hailgenetics --password-stdin docker.io; 	popd. Next we need a valid pypirc:. download-secret pypi-credentials; cp contents/pypirc $HOME/.pypirc; 	popd. Next we need a valid github-oauth token (for creating GitHub releases):. download-secret hail-ci-0-1-github-oauth-token; printf 'Authorization: token ' > $RELEASE_ARTIFACTS_DIR/github-oauth; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/github-oauth. We use those same credentials to automatically create releases against DSP's repositories:. printf '#!/bin/bash\necho ' > $RELEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `release.sh` invocation. Find the invocation in the ""command""",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:2849,Availability,down,download-secret,2849,"d/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/www.tar.gz: 1 files, 43.5 MB. Download all these files except the repo (which you do not need, because you checked out the commit):. BUILD_TOKEN=9cabeeb4ba047d1722e6f8da0383ab97; mkdir $BUILD_TOKEN; 	RELEASE_ARTIFACTS_DIR=$(realpath $BUILD_TOKEN); gcloud storage cp -r \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_pip_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/git_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/azure-wheel \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/www.tar.gz \; $BUILD_TOKEN. Note that the `-r` is necessary because some of these things like `azure-wheel` are folders. Next we need to authenticate with DockerHub. Download the secret and authenticate skopeo with; it. `download-secret` is a function stored in `devbin/functions.sh`. download-secret docker-hub-hailgenetics; cat contents/password | skopeo login --username hailgenetics --password-stdin docker.io; 	popd. Next we need a valid pypirc:. download-secret pypi-credentials; cp contents/pypirc $HOME/.pypirc; 	popd. Next we need a valid github-oauth token (for creating GitHub releases):. download-secret hail-ci-0-1-github-oauth-token; printf 'Authorization: token ' > $RELEASE_ARTIFACTS_DIR/github-oauth; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/github-oauth. We use those same credentials to automatically create releases against DSP's repositories:. printf '#!/bin/bash\necho ' > $RELEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `release.sh` invocation. Find the invocation in the ""command"" part of the; ""Job Specification"" table. It should look like:. bash scripts/release.sh $(cat /io/h",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:3016,Availability,down,download-secret,3016,"z: 1 files, 43.5 MB. Download all these files except the repo (which you do not need, because you checked out the commit):. BUILD_TOKEN=9cabeeb4ba047d1722e6f8da0383ab97; mkdir $BUILD_TOKEN; 	RELEASE_ARTIFACTS_DIR=$(realpath $BUILD_TOKEN); gcloud storage cp -r \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_pip_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/git_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/azure-wheel \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/www.tar.gz \; $BUILD_TOKEN. Note that the `-r` is necessary because some of these things like `azure-wheel` are folders. Next we need to authenticate with DockerHub. Download the secret and authenticate skopeo with; it. `download-secret` is a function stored in `devbin/functions.sh`. download-secret docker-hub-hailgenetics; cat contents/password | skopeo login --username hailgenetics --password-stdin docker.io; 	popd. Next we need a valid pypirc:. download-secret pypi-credentials; cp contents/pypirc $HOME/.pypirc; 	popd. Next we need a valid github-oauth token (for creating GitHub releases):. download-secret hail-ci-0-1-github-oauth-token; printf 'Authorization: token ' > $RELEASE_ARTIFACTS_DIR/github-oauth; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/github-oauth. We use those same credentials to automatically create releases against DSP's repositories:. printf '#!/bin/bash\necho ' > $RELEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `release.sh` invocation. Find the invocation in the ""command"" part of the; ""Job Specification"" table. It should look like:. bash scripts/release.sh $(cat /io/hail_pip_version) \; $(cat /io/hail_version) \; $(cat /io/git_version) \; origin \; /io/repo/hail/build/deploy/dist/hail-*-py3-none-any",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:3164,Availability,down,download-secret,3164,"DIR=$(realpath $BUILD_TOKEN); gcloud storage cp -r \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_pip_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/git_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/azure-wheel \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/www.tar.gz \; $BUILD_TOKEN. Note that the `-r` is necessary because some of these things like `azure-wheel` are folders. Next we need to authenticate with DockerHub. Download the secret and authenticate skopeo with; it. `download-secret` is a function stored in `devbin/functions.sh`. download-secret docker-hub-hailgenetics; cat contents/password | skopeo login --username hailgenetics --password-stdin docker.io; 	popd. Next we need a valid pypirc:. download-secret pypi-credentials; cp contents/pypirc $HOME/.pypirc; 	popd. Next we need a valid github-oauth token (for creating GitHub releases):. download-secret hail-ci-0-1-github-oauth-token; printf 'Authorization: token ' > $RELEASE_ARTIFACTS_DIR/github-oauth; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/github-oauth. We use those same credentials to automatically create releases against DSP's repositories:. printf '#!/bin/bash\necho ' > $RELEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `release.sh` invocation. Find the invocation in the ""command"" part of the; ""Job Specification"" table. It should look like:. bash scripts/release.sh $(cat /io/hail_pip_version) \; $(cat /io/hail_version) \; $(cat /io/git_version) \; origin \; /io/repo/hail/build/deploy/dist/hail-*-py3-none-any.whl \; /io/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-d",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:4745,Availability,down,downloaded,4745,". Now we can construct a `release.sh` invocation. Find the invocation in the ""command"" part of the; ""Job Specification"" table. It should look like:. bash scripts/release.sh $(cat /io/hail_pip_version) \; $(cat /io/hail_version) \; $(cat /io/git_version) \; origin \; /io/repo/hail/build/deploy/dist/hail-*-py3-none-any.whl \; /io/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; $RELEASE_ARTIFACTS_DIR/azure-wheel/hail-*-py3-none-any.whl \; $RELEAS",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:5990,Availability,down,download,5990,"netics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; $RELEASE_ARTIFACTS_DIR/azure-wheel/hail-*-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/www.tar.gz'. When you are complete, delete all the credentials:. rm $RELEASE_ARTIFACTS_DIR/git-askpass; rm $RELEASE_ARTIFACTS_DIR/github-oauth; rm $HOME/.pypirc; 	skopeo logout docker.io. You should also delete the temporary directories used to download the credentials. On Mac OS X,; those directories are all under $TMPDIR which looks like; `/var/folders/x1/601098gx0v11qjx2l_7qfw2c0000gq/T/`. If you're comfortable deleting all of $TMPDIR,; just run:. rm -rf $TMPDIR. ## Failure due to a tag or a release already existing. If you are hand releasing and the release script exits because the tag or release already exists,; you can safely comment out the lines that check for that and the lines that create those; things. Then you may execute the script to continue with the rest of the release.; ",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:55,Deployability,release,release,55,"# Releasing FAQ. ## Commits were merged after a broken release. If the release process broke and new commits have merged since we modified the change log, which; commit should we release?. If a commit has been tagged, release that commit. If fixes are necessary, create a branch from the; tagged commit, add commits as necessary, modify the tag, and hand release that. ## Failure due to ""active Temporary Hold"". The release build.yaml job fails due to ""'hail-common/dataproc/0.2.XXX/vep-GRCh37.sh' is under; active Temporary Hold"". What do I do?. There are four files uploaded by Hail for use by Dataproc clusters (a wheel, two VEP scripts, and a; notebook initialization script). We place a temporary hold on these files to prevent them from being; inadvertently deleted. If all four files were successfully uploaded, you can continue the release; from this point by directly executing release.sh for the particular commit used to generate the; uploaded wheel. You can check the commit by downloading the wheel, installing it, and running. python3 -c 'import hail; print(hail.version())'. Checkout this commit locally. Then find the corresponding release batch by searching for:. sha = THE_FULL_SHA. Change directories into the Hail directory:. cd /PATH/TO/REPO/hail. You can download all the necessary files to execute release.sh by downloading them from the; hail-ci-bpk3h bucket. The necessary files are listed under ""Sources: "" in the ""Inputs"" log of the; ""release"" build step. They should look something like:. Sources:; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_version: 1 files, 21 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_pip_version: 1 files, 8 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/buil",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:71,Deployability,release,release,71,"# Releasing FAQ. ## Commits were merged after a broken release. If the release process broke and new commits have merged since we modified the change log, which; commit should we release?. If a commit has been tagged, release that commit. If fixes are necessary, create a branch from the; tagged commit, add commits as necessary, modify the tag, and hand release that. ## Failure due to ""active Temporary Hold"". The release build.yaml job fails due to ""'hail-common/dataproc/0.2.XXX/vep-GRCh37.sh' is under; active Temporary Hold"". What do I do?. There are four files uploaded by Hail for use by Dataproc clusters (a wheel, two VEP scripts, and a; notebook initialization script). We place a temporary hold on these files to prevent them from being; inadvertently deleted. If all four files were successfully uploaded, you can continue the release; from this point by directly executing release.sh for the particular commit used to generate the; uploaded wheel. You can check the commit by downloading the wheel, installing it, and running. python3 -c 'import hail; print(hail.version())'. Checkout this commit locally. Then find the corresponding release batch by searching for:. sha = THE_FULL_SHA. Change directories into the Hail directory:. cd /PATH/TO/REPO/hail. You can download all the necessary files to execute release.sh by downloading them from the; hail-ci-bpk3h bucket. The necessary files are listed under ""Sources: "" in the ""Inputs"" log of the; ""release"" build step. They should look something like:. Sources:; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_version: 1 files, 21 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_pip_version: 1 files, 8 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/buil",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:179,Deployability,release,release,179,"# Releasing FAQ. ## Commits were merged after a broken release. If the release process broke and new commits have merged since we modified the change log, which; commit should we release?. If a commit has been tagged, release that commit. If fixes are necessary, create a branch from the; tagged commit, add commits as necessary, modify the tag, and hand release that. ## Failure due to ""active Temporary Hold"". The release build.yaml job fails due to ""'hail-common/dataproc/0.2.XXX/vep-GRCh37.sh' is under; active Temporary Hold"". What do I do?. There are four files uploaded by Hail for use by Dataproc clusters (a wheel, two VEP scripts, and a; notebook initialization script). We place a temporary hold on these files to prevent them from being; inadvertently deleted. If all four files were successfully uploaded, you can continue the release; from this point by directly executing release.sh for the particular commit used to generate the; uploaded wheel. You can check the commit by downloading the wheel, installing it, and running. python3 -c 'import hail; print(hail.version())'. Checkout this commit locally. Then find the corresponding release batch by searching for:. sha = THE_FULL_SHA. Change directories into the Hail directory:. cd /PATH/TO/REPO/hail. You can download all the necessary files to execute release.sh by downloading them from the; hail-ci-bpk3h bucket. The necessary files are listed under ""Sources: "" in the ""Inputs"" log of the; ""release"" build step. They should look something like:. Sources:; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_version: 1 files, 21 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_pip_version: 1 files, 8 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/buil",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:218,Deployability,release,release,218,"# Releasing FAQ. ## Commits were merged after a broken release. If the release process broke and new commits have merged since we modified the change log, which; commit should we release?. If a commit has been tagged, release that commit. If fixes are necessary, create a branch from the; tagged commit, add commits as necessary, modify the tag, and hand release that. ## Failure due to ""active Temporary Hold"". The release build.yaml job fails due to ""'hail-common/dataproc/0.2.XXX/vep-GRCh37.sh' is under; active Temporary Hold"". What do I do?. There are four files uploaded by Hail for use by Dataproc clusters (a wheel, two VEP scripts, and a; notebook initialization script). We place a temporary hold on these files to prevent them from being; inadvertently deleted. If all four files were successfully uploaded, you can continue the release; from this point by directly executing release.sh for the particular commit used to generate the; uploaded wheel. You can check the commit by downloading the wheel, installing it, and running. python3 -c 'import hail; print(hail.version())'. Checkout this commit locally. Then find the corresponding release batch by searching for:. sha = THE_FULL_SHA. Change directories into the Hail directory:. cd /PATH/TO/REPO/hail. You can download all the necessary files to execute release.sh by downloading them from the; hail-ci-bpk3h bucket. The necessary files are listed under ""Sources: "" in the ""Inputs"" log of the; ""release"" build step. They should look something like:. Sources:; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_version: 1 files, 21 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_pip_version: 1 files, 8 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/buil",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:355,Deployability,release,release,355,"# Releasing FAQ. ## Commits were merged after a broken release. If the release process broke and new commits have merged since we modified the change log, which; commit should we release?. If a commit has been tagged, release that commit. If fixes are necessary, create a branch from the; tagged commit, add commits as necessary, modify the tag, and hand release that. ## Failure due to ""active Temporary Hold"". The release build.yaml job fails due to ""'hail-common/dataproc/0.2.XXX/vep-GRCh37.sh' is under; active Temporary Hold"". What do I do?. There are four files uploaded by Hail for use by Dataproc clusters (a wheel, two VEP scripts, and a; notebook initialization script). We place a temporary hold on these files to prevent them from being; inadvertently deleted. If all four files were successfully uploaded, you can continue the release; from this point by directly executing release.sh for the particular commit used to generate the; uploaded wheel. You can check the commit by downloading the wheel, installing it, and running. python3 -c 'import hail; print(hail.version())'. Checkout this commit locally. Then find the corresponding release batch by searching for:. sha = THE_FULL_SHA. Change directories into the Hail directory:. cd /PATH/TO/REPO/hail. You can download all the necessary files to execute release.sh by downloading them from the; hail-ci-bpk3h bucket. The necessary files are listed under ""Sources: "" in the ""Inputs"" log of the; ""release"" build step. They should look something like:. Sources:; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_version: 1 files, 21 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_pip_version: 1 files, 8 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/buil",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:416,Deployability,release,release,416,"# Releasing FAQ. ## Commits were merged after a broken release. If the release process broke and new commits have merged since we modified the change log, which; commit should we release?. If a commit has been tagged, release that commit. If fixes are necessary, create a branch from the; tagged commit, add commits as necessary, modify the tag, and hand release that. ## Failure due to ""active Temporary Hold"". The release build.yaml job fails due to ""'hail-common/dataproc/0.2.XXX/vep-GRCh37.sh' is under; active Temporary Hold"". What do I do?. There are four files uploaded by Hail for use by Dataproc clusters (a wheel, two VEP scripts, and a; notebook initialization script). We place a temporary hold on these files to prevent them from being; inadvertently deleted. If all four files were successfully uploaded, you can continue the release; from this point by directly executing release.sh for the particular commit used to generate the; uploaded wheel. You can check the commit by downloading the wheel, installing it, and running. python3 -c 'import hail; print(hail.version())'. Checkout this commit locally. Then find the corresponding release batch by searching for:. sha = THE_FULL_SHA. Change directories into the Hail directory:. cd /PATH/TO/REPO/hail. You can download all the necessary files to execute release.sh by downloading them from the; hail-ci-bpk3h bucket. The necessary files are listed under ""Sources: "" in the ""Inputs"" log of the; ""release"" build step. They should look something like:. Sources:; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_version: 1 files, 21 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_pip_version: 1 files, 8 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/buil",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:840,Deployability,release,release,840,"# Releasing FAQ. ## Commits were merged after a broken release. If the release process broke and new commits have merged since we modified the change log, which; commit should we release?. If a commit has been tagged, release that commit. If fixes are necessary, create a branch from the; tagged commit, add commits as necessary, modify the tag, and hand release that. ## Failure due to ""active Temporary Hold"". The release build.yaml job fails due to ""'hail-common/dataproc/0.2.XXX/vep-GRCh37.sh' is under; active Temporary Hold"". What do I do?. There are four files uploaded by Hail for use by Dataproc clusters (a wheel, two VEP scripts, and a; notebook initialization script). We place a temporary hold on these files to prevent them from being; inadvertently deleted. If all four files were successfully uploaded, you can continue the release; from this point by directly executing release.sh for the particular commit used to generate the; uploaded wheel. You can check the commit by downloading the wheel, installing it, and running. python3 -c 'import hail; print(hail.version())'. Checkout this commit locally. Then find the corresponding release batch by searching for:. sha = THE_FULL_SHA. Change directories into the Hail directory:. cd /PATH/TO/REPO/hail. You can download all the necessary files to execute release.sh by downloading them from the; hail-ci-bpk3h bucket. The necessary files are listed under ""Sources: "" in the ""Inputs"" log of the; ""release"" build step. They should look something like:. Sources:; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_version: 1 files, 21 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_pip_version: 1 files, 8 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/buil",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:887,Deployability,release,release,887,"# Releasing FAQ. ## Commits were merged after a broken release. If the release process broke and new commits have merged since we modified the change log, which; commit should we release?. If a commit has been tagged, release that commit. If fixes are necessary, create a branch from the; tagged commit, add commits as necessary, modify the tag, and hand release that. ## Failure due to ""active Temporary Hold"". The release build.yaml job fails due to ""'hail-common/dataproc/0.2.XXX/vep-GRCh37.sh' is under; active Temporary Hold"". What do I do?. There are four files uploaded by Hail for use by Dataproc clusters (a wheel, two VEP scripts, and a; notebook initialization script). We place a temporary hold on these files to prevent them from being; inadvertently deleted. If all four files were successfully uploaded, you can continue the release; from this point by directly executing release.sh for the particular commit used to generate the; uploaded wheel. You can check the commit by downloading the wheel, installing it, and running. python3 -c 'import hail; print(hail.version())'. Checkout this commit locally. Then find the corresponding release batch by searching for:. sha = THE_FULL_SHA. Change directories into the Hail directory:. cd /PATH/TO/REPO/hail. You can download all the necessary files to execute release.sh by downloading them from the; hail-ci-bpk3h bucket. The necessary files are listed under ""Sources: "" in the ""Inputs"" log of the; ""release"" build step. They should look something like:. Sources:; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_version: 1 files, 21 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_pip_version: 1 files, 8 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/buil",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:1013,Deployability,install,installing,1013,"# Releasing FAQ. ## Commits were merged after a broken release. If the release process broke and new commits have merged since we modified the change log, which; commit should we release?. If a commit has been tagged, release that commit. If fixes are necessary, create a branch from the; tagged commit, add commits as necessary, modify the tag, and hand release that. ## Failure due to ""active Temporary Hold"". The release build.yaml job fails due to ""'hail-common/dataproc/0.2.XXX/vep-GRCh37.sh' is under; active Temporary Hold"". What do I do?. There are four files uploaded by Hail for use by Dataproc clusters (a wheel, two VEP scripts, and a; notebook initialization script). We place a temporary hold on these files to prevent them from being; inadvertently deleted. If all four files were successfully uploaded, you can continue the release; from this point by directly executing release.sh for the particular commit used to generate the; uploaded wheel. You can check the commit by downloading the wheel, installing it, and running. python3 -c 'import hail; print(hail.version())'. Checkout this commit locally. Then find the corresponding release batch by searching for:. sha = THE_FULL_SHA. Change directories into the Hail directory:. cd /PATH/TO/REPO/hail. You can download all the necessary files to execute release.sh by downloading them from the; hail-ci-bpk3h bucket. The necessary files are listed under ""Sources: "" in the ""Inputs"" log of the; ""release"" build step. They should look something like:. Sources:; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_version: 1 files, 21 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_pip_version: 1 files, 8 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/buil",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:1148,Deployability,release,release,1148," log, which; commit should we release?. If a commit has been tagged, release that commit. If fixes are necessary, create a branch from the; tagged commit, add commits as necessary, modify the tag, and hand release that. ## Failure due to ""active Temporary Hold"". The release build.yaml job fails due to ""'hail-common/dataproc/0.2.XXX/vep-GRCh37.sh' is under; active Temporary Hold"". What do I do?. There are four files uploaded by Hail for use by Dataproc clusters (a wheel, two VEP scripts, and a; notebook initialization script). We place a temporary hold on these files to prevent them from being; inadvertently deleted. If all four files were successfully uploaded, you can continue the release; from this point by directly executing release.sh for the particular commit used to generate the; uploaded wheel. You can check the commit by downloading the wheel, installing it, and running. python3 -c 'import hail; print(hail.version())'. Checkout this commit locally. Then find the corresponding release batch by searching for:. sha = THE_FULL_SHA. Change directories into the Hail directory:. cd /PATH/TO/REPO/hail. You can download all the necessary files to execute release.sh by downloading them from the; hail-ci-bpk3h bucket. The necessary files are listed under ""Sources: "" in the ""Inputs"" log of the; ""release"" build step. They should look something like:. Sources:; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_version: 1 files, 21 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_pip_version: 1 files, 8 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/www.tar.gz: 1 files, 43.5 MB. Download all these files except the repo (which you do not need, because you checked",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:1321,Deployability,release,release,1321,"mmit, add commits as necessary, modify the tag, and hand release that. ## Failure due to ""active Temporary Hold"". The release build.yaml job fails due to ""'hail-common/dataproc/0.2.XXX/vep-GRCh37.sh' is under; active Temporary Hold"". What do I do?. There are four files uploaded by Hail for use by Dataproc clusters (a wheel, two VEP scripts, and a; notebook initialization script). We place a temporary hold on these files to prevent them from being; inadvertently deleted. If all four files were successfully uploaded, you can continue the release; from this point by directly executing release.sh for the particular commit used to generate the; uploaded wheel. You can check the commit by downloading the wheel, installing it, and running. python3 -c 'import hail; print(hail.version())'. Checkout this commit locally. Then find the corresponding release batch by searching for:. sha = THE_FULL_SHA. Change directories into the Hail directory:. cd /PATH/TO/REPO/hail. You can download all the necessary files to execute release.sh by downloading them from the; hail-ci-bpk3h bucket. The necessary files are listed under ""Sources: "" in the ""Inputs"" log of the; ""release"" build step. They should look something like:. Sources:; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_version: 1 files, 21 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_pip_version: 1 files, 8 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/www.tar.gz: 1 files, 43.5 MB. Download all these files except the repo (which you do not need, because you checked out the commit):. BUILD_TOKEN=9cabeeb4ba047d1722e6f8da0383ab97; mkdir $BUILD_TOKEN; 	RELEASE_ARTIFACTS_DIR=$(realpath $BUILD_TOKEN); gcloud storage ",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:1462,Deployability,release,release,1462,"ml job fails due to ""'hail-common/dataproc/0.2.XXX/vep-GRCh37.sh' is under; active Temporary Hold"". What do I do?. There are four files uploaded by Hail for use by Dataproc clusters (a wheel, two VEP scripts, and a; notebook initialization script). We place a temporary hold on these files to prevent them from being; inadvertently deleted. If all four files were successfully uploaded, you can continue the release; from this point by directly executing release.sh for the particular commit used to generate the; uploaded wheel. You can check the commit by downloading the wheel, installing it, and running. python3 -c 'import hail; print(hail.version())'. Checkout this commit locally. Then find the corresponding release batch by searching for:. sha = THE_FULL_SHA. Change directories into the Hail directory:. cd /PATH/TO/REPO/hail. You can download all the necessary files to execute release.sh by downloading them from the; hail-ci-bpk3h bucket. The necessary files are listed under ""Sources: "" in the ""Inputs"" log of the; ""release"" build step. They should look something like:. Sources:; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_version: 1 files, 21 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_pip_version: 1 files, 8 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/www.tar.gz: 1 files, 43.5 MB. Download all these files except the repo (which you do not need, because you checked out the commit):. BUILD_TOKEN=9cabeeb4ba047d1722e6f8da0383ab97; mkdir $BUILD_TOKEN; 	RELEASE_ARTIFACTS_DIR=$(realpath $BUILD_TOKEN); gcloud storage cp -r \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_pip_version \; gs://hail-ci-",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:3152,Deployability,release,releases,3152,"eed, because you checked out the commit):. BUILD_TOKEN=9cabeeb4ba047d1722e6f8da0383ab97; mkdir $BUILD_TOKEN; 	RELEASE_ARTIFACTS_DIR=$(realpath $BUILD_TOKEN); gcloud storage cp -r \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_pip_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/git_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/azure-wheel \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/www.tar.gz \; $BUILD_TOKEN. Note that the `-r` is necessary because some of these things like `azure-wheel` are folders. Next we need to authenticate with DockerHub. Download the secret and authenticate skopeo with; it. `download-secret` is a function stored in `devbin/functions.sh`. download-secret docker-hub-hailgenetics; cat contents/password | skopeo login --username hailgenetics --password-stdin docker.io; 	popd. Next we need a valid pypirc:. download-secret pypi-credentials; cp contents/pypirc $HOME/.pypirc; 	popd. Next we need a valid github-oauth token (for creating GitHub releases):. download-secret hail-ci-0-1-github-oauth-token; printf 'Authorization: token ' > $RELEASE_ARTIFACTS_DIR/github-oauth; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/github-oauth. We use those same credentials to automatically create releases against DSP's repositories:. printf '#!/bin/bash\necho ' > $RELEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `release.sh` invocation. Find the invocation in the ""command"" part of the; ""Job Specification"" table. It should look like:. bash scripts/release.sh $(cat /io/hail_pip_version) \; $(cat /io/hail_version) \; $(cat /io/git_version) \; origin \; /io/repo/hail/build/deploy/dist/hail-*-py3-none-any.whl \; /io/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:3400,Deployability,release,releases,3400,"LD_TOKEN/hail_pip_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/git_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/azure-wheel \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/www.tar.gz \; $BUILD_TOKEN. Note that the `-r` is necessary because some of these things like `azure-wheel` are folders. Next we need to authenticate with DockerHub. Download the secret and authenticate skopeo with; it. `download-secret` is a function stored in `devbin/functions.sh`. download-secret docker-hub-hailgenetics; cat contents/password | skopeo login --username hailgenetics --password-stdin docker.io; 	popd. Next we need a valid pypirc:. download-secret pypi-credentials; cp contents/pypirc $HOME/.pypirc; 	popd. Next we need a valid github-oauth token (for creating GitHub releases):. download-secret hail-ci-0-1-github-oauth-token; printf 'Authorization: token ' > $RELEASE_ARTIFACTS_DIR/github-oauth; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/github-oauth. We use those same credentials to automatically create releases against DSP's repositories:. printf '#!/bin/bash\necho ' > $RELEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `release.sh` invocation. Find the invocation in the ""command"" part of the; ""Job Specification"" table. It should look like:. bash scripts/release.sh $(cat /io/hail_pip_version) \; $(cat /io/hail_version) \; $(cat /io/git_version) \; origin \; /io/repo/hail/build/deploy/dist/hail-*-py3-none-any.whl \; /io/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:3754,Deployability,release,release,3754,"cret and authenticate skopeo with; it. `download-secret` is a function stored in `devbin/functions.sh`. download-secret docker-hub-hailgenetics; cat contents/password | skopeo login --username hailgenetics --password-stdin docker.io; 	popd. Next we need a valid pypirc:. download-secret pypi-credentials; cp contents/pypirc $HOME/.pypirc; 	popd. Next we need a valid github-oauth token (for creating GitHub releases):. download-secret hail-ci-0-1-github-oauth-token; printf 'Authorization: token ' > $RELEASE_ARTIFACTS_DIR/github-oauth; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/github-oauth. We use those same credentials to automatically create releases against DSP's repositories:. printf '#!/bin/bash\necho ' > $RELEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `release.sh` invocation. Find the invocation in the ""command"" part of the; ""Job Specification"" table. It should look like:. bash scripts/release.sh $(cat /io/hail_pip_version) \; $(cat /io/hail_version) \; $(cat /io/git_version) \; origin \; /io/repo/hail/build/deploy/dist/hail-*-py3-none-any.whl \; /io/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we d",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:3890,Deployability,release,release,3890,"s; cat contents/password | skopeo login --username hailgenetics --password-stdin docker.io; 	popd. Next we need a valid pypirc:. download-secret pypi-credentials; cp contents/pypirc $HOME/.pypirc; 	popd. Next we need a valid github-oauth token (for creating GitHub releases):. download-secret hail-ci-0-1-github-oauth-token; printf 'Authorization: token ' > $RELEASE_ARTIFACTS_DIR/github-oauth; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/github-oauth. We use those same credentials to automatically create releases against DSP's repositories:. printf '#!/bin/bash\necho ' > $RELEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `release.sh` invocation. Find the invocation in the ""command"" part of the; ""Job Specification"" table. It should look like:. bash scripts/release.sh $(cat /io/hail_pip_version) \; $(cat /io/hail_version) \; $(cat /io/git_version) \; origin \; /io/repo/hail/build/deploy/dist/hail-*-py3-none-any.whl \; /io/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:4015,Deployability,deploy,deploy,4015,"er.io; 	popd. Next we need a valid pypirc:. download-secret pypi-credentials; cp contents/pypirc $HOME/.pypirc; 	popd. Next we need a valid github-oauth token (for creating GitHub releases):. download-secret hail-ci-0-1-github-oauth-token; printf 'Authorization: token ' > $RELEASE_ARTIFACTS_DIR/github-oauth; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/github-oauth. We use those same credentials to automatically create releases against DSP's repositories:. printf '#!/bin/bash\necho ' > $RELEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `release.sh` invocation. Find the invocation in the ""command"" part of the; ""Job Specification"" table. It should look like:. bash scripts/release.sh $(cat /io/hail_pip_version) \; $(cat /io/hail_version) \; $(cat /io/git_version) \; origin \; /io/repo/hail/build/deploy/dist/hail-*-py3-none-any.whl \; /io/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_ver",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:4133,Deployability,deploy,deploy-,4133,"creating GitHub releases):. download-secret hail-ci-0-1-github-oauth-token; printf 'Authorization: token ' > $RELEASE_ARTIFACTS_DIR/github-oauth; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/github-oauth. We use those same credentials to automatically create releases against DSP's repositories:. printf '#!/bin/bash\necho ' > $RELEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `release.sh` invocation. Find the invocation in the ""command"" part of the; ""Job Specification"" table. It should look like:. bash scripts/release.sh $(cat /io/hail_pip_version) \; $(cat /io/hail_version) \; $(cat /io/git_version) \; origin \; /io/repo/hail/build/deploy/dist/hail-*-py3-none-any.whl \; /io/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/githu",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:4218,Deployability,deploy,deploy-,4218,"'Authorization: token ' > $RELEASE_ARTIFACTS_DIR/github-oauth; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/github-oauth. We use those same credentials to automatically create releases against DSP's repositories:. printf '#!/bin/bash\necho ' > $RELEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `release.sh` invocation. Find the invocation in the ""command"" part of the; ""Job Specification"" table. It should look like:. bash scripts/release.sh $(cat /io/hail_pip_version) \; $(cat /io/hail_version) \; $(cat /io/git_version) \; origin \; /io/repo/hail/build/deploy/dist/hail-*-py3-none-any.whl \; /io/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:4300,Deployability,deploy,deploy-,4300,"ken >>$RELEASE_ARTIFACTS_DIR/github-oauth. We use those same credentials to automatically create releases against DSP's repositories:. printf '#!/bin/bash\necho ' > $RELEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `release.sh` invocation. Find the invocation in the ""command"" part of the; ""Job Specification"" table. It should look like:. bash scripts/release.sh $(cat /io/hail_pip_version) \; $(cat /io/hail_version) \; $(cat /io/git_version) \; origin \; /io/repo/hail/build/deploy/dist/hail-*-py3-none-any.whl \; /io/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3ops",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:4382,Deployability,deploy,deploy-,4382,"tically create releases against DSP's repositories:. printf '#!/bin/bash\necho ' > $RELEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `release.sh` invocation. Find the invocation in the ""command"" part of the; ""Job Specification"" table. It should look like:. bash scripts/release.sh $(cat /io/hail_pip_version) \; $(cat /io/hail_version) \; $(cat /io/git_version) \; origin \; /io/repo/hail/build/deploy/dist/hail-*-py3-none-any.whl \; /io/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcp",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:4473,Deployability,deploy,deploy-,4473,"LEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `release.sh` invocation. Find the invocation in the ""command"" part of the; ""Job Specification"" table. It should look like:. bash scripts/release.sh $(cat /io/hail_pip_version) \; $(cat /io/hail_version) \; $(cat /io/git_version) \; origin \; /io/repo/hail/build/deploy/dist/hail-*-py3-none-any.whl \; /io/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:4564,Deployability,deploy,deploy-,4564,"d 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `release.sh` invocation. Find the invocation in the ""command"" part of the; ""Job Specification"" table. It should look like:. bash scripts/release.sh $(cat /io/hail_pip_version) \; $(cat /io/hail_version) \; $(cat /io/git_version) \; origin \; /io/repo/hail/build/deploy/dist/hail-*-py3-none-any.whl \; /io/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; do",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:4872,Deployability,release,release,4872," like:. bash scripts/release.sh $(cat /io/hail_pip_version) \; $(cat /io/hail_version) \; $(cat /io/git_version) \; origin \; /io/repo/hail/build/deploy/dist/hail-*-py3-none-any.whl \; /io/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; $RELEASE_ARTIFACTS_DIR/azure-wheel/hail-*-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/www.tar.gz'. When you are complete, delete all the credentials:. rm $RELEASE_ARTIFACTS_DIR/git-askpass; rm $RELEASE_ARTIFACT",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:5206,Deployability,deploy,deploy-,5206,"/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; $RELEASE_ARTIFACTS_DIR/azure-wheel/hail-*-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/www.tar.gz'. When you are complete, delete all the credentials:. rm $RELEASE_ARTIFACTS_DIR/git-askpass; rm $RELEASE_ARTIFACTS_DIR/github-oauth; rm $HOME/.pypirc; 	skopeo logout docker.io. You should also delete the temporary directories used to download the credentials. On Mac OS X,; those directories are all under $TMPDIR which looks like; `/var/folders/x1/601098gx0v11qjx2l_7qfw2c0000gq/T/`. If you're comfortable deleting all of $TMPDIR,; just run:. rm -rf $T",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:5291,Deployability,deploy,deploy-,5291,"cs/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; $RELEASE_ARTIFACTS_DIR/azure-wheel/hail-*-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/www.tar.gz'. When you are complete, delete all the credentials:. rm $RELEASE_ARTIFACTS_DIR/git-askpass; rm $RELEASE_ARTIFACTS_DIR/github-oauth; rm $HOME/.pypirc; 	skopeo logout docker.io. You should also delete the temporary directories used to download the credentials. On Mac OS X,; those directories are all under $TMPDIR which looks like; `/var/folders/x1/601098gx0v11qjx2l_7qfw2c0000gq/T/`. If you're comfortable deleting all of $TMPDIR,; just run:. rm -rf $TMPDIR. ## Failure due to a tag or a release already existing. If you are hand releas",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:5373,Deployability,deploy,deploy-,5373,"/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; $RELEASE_ARTIFACTS_DIR/azure-wheel/hail-*-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/www.tar.gz'. When you are complete, delete all the credentials:. rm $RELEASE_ARTIFACTS_DIR/git-askpass; rm $RELEASE_ARTIFACTS_DIR/github-oauth; rm $HOME/.pypirc; 	skopeo logout docker.io. You should also delete the temporary directories used to download the credentials. On Mac OS X,; those directories are all under $TMPDIR which looks like; `/var/folders/x1/601098gx0v11qjx2l_7qfw2c0000gq/T/`. If you're comfortable deleting all of $TMPDIR,; just run:. rm -rf $TMPDIR. ## Failure due to a tag or a release already existing. If you are hand releasing and the release script exits because the tag or release already exists,; you ca",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:5455,Deployability,deploy,deploy-,5455,"/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; $RELEASE_ARTIFACTS_DIR/azure-wheel/hail-*-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/www.tar.gz'. When you are complete, delete all the credentials:. rm $RELEASE_ARTIFACTS_DIR/git-askpass; rm $RELEASE_ARTIFACTS_DIR/github-oauth; rm $HOME/.pypirc; 	skopeo logout docker.io. You should also delete the temporary directories used to download the credentials. On Mac OS X,; those directories are all under $TMPDIR which looks like; `/var/folders/x1/601098gx0v11qjx2l_7qfw2c0000gq/T/`. If you're comfortable deleting all of $TMPDIR,; just run:. rm -rf $TMPDIR. ## Failure due to a tag or a release already existing. If you are hand releasing and the release script exits because the tag or release already exists,; you can safely comment out the lines that check for that and the lines that create those",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:5546,Deployability,deploy,deploy-,5546,"netics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; $RELEASE_ARTIFACTS_DIR/azure-wheel/hail-*-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/www.tar.gz'. When you are complete, delete all the credentials:. rm $RELEASE_ARTIFACTS_DIR/git-askpass; rm $RELEASE_ARTIFACTS_DIR/github-oauth; rm $HOME/.pypirc; 	skopeo logout docker.io. You should also delete the temporary directories used to download the credentials. On Mac OS X,; those directories are all under $TMPDIR which looks like; `/var/folders/x1/601098gx0v11qjx2l_7qfw2c0000gq/T/`. If you're comfortable deleting all of $TMPDIR,; just run:. rm -rf $TMPDIR. ## Failure due to a tag or a release already existing. If you are hand releasing and the release script exits because the tag or release already exists,; you can safely comment out the lines that check for that and the lines that create those; things. Then you may execute the script to continue with the rest of the release.; ",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:5637,Deployability,deploy,deploy-,5637,"netics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; $RELEASE_ARTIFACTS_DIR/azure-wheel/hail-*-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/www.tar.gz'. When you are complete, delete all the credentials:. rm $RELEASE_ARTIFACTS_DIR/git-askpass; rm $RELEASE_ARTIFACTS_DIR/github-oauth; rm $HOME/.pypirc; 	skopeo logout docker.io. You should also delete the temporary directories used to download the credentials. On Mac OS X,; those directories are all under $TMPDIR which looks like; `/var/folders/x1/601098gx0v11qjx2l_7qfw2c0000gq/T/`. If you're comfortable deleting all of $TMPDIR,; just run:. rm -rf $TMPDIR. ## Failure due to a tag or a release already existing. If you are hand releasing and the release script exits because the tag or release already exists,; you can safely comment out the lines that check for that and the lines that create those; things. Then you may execute the script to continue with the rest of the release.; ",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:6245,Deployability,release,release,6245,"netics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; $RELEASE_ARTIFACTS_DIR/azure-wheel/hail-*-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/www.tar.gz'. When you are complete, delete all the credentials:. rm $RELEASE_ARTIFACTS_DIR/git-askpass; rm $RELEASE_ARTIFACTS_DIR/github-oauth; rm $HOME/.pypirc; 	skopeo logout docker.io. You should also delete the temporary directories used to download the credentials. On Mac OS X,; those directories are all under $TMPDIR which looks like; `/var/folders/x1/601098gx0v11qjx2l_7qfw2c0000gq/T/`. If you're comfortable deleting all of $TMPDIR,; just run:. rm -rf $TMPDIR. ## Failure due to a tag or a release already existing. If you are hand releasing and the release script exits because the tag or release already exists,; you can safely comment out the lines that check for that and the lines that create those; things. Then you may execute the script to continue with the rest of the release.; ",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:6305,Deployability,release,release,6305,"netics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; $RELEASE_ARTIFACTS_DIR/azure-wheel/hail-*-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/www.tar.gz'. When you are complete, delete all the credentials:. rm $RELEASE_ARTIFACTS_DIR/git-askpass; rm $RELEASE_ARTIFACTS_DIR/github-oauth; rm $HOME/.pypirc; 	skopeo logout docker.io. You should also delete the temporary directories used to download the credentials. On Mac OS X,; those directories are all under $TMPDIR which looks like; `/var/folders/x1/601098gx0v11qjx2l_7qfw2c0000gq/T/`. If you're comfortable deleting all of $TMPDIR,; just run:. rm -rf $TMPDIR. ## Failure due to a tag or a release already existing. If you are hand releasing and the release script exits because the tag or release already exists,; you can safely comment out the lines that check for that and the lines that create those; things. Then you may execute the script to continue with the rest of the release.; ",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:6345,Deployability,release,release,6345,"netics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; $RELEASE_ARTIFACTS_DIR/azure-wheel/hail-*-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/www.tar.gz'. When you are complete, delete all the credentials:. rm $RELEASE_ARTIFACTS_DIR/git-askpass; rm $RELEASE_ARTIFACTS_DIR/github-oauth; rm $HOME/.pypirc; 	skopeo logout docker.io. You should also delete the temporary directories used to download the credentials. On Mac OS X,; those directories are all under $TMPDIR which looks like; `/var/folders/x1/601098gx0v11qjx2l_7qfw2c0000gq/T/`. If you're comfortable deleting all of $TMPDIR,; just run:. rm -rf $TMPDIR. ## Failure due to a tag or a release already existing. If you are hand releasing and the release script exits because the tag or release already exists,; you can safely comment out the lines that check for that and the lines that create those; things. Then you may execute the script to continue with the rest of the release.; ",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:6533,Deployability,release,release,6533,"netics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; $RELEASE_ARTIFACTS_DIR/azure-wheel/hail-*-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/www.tar.gz'. When you are complete, delete all the credentials:. rm $RELEASE_ARTIFACTS_DIR/git-askpass; rm $RELEASE_ARTIFACTS_DIR/github-oauth; rm $HOME/.pypirc; 	skopeo logout docker.io. You should also delete the temporary directories used to download the credentials. On Mac OS X,; those directories are all under $TMPDIR which looks like; `/var/folders/x1/601098gx0v11qjx2l_7qfw2c0000gq/T/`. If you're comfortable deleting all of $TMPDIR,; just run:. rm -rf $TMPDIR. ## Failure due to a tag or a release already existing. If you are hand releasing and the release script exits because the tag or release already exists,; you can safely comment out the lines that check for that and the lines that create those; things. Then you may execute the script to continue with the rest of the release.; ",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:6378,Safety,safe,safely,6378,"netics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; $RELEASE_ARTIFACTS_DIR/azure-wheel/hail-*-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/www.tar.gz'. When you are complete, delete all the credentials:. rm $RELEASE_ARTIFACTS_DIR/git-askpass; rm $RELEASE_ARTIFACTS_DIR/github-oauth; rm $HOME/.pypirc; 	skopeo logout docker.io. You should also delete the temporary directories used to download the credentials. On Mac OS X,; those directories are all under $TMPDIR which looks like; `/var/folders/x1/601098gx0v11qjx2l_7qfw2c0000gq/T/`. If you're comfortable deleting all of $TMPDIR,; just run:. rm -rf $TMPDIR. ## Failure due to a tag or a release already existing. If you are hand releasing and the release script exits because the tag or release already exists,; you can safely comment out the lines that check for that and the lines that create those; things. Then you may execute the script to continue with the rest of the release.; ",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:2701,Security,authenticat,authenticate,2701,"ytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/www.tar.gz: 1 files, 43.5 MB. Download all these files except the repo (which you do not need, because you checked out the commit):. BUILD_TOKEN=9cabeeb4ba047d1722e6f8da0383ab97; mkdir $BUILD_TOKEN; 	RELEASE_ARTIFACTS_DIR=$(realpath $BUILD_TOKEN); gcloud storage cp -r \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_pip_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/git_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/azure-wheel \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/www.tar.gz \; $BUILD_TOKEN. Note that the `-r` is necessary because some of these things like `azure-wheel` are folders. Next we need to authenticate with DockerHub. Download the secret and authenticate skopeo with; it. `download-secret` is a function stored in `devbin/functions.sh`. download-secret docker-hub-hailgenetics; cat contents/password | skopeo login --username hailgenetics --password-stdin docker.io; 	popd. Next we need a valid pypirc:. download-secret pypi-credentials; cp contents/pypirc $HOME/.pypirc; 	popd. Next we need a valid github-oauth token (for creating GitHub releases):. download-secret hail-ci-0-1-github-oauth-token; printf 'Authorization: token ' > $RELEASE_ARTIFACTS_DIR/github-oauth; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/github-oauth. We use those same credentials to automatically create releases against DSP's repositories:. printf '#!/bin/bash\necho ' > $RELEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of th",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:2754,Security,authenticat,authenticate,2754,"6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/www.tar.gz: 1 files, 43.5 MB. Download all these files except the repo (which you do not need, because you checked out the commit):. BUILD_TOKEN=9cabeeb4ba047d1722e6f8da0383ab97; mkdir $BUILD_TOKEN; 	RELEASE_ARTIFACTS_DIR=$(realpath $BUILD_TOKEN); gcloud storage cp -r \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_pip_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/git_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/azure-wheel \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/www.tar.gz \; $BUILD_TOKEN. Note that the `-r` is necessary because some of these things like `azure-wheel` are folders. Next we need to authenticate with DockerHub. Download the secret and authenticate skopeo with; it. `download-secret` is a function stored in `devbin/functions.sh`. download-secret docker-hub-hailgenetics; cat contents/password | skopeo login --username hailgenetics --password-stdin docker.io; 	popd. Next we need a valid pypirc:. download-secret pypi-credentials; cp contents/pypirc $HOME/.pypirc; 	popd. Next we need a valid github-oauth token (for creating GitHub releases):. download-secret hail-ci-0-1-github-oauth-token; printf 'Authorization: token ' > $RELEASE_ARTIFACTS_DIR/github-oauth; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/github-oauth. We use those same credentials to automatically create releases against DSP's repositories:. printf '#!/bin/bash\necho ' > $RELEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `re",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:2903,Security,password,password,2903,"d/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/www.tar.gz: 1 files, 43.5 MB. Download all these files except the repo (which you do not need, because you checked out the commit):. BUILD_TOKEN=9cabeeb4ba047d1722e6f8da0383ab97; mkdir $BUILD_TOKEN; 	RELEASE_ARTIFACTS_DIR=$(realpath $BUILD_TOKEN); gcloud storage cp -r \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_pip_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/git_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/azure-wheel \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/www.tar.gz \; $BUILD_TOKEN. Note that the `-r` is necessary because some of these things like `azure-wheel` are folders. Next we need to authenticate with DockerHub. Download the secret and authenticate skopeo with; it. `download-secret` is a function stored in `devbin/functions.sh`. download-secret docker-hub-hailgenetics; cat contents/password | skopeo login --username hailgenetics --password-stdin docker.io; 	popd. Next we need a valid pypirc:. download-secret pypi-credentials; cp contents/pypirc $HOME/.pypirc; 	popd. Next we need a valid github-oauth token (for creating GitHub releases):. download-secret hail-ci-0-1-github-oauth-token; printf 'Authorization: token ' > $RELEASE_ARTIFACTS_DIR/github-oauth; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/github-oauth. We use those same credentials to automatically create releases against DSP's repositories:. printf '#!/bin/bash\necho ' > $RELEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `release.sh` invocation. Find the invocation in the ""command"" part of the; ""Job Specification"" table. It should look like:. bash scripts/release.sh $(cat /io/h",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:2953,Security,password,password-stdin,2953,"d/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/www.tar.gz: 1 files, 43.5 MB. Download all these files except the repo (which you do not need, because you checked out the commit):. BUILD_TOKEN=9cabeeb4ba047d1722e6f8da0383ab97; mkdir $BUILD_TOKEN; 	RELEASE_ARTIFACTS_DIR=$(realpath $BUILD_TOKEN); gcloud storage cp -r \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_pip_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/git_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/azure-wheel \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/www.tar.gz \; $BUILD_TOKEN. Note that the `-r` is necessary because some of these things like `azure-wheel` are folders. Next we need to authenticate with DockerHub. Download the secret and authenticate skopeo with; it. `download-secret` is a function stored in `devbin/functions.sh`. download-secret docker-hub-hailgenetics; cat contents/password | skopeo login --username hailgenetics --password-stdin docker.io; 	popd. Next we need a valid pypirc:. download-secret pypi-credentials; cp contents/pypirc $HOME/.pypirc; 	popd. Next we need a valid github-oauth token (for creating GitHub releases):. download-secret hail-ci-0-1-github-oauth-token; printf 'Authorization: token ' > $RELEASE_ARTIFACTS_DIR/github-oauth; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/github-oauth. We use those same credentials to automatically create releases against DSP's repositories:. printf '#!/bin/bash\necho ' > $RELEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `release.sh` invocation. Find the invocation in the ""command"" part of the; ""Job Specification"" table. It should look like:. bash scripts/release.sh $(cat /io/h",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:150,Testability,log,log,150,"# Releasing FAQ. ## Commits were merged after a broken release. If the release process broke and new commits have merged since we modified the change log, which; commit should we release?. If a commit has been tagged, release that commit. If fixes are necessary, create a branch from the; tagged commit, add commits as necessary, modify the tag, and hand release that. ## Failure due to ""active Temporary Hold"". The release build.yaml job fails due to ""'hail-common/dataproc/0.2.XXX/vep-GRCh37.sh' is under; active Temporary Hold"". What do I do?. There are four files uploaded by Hail for use by Dataproc clusters (a wheel, two VEP scripts, and a; notebook initialization script). We place a temporary hold on these files to prevent them from being; inadvertently deleted. If all four files were successfully uploaded, you can continue the release; from this point by directly executing release.sh for the particular commit used to generate the; uploaded wheel. You can check the commit by downloading the wheel, installing it, and running. python3 -c 'import hail; print(hail.version())'. Checkout this commit locally. Then find the corresponding release batch by searching for:. sha = THE_FULL_SHA. Change directories into the Hail directory:. cd /PATH/TO/REPO/hail. You can download all the necessary files to execute release.sh by downloading them from the; hail-ci-bpk3h bucket. The necessary files are listed under ""Sources: "" in the ""Inputs"" log of the; ""release"" build step. They should look something like:. Sources:; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_version: 1 files, 21 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_pip_version: 1 files, 8 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/buil",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:1449,Testability,log,log,1449,"ml job fails due to ""'hail-common/dataproc/0.2.XXX/vep-GRCh37.sh' is under; active Temporary Hold"". What do I do?. There are four files uploaded by Hail for use by Dataproc clusters (a wheel, two VEP scripts, and a; notebook initialization script). We place a temporary hold on these files to prevent them from being; inadvertently deleted. If all four files were successfully uploaded, you can continue the release; from this point by directly executing release.sh for the particular commit used to generate the; uploaded wheel. You can check the commit by downloading the wheel, installing it, and running. python3 -c 'import hail; print(hail.version())'. Checkout this commit locally. Then find the corresponding release batch by searching for:. sha = THE_FULL_SHA. Change directories into the Hail directory:. cd /PATH/TO/REPO/hail. You can download all the necessary files to execute release.sh by downloading them from the; hail-ci-bpk3h bucket. The necessary files are listed under ""Sources: "" in the ""Inputs"" log of the; ""release"" build step. They should look something like:. Sources:; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_version: 1 files, 21 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/hail_pip_version: 1 files, 8 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/git_version: 1 files, 41 Bytes; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/repo: 6272 files, 205.1 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/www.tar.gz: 1 files, 43.5 MB. Download all these files except the repo (which you do not need, because you checked out the commit):. BUILD_TOKEN=9cabeeb4ba047d1722e6f8da0383ab97; mkdir $BUILD_TOKEN; 	RELEASE_ARTIFACTS_DIR=$(realpath $BUILD_TOKEN); gcloud storage cp -r \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_pip_version \; gs://hail-ci-",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:2921,Testability,log,login,2921,"d/9cabeeb4ba047d1722e6f8da0383ab97/azure-wheel: 1 files, 144.5 MB; gs://hail-ci-bpk3h/build/9cabeeb4ba047d1722e6f8da0383ab97/www.tar.gz: 1 files, 43.5 MB. Download all these files except the repo (which you do not need, because you checked out the commit):. BUILD_TOKEN=9cabeeb4ba047d1722e6f8da0383ab97; mkdir $BUILD_TOKEN; 	RELEASE_ARTIFACTS_DIR=$(realpath $BUILD_TOKEN); gcloud storage cp -r \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/hail_pip_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/git_version \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/azure-wheel \; gs://hail-ci-bpk3h/build/$BUILD_TOKEN/www.tar.gz \; $BUILD_TOKEN. Note that the `-r` is necessary because some of these things like `azure-wheel` are folders. Next we need to authenticate with DockerHub. Download the secret and authenticate skopeo with; it. `download-secret` is a function stored in `devbin/functions.sh`. download-secret docker-hub-hailgenetics; cat contents/password | skopeo login --username hailgenetics --password-stdin docker.io; 	popd. Next we need a valid pypirc:. download-secret pypi-credentials; cp contents/pypirc $HOME/.pypirc; 	popd. Next we need a valid github-oauth token (for creating GitHub releases):. download-secret hail-ci-0-1-github-oauth-token; printf 'Authorization: token ' > $RELEASE_ARTIFACTS_DIR/github-oauth; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/github-oauth. We use those same credentials to automatically create releases against DSP's repositories:. printf '#!/bin/bash\necho ' > $RELEASE_ARTIFACTS_DIR/git-askpass; cat contents/oauth-token >>$RELEASE_ARTIFACTS_DIR/git-askpass; chmod 755 git-askpass; export GIT_ASKPASS=$RELEASE_ARTIFACTS_DIR/git-askpass; 	popd. Ensure you have returned to the `hail` sub-folder of the Hail git repository. Now we can construct a `release.sh` invocation. Find the invocation in the ""command"" part of the; ""Job Specification"" table. It should look like:. bash scripts/release.sh $(cat /io/h",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md:5915,Testability,log,logout,5915,"netics/vep-grch38-95:deploy-dv77x7gtm8ns \; /io/azure-wheel/hail-*-py3-none-any.whl \; /io/www.tar.gz'. We need to make two replacements:. 1. Replace the path to the wheel with the path to the wheel we downloaded from hail-common. 2. Replace `/io` with `$RELEASE_ARTIFACTS_DIR`. It should look something like this:. bash scripts/release.sh \; $(cat $RELEASE_ARTIFACTS_DIR/hail_pip_version) \; $(cat $RELEASE_ARTIFACTS_DIR/hail_version) \; $(cat $RELEASE_ARTIFACTS_DIR/git_version) \; origin \; /PATH/TO/DOWNLOADED/HAIL-COMMON/WHEEL/hail-0.2.XXX-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/github-oauth \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-syrodsx1m9j7 \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-a3opsijrtgir \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-tmdcpjx6zbvh \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-w1ehxyfzy2jl \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-f51bxmvgmwsb \; docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-dv77x7gtm8ns \; $RELEASE_ARTIFACTS_DIR/azure-wheel/hail-*-py3-none-any.whl \; $RELEASE_ARTIFACTS_DIR/www.tar.gz'. When you are complete, delete all the credentials:. rm $RELEASE_ARTIFACTS_DIR/git-askpass; rm $RELEASE_ARTIFACTS_DIR/github-oauth; rm $HOME/.pypirc; 	skopeo logout docker.io. You should also delete the temporary directories used to download the credentials. On Mac OS X,; those directories are all under $TMPDIR which looks like; `/var/folders/x1/601098gx0v11qjx2l_7qfw2c0000gq/T/`. If you're comfortable deleting all of $TMPDIR,; just run:. rm -rf $TMPDIR. ## Failure due to a tag or a release already existing. If you are hand releasing and the release script exits because the tag or release already exists,; you can safely comment out the lines that check for that and the lines that create those; things. Then you may execute the script to continue with the rest of the release.; ",MatchSource.DOCS,dev-docs/releasing.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/releasing.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/compiler-team/development_tools.md:567,Availability,avail,available,567,"# Hail development tools. This document describes and links tools used by the Hail compiler team.; The document is written for the most common operating system used by; the team, macOS. ## General tools. ##### Homebrew - macOS package manager. Homebrew is hard to live without. Use it to install many of the other tools; used by the team. https://brew.sh/. ##### git - version control. It's nice to have a relatively recent version of git. Install this with; brew:. brew install git. It will probably be necessary to change system paths so that the; installed git is available before system git, as [described here](https://ajahne.github.io/blog/tools/2018/06/11/how-to-upgrade-git-mac.html). Once this is working, you should fork the hail-is/hail repository into; your own user space, then clone the repository locally:. git clone https://github.com/username/hail.git. Then add a remote for the main repository to pull in changes:. git remote add hi https://github.com/hail-is/hail.git. ##### Zulip - dev / user chat. We use Zulip for development discussion and conversations with users; (though not typically for user support). Get it here:. https://zulip.com/. Our Zulip server is https://hail.zulipchat.com. ##### Anaconda - manage Python installations and packages. https://www.anaconda.com/download/#macos. After installing Anaconda, you should create a new dev environment; for Hail with:. conda create --name hail python=3.9. and. conda activate hail. (put the latter in a shell .rc file so this is done on shell startup). ##### IntelliJ IDEA - IDE for java/scala/python. https://www.jetbrains.com/idea/. Configuration is hard to document here, get help by asking the team. ##### iTerm2 - terminal replacement. iTerm2 is (subjectively) nicer to use and objectively more customizable; than the built-in macOS terminal. https://iterm2.com/. ##### Google cloud utilities. We primarily use Google Cloud for development. Get the SDK here:. https://cloud.google.com/sdk/docs/install; ",MatchSource.DOCS,dev-docs/compiler-team/development_tools.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/compiler-team/development_tools.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/compiler-team/development_tools.md:1296,Availability,down,download,1296,"# Hail development tools. This document describes and links tools used by the Hail compiler team.; The document is written for the most common operating system used by; the team, macOS. ## General tools. ##### Homebrew - macOS package manager. Homebrew is hard to live without. Use it to install many of the other tools; used by the team. https://brew.sh/. ##### git - version control. It's nice to have a relatively recent version of git. Install this with; brew:. brew install git. It will probably be necessary to change system paths so that the; installed git is available before system git, as [described here](https://ajahne.github.io/blog/tools/2018/06/11/how-to-upgrade-git-mac.html). Once this is working, you should fork the hail-is/hail repository into; your own user space, then clone the repository locally:. git clone https://github.com/username/hail.git. Then add a remote for the main repository to pull in changes:. git remote add hi https://github.com/hail-is/hail.git. ##### Zulip - dev / user chat. We use Zulip for development discussion and conversations with users; (though not typically for user support). Get it here:. https://zulip.com/. Our Zulip server is https://hail.zulipchat.com. ##### Anaconda - manage Python installations and packages. https://www.anaconda.com/download/#macos. After installing Anaconda, you should create a new dev environment; for Hail with:. conda create --name hail python=3.9. and. conda activate hail. (put the latter in a shell .rc file so this is done on shell startup). ##### IntelliJ IDEA - IDE for java/scala/python. https://www.jetbrains.com/idea/. Configuration is hard to document here, get help by asking the team. ##### iTerm2 - terminal replacement. iTerm2 is (subjectively) nicer to use and objectively more customizable; than the built-in macOS terminal. https://iterm2.com/. ##### Google cloud utilities. We primarily use Google Cloud for development. Get the SDK here:. https://cloud.google.com/sdk/docs/install; ",MatchSource.DOCS,dev-docs/compiler-team/development_tools.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/compiler-team/development_tools.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/compiler-team/development_tools.md:288,Deployability,install,install,288,"# Hail development tools. This document describes and links tools used by the Hail compiler team.; The document is written for the most common operating system used by; the team, macOS. ## General tools. ##### Homebrew - macOS package manager. Homebrew is hard to live without. Use it to install many of the other tools; used by the team. https://brew.sh/. ##### git - version control. It's nice to have a relatively recent version of git. Install this with; brew:. brew install git. It will probably be necessary to change system paths so that the; installed git is available before system git, as [described here](https://ajahne.github.io/blog/tools/2018/06/11/how-to-upgrade-git-mac.html). Once this is working, you should fork the hail-is/hail repository into; your own user space, then clone the repository locally:. git clone https://github.com/username/hail.git. Then add a remote for the main repository to pull in changes:. git remote add hi https://github.com/hail-is/hail.git. ##### Zulip - dev / user chat. We use Zulip for development discussion and conversations with users; (though not typically for user support). Get it here:. https://zulip.com/. Our Zulip server is https://hail.zulipchat.com. ##### Anaconda - manage Python installations and packages. https://www.anaconda.com/download/#macos. After installing Anaconda, you should create a new dev environment; for Hail with:. conda create --name hail python=3.9. and. conda activate hail. (put the latter in a shell .rc file so this is done on shell startup). ##### IntelliJ IDEA - IDE for java/scala/python. https://www.jetbrains.com/idea/. Configuration is hard to document here, get help by asking the team. ##### iTerm2 - terminal replacement. iTerm2 is (subjectively) nicer to use and objectively more customizable; than the built-in macOS terminal. https://iterm2.com/. ##### Google cloud utilities. We primarily use Google Cloud for development. Get the SDK here:. https://cloud.google.com/sdk/docs/install; ",MatchSource.DOCS,dev-docs/compiler-team/development_tools.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/compiler-team/development_tools.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/compiler-team/development_tools.md:471,Deployability,install,install,471,"# Hail development tools. This document describes and links tools used by the Hail compiler team.; The document is written for the most common operating system used by; the team, macOS. ## General tools. ##### Homebrew - macOS package manager. Homebrew is hard to live without. Use it to install many of the other tools; used by the team. https://brew.sh/. ##### git - version control. It's nice to have a relatively recent version of git. Install this with; brew:. brew install git. It will probably be necessary to change system paths so that the; installed git is available before system git, as [described here](https://ajahne.github.io/blog/tools/2018/06/11/how-to-upgrade-git-mac.html). Once this is working, you should fork the hail-is/hail repository into; your own user space, then clone the repository locally:. git clone https://github.com/username/hail.git. Then add a remote for the main repository to pull in changes:. git remote add hi https://github.com/hail-is/hail.git. ##### Zulip - dev / user chat. We use Zulip for development discussion and conversations with users; (though not typically for user support). Get it here:. https://zulip.com/. Our Zulip server is https://hail.zulipchat.com. ##### Anaconda - manage Python installations and packages. https://www.anaconda.com/download/#macos. After installing Anaconda, you should create a new dev environment; for Hail with:. conda create --name hail python=3.9. and. conda activate hail. (put the latter in a shell .rc file so this is done on shell startup). ##### IntelliJ IDEA - IDE for java/scala/python. https://www.jetbrains.com/idea/. Configuration is hard to document here, get help by asking the team. ##### iTerm2 - terminal replacement. iTerm2 is (subjectively) nicer to use and objectively more customizable; than the built-in macOS terminal. https://iterm2.com/. ##### Google cloud utilities. We primarily use Google Cloud for development. Get the SDK here:. https://cloud.google.com/sdk/docs/install; ",MatchSource.DOCS,dev-docs/compiler-team/development_tools.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/compiler-team/development_tools.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/compiler-team/development_tools.md:550,Deployability,install,installed,550,"# Hail development tools. This document describes and links tools used by the Hail compiler team.; The document is written for the most common operating system used by; the team, macOS. ## General tools. ##### Homebrew - macOS package manager. Homebrew is hard to live without. Use it to install many of the other tools; used by the team. https://brew.sh/. ##### git - version control. It's nice to have a relatively recent version of git. Install this with; brew:. brew install git. It will probably be necessary to change system paths so that the; installed git is available before system git, as [described here](https://ajahne.github.io/blog/tools/2018/06/11/how-to-upgrade-git-mac.html). Once this is working, you should fork the hail-is/hail repository into; your own user space, then clone the repository locally:. git clone https://github.com/username/hail.git. Then add a remote for the main repository to pull in changes:. git remote add hi https://github.com/hail-is/hail.git. ##### Zulip - dev / user chat. We use Zulip for development discussion and conversations with users; (though not typically for user support). Get it here:. https://zulip.com/. Our Zulip server is https://hail.zulipchat.com. ##### Anaconda - manage Python installations and packages. https://www.anaconda.com/download/#macos. After installing Anaconda, you should create a new dev environment; for Hail with:. conda create --name hail python=3.9. and. conda activate hail. (put the latter in a shell .rc file so this is done on shell startup). ##### IntelliJ IDEA - IDE for java/scala/python. https://www.jetbrains.com/idea/. Configuration is hard to document here, get help by asking the team. ##### iTerm2 - terminal replacement. iTerm2 is (subjectively) nicer to use and objectively more customizable; than the built-in macOS terminal. https://iterm2.com/. ##### Google cloud utilities. We primarily use Google Cloud for development. Get the SDK here:. https://cloud.google.com/sdk/docs/install; ",MatchSource.DOCS,dev-docs/compiler-team/development_tools.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/compiler-team/development_tools.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/compiler-team/development_tools.md:670,Deployability,upgrade,upgrade-git-mac,670,"# Hail development tools. This document describes and links tools used by the Hail compiler team.; The document is written for the most common operating system used by; the team, macOS. ## General tools. ##### Homebrew - macOS package manager. Homebrew is hard to live without. Use it to install many of the other tools; used by the team. https://brew.sh/. ##### git - version control. It's nice to have a relatively recent version of git. Install this with; brew:. brew install git. It will probably be necessary to change system paths so that the; installed git is available before system git, as [described here](https://ajahne.github.io/blog/tools/2018/06/11/how-to-upgrade-git-mac.html). Once this is working, you should fork the hail-is/hail repository into; your own user space, then clone the repository locally:. git clone https://github.com/username/hail.git. Then add a remote for the main repository to pull in changes:. git remote add hi https://github.com/hail-is/hail.git. ##### Zulip - dev / user chat. We use Zulip for development discussion and conversations with users; (though not typically for user support). Get it here:. https://zulip.com/. Our Zulip server is https://hail.zulipchat.com. ##### Anaconda - manage Python installations and packages. https://www.anaconda.com/download/#macos. After installing Anaconda, you should create a new dev environment; for Hail with:. conda create --name hail python=3.9. and. conda activate hail. (put the latter in a shell .rc file so this is done on shell startup). ##### IntelliJ IDEA - IDE for java/scala/python. https://www.jetbrains.com/idea/. Configuration is hard to document here, get help by asking the team. ##### iTerm2 - terminal replacement. iTerm2 is (subjectively) nicer to use and objectively more customizable; than the built-in macOS terminal. https://iterm2.com/. ##### Google cloud utilities. We primarily use Google Cloud for development. Get the SDK here:. https://cloud.google.com/sdk/docs/install; ",MatchSource.DOCS,dev-docs/compiler-team/development_tools.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/compiler-team/development_tools.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/compiler-team/development_tools.md:1243,Deployability,install,installations,1243,"# Hail development tools. This document describes and links tools used by the Hail compiler team.; The document is written for the most common operating system used by; the team, macOS. ## General tools. ##### Homebrew - macOS package manager. Homebrew is hard to live without. Use it to install many of the other tools; used by the team. https://brew.sh/. ##### git - version control. It's nice to have a relatively recent version of git. Install this with; brew:. brew install git. It will probably be necessary to change system paths so that the; installed git is available before system git, as [described here](https://ajahne.github.io/blog/tools/2018/06/11/how-to-upgrade-git-mac.html). Once this is working, you should fork the hail-is/hail repository into; your own user space, then clone the repository locally:. git clone https://github.com/username/hail.git. Then add a remote for the main repository to pull in changes:. git remote add hi https://github.com/hail-is/hail.git. ##### Zulip - dev / user chat. We use Zulip for development discussion and conversations with users; (though not typically for user support). Get it here:. https://zulip.com/. Our Zulip server is https://hail.zulipchat.com. ##### Anaconda - manage Python installations and packages. https://www.anaconda.com/download/#macos. After installing Anaconda, you should create a new dev environment; for Hail with:. conda create --name hail python=3.9. and. conda activate hail. (put the latter in a shell .rc file so this is done on shell startup). ##### IntelliJ IDEA - IDE for java/scala/python. https://www.jetbrains.com/idea/. Configuration is hard to document here, get help by asking the team. ##### iTerm2 - terminal replacement. iTerm2 is (subjectively) nicer to use and objectively more customizable; than the built-in macOS terminal. https://iterm2.com/. ##### Google cloud utilities. We primarily use Google Cloud for development. Get the SDK here:. https://cloud.google.com/sdk/docs/install; ",MatchSource.DOCS,dev-docs/compiler-team/development_tools.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/compiler-team/development_tools.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/compiler-team/development_tools.md:1319,Deployability,install,installing,1319,"# Hail development tools. This document describes and links tools used by the Hail compiler team.; The document is written for the most common operating system used by; the team, macOS. ## General tools. ##### Homebrew - macOS package manager. Homebrew is hard to live without. Use it to install many of the other tools; used by the team. https://brew.sh/. ##### git - version control. It's nice to have a relatively recent version of git. Install this with; brew:. brew install git. It will probably be necessary to change system paths so that the; installed git is available before system git, as [described here](https://ajahne.github.io/blog/tools/2018/06/11/how-to-upgrade-git-mac.html). Once this is working, you should fork the hail-is/hail repository into; your own user space, then clone the repository locally:. git clone https://github.com/username/hail.git. Then add a remote for the main repository to pull in changes:. git remote add hi https://github.com/hail-is/hail.git. ##### Zulip - dev / user chat. We use Zulip for development discussion and conversations with users; (though not typically for user support). Get it here:. https://zulip.com/. Our Zulip server is https://hail.zulipchat.com. ##### Anaconda - manage Python installations and packages. https://www.anaconda.com/download/#macos. After installing Anaconda, you should create a new dev environment; for Hail with:. conda create --name hail python=3.9. and. conda activate hail. (put the latter in a shell .rc file so this is done on shell startup). ##### IntelliJ IDEA - IDE for java/scala/python. https://www.jetbrains.com/idea/. Configuration is hard to document here, get help by asking the team. ##### iTerm2 - terminal replacement. iTerm2 is (subjectively) nicer to use and objectively more customizable; than the built-in macOS terminal. https://iterm2.com/. ##### Google cloud utilities. We primarily use Google Cloud for development. Get the SDK here:. https://cloud.google.com/sdk/docs/install; ",MatchSource.DOCS,dev-docs/compiler-team/development_tools.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/compiler-team/development_tools.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/compiler-team/development_tools.md:1977,Deployability,install,install,1977,"# Hail development tools. This document describes and links tools used by the Hail compiler team.; The document is written for the most common operating system used by; the team, macOS. ## General tools. ##### Homebrew - macOS package manager. Homebrew is hard to live without. Use it to install many of the other tools; used by the team. https://brew.sh/. ##### git - version control. It's nice to have a relatively recent version of git. Install this with; brew:. brew install git. It will probably be necessary to change system paths so that the; installed git is available before system git, as [described here](https://ajahne.github.io/blog/tools/2018/06/11/how-to-upgrade-git-mac.html). Once this is working, you should fork the hail-is/hail repository into; your own user space, then clone the repository locally:. git clone https://github.com/username/hail.git. Then add a remote for the main repository to pull in changes:. git remote add hi https://github.com/hail-is/hail.git. ##### Zulip - dev / user chat. We use Zulip for development discussion and conversations with users; (though not typically for user support). Get it here:. https://zulip.com/. Our Zulip server is https://hail.zulipchat.com. ##### Anaconda - manage Python installations and packages. https://www.anaconda.com/download/#macos. After installing Anaconda, you should create a new dev environment; for Hail with:. conda create --name hail python=3.9. and. conda activate hail. (put the latter in a shell .rc file so this is done on shell startup). ##### IntelliJ IDEA - IDE for java/scala/python. https://www.jetbrains.com/idea/. Configuration is hard to document here, get help by asking the team. ##### iTerm2 - terminal replacement. iTerm2 is (subjectively) nicer to use and objectively more customizable; than the built-in macOS terminal. https://iterm2.com/. ##### Google cloud utilities. We primarily use Google Cloud for development. Get the SDK here:. https://cloud.google.com/sdk/docs/install; ",MatchSource.DOCS,dev-docs/compiler-team/development_tools.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/compiler-team/development_tools.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md:1162,Deployability,pipeline,pipeline,1162,"query that will be sent to the backend for; execution. ```; >>> import hail as hl; >>> t = hl.utils.range_table(100); ```. Python objects that represent (possibly partial) computations; (expression, table, matrix table or block matrix) carry an IR that; implements the computation. The IR can be printed like this:. ```; >>> t = hl.utils.range_table(100); >>> print(t._tir); (TableRange 100 None); ```. The IR is stored as `_ir` for expressions, `_mir` for matrix tables, and `_bmir` for; block matrices. Let's filter `t`. Here's another example:. ```; >>> c = t.idx % 7 < 4; >>> print(c._ir); (ApplyComparisonOp `<` (Apply mod () Int32 (GetField idx (Ref row)) (I32 7)) (I32 4)); >>> t = t.filter(c); >>> print(t._tir); (TableFilter (TableRange 100 None) (Coalesce (ApplyComparisonOp `<` (Apply mod () Int32 (GetField idx (Ref row)) (I32 7)) (I32 4)) (False))); ```. In the repo, the Python implementation of the IR lives in; $HAIL/hail/python/hail/ir. Next, suppose we perform an operation that requires the lazy pipeline; to be executed, `Table.count`, say. Here's the implementation:. ```; def count(self):; return Env.backend().execute(ir.TableCount(self._tir)); ```. The IR is sent to the backend to execute. There are three backends in; Python, each implementing the abstract base class; hail.backend.Backend:; - SparkBackend,; - LocalBackend,; - ServiceBackend. The Python Spark and local backends are implemented by a JVM running; in parallel with Python. The backends on the JVM are implemented by; classes extending is.hail.backend.Backend. These backends works by; calling into the JVM backends via Py4J (soon to be replaced with a; unix domain socket). The Python service backend issues batches to Hail Batch; to execute pipelines. The query service is implemented in Python but; again has a parallel JVM with a ServiceBackend that does the heavy; lifting. When invoked from Python, the JVM backends perform the following:; - The IR is serialized as a string, and it is parsed by IRParser",MatchSource.DOCS,dev-docs/hail-query/hail-query-lifecycle.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md:1881,Deployability,pipeline,pipelines,1881,"> print(t._tir); (TableFilter (TableRange 100 None) (Coalesce (ApplyComparisonOp `<` (Apply mod () Int32 (GetField idx (Ref row)) (I32 7)) (I32 4)) (False))); ```. In the repo, the Python implementation of the IR lives in; $HAIL/hail/python/hail/ir. Next, suppose we perform an operation that requires the lazy pipeline; to be executed, `Table.count`, say. Here's the implementation:. ```; def count(self):; return Env.backend().execute(ir.TableCount(self._tir)); ```. The IR is sent to the backend to execute. There are three backends in; Python, each implementing the abstract base class; hail.backend.Backend:; - SparkBackend,; - LocalBackend,; - ServiceBackend. The Python Spark and local backends are implemented by a JVM running; in parallel with Python. The backends on the JVM are implemented by; classes extending is.hail.backend.Backend. These backends works by; calling into the JVM backends via Py4J (soon to be replaced with a; unix domain socket). The Python service backend issues batches to Hail Batch; to execute pipelines. The query service is implemented in Python but; again has a parallel JVM with a ServiceBackend that does the heavy; lifting. When invoked from Python, the JVM backends perform the following:; - The IR is serialized as a string, and it is parsed by IRParser.; - The IR is type checked, see `TypeCheck`.; - The IR is optimized and lowered. There are a few versions of this,; but the full version looks like:; - Lower MatrixTable IR in terms of TableIR. All MatrixTable IR are; now eliminated. See `LowerMatrixIR`.; - Lower TableIR and BlockMatrixIR to expressions and a; CollectDistributedArray IR which represents the execution of a; stage of the pipeline. See `LowerTableIR`.; - Shuffles (distributed sorts) are implemented on a per-backend; basis by `Backend.lowerDistributedSort`. A new distribution-sort; implementation of a distributed sort is in progress.; - The IR is optimized after parsing and after each lowering step.; - JVM bytecode is generated for",MatchSource.DOCS,dev-docs/hail-query/hail-query-lifecycle.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md:2538,Deployability,pipeline,pipeline,2538,"kend,; - LocalBackend,; - ServiceBackend. The Python Spark and local backends are implemented by a JVM running; in parallel with Python. The backends on the JVM are implemented by; classes extending is.hail.backend.Backend. These backends works by; calling into the JVM backends via Py4J (soon to be replaced with a; unix domain socket). The Python service backend issues batches to Hail Batch; to execute pipelines. The query service is implemented in Python but; again has a parallel JVM with a ServiceBackend that does the heavy; lifting. When invoked from Python, the JVM backends perform the following:; - The IR is serialized as a string, and it is parsed by IRParser.; - The IR is type checked, see `TypeCheck`.; - The IR is optimized and lowered. There are a few versions of this,; but the full version looks like:; - Lower MatrixTable IR in terms of TableIR. All MatrixTable IR are; now eliminated. See `LowerMatrixIR`.; - Lower TableIR and BlockMatrixIR to expressions and a; CollectDistributedArray IR which represents the execution of a; stage of the pipeline. See `LowerTableIR`.; - Shuffles (distributed sorts) are implemented on a per-backend; basis by `Backend.lowerDistributedSort`. A new distribution-sort; implementation of a distributed sort is in progress.; - The IR is optimized after parsing and after each lowering step.; - JVM bytecode is generated for the lowered, optimized IR. See; `Emit`. Again, each backend invokes the generated bytecode; differently:; - For all pipelines, code not inside of a parallel operation; (collecting a distributed array) is executed in the driver.; - The Spark backend executes code on Spark workers by wrapping it; in an RDD.; - The local backend executes everything locally.; - The service backend serializes the generated code to GCS and; submits a Hail Batch to execute that code on Batch workers.; - The final result is serialized and sent back to the Python caller; which is then returned to the user. Lowering is a work in progress, so",MatchSource.DOCS,dev-docs/hail-query/hail-query-lifecycle.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md:2969,Deployability,pipeline,pipelines,2969,". The Python service backend issues batches to Hail Batch; to execute pipelines. The query service is implemented in Python but; again has a parallel JVM with a ServiceBackend that does the heavy; lifting. When invoked from Python, the JVM backends perform the following:; - The IR is serialized as a string, and it is parsed by IRParser.; - The IR is type checked, see `TypeCheck`.; - The IR is optimized and lowered. There are a few versions of this,; but the full version looks like:; - Lower MatrixTable IR in terms of TableIR. All MatrixTable IR are; now eliminated. See `LowerMatrixIR`.; - Lower TableIR and BlockMatrixIR to expressions and a; CollectDistributedArray IR which represents the execution of a; stage of the pipeline. See `LowerTableIR`.; - Shuffles (distributed sorts) are implemented on a per-backend; basis by `Backend.lowerDistributedSort`. A new distribution-sort; implementation of a distributed sort is in progress.; - The IR is optimized after parsing and after each lowering step.; - JVM bytecode is generated for the lowered, optimized IR. See; `Emit`. Again, each backend invokes the generated bytecode; differently:; - For all pipelines, code not inside of a parallel operation; (collecting a distributed array) is executed in the driver.; - The Spark backend executes code on Spark workers by wrapping it; in an RDD.; - The local backend executes everything locally.; - The service backend serializes the generated code to GCS and; submits a Hail Batch to execute that code on Batch workers.; - The final result is serialized and sent back to the Python caller; which is then returned to the user. Lowering is a work in progress, so not all pipelines run on the local; and service backends. The Spark backend has a second, legacy; execution strategy which lowers MatrixIR to TableIR, but then; interprets the TableIR by calling `TableIR.execute` rather than; lowering to CollectDistributedArray. When lowering is feature; complete, `TableIR.execute` will be removed.; ",MatchSource.DOCS,dev-docs/hail-query/hail-query-lifecycle.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md:3484,Deployability,pipeline,pipelines,3484,". The Python service backend issues batches to Hail Batch; to execute pipelines. The query service is implemented in Python but; again has a parallel JVM with a ServiceBackend that does the heavy; lifting. When invoked from Python, the JVM backends perform the following:; - The IR is serialized as a string, and it is parsed by IRParser.; - The IR is type checked, see `TypeCheck`.; - The IR is optimized and lowered. There are a few versions of this,; but the full version looks like:; - Lower MatrixTable IR in terms of TableIR. All MatrixTable IR are; now eliminated. See `LowerMatrixIR`.; - Lower TableIR and BlockMatrixIR to expressions and a; CollectDistributedArray IR which represents the execution of a; stage of the pipeline. See `LowerTableIR`.; - Shuffles (distributed sorts) are implemented on a per-backend; basis by `Backend.lowerDistributedSort`. A new distribution-sort; implementation of a distributed sort is in progress.; - The IR is optimized after parsing and after each lowering step.; - JVM bytecode is generated for the lowered, optimized IR. See; `Emit`. Again, each backend invokes the generated bytecode; differently:; - For all pipelines, code not inside of a parallel operation; (collecting a distributed array) is executed in the driver.; - The Spark backend executes code on Spark workers by wrapping it; in an RDD.; - The local backend executes everything locally.; - The service backend serializes the generated code to GCS and; submits a Hail Batch to execute that code on Batch workers.; - The final result is serialized and sent back to the Python caller; which is then returned to the user. Lowering is a work in progress, so not all pipelines run on the local; and service backends. The Spark backend has a second, legacy; execution strategy which lowers MatrixIR to TableIR, but then; interprets the TableIR by calling `TableIR.execute` rather than; lowering to CollectDistributedArray. When lowering is feature; complete, `TableIR.execute` will be removed.; ",MatchSource.DOCS,dev-docs/hail-query/hail-query-lifecycle.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md:3136,Integrability,wrap,wrapping,3136,". The Python service backend issues batches to Hail Batch; to execute pipelines. The query service is implemented in Python but; again has a parallel JVM with a ServiceBackend that does the heavy; lifting. When invoked from Python, the JVM backends perform the following:; - The IR is serialized as a string, and it is parsed by IRParser.; - The IR is type checked, see `TypeCheck`.; - The IR is optimized and lowered. There are a few versions of this,; but the full version looks like:; - Lower MatrixTable IR in terms of TableIR. All MatrixTable IR are; now eliminated. See `LowerMatrixIR`.; - Lower TableIR and BlockMatrixIR to expressions and a; CollectDistributedArray IR which represents the execution of a; stage of the pipeline. See `LowerTableIR`.; - Shuffles (distributed sorts) are implemented on a per-backend; basis by `Backend.lowerDistributedSort`. A new distribution-sort; implementation of a distributed sort is in progress.; - The IR is optimized after parsing and after each lowering step.; - JVM bytecode is generated for the lowered, optimized IR. See; `Emit`. Again, each backend invokes the generated bytecode; differently:; - For all pipelines, code not inside of a parallel operation; (collecting a distributed array) is executed in the driver.; - The Spark backend executes code on Spark workers by wrapping it; in an RDD.; - The local backend executes everything locally.; - The service backend serializes the generated code to GCS and; submits a Hail Batch to execute that code on Batch workers.; - The final result is serialized and sent back to the Python caller; which is then returned to the user. Lowering is a work in progress, so not all pipelines run on the local; and service backends. The Spark backend has a second, legacy; execution strategy which lowers MatrixIR to TableIR, but then; interprets the TableIR by calling `TableIR.execute` rather than; lowering to CollectDistributedArray. When lowering is feature; complete, `TableIR.execute` will be removed.; ",MatchSource.DOCS,dev-docs/hail-query/hail-query-lifecycle.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md:1664,Modifiability,extend,extending,1664,"trices. Let's filter `t`. Here's another example:. ```; >>> c = t.idx % 7 < 4; >>> print(c._ir); (ApplyComparisonOp `<` (Apply mod () Int32 (GetField idx (Ref row)) (I32 7)) (I32 4)); >>> t = t.filter(c); >>> print(t._tir); (TableFilter (TableRange 100 None) (Coalesce (ApplyComparisonOp `<` (Apply mod () Int32 (GetField idx (Ref row)) (I32 7)) (I32 4)) (False))); ```. In the repo, the Python implementation of the IR lives in; $HAIL/hail/python/hail/ir. Next, suppose we perform an operation that requires the lazy pipeline; to be executed, `Table.count`, say. Here's the implementation:. ```; def count(self):; return Env.backend().execute(ir.TableCount(self._tir)); ```. The IR is sent to the backend to execute. There are three backends in; Python, each implementing the abstract base class; hail.backend.Backend:; - SparkBackend,; - LocalBackend,; - ServiceBackend. The Python Spark and local backends are implemented by a JVM running; in parallel with Python. The backends on the JVM are implemented by; classes extending is.hail.backend.Backend. These backends works by; calling into the JVM backends via Py4J (soon to be replaced with a; unix domain socket). The Python service backend issues batches to Hail Batch; to execute pipelines. The query service is implemented in Python but; again has a parallel JVM with a ServiceBackend that does the heavy; lifting. When invoked from Python, the JVM backends perform the following:; - The IR is serialized as a string, and it is parsed by IRParser.; - The IR is type checked, see `TypeCheck`.; - The IR is optimized and lowered. There are a few versions of this,; but the full version looks like:; - Lower MatrixTable IR in terms of TableIR. All MatrixTable IR are; now eliminated. See `LowerMatrixIR`.; - Lower TableIR and BlockMatrixIR to expressions and a; CollectDistributedArray IR which represents the execution of a; stage of the pipeline. See `LowerTableIR`.; - Shuffles (distributed sorts) are implemented on a per-backend; basis by `",MatchSource.DOCS,dev-docs/hail-query/hail-query-lifecycle.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md:1118,Performance,perform,perform,1118,"query that will be sent to the backend for; execution. ```; >>> import hail as hl; >>> t = hl.utils.range_table(100); ```. Python objects that represent (possibly partial) computations; (expression, table, matrix table or block matrix) carry an IR that; implements the computation. The IR can be printed like this:. ```; >>> t = hl.utils.range_table(100); >>> print(t._tir); (TableRange 100 None); ```. The IR is stored as `_ir` for expressions, `_mir` for matrix tables, and `_bmir` for; block matrices. Let's filter `t`. Here's another example:. ```; >>> c = t.idx % 7 < 4; >>> print(c._ir); (ApplyComparisonOp `<` (Apply mod () Int32 (GetField idx (Ref row)) (I32 7)) (I32 4)); >>> t = t.filter(c); >>> print(t._tir); (TableFilter (TableRange 100 None) (Coalesce (ApplyComparisonOp `<` (Apply mod () Int32 (GetField idx (Ref row)) (I32 7)) (I32 4)) (False))); ```. In the repo, the Python implementation of the IR lives in; $HAIL/hail/python/hail/ir. Next, suppose we perform an operation that requires the lazy pipeline; to be executed, `Table.count`, say. Here's the implementation:. ```; def count(self):; return Env.backend().execute(ir.TableCount(self._tir)); ```. The IR is sent to the backend to execute. There are three backends in; Python, each implementing the abstract base class; hail.backend.Backend:; - SparkBackend,; - LocalBackend,; - ServiceBackend. The Python Spark and local backends are implemented by a JVM running; in parallel with Python. The backends on the JVM are implemented by; classes extending is.hail.backend.Backend. These backends works by; calling into the JVM backends via Py4J (soon to be replaced with a; unix domain socket). The Python service backend issues batches to Hail Batch; to execute pipelines. The query service is implemented in Python but; again has a parallel JVM with a ServiceBackend that does the heavy; lifting. When invoked from Python, the JVM backends perform the following:; - The IR is serialized as a string, and it is parsed by IRParser",MatchSource.DOCS,dev-docs/hail-query/hail-query-lifecycle.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md:2060,Performance,perform,perform,2060,"il/python/hail/ir. Next, suppose we perform an operation that requires the lazy pipeline; to be executed, `Table.count`, say. Here's the implementation:. ```; def count(self):; return Env.backend().execute(ir.TableCount(self._tir)); ```. The IR is sent to the backend to execute. There are three backends in; Python, each implementing the abstract base class; hail.backend.Backend:; - SparkBackend,; - LocalBackend,; - ServiceBackend. The Python Spark and local backends are implemented by a JVM running; in parallel with Python. The backends on the JVM are implemented by; classes extending is.hail.backend.Backend. These backends works by; calling into the JVM backends via Py4J (soon to be replaced with a; unix domain socket). The Python service backend issues batches to Hail Batch; to execute pipelines. The query service is implemented in Python but; again has a parallel JVM with a ServiceBackend that does the heavy; lifting. When invoked from Python, the JVM backends perform the following:; - The IR is serialized as a string, and it is parsed by IRParser.; - The IR is type checked, see `TypeCheck`.; - The IR is optimized and lowered. There are a few versions of this,; but the full version looks like:; - Lower MatrixTable IR in terms of TableIR. All MatrixTable IR are; now eliminated. See `LowerMatrixIR`.; - Lower TableIR and BlockMatrixIR to expressions and a; CollectDistributedArray IR which represents the execution of a; stage of the pipeline. See `LowerTableIR`.; - Shuffles (distributed sorts) are implemented on a per-backend; basis by `Backend.lowerDistributedSort`. A new distribution-sort; implementation of a distributed sort is in progress.; - The IR is optimized after parsing and after each lowering step.; - JVM bytecode is generated for the lowered, optimized IR. See; `Emit`. Again, each backend invokes the generated bytecode; differently:; - For all pipelines, code not inside of a parallel operation; (collecting a distributed array) is executed in the driver.; -",MatchSource.DOCS,dev-docs/hail-query/hail-query-lifecycle.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md:2207,Performance,optimiz,optimized,2207,"e's the implementation:. ```; def count(self):; return Env.backend().execute(ir.TableCount(self._tir)); ```. The IR is sent to the backend to execute. There are three backends in; Python, each implementing the abstract base class; hail.backend.Backend:; - SparkBackend,; - LocalBackend,; - ServiceBackend. The Python Spark and local backends are implemented by a JVM running; in parallel with Python. The backends on the JVM are implemented by; classes extending is.hail.backend.Backend. These backends works by; calling into the JVM backends via Py4J (soon to be replaced with a; unix domain socket). The Python service backend issues batches to Hail Batch; to execute pipelines. The query service is implemented in Python but; again has a parallel JVM with a ServiceBackend that does the heavy; lifting. When invoked from Python, the JVM backends perform the following:; - The IR is serialized as a string, and it is parsed by IRParser.; - The IR is type checked, see `TypeCheck`.; - The IR is optimized and lowered. There are a few versions of this,; but the full version looks like:; - Lower MatrixTable IR in terms of TableIR. All MatrixTable IR are; now eliminated. See `LowerMatrixIR`.; - Lower TableIR and BlockMatrixIR to expressions and a; CollectDistributedArray IR which represents the execution of a; stage of the pipeline. See `LowerTableIR`.; - Shuffles (distributed sorts) are implemented on a per-backend; basis by `Backend.lowerDistributedSort`. A new distribution-sort; implementation of a distributed sort is in progress.; - The IR is optimized after parsing and after each lowering step.; - JVM bytecode is generated for the lowered, optimized IR. See; `Emit`. Again, each backend invokes the generated bytecode; differently:; - For all pipelines, code not inside of a parallel operation; (collecting a distributed array) is executed in the driver.; - The Spark backend executes code on Spark workers by wrapping it; in an RDD.; - The local backend executes everything locally.; ",MatchSource.DOCS,dev-docs/hail-query/hail-query-lifecycle.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md:2766,Performance,optimiz,optimized,2766,"ith a; unix domain socket). The Python service backend issues batches to Hail Batch; to execute pipelines. The query service is implemented in Python but; again has a parallel JVM with a ServiceBackend that does the heavy; lifting. When invoked from Python, the JVM backends perform the following:; - The IR is serialized as a string, and it is parsed by IRParser.; - The IR is type checked, see `TypeCheck`.; - The IR is optimized and lowered. There are a few versions of this,; but the full version looks like:; - Lower MatrixTable IR in terms of TableIR. All MatrixTable IR are; now eliminated. See `LowerMatrixIR`.; - Lower TableIR and BlockMatrixIR to expressions and a; CollectDistributedArray IR which represents the execution of a; stage of the pipeline. See `LowerTableIR`.; - Shuffles (distributed sorts) are implemented on a per-backend; basis by `Backend.lowerDistributedSort`. A new distribution-sort; implementation of a distributed sort is in progress.; - The IR is optimized after parsing and after each lowering step.; - JVM bytecode is generated for the lowered, optimized IR. See; `Emit`. Again, each backend invokes the generated bytecode; differently:; - For all pipelines, code not inside of a parallel operation; (collecting a distributed array) is executed in the driver.; - The Spark backend executes code on Spark workers by wrapping it; in an RDD.; - The local backend executes everything locally.; - The service backend serializes the generated code to GCS and; submits a Hail Batch to execute that code on Batch workers.; - The final result is serialized and sent back to the Python caller; which is then returned to the user. Lowering is a work in progress, so not all pipelines run on the local; and service backends. The Spark backend has a second, legacy; execution strategy which lowers MatrixIR to TableIR, but then; interprets the TableIR by calling `TableIR.execute` rather than; lowering to CollectDistributedArray. When lowering is feature; complete, `TableIR.ex",MatchSource.DOCS,dev-docs/hail-query/hail-query-lifecycle.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md:2866,Performance,optimiz,optimized,2866,". The Python service backend issues batches to Hail Batch; to execute pipelines. The query service is implemented in Python but; again has a parallel JVM with a ServiceBackend that does the heavy; lifting. When invoked from Python, the JVM backends perform the following:; - The IR is serialized as a string, and it is parsed by IRParser.; - The IR is type checked, see `TypeCheck`.; - The IR is optimized and lowered. There are a few versions of this,; but the full version looks like:; - Lower MatrixTable IR in terms of TableIR. All MatrixTable IR are; now eliminated. See `LowerMatrixIR`.; - Lower TableIR and BlockMatrixIR to expressions and a; CollectDistributedArray IR which represents the execution of a; stage of the pipeline. See `LowerTableIR`.; - Shuffles (distributed sorts) are implemented on a per-backend; basis by `Backend.lowerDistributedSort`. A new distribution-sort; implementation of a distributed sort is in progress.; - The IR is optimized after parsing and after each lowering step.; - JVM bytecode is generated for the lowered, optimized IR. See; `Emit`. Again, each backend invokes the generated bytecode; differently:; - For all pipelines, code not inside of a parallel operation; (collecting a distributed array) is executed in the driver.; - The Spark backend executes code on Spark workers by wrapping it; in an RDD.; - The local backend executes everything locally.; - The service backend serializes the generated code to GCS and; submits a Hail Batch to execute that code on Batch workers.; - The final result is serialized and sent back to the Python caller; which is then returned to the user. Lowering is a work in progress, so not all pipelines run on the local; and service backends. The Spark backend has a second, legacy; execution strategy which lowers MatrixIR to TableIR, but then; interprets the TableIR by calling `TableIR.execute` rather than; lowering to CollectDistributedArray. When lowering is feature; complete, `TableIR.execute` will be removed.; ",MatchSource.DOCS,dev-docs/hail-query/hail-query-lifecycle.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/hail-query-lifecycle.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md:5327,Availability,down,downloaded,5327," - `parallelizeAndComputeWithIndex`. There are many other operations like `tableType` which are best thought of as remote procedure calls; from Python to the JVM but where the procedure is executed in a Hail Batch job. This was perhaps a; mistake, but it freed the users from installing Java and empowered us to fully control the run-time; environment of the driver (for example, its core count and memory). ### `broadcast`. Broadcast explicitly serializes a value for use in jobs. In Spark, broadcasted files are; deserialized once per JVM. In Query-on-Batch, broadcasted files are just serialized along with the; code. Improving this requires Batch to support some form of shared/cached data. At time of writing,; the only caching/sharing that Batch supports is sharing of container images. ### `addReference`. A reference is a relatively large (a few gigabytes) sequence of DNA bases. Hail treats them as a; first class object that is used with the Locus datatype. In Spark, these are downloaded onto the; filesystem once per JVM. In Query-on-Batch, the bucket containing the references is mounted via; cloudfuse (gcsfuse in GCP, blobfuse in Azure). Note that gcsfuse requires the use of a Class A; Operation for each directory in a path making this more expensive than the ideal. ### `parallelizeAndComputeWithIndex`. This creates one job for each element of `collection`. Each job executes `f` on its element. If no; job raises an exception, the results are returned in the same order as `collection`. In Spark, we; implement this operation with an RDD. In Query-on-Batch, `submitAndWaitForBatch`:. 1. Uploads the bytecode of the function to one object.; 2. Uploads the contexts and an index thereof to another object.; 3. Creates one job per context encoding in the command an index into the context array (as well as; the length of the context lengths). The memory and cores of the workers are controlled by `hl.init` parameters: `worker_memory` and; `worker_cores`. Hail Query workers cannot ",MatchSource.DOCS,dev-docs/hail-query/query-on-batch.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md:6415,Availability,avail,available,6415,"n Query-on-Batch, the bucket containing the references is mounted via; cloudfuse (gcsfuse in GCP, blobfuse in Azure). Note that gcsfuse requires the use of a Class A; Operation for each directory in a path making this more expensive than the ideal. ### `parallelizeAndComputeWithIndex`. This creates one job for each element of `collection`. Each job executes `f` on its element. If no; job raises an exception, the results are returned in the same order as `collection`. In Spark, we; implement this operation with an RDD. In Query-on-Batch, `submitAndWaitForBatch`:. 1. Uploads the bytecode of the function to one object.; 2. Uploads the contexts and an index thereof to another object.; 3. Creates one job per context encoding in the command an index into the context array (as well as; the length of the context lengths). The memory and cores of the workers are controlled by `hl.init` parameters: `worker_memory` and; `worker_cores`. Hail Query workers cannot use more than one core but larger core counts effectively; make more memory available to the job. The jobs are always added to the same batch in which the driver exists. `submitAndWaitForBatch` polls the Batch API until all the jobs are complete. At time of writing,; Query-on-Batch does not use job groups. Instead, it assumes that a batch contains exactly one; Query-on-Batch driver (itself). The driver considers the distributed execution complete when the; number of complete jobs is one less than the number of jobs (nb: the driver itself is still; running). ## JVM Jobs. Query-on-Batch does not use normal Hail Batch jobs. Instead, it uses ""JVM jobs"". A JVM job is built; to enable Query-on-Batch. Every Hail Batch keeps 31 JVMs running: 1 16-core, 2 8-core, ..., and 16; 1-core JVMs. Each JVM is running the `jvm-entryway`, which classloads a JAR (if not already loaded),; instantiates a given class, and invokes its `main` method. Hail Batch maintains an allow-list of JAR locations which are only writable by the Hail team and",MatchSource.DOCS,dev-docs/hail-query/query-on-batch.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md:8119,Availability,checkpoint,checkpointed,8119,"ontext array (as well as; the length of the context lengths). The memory and cores of the workers are controlled by `hl.init` parameters: `worker_memory` and; `worker_cores`. Hail Query workers cannot use more than one core but larger core counts effectively; make more memory available to the job. The jobs are always added to the same batch in which the driver exists. `submitAndWaitForBatch` polls the Batch API until all the jobs are complete. At time of writing,; Query-on-Batch does not use job groups. Instead, it assumes that a batch contains exactly one; Query-on-Batch driver (itself). The driver considers the distributed execution complete when the; number of complete jobs is one less than the number of jobs (nb: the driver itself is still; running). ## JVM Jobs. Query-on-Batch does not use normal Hail Batch jobs. Instead, it uses ""JVM jobs"". A JVM job is built; to enable Query-on-Batch. Every Hail Batch keeps 31 JVMs running: 1 16-core, 2 8-core, ..., and 16; 1-core JVMs. Each JVM is running the `jvm-entryway`, which classloads a JAR (if not already loaded),; instantiates a given class, and invokes its `main` method. Hail Batch maintains an allow-list of JAR locations which are only writable by the Hail team and; CI. A JAR for every main commit of Hail is uploaded to this location. In `batch/batch/worker.py`,; the main class is hard-coded to `is.hail.backend.service.Main`. `Main` dispatches to either; `is.hail.backend.service.Worker.main` or `is.hail.backend.service.ServiceBackendAPI.main`. The; latter is for driver jobs. The name is unfortunate and ought to be changed. We keep JVMs running on the workers because a cold JVM executes Hail Query code substantially slower; than a warm JVM. A particularly glaring example is the Google Cloud Storage library: it is about an; order of magnitude slower on a cold JVM than a warm one. We are exploring the possibility of; removing JVM Jobs and replacing them with normal jobs which run a warmed and CRaC-checkpointed JVM.; ",MatchSource.DOCS,dev-docs/hail-query/query-on-batch.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md:4209,Deployability,configurat,configuration,4209,"h starts workers. For EXECUTE, the driver parses the; IR, reads metadata about the datasets, plans a query, compiles ""small data"" code, and then executes; each partition of the query in its own job. ## Worker. The entry point to the worker is `Worker`. It simply reads the function object, reads the context; element, classloads the function, invokes the function on the context element, and serializes the; result (or an exception) to cloud storage. Conceptually, a worker can either:. 1. Complete successfully and serialize a small Hail value to cloud storage.; 2. Raise an exception while executing the function and serialize that exception to cloud storage.; 3. Raise an exception at any other point. Both (2) and (3) will cause the job to appear failed in the Hail Batch UI, but if (3) happens, the; driver will fail because an output file is missing. (3) is almost certainly the result of a bug in; `Worker`. ## Backend. In terms of code, Query-on-Batch is implemented as a `Backend` which defines how to serialize; configuration, execute jobs, and deserialize results. The three key operations of `Backend` are:. - `broadcast`; - `addReference`; - `parallelizeAndComputeWithIndex`. There are many other operations like `tableType` which are best thought of as remote procedure calls; from Python to the JVM but where the procedure is executed in a Hail Batch job. This was perhaps a; mistake, but it freed the users from installing Java and empowered us to fully control the run-time; environment of the driver (for example, its core count and memory). ### `broadcast`. Broadcast explicitly serializes a value for use in jobs. In Spark, broadcasted files are; deserialized once per JVM. In Query-on-Batch, broadcasted files are just serialized along with the; code. Improving this requires Batch to support some form of shared/cached data. At time of writing,; the only caching/sharing that Batch supports is sharing of container images. ### `addReference`. A reference is a relatively large (a",MatchSource.DOCS,dev-docs/hail-query/query-on-batch.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md:4615,Deployability,install,installing,4615," a worker can either:. 1. Complete successfully and serialize a small Hail value to cloud storage.; 2. Raise an exception while executing the function and serialize that exception to cloud storage.; 3. Raise an exception at any other point. Both (2) and (3) will cause the job to appear failed in the Hail Batch UI, but if (3) happens, the; driver will fail because an output file is missing. (3) is almost certainly the result of a bug in; `Worker`. ## Backend. In terms of code, Query-on-Batch is implemented as a `Backend` which defines how to serialize; configuration, execute jobs, and deserialize results. The three key operations of `Backend` are:. - `broadcast`; - `addReference`; - `parallelizeAndComputeWithIndex`. There are many other operations like `tableType` which are best thought of as remote procedure calls; from Python to the JVM but where the procedure is executed in a Hail Batch job. This was perhaps a; mistake, but it freed the users from installing Java and empowered us to fully control the run-time; environment of the driver (for example, its core count and memory). ### `broadcast`. Broadcast explicitly serializes a value for use in jobs. In Spark, broadcasted files are; deserialized once per JVM. In Query-on-Batch, broadcasted files are just serialized along with the; code. Improving this requires Batch to support some form of shared/cached data. At time of writing,; the only caching/sharing that Batch supports is sharing of container images. ### `addReference`. A reference is a relatively large (a few gigabytes) sequence of DNA bases. Hail treats them as a; first class object that is used with the Locus datatype. In Spark, these are downloaded onto the; filesystem once per JVM. In Query-on-Batch, the bucket containing the references is mounted via; cloudfuse (gcsfuse in GCP, blobfuse in Azure). Note that gcsfuse requires the use of a Class A; Operation for each directory in a path making this more expensive than the ideal. ### `parallelizeAndComputeWit",MatchSource.DOCS,dev-docs/hail-query/query-on-batch.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md:4209,Modifiability,config,configuration,4209,"h starts workers. For EXECUTE, the driver parses the; IR, reads metadata about the datasets, plans a query, compiles ""small data"" code, and then executes; each partition of the query in its own job. ## Worker. The entry point to the worker is `Worker`. It simply reads the function object, reads the context; element, classloads the function, invokes the function on the context element, and serializes the; result (or an exception) to cloud storage. Conceptually, a worker can either:. 1. Complete successfully and serialize a small Hail value to cloud storage.; 2. Raise an exception while executing the function and serialize that exception to cloud storage.; 3. Raise an exception at any other point. Both (2) and (3) will cause the job to appear failed in the Hail Batch UI, but if (3) happens, the; driver will fail because an output file is missing. (3) is almost certainly the result of a bug in; `Worker`. ## Backend. In terms of code, Query-on-Batch is implemented as a `Backend` which defines how to serialize; configuration, execute jobs, and deserialize results. The three key operations of `Backend` are:. - `broadcast`; - `addReference`; - `parallelizeAndComputeWithIndex`. There are many other operations like `tableType` which are best thought of as remote procedure calls; from Python to the JVM but where the procedure is executed in a Hail Batch job. This was perhaps a; mistake, but it freed the users from installing Java and empowered us to fully control the run-time; environment of the driver (for example, its core count and memory). ### `broadcast`. Broadcast explicitly serializes a value for use in jobs. In Spark, broadcasted files are; deserialized once per JVM. In Query-on-Batch, broadcasted files are just serialized along with the; code. Improving this requires Batch to support some form of shared/cached data. At time of writing,; the only caching/sharing that Batch supports is sharing of container images. ### `addReference`. A reference is a relatively large (a",MatchSource.DOCS,dev-docs/hail-query/query-on-batch.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md:5021,Performance,cache,cached,5021,"er will fail because an output file is missing. (3) is almost certainly the result of a bug in; `Worker`. ## Backend. In terms of code, Query-on-Batch is implemented as a `Backend` which defines how to serialize; configuration, execute jobs, and deserialize results. The three key operations of `Backend` are:. - `broadcast`; - `addReference`; - `parallelizeAndComputeWithIndex`. There are many other operations like `tableType` which are best thought of as remote procedure calls; from Python to the JVM but where the procedure is executed in a Hail Batch job. This was perhaps a; mistake, but it freed the users from installing Java and empowered us to fully control the run-time; environment of the driver (for example, its core count and memory). ### `broadcast`. Broadcast explicitly serializes a value for use in jobs. In Spark, broadcasted files are; deserialized once per JVM. In Query-on-Batch, broadcasted files are just serialized along with the; code. Improving this requires Batch to support some form of shared/cached data. At time of writing,; the only caching/sharing that Batch supports is sharing of container images. ### `addReference`. A reference is a relatively large (a few gigabytes) sequence of DNA bases. Hail treats them as a; first class object that is used with the Locus datatype. In Spark, these are downloaded onto the; filesystem once per JVM. In Query-on-Batch, the bucket containing the references is mounted via; cloudfuse (gcsfuse in GCP, blobfuse in Azure). Note that gcsfuse requires the use of a Class A; Operation for each directory in a path making this more expensive than the ideal. ### `parallelizeAndComputeWithIndex`. This creates one job for each element of `collection`. Each job executes `f` on its element. If no; job raises an exception, the results are returned in the same order as `collection`. In Spark, we; implement this operation with an RDD. In Query-on-Batch, `submitAndWaitForBatch`:. 1. Uploads the bytecode of the function to one object",MatchSource.DOCS,dev-docs/hail-query/query-on-batch.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md:7209,Performance,load,loaded,7209,"ontext array (as well as; the length of the context lengths). The memory and cores of the workers are controlled by `hl.init` parameters: `worker_memory` and; `worker_cores`. Hail Query workers cannot use more than one core but larger core counts effectively; make more memory available to the job. The jobs are always added to the same batch in which the driver exists. `submitAndWaitForBatch` polls the Batch API until all the jobs are complete. At time of writing,; Query-on-Batch does not use job groups. Instead, it assumes that a batch contains exactly one; Query-on-Batch driver (itself). The driver considers the distributed execution complete when the; number of complete jobs is one less than the number of jobs (nb: the driver itself is still; running). ## JVM Jobs. Query-on-Batch does not use normal Hail Batch jobs. Instead, it uses ""JVM jobs"". A JVM job is built; to enable Query-on-Batch. Every Hail Batch keeps 31 JVMs running: 1 16-core, 2 8-core, ..., and 16; 1-core JVMs. Each JVM is running the `jvm-entryway`, which classloads a JAR (if not already loaded),; instantiates a given class, and invokes its `main` method. Hail Batch maintains an allow-list of JAR locations which are only writable by the Hail team and; CI. A JAR for every main commit of Hail is uploaded to this location. In `batch/batch/worker.py`,; the main class is hard-coded to `is.hail.backend.service.Main`. `Main` dispatches to either; `is.hail.backend.service.Worker.main` or `is.hail.backend.service.ServiceBackendAPI.main`. The; latter is for driver jobs. The name is unfortunate and ought to be changed. We keep JVMs running on the workers because a cold JVM executes Hail Query code substantially slower; than a warm JVM. A particularly glaring example is the Google Cloud Storage library: it is about an; order of magnitude slower on a cold JVM than a warm one. We are exploring the possibility of; removing JVM Jobs and replacing them with normal jobs which run a warmed and CRaC-checkpointed JVM.; ",MatchSource.DOCS,dev-docs/hail-query/query-on-batch.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md:3071,Usability,simpl,simple,3071,"r; that an array of objects repeats the field names). Instead, we select, for each type exactly one; EType which we implement in both Scala and Python. The implementation in Python is the; `_from_encoding` method of the Type class hierarchy. There is exactly one place where Hail values are sent in the opposite direction from Python to; Scala: `EncodedLiteral`. Encoded literals are used to transfer large objects like a large dictionary; or a long list from Python to Scala. The value is serialized to the same EType mentioned above using; the `_to_encoding` method of the Type class hierarchy. A encoded literal is created by `hl.literal`; for any non-primitive, non-missing value. ## Driver. The entry point to the driver is the `ServiceBackendAPI` which is instantiated once per remote; procedure call. Every call corresponds to exactly one batch job. Each job is a JVM Job which is; described in more detail below under ""JVM Jobs"". For most of the operations, the driver quickly; executes a simple function and returns its value without starting any workers. The EXECUTE operation is the only operation which starts workers. For EXECUTE, the driver parses the; IR, reads metadata about the datasets, plans a query, compiles ""small data"" code, and then executes; each partition of the query in its own job. ## Worker. The entry point to the worker is `Worker`. It simply reads the function object, reads the context; element, classloads the function, invokes the function on the context element, and serializes the; result (or an exception) to cloud storage. Conceptually, a worker can either:. 1. Complete successfully and serialize a small Hail value to cloud storage.; 2. Raise an exception while executing the function and serialize that exception to cloud storage.; 3. Raise an exception at any other point. Both (2) and (3) will cause the job to appear failed in the Hail Batch UI, but if (3) happens, the; driver will fail because an output file is missing. (3) is almost certainly the res",MatchSource.DOCS,dev-docs/hail-query/query-on-batch.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md:3443,Usability,simpl,simply,3443,"on to Scala. The value is serialized to the same EType mentioned above using; the `_to_encoding` method of the Type class hierarchy. A encoded literal is created by `hl.literal`; for any non-primitive, non-missing value. ## Driver. The entry point to the driver is the `ServiceBackendAPI` which is instantiated once per remote; procedure call. Every call corresponds to exactly one batch job. Each job is a JVM Job which is; described in more detail below under ""JVM Jobs"". For most of the operations, the driver quickly; executes a simple function and returns its value without starting any workers. The EXECUTE operation is the only operation which starts workers. For EXECUTE, the driver parses the; IR, reads metadata about the datasets, plans a query, compiles ""small data"" code, and then executes; each partition of the query in its own job. ## Worker. The entry point to the worker is `Worker`. It simply reads the function object, reads the context; element, classloads the function, invokes the function on the context element, and serializes the; result (or an exception) to cloud storage. Conceptually, a worker can either:. 1. Complete successfully and serialize a small Hail value to cloud storage.; 2. Raise an exception while executing the function and serialize that exception to cloud storage.; 3. Raise an exception at any other point. Both (2) and (3) will cause the job to appear failed in the Hail Batch UI, but if (3) happens, the; driver will fail because an output file is missing. (3) is almost certainly the result of a bug in; `Worker`. ## Backend. In terms of code, Query-on-Batch is implemented as a `Backend` which defines how to serialize; configuration, execute jobs, and deserialize results. The three key operations of `Backend` are:. - `broadcast`; - `addReference`; - `parallelizeAndComputeWithIndex`. There are many other operations like `tableType` which are best thought of as remote procedure calls; from Python to the JVM but where the procedure is executed i",MatchSource.DOCS,dev-docs/hail-query/query-on-batch.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/query-on-batch.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:8705,Deployability,pipeline,pipeline,8705,"om function). This minimizes the overhead of random number generation in inner loops, and is the reason for choose PMAC. # UIDs; To use the above PMAC scheme, we need to assign a ""message"" to every random function invocation in the program. As long as each invocation gets a distinct message, the PMAC random function generates approximately independent randomness for each invocation. We fix a key for the block cipher once and for all. It was generated randomly, and is hard coded in the compiler. This saves us from issues of users specifying ""bad"" keys. Instead, we reserve a part of the message to encode a session scoped uid. By changing that uid between sessions, we allow running identical code repeatedly with independent randomness. ## Static UIDs; We split the message into static and dynamic components. The static component consists of two longs. The first, called the ""rng nonce"", is a hail session constant. It replaces the old ""global seed"", allowing the same pipeline to run with independent randomness each session, unless determinism is specifically requested. The second component is stored in the `ApplySeeded` IR node. We simply maintain a global counter, and increment it each time an `ApplySeeded` node is constructed, ensuring that each node in a pipeline has a distinct static uid. The dynamic component is needed to distinguish between different invocations of a single `ApplySeeded` node inside a looping construct. It is an arbitrary length message (though it will typically be quite small, probably less than 10 longs). It is constructed as follows:. ## Dynamic UIDs; Every stream, table, or matrix table pipeline is transformed to explicitly generate a unique uid per stream entry, table row, and matrix table row/column. These uids are explicit in the IR as ordinary values/fields, so the compiler automatically preserves the RNG determinism. ## Putting it all together; Consider the example pipeline; ```; mt = hl.utils.range_matrix_table(10, 10); mt = mt.annotate_ent",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:9001,Deployability,pipeline,pipeline,9001,"n in the program. As long as each invocation gets a distinct message, the PMAC random function generates approximately independent randomness for each invocation. We fix a key for the block cipher once and for all. It was generated randomly, and is hard coded in the compiler. This saves us from issues of users specifying ""bad"" keys. Instead, we reserve a part of the message to encode a session scoped uid. By changing that uid between sessions, we allow running identical code repeatedly with independent randomness. ## Static UIDs; We split the message into static and dynamic components. The static component consists of two longs. The first, called the ""rng nonce"", is a hail session constant. It replaces the old ""global seed"", allowing the same pipeline to run with independent randomness each session, unless determinism is specifically requested. The second component is stored in the `ApplySeeded` IR node. We simply maintain a global counter, and increment it each time an `ApplySeeded` node is constructed, ensuring that each node in a pipeline has a distinct static uid. The dynamic component is needed to distinguish between different invocations of a single `ApplySeeded` node inside a looping construct. It is an arbitrary length message (though it will typically be quite small, probably less than 10 longs). It is constructed as follows:. ## Dynamic UIDs; Every stream, table, or matrix table pipeline is transformed to explicitly generate a unique uid per stream entry, table row, and matrix table row/column. These uids are explicit in the IR as ordinary values/fields, so the compiler automatically preserves the RNG determinism. ## Putting it all together; Consider the example pipeline; ```; mt = hl.utils.range_matrix_table(10, 10); mt = mt.annotate_entries(a=hl.range(10).map(lambda i: hl.rand_int32(100))); ```; Before elaborating UIDs in the IR in python, the IR looks like this (after a little tidying):; ```; !1 = MatrixRead [DropRowColUIDs, ...] // don't add uid fields;",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:9364,Deployability,pipeline,pipeline,9364,"ons, we allow running identical code repeatedly with independent randomness. ## Static UIDs; We split the message into static and dynamic components. The static component consists of two longs. The first, called the ""rng nonce"", is a hail session constant. It replaces the old ""global seed"", allowing the same pipeline to run with independent randomness each session, unless determinism is specifically requested. The second component is stored in the `ApplySeeded` IR node. We simply maintain a global counter, and increment it each time an `ApplySeeded` node is constructed, ensuring that each node in a pipeline has a distinct static uid. The dynamic component is needed to distinguish between different invocations of a single `ApplySeeded` node inside a looping construct. It is an arbitrary length message (though it will typically be quite small, probably less than 10 longs). It is constructed as follows:. ## Dynamic UIDs; Every stream, table, or matrix table pipeline is transformed to explicitly generate a unique uid per stream entry, table row, and matrix table row/column. These uids are explicit in the IR as ordinary values/fields, so the compiler automatically preserves the RNG determinism. ## Putting it all together; Consider the example pipeline; ```; mt = hl.utils.range_matrix_table(10, 10); mt = mt.annotate_entries(a=hl.range(10).map(lambda i: hl.rand_int32(100))); ```; Before elaborating UIDs in the IR in python, the IR looks like this (after a little tidying):; ```; !1 = MatrixRead [DropRowColUIDs, ...] // don't add uid fields; !3 = MatrixMapEntries(!1) {; (%g, %col, %row, %entry) =>; !c0 = I32 [0]; !c10 = I32 [10]; !c1 = I32 [1]; !s = StreamRange(!c0, !c10, !c1) [1, False]; !s2 = StreamMap(!s) { (%elt) =>; !c100 = I32 [100]; ApplySeeded(!c100, %__rng_state) [rand_int32, 0, Int32] // unbound %__rng_state; }; !2 = ToArray(!s2); InsertFields !entry (a: !2); }; ```; Note that the `ApplySeeded` node is tageed with a static UID `0`, and references an unbound variabl",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:9653,Deployability,pipeline,pipeline,9653,"on constant. It replaces the old ""global seed"", allowing the same pipeline to run with independent randomness each session, unless determinism is specifically requested. The second component is stored in the `ApplySeeded` IR node. We simply maintain a global counter, and increment it each time an `ApplySeeded` node is constructed, ensuring that each node in a pipeline has a distinct static uid. The dynamic component is needed to distinguish between different invocations of a single `ApplySeeded` node inside a looping construct. It is an arbitrary length message (though it will typically be quite small, probably less than 10 longs). It is constructed as follows:. ## Dynamic UIDs; Every stream, table, or matrix table pipeline is transformed to explicitly generate a unique uid per stream entry, table row, and matrix table row/column. These uids are explicit in the IR as ordinary values/fields, so the compiler automatically preserves the RNG determinism. ## Putting it all together; Consider the example pipeline; ```; mt = hl.utils.range_matrix_table(10, 10); mt = mt.annotate_entries(a=hl.range(10).map(lambda i: hl.rand_int32(100))); ```; Before elaborating UIDs in the IR in python, the IR looks like this (after a little tidying):; ```; !1 = MatrixRead [DropRowColUIDs, ...] // don't add uid fields; !3 = MatrixMapEntries(!1) {; (%g, %col, %row, %entry) =>; !c0 = I32 [0]; !c10 = I32 [10]; !c1 = I32 [1]; !s = StreamRange(!c0, !c10, !c1) [1, False]; !s2 = StreamMap(!s) { (%elt) =>; !c100 = I32 [100]; ApplySeeded(!c100, %__rng_state) [rand_int32, 0, Int32] // unbound %__rng_state; }; !2 = ToArray(!s2); InsertFields !entry (a: !2); }; ```; Note that the `ApplySeeded` node is tageed with a static UID `0`, and references an unbound variable `__rng_state`. It is the responsibility of the `handle_randomness` pass to give proper definitions of `__rng_state` in any scope that needs it. After `handle_randomness` (and some more tidying), the IR looks like:; ```; // Now MatrixRead adds ",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:12485,Deployability,pipeline,pipeline,12485,"c100, %9) [rand_int32, 0, Int32]; }; !10 = ToArray(!s3); InsertFields !entry (a: !10); }; ```; Note that because only 3 longs are added to the message, none of the `RNGSplit` calls generate any runtime code. They simply encode statically that the last block of the message at the time of the `ApplySeeded` call consists of the locals `[!3, !4, !8]`. Then the `ApplySeeded` just needs to pad the last block, xor it with the running sum (which is the encrypted static block, embedded as a constant in the code), and call the Threefry `encrypt` function just once. # Security; Cryptogrophers have developed a very pragmatic theory of what makes for ""good"" pseudorandomness. One of the benefits of using cryptographic primitives (even while weakening some of the components for performance, as we do with Threefish) is that we can use this framework to evaluate how well users can trust the outputs of the RNG. Using this theory for a quick sanity check, consider a pipeline with a 1e7 row by 1e7 column matrixtable, with 1e4 random function invocations per entry, running for a year on 1e23 cores. Let `b` be any boolean output of this pipeline. Let `P_1` and `P_2` be the probabilities that `b=1` in the scenarios where random functions are truly random, and using the above scheme, respectively. Then `abs(P_1 - P_2) < 3e-40`. The only assumption in this bound is that Threefry is a secure block cipher, i.e. that the best attack against it is a brute force search of the space of all keys. The time bound comes from limiting how much of the key space the program is able to search. Clearly this will never be the weak link, and we can focus on how many random numbers are generated. This is a very practically reasurring result. It says that users can really trust that their results--interpreted under a model of true randomness--are not skewed by our implementation of pseudorandomness. # User interface; For the most part, users should not need to interact directly with the randomness infrastruct",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:12656,Deployability,pipeline,pipeline,12656,"3 longs are added to the message, none of the `RNGSplit` calls generate any runtime code. They simply encode statically that the last block of the message at the time of the `ApplySeeded` call consists of the locals `[!3, !4, !8]`. Then the `ApplySeeded` just needs to pad the last block, xor it with the running sum (which is the encrypted static block, embedded as a constant in the code), and call the Threefry `encrypt` function just once. # Security; Cryptogrophers have developed a very pragmatic theory of what makes for ""good"" pseudorandomness. One of the benefits of using cryptographic primitives (even while weakening some of the components for performance, as we do with Threefish) is that we can use this framework to evaluate how well users can trust the outputs of the RNG. Using this theory for a quick sanity check, consider a pipeline with a 1e7 row by 1e7 column matrixtable, with 1e4 random function invocations per entry, running for a year on 1e23 cores. Let `b` be any boolean output of this pipeline. Let `P_1` and `P_2` be the probabilities that `b=1` in the scenarios where random functions are truly random, and using the above scheme, respectively. Then `abs(P_1 - P_2) < 3e-40`. The only assumption in this bound is that Threefry is a secure block cipher, i.e. that the best attack against it is a brute force search of the space of all keys. The time bound comes from limiting how much of the key space the program is able to search. Clearly this will never be the weak link, and we can focus on how many random numbers are generated. This is a very practically reasurring result. It says that users can really trust that their results--interpreted under a model of true randomness--are not skewed by our implementation of pseudorandomness. # User interface; For the most part, users should not need to interact directly with the randomness infrastructure. If they don't, the default semantics are:; * Evaluating a hail expression multiple times in the same session alwa",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:13825,Deployability,pipeline,pipeline,13825,"obabilities that `b=1` in the scenarios where random functions are truly random, and using the above scheme, respectively. Then `abs(P_1 - P_2) < 3e-40`. The only assumption in this bound is that Threefry is a secure block cipher, i.e. that the best attack against it is a brute force search of the space of all keys. The time bound comes from limiting how much of the key space the program is able to search. Clearly this will never be the weak link, and we can focus on how many random numbers are generated. This is a very practically reasurring result. It says that users can really trust that their results--interpreted under a model of true randomness--are not skewed by our implementation of pseudorandomness. # User interface; For the most part, users should not need to interact directly with the randomness infrastructure. If they don't, the default semantics are:; * Evaluating a hail expression multiple times in the same session always produces the same results; * Rebuilding an identical hail expression (e.g. `x = hl.rand_unif()` and `y = hl.rand_unif()`) evaluates with independent randomness.; * Running the same pipeline in multiple hail sessions uses independend randomness each time. The last two can be overridden if needed:; * To build identical expressions using the same randomness, manually specify ""seeds"" (should we rename this?) on each random function call. E.g. `x = hl.rand_unif(seed=0)`. This overrides using the global counter to populate the static uid. It is guaranteed that user specified static uids never clash with automatically generated ones.; * To run the same pipeline in multiple sessions with the same randomness, manually specify the ""global seed"" on init: `hl.init(global_seed=0)`. [1] ""Splittable pseudorandom number generators using cryptographic hashing""; [2] ""The Skein Hash Function Family""; [3] ""Parallel random numbers: as easy as 1, 2, 3""; [4] Rogaway, ""Efficient Instantiations of Tweakable Blockciphers and Refinements to Modes OCB and PMAC""; ",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:14298,Deployability,pipeline,pipeline,14298,"obabilities that `b=1` in the scenarios where random functions are truly random, and using the above scheme, respectively. Then `abs(P_1 - P_2) < 3e-40`. The only assumption in this bound is that Threefry is a secure block cipher, i.e. that the best attack against it is a brute force search of the space of all keys. The time bound comes from limiting how much of the key space the program is able to search. Clearly this will never be the weak link, and we can focus on how many random numbers are generated. This is a very practically reasurring result. It says that users can really trust that their results--interpreted under a model of true randomness--are not skewed by our implementation of pseudorandomness. # User interface; For the most part, users should not need to interact directly with the randomness infrastructure. If they don't, the default semantics are:; * Evaluating a hail expression multiple times in the same session always produces the same results; * Rebuilding an identical hail expression (e.g. `x = hl.rand_unif()` and `y = hl.rand_unif()`) evaluates with independent randomness.; * Running the same pipeline in multiple hail sessions uses independend randomness each time. The last two can be overridden if needed:; * To build identical expressions using the same randomness, manually specify ""seeds"" (should we rename this?) on each random function call. E.g. `x = hl.rand_unif(seed=0)`. This overrides using the global counter to populate the static uid. It is guaranteed that user specified static uids never clash with automatically generated ones.; * To run the same pipeline in multiple sessions with the same randomness, manually specify the ""global seed"" on init: `hl.init(global_seed=0)`. [1] ""Splittable pseudorandom number generators using cryptographic hashing""; [2] ""The Skein Hash Function Family""; [3] ""Parallel random numbers: as easy as 1, 2, 3""; [4] Rogaway, ""Efficient Instantiations of Tweakable Blockciphers and Refinements to Modes OCB and PMAC""; ",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:1068,Energy Efficiency,reduce,reduced-round,1068,"om number generation is inspired by [1], but several details differ. At a high level, the idea is:; * Assign to each random function invocation some unique identifier. In general we can't bound the size of the identifier. We use arrays of longs.; * Use a construction of a psuedo-random function to map unique identifiers to random streams of bits. Intuitively, it's as if we used the identifier to seed a stateful RNG. The key property is that random function invocations with distinct identifiers produce independent random results, while invocations with the same identifier always produce the same result. Thus random function invocations are actually pure functions, with no side effects, which gives the compiler great freedom to optimize queries without affecting the results. Psuedo-random functions are important building blocks in cryptography, and so they are very well studied, with many different practical constructions. We use the PMAC message authentication code, which depends on a tweakable block cipher, for which we use a reduced-round Threefish. Either or both of these pieces could be replaced with little effort, e.g. to improve performance. # Threefish/Threefry:; We use the Threefish [2] block cipher, modified to use 20 rounds for efficiency (the full Threefish4x64 uses 72 rounds), as suggested by [3] (although we make use of the Threefish tweak). Reference implementation is `Threefry.encrypt`. `threefish4x64` takes:; * key `K = (k_0, ..., k_3)`: 4 words; * tweak `T = (t_0, t_1)`: 2 words; * plaintext `P = (p_0, ..., p_3)`: 4 words. Intutively, this is a function taking a key and tweak as input, and returning a permutation on the space of all 256-bit blocks. The security claim is that if the key is chosen randomly, then for any choice of tweak, the resulting permutation ""looks like"" a uniformly chosen random permutation. Like most (all?) block ciphers, it is constructed as a sequence of simpler permutations. Think of shuffling a deck of cards: each shuffle isn",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:2536,Energy Efficiency,schedul,schedule,2536,"., p_3)`: 4 words. Intutively, this is a function taking a key and tweak as input, and returning a permutation on the space of all 256-bit blocks. The security claim is that if the key is chosen randomly, then for any choice of tweak, the resulting permutation ""looks like"" a uniformly chosen random permutation. Like most (all?) block ciphers, it is constructed as a sequence of simpler permutations. Think of shuffling a deck of cards: each shuffle isn't that random (is easily distinguishable from a completely random permutation), but a sequence of seven shuffles is indistinguishable from a random permutation. The simple permutations are called ""rounds"". Each round consists of applying a function ""Mix"" to pairs of 64-bit words, which is a bit-level permutation, followed by a permutation of the four words. <img width=""440"" alt=""threefish"" src=""https://user-images.githubusercontent.com/3430459/197852931-59bb6734-917b-4c4a-b3a2-7e2e9302a09f.png"">. ## key schedule; The key schedule turns the key and tweak into 6 subkeys, each 4 words. Subkey `s` is denoted `(k_{s,0}, ..., k_{s,3})`. First compute two additional words `k_4 = C ^ k_0 ^ k_1 ^ k_2 ^ k_3` and `t_2 = t0 ^ t_1`, where `C = 0x1BD11BDAA9FC1A22`. Then; ```; k_{s,0} = k_{s mod 5}; k_{s,1} = k_{s+1 mod 5} + t_{s mod 3}; k_{s,2} = k_{s+2 mod 5} + t_{s+1 mod 3}; k_{s,3} = k_{s+3 mod 5} + s; ```. ## an encryption round; Encryption is performed over 20 rounds. Let `v_i` be the `i`th word of the encryption state, initialized; ```; v_i = p_i; ```; Before round `d` if `d mod 4 = 0`, add subkey `s = d/4`; ```; v_i += k_{s,i}; ```; Then apply the `mix` function to adjacent pairs of words, where the rotation constant `r = R[d mod 8][j]` is looked up in a table.; ```; mix(v_{2j}, v_{2j+1}, r); ```; `mix` is defined; ```; mix(x0, x1, r) {; x0 += x1; rotL(x1, r); x1 ^= x0; }; ```; <img width=""190"" alt=""MIX"" src=""https://user-images.githubusercontent.com/3430459/197853087-ff3cee9d-002e-43a0-955b-7dd6747b90f2.png"">. Lastly, the wor",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:2554,Energy Efficiency,schedul,schedule,2554,"., p_3)`: 4 words. Intutively, this is a function taking a key and tweak as input, and returning a permutation on the space of all 256-bit blocks. The security claim is that if the key is chosen randomly, then for any choice of tweak, the resulting permutation ""looks like"" a uniformly chosen random permutation. Like most (all?) block ciphers, it is constructed as a sequence of simpler permutations. Think of shuffling a deck of cards: each shuffle isn't that random (is easily distinguishable from a completely random permutation), but a sequence of seven shuffles is indistinguishable from a random permutation. The simple permutations are called ""rounds"". Each round consists of applying a function ""Mix"" to pairs of 64-bit words, which is a bit-level permutation, followed by a permutation of the four words. <img width=""440"" alt=""threefish"" src=""https://user-images.githubusercontent.com/3430459/197852931-59bb6734-917b-4c4a-b3a2-7e2e9302a09f.png"">. ## key schedule; The key schedule turns the key and tweak into 6 subkeys, each 4 words. Subkey `s` is denoted `(k_{s,0}, ..., k_{s,3})`. First compute two additional words `k_4 = C ^ k_0 ^ k_1 ^ k_2 ^ k_3` and `t_2 = t0 ^ t_1`, where `C = 0x1BD11BDAA9FC1A22`. Then; ```; k_{s,0} = k_{s mod 5}; k_{s,1} = k_{s+1 mod 5} + t_{s mod 3}; k_{s,2} = k_{s+2 mod 5} + t_{s+1 mod 3}; k_{s,3} = k_{s+3 mod 5} + s; ```. ## an encryption round; Encryption is performed over 20 rounds. Let `v_i` be the `i`th word of the encryption state, initialized; ```; v_i = p_i; ```; Before round `d` if `d mod 4 = 0`, add subkey `s = d/4`; ```; v_i += k_{s,i}; ```; Then apply the `mix` function to adjacent pairs of words, where the rotation constant `r = R[d mod 8][j]` is looked up in a table.; ```; mix(v_{2j}, v_{2j+1}, r); ```; `mix` is defined; ```; mix(x0, x1, r) {; x0 += x1; rotL(x1, r); x1 ^= x0; }; ```; <img width=""190"" alt=""MIX"" src=""https://user-images.githubusercontent.com/3430459/197853087-ff3cee9d-002e-43a0-955b-7dd6747b90f2.png"">. Lastly, the wor",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:5869,Energy Efficiency,reduce,reduced,5869,"64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into blocks of 4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets reduced to a 256 bit hash, such that distinct messages are highly unlikely to have the same hash. Then for each value of the counter, we use a distinct random function (really a random permutation) from the space of hashes to the space of random outputs. ## Lazy computation; In practice, we don't need to save entire messages in memory. Instead we compute the hash on the fly. The new type is `RNGState`. A value of this type consists of the data:; * `runningSum: IndexedSeq[Long]`: the xor of the encrypted contents of all full blocks; * `lastDynBlock: IndexedSeq[Long]`: the partial contents of the last block. The length of the sequence is `numWordsInLastDynBlock`; * `numWordsInLastDynBlock: Int`: the number of words (longs), in the range `[0, 4)`, currently contained in `lastDynBlock`; * `hasStaticSplit: Boolean`: whether the static block has been incorporated into `runningSum`; * `numDynBlocks: Int`: the number of completed blocks, not including the static block. This system is implemented using three ",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:977,Integrability,message,message,977,"om number generation is inspired by [1], but several details differ. At a high level, the idea is:; * Assign to each random function invocation some unique identifier. In general we can't bound the size of the identifier. We use arrays of longs.; * Use a construction of a psuedo-random function to map unique identifiers to random streams of bits. Intuitively, it's as if we used the identifier to seed a stateful RNG. The key property is that random function invocations with distinct identifiers produce independent random results, while invocations with the same identifier always produce the same result. Thus random function invocations are actually pure functions, with no side effects, which gives the compiler great freedom to optimize queries without affecting the results. Psuedo-random functions are important building blocks in cryptography, and so they are very well studied, with many different practical constructions. We use the PMAC message authentication code, which depends on a tweakable block cipher, for which we use a reduced-round Threefish. Either or both of these pieces could be replaced with little effort, e.g. to improve performance. # Threefish/Threefry:; We use the Threefish [2] block cipher, modified to use 20 rounds for efficiency (the full Threefish4x64 uses 72 rounds), as suggested by [3] (although we make use of the Threefish tweak). Reference implementation is `Threefry.encrypt`. `threefish4x64` takes:; * key `K = (k_0, ..., k_3)`: 4 words; * tweak `T = (t_0, t_1)`: 2 words; * plaintext `P = (p_0, ..., p_3)`: 4 words. Intutively, this is a function taking a key and tweak as input, and returning a permutation on the space of all 256-bit blocks. The security claim is that if the key is chosen randomly, then for any choice of tweak, the resulting permutation ""looks like"" a uniformly chosen random permutation. Like most (all?) block ciphers, it is constructed as a sequence of simpler permutations. Think of shuffling a deck of cards: each shuffle isn",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:1012,Integrability,depend,depends,1012,"om number generation is inspired by [1], but several details differ. At a high level, the idea is:; * Assign to each random function invocation some unique identifier. In general we can't bound the size of the identifier. We use arrays of longs.; * Use a construction of a psuedo-random function to map unique identifiers to random streams of bits. Intuitively, it's as if we used the identifier to seed a stateful RNG. The key property is that random function invocations with distinct identifiers produce independent random results, while invocations with the same identifier always produce the same result. Thus random function invocations are actually pure functions, with no side effects, which gives the compiler great freedom to optimize queries without affecting the results. Psuedo-random functions are important building blocks in cryptography, and so they are very well studied, with many different practical constructions. We use the PMAC message authentication code, which depends on a tweakable block cipher, for which we use a reduced-round Threefish. Either or both of these pieces could be replaced with little effort, e.g. to improve performance. # Threefish/Threefry:; We use the Threefish [2] block cipher, modified to use 20 rounds for efficiency (the full Threefish4x64 uses 72 rounds), as suggested by [3] (although we make use of the Threefish tweak). Reference implementation is `Threefry.encrypt`. `threefish4x64` takes:; * key `K = (k_0, ..., k_3)`: 4 words; * tweak `T = (t_0, t_1)`: 2 words; * plaintext `P = (p_0, ..., p_3)`: 4 words. Intutively, this is a function taking a key and tweak as input, and returning a permutation on the space of all 256-bit blocks. The security claim is that if the key is chosen randomly, then for any choice of tweak, the resulting permutation ""looks like"" a uniformly chosen random permutation. Like most (all?) block ciphers, it is constructed as a sequence of simpler permutations. Think of shuffling a deck of cards: each shuffle isn",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:3639,Integrability,message,message,3639,"{s,0}, ..., k_{s,3})`. First compute two additional words `k_4 = C ^ k_0 ^ k_1 ^ k_2 ^ k_3` and `t_2 = t0 ^ t_1`, where `C = 0x1BD11BDAA9FC1A22`. Then; ```; k_{s,0} = k_{s mod 5}; k_{s,1} = k_{s+1 mod 5} + t_{s mod 3}; k_{s,2} = k_{s+2 mod 5} + t_{s+1 mod 3}; k_{s,3} = k_{s+3 mod 5} + s; ```. ## an encryption round; Encryption is performed over 20 rounds. Let `v_i` be the `i`th word of the encryption state, initialized; ```; v_i = p_i; ```; Before round `d` if `d mod 4 = 0`, add subkey `s = d/4`; ```; v_i += k_{s,i}; ```; Then apply the `mix` function to adjacent pairs of words, where the rotation constant `r = R[d mod 8][j]` is looked up in a table.; ```; mix(v_{2j}, v_{2j+1}, r); ```; `mix` is defined; ```; mix(x0, x1, r) {; x0 += x1; rotL(x1, r); x1 ^= x0; }; ```; <img width=""190"" alt=""MIX"" src=""https://user-images.githubusercontent.com/3430459/197853087-ff3cee9d-002e-43a0-955b-7dd6747b90f2.png"">. Lastly, the words are permuted; ```; v_1, v_3 = v_3, v_1 ; ```. # PMAC; PMAC is a message authentication code. Intuitively, a MAC uses a block cipher to construct a function from abritrary length messages to 256 bit message tags. We extend this to a function from arbitrary length messages to ""infinite"" length message tags (really a very large finite length). The security claim is that if the block cipher used ""looks like"" a random permutation, then the MAC ""looks like"" a random function. In particular, for each message `m`, `pmac(m)` looks like a stream of random bits, and for distinct messages `m1` and `m2`, `pmac(m1)` and `pmac(m2)` look like completely independent streams of random bits. Yet this is a deterministic function, so computing `pmac` on the same message always produces the same stream of bits. <img width=""654"" alt=""Screen Shot 2022-10-25 at 11 53 13 AM"" src=""https://user-images.githubusercontent.com/3430459/197853175-d96ea5b0-4618-4f6c-b92a-0dfbf5da7fe2.png"">. Many MAC constructions must process blocks sequentially. As we'll see below, this would add signif",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:3753,Integrability,message,messages,3753,"2 ^ k_3` and `t_2 = t0 ^ t_1`, where `C = 0x1BD11BDAA9FC1A22`. Then; ```; k_{s,0} = k_{s mod 5}; k_{s,1} = k_{s+1 mod 5} + t_{s mod 3}; k_{s,2} = k_{s+2 mod 5} + t_{s+1 mod 3}; k_{s,3} = k_{s+3 mod 5} + s; ```. ## an encryption round; Encryption is performed over 20 rounds. Let `v_i` be the `i`th word of the encryption state, initialized; ```; v_i = p_i; ```; Before round `d` if `d mod 4 = 0`, add subkey `s = d/4`; ```; v_i += k_{s,i}; ```; Then apply the `mix` function to adjacent pairs of words, where the rotation constant `r = R[d mod 8][j]` is looked up in a table.; ```; mix(v_{2j}, v_{2j+1}, r); ```; `mix` is defined; ```; mix(x0, x1, r) {; x0 += x1; rotL(x1, r); x1 ^= x0; }; ```; <img width=""190"" alt=""MIX"" src=""https://user-images.githubusercontent.com/3430459/197853087-ff3cee9d-002e-43a0-955b-7dd6747b90f2.png"">. Lastly, the words are permuted; ```; v_1, v_3 = v_3, v_1 ; ```. # PMAC; PMAC is a message authentication code. Intuitively, a MAC uses a block cipher to construct a function from abritrary length messages to 256 bit message tags. We extend this to a function from arbitrary length messages to ""infinite"" length message tags (really a very large finite length). The security claim is that if the block cipher used ""looks like"" a random permutation, then the MAC ""looks like"" a random function. In particular, for each message `m`, `pmac(m)` looks like a stream of random bits, and for distinct messages `m1` and `m2`, `pmac(m1)` and `pmac(m2)` look like completely independent streams of random bits. Yet this is a deterministic function, so computing `pmac` on the same message always produces the same stream of bits. <img width=""654"" alt=""Screen Shot 2022-10-25 at 11 53 13 AM"" src=""https://user-images.githubusercontent.com/3430459/197853175-d96ea5b0-4618-4f6c-b92a-0dfbf5da7fe2.png"">. Many MAC constructions must process blocks sequentially. As we'll see below, this would add significant overhead to random number generation. PMAC has the property that blocks of th",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:3773,Integrability,message,message,3773,"2 ^ k_3` and `t_2 = t0 ^ t_1`, where `C = 0x1BD11BDAA9FC1A22`. Then; ```; k_{s,0} = k_{s mod 5}; k_{s,1} = k_{s+1 mod 5} + t_{s mod 3}; k_{s,2} = k_{s+2 mod 5} + t_{s+1 mod 3}; k_{s,3} = k_{s+3 mod 5} + s; ```. ## an encryption round; Encryption is performed over 20 rounds. Let `v_i` be the `i`th word of the encryption state, initialized; ```; v_i = p_i; ```; Before round `d` if `d mod 4 = 0`, add subkey `s = d/4`; ```; v_i += k_{s,i}; ```; Then apply the `mix` function to adjacent pairs of words, where the rotation constant `r = R[d mod 8][j]` is looked up in a table.; ```; mix(v_{2j}, v_{2j+1}, r); ```; `mix` is defined; ```; mix(x0, x1, r) {; x0 += x1; rotL(x1, r); x1 ^= x0; }; ```; <img width=""190"" alt=""MIX"" src=""https://user-images.githubusercontent.com/3430459/197853087-ff3cee9d-002e-43a0-955b-7dd6747b90f2.png"">. Lastly, the words are permuted; ```; v_1, v_3 = v_3, v_1 ; ```. # PMAC; PMAC is a message authentication code. Intuitively, a MAC uses a block cipher to construct a function from abritrary length messages to 256 bit message tags. We extend this to a function from arbitrary length messages to ""infinite"" length message tags (really a very large finite length). The security claim is that if the block cipher used ""looks like"" a random permutation, then the MAC ""looks like"" a random function. In particular, for each message `m`, `pmac(m)` looks like a stream of random bits, and for distinct messages `m1` and `m2`, `pmac(m1)` and `pmac(m2)` look like completely independent streams of random bits. Yet this is a deterministic function, so computing `pmac` on the same message always produces the same stream of bits. <img width=""654"" alt=""Screen Shot 2022-10-25 at 11 53 13 AM"" src=""https://user-images.githubusercontent.com/3430459/197853175-d96ea5b0-4618-4f6c-b92a-0dfbf5da7fe2.png"">. Many MAC constructions must process blocks sequentially. As we'll see below, this would add significant overhead to random number generation. PMAC has the property that blocks of th",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:3838,Integrability,message,messages,3838,"{s mod 3}; k_{s,2} = k_{s+2 mod 5} + t_{s+1 mod 3}; k_{s,3} = k_{s+3 mod 5} + s; ```. ## an encryption round; Encryption is performed over 20 rounds. Let `v_i` be the `i`th word of the encryption state, initialized; ```; v_i = p_i; ```; Before round `d` if `d mod 4 = 0`, add subkey `s = d/4`; ```; v_i += k_{s,i}; ```; Then apply the `mix` function to adjacent pairs of words, where the rotation constant `r = R[d mod 8][j]` is looked up in a table.; ```; mix(v_{2j}, v_{2j+1}, r); ```; `mix` is defined; ```; mix(x0, x1, r) {; x0 += x1; rotL(x1, r); x1 ^= x0; }; ```; <img width=""190"" alt=""MIX"" src=""https://user-images.githubusercontent.com/3430459/197853087-ff3cee9d-002e-43a0-955b-7dd6747b90f2.png"">. Lastly, the words are permuted; ```; v_1, v_3 = v_3, v_1 ; ```. # PMAC; PMAC is a message authentication code. Intuitively, a MAC uses a block cipher to construct a function from abritrary length messages to 256 bit message tags. We extend this to a function from arbitrary length messages to ""infinite"" length message tags (really a very large finite length). The security claim is that if the block cipher used ""looks like"" a random permutation, then the MAC ""looks like"" a random function. In particular, for each message `m`, `pmac(m)` looks like a stream of random bits, and for distinct messages `m1` and `m2`, `pmac(m1)` and `pmac(m2)` look like completely independent streams of random bits. Yet this is a deterministic function, so computing `pmac` on the same message always produces the same stream of bits. <img width=""654"" alt=""Screen Shot 2022-10-25 at 11 53 13 AM"" src=""https://user-images.githubusercontent.com/3430459/197853175-d96ea5b0-4618-4f6c-b92a-0dfbf5da7fe2.png"">. Many MAC constructions must process blocks sequentially. As we'll see below, this would add significant overhead to random number generation. PMAC has the property that blocks of the message can be processed in any order. In our case, we use a modification of the PMAC1 construction in [4]. We restrict the",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:3868,Integrability,message,message,3868,"{s mod 3}; k_{s,2} = k_{s+2 mod 5} + t_{s+1 mod 3}; k_{s,3} = k_{s+3 mod 5} + s; ```. ## an encryption round; Encryption is performed over 20 rounds. Let `v_i` be the `i`th word of the encryption state, initialized; ```; v_i = p_i; ```; Before round `d` if `d mod 4 = 0`, add subkey `s = d/4`; ```; v_i += k_{s,i}; ```; Then apply the `mix` function to adjacent pairs of words, where the rotation constant `r = R[d mod 8][j]` is looked up in a table.; ```; mix(v_{2j}, v_{2j+1}, r); ```; `mix` is defined; ```; mix(x0, x1, r) {; x0 += x1; rotL(x1, r); x1 ^= x0; }; ```; <img width=""190"" alt=""MIX"" src=""https://user-images.githubusercontent.com/3430459/197853087-ff3cee9d-002e-43a0-955b-7dd6747b90f2.png"">. Lastly, the words are permuted; ```; v_1, v_3 = v_3, v_1 ; ```. # PMAC; PMAC is a message authentication code. Intuitively, a MAC uses a block cipher to construct a function from abritrary length messages to 256 bit message tags. We extend this to a function from arbitrary length messages to ""infinite"" length message tags (really a very large finite length). The security claim is that if the block cipher used ""looks like"" a random permutation, then the MAC ""looks like"" a random function. In particular, for each message `m`, `pmac(m)` looks like a stream of random bits, and for distinct messages `m1` and `m2`, `pmac(m1)` and `pmac(m2)` look like completely independent streams of random bits. Yet this is a deterministic function, so computing `pmac` on the same message always produces the same stream of bits. <img width=""654"" alt=""Screen Shot 2022-10-25 at 11 53 13 AM"" src=""https://user-images.githubusercontent.com/3430459/197853175-d96ea5b0-4618-4f6c-b92a-0dfbf5da7fe2.png"">. Many MAC constructions must process blocks sequentially. As we'll see below, this would add significant overhead to random number generation. PMAC has the property that blocks of the message can be processed in any order. In our case, we use a modification of the PMAC1 construction in [4]. We restrict the",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:4074,Integrability,message,message,4074,"i += k_{s,i}; ```; Then apply the `mix` function to adjacent pairs of words, where the rotation constant `r = R[d mod 8][j]` is looked up in a table.; ```; mix(v_{2j}, v_{2j+1}, r); ```; `mix` is defined; ```; mix(x0, x1, r) {; x0 += x1; rotL(x1, r); x1 ^= x0; }; ```; <img width=""190"" alt=""MIX"" src=""https://user-images.githubusercontent.com/3430459/197853087-ff3cee9d-002e-43a0-955b-7dd6747b90f2.png"">. Lastly, the words are permuted; ```; v_1, v_3 = v_3, v_1 ; ```. # PMAC; PMAC is a message authentication code. Intuitively, a MAC uses a block cipher to construct a function from abritrary length messages to 256 bit message tags. We extend this to a function from arbitrary length messages to ""infinite"" length message tags (really a very large finite length). The security claim is that if the block cipher used ""looks like"" a random permutation, then the MAC ""looks like"" a random function. In particular, for each message `m`, `pmac(m)` looks like a stream of random bits, and for distinct messages `m1` and `m2`, `pmac(m1)` and `pmac(m2)` look like completely independent streams of random bits. Yet this is a deterministic function, so computing `pmac` on the same message always produces the same stream of bits. <img width=""654"" alt=""Screen Shot 2022-10-25 at 11 53 13 AM"" src=""https://user-images.githubusercontent.com/3430459/197853175-d96ea5b0-4618-4f6c-b92a-0dfbf5da7fe2.png"">. Many MAC constructions must process blocks sequentially. As we'll see below, this would add significant overhead to random number generation. PMAC has the property that blocks of the message can be processed in any order. In our case, we use a modification of the PMAC1 construction in [4]. We restrict the message length to multiples of 64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:4150,Integrability,message,messages,4150,"i += k_{s,i}; ```; Then apply the `mix` function to adjacent pairs of words, where the rotation constant `r = R[d mod 8][j]` is looked up in a table.; ```; mix(v_{2j}, v_{2j+1}, r); ```; `mix` is defined; ```; mix(x0, x1, r) {; x0 += x1; rotL(x1, r); x1 ^= x0; }; ```; <img width=""190"" alt=""MIX"" src=""https://user-images.githubusercontent.com/3430459/197853087-ff3cee9d-002e-43a0-955b-7dd6747b90f2.png"">. Lastly, the words are permuted; ```; v_1, v_3 = v_3, v_1 ; ```. # PMAC; PMAC is a message authentication code. Intuitively, a MAC uses a block cipher to construct a function from abritrary length messages to 256 bit message tags. We extend this to a function from arbitrary length messages to ""infinite"" length message tags (really a very large finite length). The security claim is that if the block cipher used ""looks like"" a random permutation, then the MAC ""looks like"" a random function. In particular, for each message `m`, `pmac(m)` looks like a stream of random bits, and for distinct messages `m1` and `m2`, `pmac(m1)` and `pmac(m2)` look like completely independent streams of random bits. Yet this is a deterministic function, so computing `pmac` on the same message always produces the same stream of bits. <img width=""654"" alt=""Screen Shot 2022-10-25 at 11 53 13 AM"" src=""https://user-images.githubusercontent.com/3430459/197853175-d96ea5b0-4618-4f6c-b92a-0dfbf5da7fe2.png"">. Many MAC constructions must process blocks sequentially. As we'll see below, this would add significant overhead to random number generation. PMAC has the property that blocks of the message can be processed in any order. In our case, we use a modification of the PMAC1 construction in [4]. We restrict the message length to multiples of 64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:4327,Integrability,message,message,4327,"2j}, v_{2j+1}, r); ```; `mix` is defined; ```; mix(x0, x1, r) {; x0 += x1; rotL(x1, r); x1 ^= x0; }; ```; <img width=""190"" alt=""MIX"" src=""https://user-images.githubusercontent.com/3430459/197853087-ff3cee9d-002e-43a0-955b-7dd6747b90f2.png"">. Lastly, the words are permuted; ```; v_1, v_3 = v_3, v_1 ; ```. # PMAC; PMAC is a message authentication code. Intuitively, a MAC uses a block cipher to construct a function from abritrary length messages to 256 bit message tags. We extend this to a function from arbitrary length messages to ""infinite"" length message tags (really a very large finite length). The security claim is that if the block cipher used ""looks like"" a random permutation, then the MAC ""looks like"" a random function. In particular, for each message `m`, `pmac(m)` looks like a stream of random bits, and for distinct messages `m1` and `m2`, `pmac(m1)` and `pmac(m2)` look like completely independent streams of random bits. Yet this is a deterministic function, so computing `pmac` on the same message always produces the same stream of bits. <img width=""654"" alt=""Screen Shot 2022-10-25 at 11 53 13 AM"" src=""https://user-images.githubusercontent.com/3430459/197853175-d96ea5b0-4618-4f6c-b92a-0dfbf5da7fe2.png"">. Many MAC constructions must process blocks sequentially. As we'll see below, this would add significant overhead to random number generation. PMAC has the property that blocks of the message can be processed in any order. In our case, we use a modification of the PMAC1 construction in [4]. We restrict the message length to multiples of 64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into blocks of 4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last ",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:4729,Integrability,message,message,4729,"ion from abritrary length messages to 256 bit message tags. We extend this to a function from arbitrary length messages to ""infinite"" length message tags (really a very large finite length). The security claim is that if the block cipher used ""looks like"" a random permutation, then the MAC ""looks like"" a random function. In particular, for each message `m`, `pmac(m)` looks like a stream of random bits, and for distinct messages `m1` and `m2`, `pmac(m1)` and `pmac(m2)` look like completely independent streams of random bits. Yet this is a deterministic function, so computing `pmac` on the same message always produces the same stream of bits. <img width=""654"" alt=""Screen Shot 2022-10-25 at 11 53 13 AM"" src=""https://user-images.githubusercontent.com/3430459/197853175-d96ea5b0-4618-4f6c-b92a-0dfbf5da7fe2.png"">. Many MAC constructions must process blocks sequentially. As we'll see below, this would add significant overhead to random number generation. PMAC has the property that blocks of the message can be processed in any order. In our case, we use a modification of the PMAC1 construction in [4]. We restrict the message length to multiples of 64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into blocks of 4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:4853,Integrability,message,message,4853,"sage tags (really a very large finite length). The security claim is that if the block cipher used ""looks like"" a random permutation, then the MAC ""looks like"" a random function. In particular, for each message `m`, `pmac(m)` looks like a stream of random bits, and for distinct messages `m1` and `m2`, `pmac(m1)` and `pmac(m2)` look like completely independent streams of random bits. Yet this is a deterministic function, so computing `pmac` on the same message always produces the same stream of bits. <img width=""654"" alt=""Screen Shot 2022-10-25 at 11 53 13 AM"" src=""https://user-images.githubusercontent.com/3430459/197853175-d96ea5b0-4618-4f6c-b92a-0dfbf5da7fe2.png"">. Many MAC constructions must process blocks sequentially. As we'll see below, this would add significant overhead to random number generation. PMAC has the property that blocks of the message can be processed in any order. In our case, we use a modification of the PMAC1 construction in [4]. We restrict the message length to multiples of 64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into blocks of 4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets red",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:4975,Integrability,message,message,4975,"dom permutation, then the MAC ""looks like"" a random function. In particular, for each message `m`, `pmac(m)` looks like a stream of random bits, and for distinct messages `m1` and `m2`, `pmac(m1)` and `pmac(m2)` look like completely independent streams of random bits. Yet this is a deterministic function, so computing `pmac` on the same message always produces the same stream of bits. <img width=""654"" alt=""Screen Shot 2022-10-25 at 11 53 13 AM"" src=""https://user-images.githubusercontent.com/3430459/197853175-d96ea5b0-4618-4f6c-b92a-0dfbf5da7fe2.png"">. Many MAC constructions must process blocks sequentially. As we'll see below, this would add significant overhead to random number generation. PMAC has the property that blocks of the message can be processed in any order. In our case, we use a modification of the PMAC1 construction in [4]. We restrict the message length to multiples of 64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into blocks of 4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets reduced to a 256 bit hash, such that distinct messages are highly unlikely to have the same hash. Then for each value o",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:5140,Integrability,message,message,5140,"s `m1` and `m2`, `pmac(m1)` and `pmac(m2)` look like completely independent streams of random bits. Yet this is a deterministic function, so computing `pmac` on the same message always produces the same stream of bits. <img width=""654"" alt=""Screen Shot 2022-10-25 at 11 53 13 AM"" src=""https://user-images.githubusercontent.com/3430459/197853175-d96ea5b0-4618-4f6c-b92a-0dfbf5da7fe2.png"">. Many MAC constructions must process blocks sequentially. As we'll see below, this would add significant overhead to random number generation. PMAC has the property that blocks of the message can be processed in any order. In our case, we use a modification of the PMAC1 construction in [4]. We restrict the message length to multiples of 64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into blocks of 4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets reduced to a 256 bit hash, such that distinct messages are highly unlikely to have the same hash. Then for each value of the counter, we use a distinct random function (really a random permutation) from the space of hashes to the space of random outputs. ## Lazy computation; In practice, ",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:5763,Integrability,message,message,5763,"andom number generation. PMAC has the property that blocks of the message can be processed in any order. In our case, we use a modification of the PMAC1 construction in [4]. We restrict the message length to multiples of 64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into blocks of 4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets reduced to a 256 bit hash, such that distinct messages are highly unlikely to have the same hash. Then for each value of the counter, we use a distinct random function (really a random permutation) from the space of hashes to the space of random outputs. ## Lazy computation; In practice, we don't need to save entire messages in memory. Instead we compute the hash on the fly. The new type is `RNGState`. A value of this type consists of the data:; * `runningSum: IndexedSeq[Long]`: the xor of the encrypted contents of all full blocks; * `lastDynBlock: IndexedSeq[Long]`: the partial contents of the last block. The length of the sequence is `numWordsInLastDynBlock`; * `numWordsInLastDynBlock: Int`: the number of words (longs), in the range `[0, 4)`, currently contained in `lastDynBlock`; *",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:5830,Integrability,message,message,5830,"64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into blocks of 4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets reduced to a 256 bit hash, such that distinct messages are highly unlikely to have the same hash. Then for each value of the counter, we use a distinct random function (really a random permutation) from the space of hashes to the space of random outputs. ## Lazy computation; In practice, we don't need to save entire messages in memory. Instead we compute the hash on the fly. The new type is `RNGState`. A value of this type consists of the data:; * `runningSum: IndexedSeq[Long]`: the xor of the encrypted contents of all full blocks; * `lastDynBlock: IndexedSeq[Long]`: the partial contents of the last block. The length of the sequence is `numWordsInLastDynBlock`; * `numWordsInLastDynBlock: Int`: the number of words (longs), in the range `[0, 4)`, currently contained in `lastDynBlock`; * `hasStaticSplit: Boolean`: whether the static block has been incorporated into `runningSum`; * `numDynBlocks: Int`: the number of completed blocks, not including the static block. This system is implemented using three ",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:5915,Integrability,message,messages,5915,"64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into blocks of 4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets reduced to a 256 bit hash, such that distinct messages are highly unlikely to have the same hash. Then for each value of the counter, we use a distinct random function (really a random permutation) from the space of hashes to the space of random outputs. ## Lazy computation; In practice, we don't need to save entire messages in memory. Instead we compute the hash on the fly. The new type is `RNGState`. A value of this type consists of the data:; * `runningSum: IndexedSeq[Long]`: the xor of the encrypted contents of all full blocks; * `lastDynBlock: IndexedSeq[Long]`: the partial contents of the last block. The length of the sequence is `numWordsInLastDynBlock`; * `numWordsInLastDynBlock: Int`: the number of words (longs), in the range `[0, 4)`, currently contained in `lastDynBlock`; * `hasStaticSplit: Boolean`: whether the static block has been incorporated into `runningSum`; * `numDynBlocks: Int`: the number of completed blocks, not including the static block. This system is implemented using three ",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:6187,Integrability,message,messages,6187,"4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets reduced to a 256 bit hash, such that distinct messages are highly unlikely to have the same hash. Then for each value of the counter, we use a distinct random function (really a random permutation) from the space of hashes to the space of random outputs. ## Lazy computation; In practice, we don't need to save entire messages in memory. Instead we compute the hash on the fly. The new type is `RNGState`. A value of this type consists of the data:; * `runningSum: IndexedSeq[Long]`: the xor of the encrypted contents of all full blocks; * `lastDynBlock: IndexedSeq[Long]`: the partial contents of the last block. The length of the sequence is `numWordsInLastDynBlock`; * `numWordsInLastDynBlock: Int`: the number of words (longs), in the range `[0, 4)`, currently contained in `lastDynBlock`; * `hasStaticSplit: Boolean`: whether the static block has been incorporated into `runningSum`; * `numDynBlocks: Int`: the number of completed blocks, not including the static block. This system is implemented using three IR nodes:; * `RNGStateLiteral` - creates an `RNGState` representing the empty message; * `RNGSplit(state: RNGState, dynBitstring: ?): RNGState` - appends to `lastDynBlock`. When the last block is full, encrypt it (using `numDynBlocks` for the tweak), and xor it into `runningSum`. ",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:6962,Integrability,message,message,6962,"e the same hash. Then for each value of the counter, we use a distinct random function (really a random permutation) from the space of hashes to the space of random outputs. ## Lazy computation; In practice, we don't need to save entire messages in memory. Instead we compute the hash on the fly. The new type is `RNGState`. A value of this type consists of the data:; * `runningSum: IndexedSeq[Long]`: the xor of the encrypted contents of all full blocks; * `lastDynBlock: IndexedSeq[Long]`: the partial contents of the last block. The length of the sequence is `numWordsInLastDynBlock`; * `numWordsInLastDynBlock: Int`: the number of words (longs), in the range `[0, 4)`, currently contained in `lastDynBlock`; * `hasStaticSplit: Boolean`: whether the static block has been incorporated into `runningSum`; * `numDynBlocks: Int`: the number of completed blocks, not including the static block. This system is implemented using three IR nodes:; * `RNGStateLiteral` - creates an `RNGState` representing the empty message; * `RNGSplit(state: RNGState, dynBitstring: ?): RNGState` - appends to `lastDynBlock`. When the last block is full, encrypt it (using `numDynBlocks` for the tweak), and xor it into `runningSum`. Here `?` is either a single long, or an arbitrary sized tuple of longs.; * `ApplySeeded(..., rngState: RNGState, staticUID: Long)`; * Statically, forms the static block `[nonce, staticUID, 0L, 0L]`, encrypts it, and embeds the result as a literal in the code.; * At runtime, only needs to xor into the `runningSum` the encryped static block and the (possibly padded) `lastDynBlock`, and encrypt the result. Hence each `ApplySeeded` call only needs one invocation of the block cipher at runtime (more precisely, one invocation per 256 random bits needed by the random function). This minimizes the overhead of random number generation in inner loops, and is the reason for choose PMAC. # UIDs; To use the above PMAC scheme, we need to assign a ""message"" to every random function invocat",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:7909,Integrability,message,message,7909,"eral` - creates an `RNGState` representing the empty message; * `RNGSplit(state: RNGState, dynBitstring: ?): RNGState` - appends to `lastDynBlock`. When the last block is full, encrypt it (using `numDynBlocks` for the tweak), and xor it into `runningSum`. Here `?` is either a single long, or an arbitrary sized tuple of longs.; * `ApplySeeded(..., rngState: RNGState, staticUID: Long)`; * Statically, forms the static block `[nonce, staticUID, 0L, 0L]`, encrypts it, and embeds the result as a literal in the code.; * At runtime, only needs to xor into the `runningSum` the encryped static block and the (possibly padded) `lastDynBlock`, and encrypt the result. Hence each `ApplySeeded` call only needs one invocation of the block cipher at runtime (more precisely, one invocation per 256 random bits needed by the random function). This minimizes the overhead of random number generation in inner loops, and is the reason for choose PMAC. # UIDs; To use the above PMAC scheme, we need to assign a ""message"" to every random function invocation in the program. As long as each invocation gets a distinct message, the PMAC random function generates approximately independent randomness for each invocation. We fix a key for the block cipher once and for all. It was generated randomly, and is hard coded in the compiler. This saves us from issues of users specifying ""bad"" keys. Instead, we reserve a part of the message to encode a session scoped uid. By changing that uid between sessions, we allow running identical code repeatedly with independent randomness. ## Static UIDs; We split the message into static and dynamic components. The static component consists of two longs. The first, called the ""rng nonce"", is a hail session constant. It replaces the old ""global seed"", allowing the same pipeline to run with independent randomness each session, unless determinism is specifically requested. The second component is stored in the `ApplySeeded` IR node. We simply maintain a global counter, an",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:8013,Integrability,message,message,8013,"`lastDynBlock`. When the last block is full, encrypt it (using `numDynBlocks` for the tweak), and xor it into `runningSum`. Here `?` is either a single long, or an arbitrary sized tuple of longs.; * `ApplySeeded(..., rngState: RNGState, staticUID: Long)`; * Statically, forms the static block `[nonce, staticUID, 0L, 0L]`, encrypts it, and embeds the result as a literal in the code.; * At runtime, only needs to xor into the `runningSum` the encryped static block and the (possibly padded) `lastDynBlock`, and encrypt the result. Hence each `ApplySeeded` call only needs one invocation of the block cipher at runtime (more precisely, one invocation per 256 random bits needed by the random function). This minimizes the overhead of random number generation in inner loops, and is the reason for choose PMAC. # UIDs; To use the above PMAC scheme, we need to assign a ""message"" to every random function invocation in the program. As long as each invocation gets a distinct message, the PMAC random function generates approximately independent randomness for each invocation. We fix a key for the block cipher once and for all. It was generated randomly, and is hard coded in the compiler. This saves us from issues of users specifying ""bad"" keys. Instead, we reserve a part of the message to encode a session scoped uid. By changing that uid between sessions, we allow running identical code repeatedly with independent randomness. ## Static UIDs; We split the message into static and dynamic components. The static component consists of two longs. The first, called the ""rng nonce"", is a hail session constant. It replaces the old ""global seed"", allowing the same pipeline to run with independent randomness each session, unless determinism is specifically requested. The second component is stored in the `ApplySeeded` IR node. We simply maintain a global counter, and increment it each time an `ApplySeeded` node is constructed, ensuring that each node in a pipeline has a distinct static uid. The d",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:8321,Integrability,message,message,8321,"atic block `[nonce, staticUID, 0L, 0L]`, encrypts it, and embeds the result as a literal in the code.; * At runtime, only needs to xor into the `runningSum` the encryped static block and the (possibly padded) `lastDynBlock`, and encrypt the result. Hence each `ApplySeeded` call only needs one invocation of the block cipher at runtime (more precisely, one invocation per 256 random bits needed by the random function). This minimizes the overhead of random number generation in inner loops, and is the reason for choose PMAC. # UIDs; To use the above PMAC scheme, we need to assign a ""message"" to every random function invocation in the program. As long as each invocation gets a distinct message, the PMAC random function generates approximately independent randomness for each invocation. We fix a key for the block cipher once and for all. It was generated randomly, and is hard coded in the compiler. This saves us from issues of users specifying ""bad"" keys. Instead, we reserve a part of the message to encode a session scoped uid. By changing that uid between sessions, we allow running identical code repeatedly with independent randomness. ## Static UIDs; We split the message into static and dynamic components. The static component consists of two longs. The first, called the ""rng nonce"", is a hail session constant. It replaces the old ""global seed"", allowing the same pipeline to run with independent randomness each session, unless determinism is specifically requested. The second component is stored in the `ApplySeeded` IR node. We simply maintain a global counter, and increment it each time an `ApplySeeded` node is constructed, ensuring that each node in a pipeline has a distinct static uid. The dynamic component is needed to distinguish between different invocations of a single `ApplySeeded` node inside a looping construct. It is an arbitrary length message (though it will typically be quite small, probably less than 10 longs). It is constructed as follows:. ## Dynamic UI",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:8501,Integrability,message,message,8501,"nd the (possibly padded) `lastDynBlock`, and encrypt the result. Hence each `ApplySeeded` call only needs one invocation of the block cipher at runtime (more precisely, one invocation per 256 random bits needed by the random function). This minimizes the overhead of random number generation in inner loops, and is the reason for choose PMAC. # UIDs; To use the above PMAC scheme, we need to assign a ""message"" to every random function invocation in the program. As long as each invocation gets a distinct message, the PMAC random function generates approximately independent randomness for each invocation. We fix a key for the block cipher once and for all. It was generated randomly, and is hard coded in the compiler. This saves us from issues of users specifying ""bad"" keys. Instead, we reserve a part of the message to encode a session scoped uid. By changing that uid between sessions, we allow running identical code repeatedly with independent randomness. ## Static UIDs; We split the message into static and dynamic components. The static component consists of two longs. The first, called the ""rng nonce"", is a hail session constant. It replaces the old ""global seed"", allowing the same pipeline to run with independent randomness each session, unless determinism is specifically requested. The second component is stored in the `ApplySeeded` IR node. We simply maintain a global counter, and increment it each time an `ApplySeeded` node is constructed, ensuring that each node in a pipeline has a distinct static uid. The dynamic component is needed to distinguish between different invocations of a single `ApplySeeded` node inside a looping construct. It is an arbitrary length message (though it will typically be quite small, probably less than 10 longs). It is constructed as follows:. ## Dynamic UIDs; Every stream, table, or matrix table pipeline is transformed to explicitly generate a unique uid per stream entry, table row, and matrix table row/column. These uids are explicit in",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:9199,Integrability,message,message,9199,"er. This saves us from issues of users specifying ""bad"" keys. Instead, we reserve a part of the message to encode a session scoped uid. By changing that uid between sessions, we allow running identical code repeatedly with independent randomness. ## Static UIDs; We split the message into static and dynamic components. The static component consists of two longs. The first, called the ""rng nonce"", is a hail session constant. It replaces the old ""global seed"", allowing the same pipeline to run with independent randomness each session, unless determinism is specifically requested. The second component is stored in the `ApplySeeded` IR node. We simply maintain a global counter, and increment it each time an `ApplySeeded` node is constructed, ensuring that each node in a pipeline has a distinct static uid. The dynamic component is needed to distinguish between different invocations of a single `ApplySeeded` node inside a looping construct. It is an arbitrary length message (though it will typically be quite small, probably less than 10 longs). It is constructed as follows:. ## Dynamic UIDs; Every stream, table, or matrix table pipeline is transformed to explicitly generate a unique uid per stream entry, table row, and matrix table row/column. These uids are explicit in the IR as ordinary values/fields, so the compiler automatically preserves the RNG determinism. ## Putting it all together; Consider the example pipeline; ```; mt = hl.utils.range_matrix_table(10, 10); mt = mt.annotate_entries(a=hl.range(10).map(lambda i: hl.rand_int32(100))); ```; Before elaborating UIDs in the IR in python, the IR looks like this (after a little tidying):; ```; !1 = MatrixRead [DropRowColUIDs, ...] // don't add uid fields; !3 = MatrixMapEntries(!1) {; (%g, %col, %row, %entry) =>; !c0 = I32 [0]; !c10 = I32 [10]; !c1 = I32 [1]; !s = StreamRange(!c0, !c10, !c1) [1, False]; !s2 = StreamMap(!s) { (%elt) =>; !c100 = I32 [100]; ApplySeeded(!c100, %__rng_state) [rand_int32, 0, Int32] // unbound %_",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:10874,Integrability,message,message,10874,"; !c100 = I32 [100]; ApplySeeded(!c100, %__rng_state) [rand_int32, 0, Int32] // unbound %__rng_state; }; !2 = ToArray(!s2); InsertFields !entry (a: !2); }; ```; Note that the `ApplySeeded` node is tageed with a static UID `0`, and references an unbound variable `__rng_state`. It is the responsibility of the `handle_randomness` pass to give proper definitions of `__rng_state` in any scope that needs it. After `handle_randomness` (and some more tidying), the IR looks like:; ```; // Now MatrixRead adds row and col uids; !1 = MatrixRead [None, False, False, (MatrixRangeReader MatrixRangeReaderParameters(10,10,None) 8)]; !11 = MatrixMapEntries(!1) {; (%g, %col, %row, %entry) =>; !2 = RNGStateLiteral // RNGState corresponding to empty message; !3 = GetField(%row) [__row_uid] // get row and col uids; !4 = GetField(%col) [__col_uid]; !5 = MakeTuple(!3, !4) [(0 1)]; %6 = RNGSplit(!2, !5) // append row and col uids to message; !c0 = I32 [0]; !c10 = I32 [10]; !c1 = I32 [1]; !s = StreamRange(!c0, !c10, !c1) [1, False]; !s2 = StreamMap(!s) { (%elt) =>; !7 = Cast(%elt) [Int64]; MakeTuple(!7, %elt) [(0 1)] // map to stream of (uid, elt) pairs; }; !s3 = StreamMap(!s2) { (%elt2) =>; !8 = GetTupleElement(%elt2) [0]; %9 = RNGSplit(%6, !8) // append stream element uid to message; !c100 = I32 [100]; // call random function with current message/RNGState %9 and static uid 0; ApplySeeded(!c100, %9) [rand_int32, 0, Int32]; }; !10 = ToArray(!s3); InsertFields !entry (a: !10); }; ```; Note that because only 3 longs are added to the message, none of the `RNGSplit` calls generate any runtime code. They simply encode statically that the last block of the message at the time of the `ApplySeeded` call consists of the locals `[!3, !4, !8]`. Then the `ApplySeeded` just needs to pad the last block, xor it with the running sum (which is the encrypted static block, embedded as a constant in the code), and call the Threefry `encrypt` function just once. # Security; Cryptogrophers have developed a very p",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:11057,Integrability,message,message,11057,"; !c100 = I32 [100]; ApplySeeded(!c100, %__rng_state) [rand_int32, 0, Int32] // unbound %__rng_state; }; !2 = ToArray(!s2); InsertFields !entry (a: !2); }; ```; Note that the `ApplySeeded` node is tageed with a static UID `0`, and references an unbound variable `__rng_state`. It is the responsibility of the `handle_randomness` pass to give proper definitions of `__rng_state` in any scope that needs it. After `handle_randomness` (and some more tidying), the IR looks like:; ```; // Now MatrixRead adds row and col uids; !1 = MatrixRead [None, False, False, (MatrixRangeReader MatrixRangeReaderParameters(10,10,None) 8)]; !11 = MatrixMapEntries(!1) {; (%g, %col, %row, %entry) =>; !2 = RNGStateLiteral // RNGState corresponding to empty message; !3 = GetField(%row) [__row_uid] // get row and col uids; !4 = GetField(%col) [__col_uid]; !5 = MakeTuple(!3, !4) [(0 1)]; %6 = RNGSplit(!2, !5) // append row and col uids to message; !c0 = I32 [0]; !c10 = I32 [10]; !c1 = I32 [1]; !s = StreamRange(!c0, !c10, !c1) [1, False]; !s2 = StreamMap(!s) { (%elt) =>; !7 = Cast(%elt) [Int64]; MakeTuple(!7, %elt) [(0 1)] // map to stream of (uid, elt) pairs; }; !s3 = StreamMap(!s2) { (%elt2) =>; !8 = GetTupleElement(%elt2) [0]; %9 = RNGSplit(%6, !8) // append stream element uid to message; !c100 = I32 [100]; // call random function with current message/RNGState %9 and static uid 0; ApplySeeded(!c100, %9) [rand_int32, 0, Int32]; }; !10 = ToArray(!s3); InsertFields !entry (a: !10); }; ```; Note that because only 3 longs are added to the message, none of the `RNGSplit` calls generate any runtime code. They simply encode statically that the last block of the message at the time of the `ApplySeeded` call consists of the locals `[!3, !4, !8]`. Then the `ApplySeeded` just needs to pad the last block, xor it with the running sum (which is the encrypted static block, embedded as a constant in the code), and call the Threefry `encrypt` function just once. # Security; Cryptogrophers have developed a very p",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:11407,Integrability,message,message,11407,"; !c100 = I32 [100]; ApplySeeded(!c100, %__rng_state) [rand_int32, 0, Int32] // unbound %__rng_state; }; !2 = ToArray(!s2); InsertFields !entry (a: !2); }; ```; Note that the `ApplySeeded` node is tageed with a static UID `0`, and references an unbound variable `__rng_state`. It is the responsibility of the `handle_randomness` pass to give proper definitions of `__rng_state` in any scope that needs it. After `handle_randomness` (and some more tidying), the IR looks like:; ```; // Now MatrixRead adds row and col uids; !1 = MatrixRead [None, False, False, (MatrixRangeReader MatrixRangeReaderParameters(10,10,None) 8)]; !11 = MatrixMapEntries(!1) {; (%g, %col, %row, %entry) =>; !2 = RNGStateLiteral // RNGState corresponding to empty message; !3 = GetField(%row) [__row_uid] // get row and col uids; !4 = GetField(%col) [__col_uid]; !5 = MakeTuple(!3, !4) [(0 1)]; %6 = RNGSplit(!2, !5) // append row and col uids to message; !c0 = I32 [0]; !c10 = I32 [10]; !c1 = I32 [1]; !s = StreamRange(!c0, !c10, !c1) [1, False]; !s2 = StreamMap(!s) { (%elt) =>; !7 = Cast(%elt) [Int64]; MakeTuple(!7, %elt) [(0 1)] // map to stream of (uid, elt) pairs; }; !s3 = StreamMap(!s2) { (%elt2) =>; !8 = GetTupleElement(%elt2) [0]; %9 = RNGSplit(%6, !8) // append stream element uid to message; !c100 = I32 [100]; // call random function with current message/RNGState %9 and static uid 0; ApplySeeded(!c100, %9) [rand_int32, 0, Int32]; }; !10 = ToArray(!s3); InsertFields !entry (a: !10); }; ```; Note that because only 3 longs are added to the message, none of the `RNGSplit` calls generate any runtime code. They simply encode statically that the last block of the message at the time of the `ApplySeeded` call consists of the locals `[!3, !4, !8]`. Then the `ApplySeeded` just needs to pad the last block, xor it with the running sum (which is the encrypted static block, embedded as a constant in the code), and call the Threefry `encrypt` function just once. # Security; Cryptogrophers have developed a very p",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:11472,Integrability,message,message,11472,"; !c100 = I32 [100]; ApplySeeded(!c100, %__rng_state) [rand_int32, 0, Int32] // unbound %__rng_state; }; !2 = ToArray(!s2); InsertFields !entry (a: !2); }; ```; Note that the `ApplySeeded` node is tageed with a static UID `0`, and references an unbound variable `__rng_state`. It is the responsibility of the `handle_randomness` pass to give proper definitions of `__rng_state` in any scope that needs it. After `handle_randomness` (and some more tidying), the IR looks like:; ```; // Now MatrixRead adds row and col uids; !1 = MatrixRead [None, False, False, (MatrixRangeReader MatrixRangeReaderParameters(10,10,None) 8)]; !11 = MatrixMapEntries(!1) {; (%g, %col, %row, %entry) =>; !2 = RNGStateLiteral // RNGState corresponding to empty message; !3 = GetField(%row) [__row_uid] // get row and col uids; !4 = GetField(%col) [__col_uid]; !5 = MakeTuple(!3, !4) [(0 1)]; %6 = RNGSplit(!2, !5) // append row and col uids to message; !c0 = I32 [0]; !c10 = I32 [10]; !c1 = I32 [1]; !s = StreamRange(!c0, !c10, !c1) [1, False]; !s2 = StreamMap(!s) { (%elt) =>; !7 = Cast(%elt) [Int64]; MakeTuple(!7, %elt) [(0 1)] // map to stream of (uid, elt) pairs; }; !s3 = StreamMap(!s2) { (%elt2) =>; !8 = GetTupleElement(%elt2) [0]; %9 = RNGSplit(%6, !8) // append stream element uid to message; !c100 = I32 [100]; // call random function with current message/RNGState %9 and static uid 0; ApplySeeded(!c100, %9) [rand_int32, 0, Int32]; }; !10 = ToArray(!s3); InsertFields !entry (a: !10); }; ```; Note that because only 3 longs are added to the message, none of the `RNGSplit` calls generate any runtime code. They simply encode statically that the last block of the message at the time of the `ApplySeeded` call consists of the locals `[!3, !4, !8]`. Then the `ApplySeeded` just needs to pad the last block, xor it with the running sum (which is the encrypted static block, embedded as a constant in the code), and call the Threefry `encrypt` function just once. # Security; Cryptogrophers have developed a very p",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:11666,Integrability,message,message,11666,"; !c100 = I32 [100]; ApplySeeded(!c100, %__rng_state) [rand_int32, 0, Int32] // unbound %__rng_state; }; !2 = ToArray(!s2); InsertFields !entry (a: !2); }; ```; Note that the `ApplySeeded` node is tageed with a static UID `0`, and references an unbound variable `__rng_state`. It is the responsibility of the `handle_randomness` pass to give proper definitions of `__rng_state` in any scope that needs it. After `handle_randomness` (and some more tidying), the IR looks like:; ```; // Now MatrixRead adds row and col uids; !1 = MatrixRead [None, False, False, (MatrixRangeReader MatrixRangeReaderParameters(10,10,None) 8)]; !11 = MatrixMapEntries(!1) {; (%g, %col, %row, %entry) =>; !2 = RNGStateLiteral // RNGState corresponding to empty message; !3 = GetField(%row) [__row_uid] // get row and col uids; !4 = GetField(%col) [__col_uid]; !5 = MakeTuple(!3, !4) [(0 1)]; %6 = RNGSplit(!2, !5) // append row and col uids to message; !c0 = I32 [0]; !c10 = I32 [10]; !c1 = I32 [1]; !s = StreamRange(!c0, !c10, !c1) [1, False]; !s2 = StreamMap(!s) { (%elt) =>; !7 = Cast(%elt) [Int64]; MakeTuple(!7, %elt) [(0 1)] // map to stream of (uid, elt) pairs; }; !s3 = StreamMap(!s2) { (%elt2) =>; !8 = GetTupleElement(%elt2) [0]; %9 = RNGSplit(%6, !8) // append stream element uid to message; !c100 = I32 [100]; // call random function with current message/RNGState %9 and static uid 0; ApplySeeded(!c100, %9) [rand_int32, 0, Int32]; }; !10 = ToArray(!s3); InsertFields !entry (a: !10); }; ```; Note that because only 3 longs are added to the message, none of the `RNGSplit` calls generate any runtime code. They simply encode statically that the last block of the message at the time of the `ApplySeeded` call consists of the locals `[!3, !4, !8]`. Then the `ApplySeeded` just needs to pad the last block, xor it with the running sum (which is the encrypted static block, embedded as a constant in the code), and call the Threefry `encrypt` function just once. # Security; Cryptogrophers have developed a very p",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:11788,Integrability,message,message,11788,"row, %entry) =>; !2 = RNGStateLiteral // RNGState corresponding to empty message; !3 = GetField(%row) [__row_uid] // get row and col uids; !4 = GetField(%col) [__col_uid]; !5 = MakeTuple(!3, !4) [(0 1)]; %6 = RNGSplit(!2, !5) // append row and col uids to message; !c0 = I32 [0]; !c10 = I32 [10]; !c1 = I32 [1]; !s = StreamRange(!c0, !c10, !c1) [1, False]; !s2 = StreamMap(!s) { (%elt) =>; !7 = Cast(%elt) [Int64]; MakeTuple(!7, %elt) [(0 1)] // map to stream of (uid, elt) pairs; }; !s3 = StreamMap(!s2) { (%elt2) =>; !8 = GetTupleElement(%elt2) [0]; %9 = RNGSplit(%6, !8) // append stream element uid to message; !c100 = I32 [100]; // call random function with current message/RNGState %9 and static uid 0; ApplySeeded(!c100, %9) [rand_int32, 0, Int32]; }; !10 = ToArray(!s3); InsertFields !entry (a: !10); }; ```; Note that because only 3 longs are added to the message, none of the `RNGSplit` calls generate any runtime code. They simply encode statically that the last block of the message at the time of the `ApplySeeded` call consists of the locals `[!3, !4, !8]`. Then the `ApplySeeded` just needs to pad the last block, xor it with the running sum (which is the encrypted static block, embedded as a constant in the code), and call the Threefry `encrypt` function just once. # Security; Cryptogrophers have developed a very pragmatic theory of what makes for ""good"" pseudorandomness. One of the benefits of using cryptographic primitives (even while weakening some of the components for performance, as we do with Threefish) is that we can use this framework to evaluate how well users can trust the outputs of the RNG. Using this theory for a quick sanity check, consider a pipeline with a 1e7 row by 1e7 column matrixtable, with 1e4 random function invocations per entry, running for a year on 1e23 cores. Let `b` be any boolean output of this pipeline. Let `P_1` and `P_2` be the probabilities that `b=1` in the scenarios where random functions are truly random, and using the above schem",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:13419,Integrability,interface,interface,13419,"eck, consider a pipeline with a 1e7 row by 1e7 column matrixtable, with 1e4 random function invocations per entry, running for a year on 1e23 cores. Let `b` be any boolean output of this pipeline. Let `P_1` and `P_2` be the probabilities that `b=1` in the scenarios where random functions are truly random, and using the above scheme, respectively. Then `abs(P_1 - P_2) < 3e-40`. The only assumption in this bound is that Threefry is a secure block cipher, i.e. that the best attack against it is a brute force search of the space of all keys. The time bound comes from limiting how much of the key space the program is able to search. Clearly this will never be the weak link, and we can focus on how many random numbers are generated. This is a very practically reasurring result. It says that users can really trust that their results--interpreted under a model of true randomness--are not skewed by our implementation of pseudorandomness. # User interface; For the most part, users should not need to interact directly with the randomness infrastructure. If they don't, the default semantics are:; * Evaluating a hail expression multiple times in the same session always produces the same results; * Rebuilding an identical hail expression (e.g. `x = hl.rand_unif()` and `y = hl.rand_unif()`) evaluates with independent randomness.; * Running the same pipeline in multiple hail sessions uses independend randomness each time. The last two can be overridden if needed:; * To build identical expressions using the same randomness, manually specify ""seeds"" (should we rename this?) on each random function call. E.g. `x = hl.rand_unif(seed=0)`. This overrides using the global counter to populate the static uid. It is guaranteed that user specified static uids never clash with automatically generated ones.; * To run the same pipeline in multiple sessions with the same randomness, manually specify the ""global seed"" on init: `hl.init(global_seed=0)`. [1] ""Splittable pseudorandom number generator",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:3790,Modifiability,extend,extend,3790,"{s mod 3}; k_{s,2} = k_{s+2 mod 5} + t_{s+1 mod 3}; k_{s,3} = k_{s+3 mod 5} + s; ```. ## an encryption round; Encryption is performed over 20 rounds. Let `v_i` be the `i`th word of the encryption state, initialized; ```; v_i = p_i; ```; Before round `d` if `d mod 4 = 0`, add subkey `s = d/4`; ```; v_i += k_{s,i}; ```; Then apply the `mix` function to adjacent pairs of words, where the rotation constant `r = R[d mod 8][j]` is looked up in a table.; ```; mix(v_{2j}, v_{2j+1}, r); ```; `mix` is defined; ```; mix(x0, x1, r) {; x0 += x1; rotL(x1, r); x1 ^= x0; }; ```; <img width=""190"" alt=""MIX"" src=""https://user-images.githubusercontent.com/3430459/197853087-ff3cee9d-002e-43a0-955b-7dd6747b90f2.png"">. Lastly, the words are permuted; ```; v_1, v_3 = v_3, v_1 ; ```. # PMAC; PMAC is a message authentication code. Intuitively, a MAC uses a block cipher to construct a function from abritrary length messages to 256 bit message tags. We extend this to a function from arbitrary length messages to ""infinite"" length message tags (really a very large finite length). The security claim is that if the block cipher used ""looks like"" a random permutation, then the MAC ""looks like"" a random function. In particular, for each message `m`, `pmac(m)` looks like a stream of random bits, and for distinct messages `m1` and `m2`, `pmac(m1)` and `pmac(m2)` look like completely independent streams of random bits. Yet this is a deterministic function, so computing `pmac` on the same message always produces the same stream of bits. <img width=""654"" alt=""Screen Shot 2022-10-25 at 11 53 13 AM"" src=""https://user-images.githubusercontent.com/3430459/197853175-d96ea5b0-4618-4f6c-b92a-0dfbf5da7fe2.png"">. Many MAC constructions must process blocks sequentially. As we'll see below, this would add significant overhead to random number generation. PMAC has the property that blocks of the message can be processed in any order. In our case, we use a modification of the PMAC1 construction in [4]. We restrict the",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:10388,Modifiability,variab,variable,10388,"ct. It is an arbitrary length message (though it will typically be quite small, probably less than 10 longs). It is constructed as follows:. ## Dynamic UIDs; Every stream, table, or matrix table pipeline is transformed to explicitly generate a unique uid per stream entry, table row, and matrix table row/column. These uids are explicit in the IR as ordinary values/fields, so the compiler automatically preserves the RNG determinism. ## Putting it all together; Consider the example pipeline; ```; mt = hl.utils.range_matrix_table(10, 10); mt = mt.annotate_entries(a=hl.range(10).map(lambda i: hl.rand_int32(100))); ```; Before elaborating UIDs in the IR in python, the IR looks like this (after a little tidying):; ```; !1 = MatrixRead [DropRowColUIDs, ...] // don't add uid fields; !3 = MatrixMapEntries(!1) {; (%g, %col, %row, %entry) =>; !c0 = I32 [0]; !c10 = I32 [10]; !c1 = I32 [1]; !s = StreamRange(!c0, !c10, !c1) [1, False]; !s2 = StreamMap(!s) { (%elt) =>; !c100 = I32 [100]; ApplySeeded(!c100, %__rng_state) [rand_int32, 0, Int32] // unbound %__rng_state; }; !2 = ToArray(!s2); InsertFields !entry (a: !2); }; ```; Note that the `ApplySeeded` node is tageed with a static UID `0`, and references an unbound variable `__rng_state`. It is the responsibility of the `handle_randomness` pass to give proper definitions of `__rng_state` in any scope that needs it. After `handle_randomness` (and some more tidying), the IR looks like:; ```; // Now MatrixRead adds row and col uids; !1 = MatrixRead [None, False, False, (MatrixRangeReader MatrixRangeReaderParameters(10,10,None) 8)]; !11 = MatrixMapEntries(!1) {; (%g, %col, %row, %entry) =>; !2 = RNGStateLiteral // RNGState corresponding to empty message; !3 = GetField(%row) [__row_uid] // get row and col uids; !4 = GetField(%col) [__col_uid]; !5 = MakeTuple(!3, !4) [(0 1)]; %6 = RNGSplit(!2, !5) // append row and col uids to message; !c0 = I32 [0]; !c10 = I32 [10]; !c1 = I32 [1]; !s = StreamRange(!c0, !c10, !c1) [1, False]; !s2 = Strea",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:762,Performance,optimiz,optimize,762,"Our design for pseudo-random number generation is inspired by [1], but several details differ. At a high level, the idea is:; * Assign to each random function invocation some unique identifier. In general we can't bound the size of the identifier. We use arrays of longs.; * Use a construction of a psuedo-random function to map unique identifiers to random streams of bits. Intuitively, it's as if we used the identifier to seed a stateful RNG. The key property is that random function invocations with distinct identifiers produce independent random results, while invocations with the same identifier always produce the same result. Thus random function invocations are actually pure functions, with no side effects, which gives the compiler great freedom to optimize queries without affecting the results. Psuedo-random functions are important building blocks in cryptography, and so they are very well studied, with many different practical constructions. We use the PMAC message authentication code, which depends on a tweakable block cipher, for which we use a reduced-round Threefish. Either or both of these pieces could be replaced with little effort, e.g. to improve performance. # Threefish/Threefry:; We use the Threefish [2] block cipher, modified to use 20 rounds for efficiency (the full Threefish4x64 uses 72 rounds), as suggested by [3] (although we make use of the Threefish tweak). Reference implementation is `Threefry.encrypt`. `threefish4x64` takes:; * key `K = (k_0, ..., k_3)`: 4 words; * tweak `T = (t_0, t_1)`: 2 words; * plaintext `P = (p_0, ..., p_3)`: 4 words. Intutively, this is a function taking a key and tweak as input, and returning a permutation on the space of all 256-bit blocks. The security claim is that if the key is chosen randomly, then for any choice of tweak, the resulting permutation ""looks like"" a uniformly chosen random permutation. Like most (all?) block ciphers, it is constructed as a sequence of simpler permutations. Think of shuffling a deck o",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:1178,Performance,perform,performance,1178,"que identifier. In general we can't bound the size of the identifier. We use arrays of longs.; * Use a construction of a psuedo-random function to map unique identifiers to random streams of bits. Intuitively, it's as if we used the identifier to seed a stateful RNG. The key property is that random function invocations with distinct identifiers produce independent random results, while invocations with the same identifier always produce the same result. Thus random function invocations are actually pure functions, with no side effects, which gives the compiler great freedom to optimize queries without affecting the results. Psuedo-random functions are important building blocks in cryptography, and so they are very well studied, with many different practical constructions. We use the PMAC message authentication code, which depends on a tweakable block cipher, for which we use a reduced-round Threefish. Either or both of these pieces could be replaced with little effort, e.g. to improve performance. # Threefish/Threefry:; We use the Threefish [2] block cipher, modified to use 20 rounds for efficiency (the full Threefish4x64 uses 72 rounds), as suggested by [3] (although we make use of the Threefish tweak). Reference implementation is `Threefry.encrypt`. `threefish4x64` takes:; * key `K = (k_0, ..., k_3)`: 4 words; * tweak `T = (t_0, t_1)`: 2 words; * plaintext `P = (p_0, ..., p_3)`: 4 words. Intutively, this is a function taking a key and tweak as input, and returning a permutation on the space of all 256-bit blocks. The security claim is that if the key is chosen randomly, then for any choice of tweak, the resulting permutation ""looks like"" a uniformly chosen random permutation. Like most (all?) block ciphers, it is constructed as a sequence of simpler permutations. Think of shuffling a deck of cards: each shuffle isn't that random (is easily distinguishable from a completely random permutation), but a sequence of seven shuffles is indistinguishable from a random per",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:2975,Performance,perform,performed,2975,"ions. Think of shuffling a deck of cards: each shuffle isn't that random (is easily distinguishable from a completely random permutation), but a sequence of seven shuffles is indistinguishable from a random permutation. The simple permutations are called ""rounds"". Each round consists of applying a function ""Mix"" to pairs of 64-bit words, which is a bit-level permutation, followed by a permutation of the four words. <img width=""440"" alt=""threefish"" src=""https://user-images.githubusercontent.com/3430459/197852931-59bb6734-917b-4c4a-b3a2-7e2e9302a09f.png"">. ## key schedule; The key schedule turns the key and tweak into 6 subkeys, each 4 words. Subkey `s` is denoted `(k_{s,0}, ..., k_{s,3})`. First compute two additional words `k_4 = C ^ k_0 ^ k_1 ^ k_2 ^ k_3` and `t_2 = t0 ^ t_1`, where `C = 0x1BD11BDAA9FC1A22`. Then; ```; k_{s,0} = k_{s mod 5}; k_{s,1} = k_{s+1 mod 5} + t_{s mod 3}; k_{s,2} = k_{s+2 mod 5} + t_{s+1 mod 3}; k_{s,3} = k_{s+3 mod 5} + s; ```. ## an encryption round; Encryption is performed over 20 rounds. Let `v_i` be the `i`th word of the encryption state, initialized; ```; v_i = p_i; ```; Before round `d` if `d mod 4 = 0`, add subkey `s = d/4`; ```; v_i += k_{s,i}; ```; Then apply the `mix` function to adjacent pairs of words, where the rotation constant `r = R[d mod 8][j]` is looked up in a table.; ```; mix(v_{2j}, v_{2j+1}, r); ```; `mix` is defined; ```; mix(x0, x1, r) {; x0 += x1; rotL(x1, r); x1 ^= x0; }; ```; <img width=""190"" alt=""MIX"" src=""https://user-images.githubusercontent.com/3430459/197853087-ff3cee9d-002e-43a0-955b-7dd6747b90f2.png"">. Lastly, the words are permuted; ```; v_1, v_3 = v_3, v_1 ; ```. # PMAC; PMAC is a message authentication code. Intuitively, a MAC uses a block cipher to construct a function from abritrary length messages to 256 bit message tags. We extend this to a function from arbitrary length messages to ""infinite"" length message tags (really a very large finite length). The security claim is that if the block cipher use",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:12297,Performance,perform,performance,12297,"lt2) =>; !8 = GetTupleElement(%elt2) [0]; %9 = RNGSplit(%6, !8) // append stream element uid to message; !c100 = I32 [100]; // call random function with current message/RNGState %9 and static uid 0; ApplySeeded(!c100, %9) [rand_int32, 0, Int32]; }; !10 = ToArray(!s3); InsertFields !entry (a: !10); }; ```; Note that because only 3 longs are added to the message, none of the `RNGSplit` calls generate any runtime code. They simply encode statically that the last block of the message at the time of the `ApplySeeded` call consists of the locals `[!3, !4, !8]`. Then the `ApplySeeded` just needs to pad the last block, xor it with the running sum (which is the encrypted static block, embedded as a constant in the code), and call the Threefry `encrypt` function just once. # Security; Cryptogrophers have developed a very pragmatic theory of what makes for ""good"" pseudorandomness. One of the benefits of using cryptographic primitives (even while weakening some of the components for performance, as we do with Threefish) is that we can use this framework to evaluate how well users can trust the outputs of the RNG. Using this theory for a quick sanity check, consider a pipeline with a 1e7 row by 1e7 column matrixtable, with 1e4 random function invocations per entry, running for a year on 1e23 cores. Let `b` be any boolean output of this pipeline. Let `P_1` and `P_2` be the probabilities that `b=1` in the scenarios where random functions are truly random, and using the above scheme, respectively. Then `abs(P_1 - P_2) < 3e-40`. The only assumption in this bound is that Threefry is a secure block cipher, i.e. that the best attack against it is a brute force search of the space of all keys. The time bound comes from limiting how much of the key space the program is able to search. Clearly this will never be the weak link, and we can focus on how many random numbers are generated. This is a very practically reasurring result. It says that users can really trust that their results--int",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:12460,Safety,sanity check,sanity check,12460,"c100, %9) [rand_int32, 0, Int32]; }; !10 = ToArray(!s3); InsertFields !entry (a: !10); }; ```; Note that because only 3 longs are added to the message, none of the `RNGSplit` calls generate any runtime code. They simply encode statically that the last block of the message at the time of the `ApplySeeded` call consists of the locals `[!3, !4, !8]`. Then the `ApplySeeded` just needs to pad the last block, xor it with the running sum (which is the encrypted static block, embedded as a constant in the code), and call the Threefry `encrypt` function just once. # Security; Cryptogrophers have developed a very pragmatic theory of what makes for ""good"" pseudorandomness. One of the benefits of using cryptographic primitives (even while weakening some of the components for performance, as we do with Threefish) is that we can use this framework to evaluate how well users can trust the outputs of the RNG. Using this theory for a quick sanity check, consider a pipeline with a 1e7 row by 1e7 column matrixtable, with 1e4 random function invocations per entry, running for a year on 1e23 cores. Let `b` be any boolean output of this pipeline. Let `P_1` and `P_2` be the probabilities that `b=1` in the scenarios where random functions are truly random, and using the above scheme, respectively. Then `abs(P_1 - P_2) < 3e-40`. The only assumption in this bound is that Threefry is a secure block cipher, i.e. that the best attack against it is a brute force search of the space of all keys. The time bound comes from limiting how much of the key space the program is able to search. Clearly this will never be the weak link, and we can focus on how many random numbers are generated. This is a very practically reasurring result. It says that users can really trust that their results--interpreted under a model of true randomness--are not skewed by our implementation of pseudorandomness. # User interface; For the most part, users should not need to interact directly with the randomness infrastruct",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:985,Security,authenticat,authentication,985,"om number generation is inspired by [1], but several details differ. At a high level, the idea is:; * Assign to each random function invocation some unique identifier. In general we can't bound the size of the identifier. We use arrays of longs.; * Use a construction of a psuedo-random function to map unique identifiers to random streams of bits. Intuitively, it's as if we used the identifier to seed a stateful RNG. The key property is that random function invocations with distinct identifiers produce independent random results, while invocations with the same identifier always produce the same result. Thus random function invocations are actually pure functions, with no side effects, which gives the compiler great freedom to optimize queries without affecting the results. Psuedo-random functions are important building blocks in cryptography, and so they are very well studied, with many different practical constructions. We use the PMAC message authentication code, which depends on a tweakable block cipher, for which we use a reduced-round Threefish. Either or both of these pieces could be replaced with little effort, e.g. to improve performance. # Threefish/Threefry:; We use the Threefish [2] block cipher, modified to use 20 rounds for efficiency (the full Threefish4x64 uses 72 rounds), as suggested by [3] (although we make use of the Threefish tweak). Reference implementation is `Threefry.encrypt`. `threefish4x64` takes:; * key `K = (k_0, ..., k_3)`: 4 words; * tweak `T = (t_0, t_1)`: 2 words; * plaintext `P = (p_0, ..., p_3)`: 4 words. Intutively, this is a function taking a key and tweak as input, and returning a permutation on the space of all 256-bit blocks. The security claim is that if the key is chosen randomly, then for any choice of tweak, the resulting permutation ""looks like"" a uniformly chosen random permutation. Like most (all?) block ciphers, it is constructed as a sequence of simpler permutations. Think of shuffling a deck of cards: each shuffle isn",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:1440,Security,encrypt,encrypt,1440,". The key property is that random function invocations with distinct identifiers produce independent random results, while invocations with the same identifier always produce the same result. Thus random function invocations are actually pure functions, with no side effects, which gives the compiler great freedom to optimize queries without affecting the results. Psuedo-random functions are important building blocks in cryptography, and so they are very well studied, with many different practical constructions. We use the PMAC message authentication code, which depends on a tweakable block cipher, for which we use a reduced-round Threefish. Either or both of these pieces could be replaced with little effort, e.g. to improve performance. # Threefish/Threefry:; We use the Threefish [2] block cipher, modified to use 20 rounds for efficiency (the full Threefish4x64 uses 72 rounds), as suggested by [3] (although we make use of the Threefish tweak). Reference implementation is `Threefry.encrypt`. `threefish4x64` takes:; * key `K = (k_0, ..., k_3)`: 4 words; * tweak `T = (t_0, t_1)`: 2 words; * plaintext `P = (p_0, ..., p_3)`: 4 words. Intutively, this is a function taking a key and tweak as input, and returning a permutation on the space of all 256-bit blocks. The security claim is that if the key is chosen randomly, then for any choice of tweak, the resulting permutation ""looks like"" a uniformly chosen random permutation. Like most (all?) block ciphers, it is constructed as a sequence of simpler permutations. Think of shuffling a deck of cards: each shuffle isn't that random (is easily distinguishable from a completely random permutation), but a sequence of seven shuffles is indistinguishable from a random permutation. The simple permutations are called ""rounds"". Each round consists of applying a function ""Mix"" to pairs of 64-bit words, which is a bit-level permutation, followed by a permutation of the four words. <img width=""440"" alt=""threefish"" src=""https://user-images.",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:1723,Security,secur,security,1723,"results. Psuedo-random functions are important building blocks in cryptography, and so they are very well studied, with many different practical constructions. We use the PMAC message authentication code, which depends on a tweakable block cipher, for which we use a reduced-round Threefish. Either or both of these pieces could be replaced with little effort, e.g. to improve performance. # Threefish/Threefry:; We use the Threefish [2] block cipher, modified to use 20 rounds for efficiency (the full Threefish4x64 uses 72 rounds), as suggested by [3] (although we make use of the Threefish tweak). Reference implementation is `Threefry.encrypt`. `threefish4x64` takes:; * key `K = (k_0, ..., k_3)`: 4 words; * tweak `T = (t_0, t_1)`: 2 words; * plaintext `P = (p_0, ..., p_3)`: 4 words. Intutively, this is a function taking a key and tweak as input, and returning a permutation on the space of all 256-bit blocks. The security claim is that if the key is chosen randomly, then for any choice of tweak, the resulting permutation ""looks like"" a uniformly chosen random permutation. Like most (all?) block ciphers, it is constructed as a sequence of simpler permutations. Think of shuffling a deck of cards: each shuffle isn't that random (is easily distinguishable from a completely random permutation), but a sequence of seven shuffles is indistinguishable from a random permutation. The simple permutations are called ""rounds"". Each round consists of applying a function ""Mix"" to pairs of 64-bit words, which is a bit-level permutation, followed by a permutation of the four words. <img width=""440"" alt=""threefish"" src=""https://user-images.githubusercontent.com/3430459/197852931-59bb6734-917b-4c4a-b3a2-7e2e9302a09f.png"">. ## key schedule; The key schedule turns the key and tweak into 6 subkeys, each 4 words. Subkey `s` is denoted `(k_{s,0}, ..., k_{s,3})`. First compute two additional words `k_4 = C ^ k_0 ^ k_1 ^ k_2 ^ k_3` and `t_2 = t0 ^ t_1`, where `C = 0x1BD11BDAA9FC1A22`. Then; ```; k",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:2943,Security,encrypt,encryption,2943,"ions. Think of shuffling a deck of cards: each shuffle isn't that random (is easily distinguishable from a completely random permutation), but a sequence of seven shuffles is indistinguishable from a random permutation. The simple permutations are called ""rounds"". Each round consists of applying a function ""Mix"" to pairs of 64-bit words, which is a bit-level permutation, followed by a permutation of the four words. <img width=""440"" alt=""threefish"" src=""https://user-images.githubusercontent.com/3430459/197852931-59bb6734-917b-4c4a-b3a2-7e2e9302a09f.png"">. ## key schedule; The key schedule turns the key and tweak into 6 subkeys, each 4 words. Subkey `s` is denoted `(k_{s,0}, ..., k_{s,3})`. First compute two additional words `k_4 = C ^ k_0 ^ k_1 ^ k_2 ^ k_3` and `t_2 = t0 ^ t_1`, where `C = 0x1BD11BDAA9FC1A22`. Then; ```; k_{s,0} = k_{s mod 5}; k_{s,1} = k_{s+1 mod 5} + t_{s mod 3}; k_{s,2} = k_{s+2 mod 5} + t_{s+1 mod 3}; k_{s,3} = k_{s+3 mod 5} + s; ```. ## an encryption round; Encryption is performed over 20 rounds. Let `v_i` be the `i`th word of the encryption state, initialized; ```; v_i = p_i; ```; Before round `d` if `d mod 4 = 0`, add subkey `s = d/4`; ```; v_i += k_{s,i}; ```; Then apply the `mix` function to adjacent pairs of words, where the rotation constant `r = R[d mod 8][j]` is looked up in a table.; ```; mix(v_{2j}, v_{2j+1}, r); ```; `mix` is defined; ```; mix(x0, x1, r) {; x0 += x1; rotL(x1, r); x1 ^= x0; }; ```; <img width=""190"" alt=""MIX"" src=""https://user-images.githubusercontent.com/3430459/197853087-ff3cee9d-002e-43a0-955b-7dd6747b90f2.png"">. Lastly, the words are permuted; ```; v_1, v_3 = v_3, v_1 ; ```. # PMAC; PMAC is a message authentication code. Intuitively, a MAC uses a block cipher to construct a function from abritrary length messages to 256 bit message tags. We extend this to a function from arbitrary length messages to ""infinite"" length message tags (really a very large finite length). The security claim is that if the block cipher use",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:3036,Security,encrypt,encryption,3036,"nguishable from a random permutation. The simple permutations are called ""rounds"". Each round consists of applying a function ""Mix"" to pairs of 64-bit words, which is a bit-level permutation, followed by a permutation of the four words. <img width=""440"" alt=""threefish"" src=""https://user-images.githubusercontent.com/3430459/197852931-59bb6734-917b-4c4a-b3a2-7e2e9302a09f.png"">. ## key schedule; The key schedule turns the key and tweak into 6 subkeys, each 4 words. Subkey `s` is denoted `(k_{s,0}, ..., k_{s,3})`. First compute two additional words `k_4 = C ^ k_0 ^ k_1 ^ k_2 ^ k_3` and `t_2 = t0 ^ t_1`, where `C = 0x1BD11BDAA9FC1A22`. Then; ```; k_{s,0} = k_{s mod 5}; k_{s,1} = k_{s+1 mod 5} + t_{s mod 3}; k_{s,2} = k_{s+2 mod 5} + t_{s+1 mod 3}; k_{s,3} = k_{s+3 mod 5} + s; ```. ## an encryption round; Encryption is performed over 20 rounds. Let `v_i` be the `i`th word of the encryption state, initialized; ```; v_i = p_i; ```; Before round `d` if `d mod 4 = 0`, add subkey `s = d/4`; ```; v_i += k_{s,i}; ```; Then apply the `mix` function to adjacent pairs of words, where the rotation constant `r = R[d mod 8][j]` is looked up in a table.; ```; mix(v_{2j}, v_{2j+1}, r); ```; `mix` is defined; ```; mix(x0, x1, r) {; x0 += x1; rotL(x1, r); x1 ^= x0; }; ```; <img width=""190"" alt=""MIX"" src=""https://user-images.githubusercontent.com/3430459/197853087-ff3cee9d-002e-43a0-955b-7dd6747b90f2.png"">. Lastly, the words are permuted; ```; v_1, v_3 = v_3, v_1 ; ```. # PMAC; PMAC is a message authentication code. Intuitively, a MAC uses a block cipher to construct a function from abritrary length messages to 256 bit message tags. We extend this to a function from arbitrary length messages to ""infinite"" length message tags (really a very large finite length). The security claim is that if the block cipher used ""looks like"" a random permutation, then the MAC ""looks like"" a random function. In particular, for each message `m`, `pmac(m)` looks like a stream of random bits, and for distinct m",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:3647,Security,authenticat,authentication,3647,"{s,0}, ..., k_{s,3})`. First compute two additional words `k_4 = C ^ k_0 ^ k_1 ^ k_2 ^ k_3` and `t_2 = t0 ^ t_1`, where `C = 0x1BD11BDAA9FC1A22`. Then; ```; k_{s,0} = k_{s mod 5}; k_{s,1} = k_{s+1 mod 5} + t_{s mod 3}; k_{s,2} = k_{s+2 mod 5} + t_{s+1 mod 3}; k_{s,3} = k_{s+3 mod 5} + s; ```. ## an encryption round; Encryption is performed over 20 rounds. Let `v_i` be the `i`th word of the encryption state, initialized; ```; v_i = p_i; ```; Before round `d` if `d mod 4 = 0`, add subkey `s = d/4`; ```; v_i += k_{s,i}; ```; Then apply the `mix` function to adjacent pairs of words, where the rotation constant `r = R[d mod 8][j]` is looked up in a table.; ```; mix(v_{2j}, v_{2j+1}, r); ```; `mix` is defined; ```; mix(x0, x1, r) {; x0 += x1; rotL(x1, r); x1 ^= x0; }; ```; <img width=""190"" alt=""MIX"" src=""https://user-images.githubusercontent.com/3430459/197853087-ff3cee9d-002e-43a0-955b-7dd6747b90f2.png"">. Lastly, the words are permuted; ```; v_1, v_3 = v_3, v_1 ; ```. # PMAC; PMAC is a message authentication code. Intuitively, a MAC uses a block cipher to construct a function from abritrary length messages to 256 bit message tags. We extend this to a function from arbitrary length messages to ""infinite"" length message tags (really a very large finite length). The security claim is that if the block cipher used ""looks like"" a random permutation, then the MAC ""looks like"" a random function. In particular, for each message `m`, `pmac(m)` looks like a stream of random bits, and for distinct messages `m1` and `m2`, `pmac(m1)` and `pmac(m2)` look like completely independent streams of random bits. Yet this is a deterministic function, so computing `pmac` on the same message always produces the same stream of bits. <img width=""654"" alt=""Screen Shot 2022-10-25 at 11 53 13 AM"" src=""https://user-images.githubusercontent.com/3430459/197853175-d96ea5b0-4618-4f6c-b92a-0dfbf5da7fe2.png"">. Many MAC constructions must process blocks sequentially. As we'll see below, this would add signif",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:3922,Security,secur,security,3922,"d over 20 rounds. Let `v_i` be the `i`th word of the encryption state, initialized; ```; v_i = p_i; ```; Before round `d` if `d mod 4 = 0`, add subkey `s = d/4`; ```; v_i += k_{s,i}; ```; Then apply the `mix` function to adjacent pairs of words, where the rotation constant `r = R[d mod 8][j]` is looked up in a table.; ```; mix(v_{2j}, v_{2j+1}, r); ```; `mix` is defined; ```; mix(x0, x1, r) {; x0 += x1; rotL(x1, r); x1 ^= x0; }; ```; <img width=""190"" alt=""MIX"" src=""https://user-images.githubusercontent.com/3430459/197853087-ff3cee9d-002e-43a0-955b-7dd6747b90f2.png"">. Lastly, the words are permuted; ```; v_1, v_3 = v_3, v_1 ; ```. # PMAC; PMAC is a message authentication code. Intuitively, a MAC uses a block cipher to construct a function from abritrary length messages to 256 bit message tags. We extend this to a function from arbitrary length messages to ""infinite"" length message tags (really a very large finite length). The security claim is that if the block cipher used ""looks like"" a random permutation, then the MAC ""looks like"" a random function. In particular, for each message `m`, `pmac(m)` looks like a stream of random bits, and for distinct messages `m1` and `m2`, `pmac(m1)` and `pmac(m2)` look like completely independent streams of random bits. Yet this is a deterministic function, so computing `pmac` on the same message always produces the same stream of bits. <img width=""654"" alt=""Screen Shot 2022-10-25 at 11 53 13 AM"" src=""https://user-images.githubusercontent.com/3430459/197853175-d96ea5b0-4618-4f6c-b92a-0dfbf5da7fe2.png"">. Many MAC constructions must process blocks sequentially. As we'll see below, this would add significant overhead to random number generation. PMAC has the property that blocks of the message can be processed in any order. In our case, we use a modification of the PMAC1 construction in [4]. We restrict the message length to multiples of 64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message:",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:5251,Security,encrypt,encrypt,5251,"is is a deterministic function, so computing `pmac` on the same message always produces the same stream of bits. <img width=""654"" alt=""Screen Shot 2022-10-25 at 11 53 13 AM"" src=""https://user-images.githubusercontent.com/3430459/197853175-d96ea5b0-4618-4f6c-b92a-0dfbf5da7fe2.png"">. Many MAC constructions must process blocks sequentially. As we'll see below, this would add significant overhead to random number generation. PMAC has the property that blocks of the message can be processed in any order. In our case, we use a modification of the PMAC1 construction in [4]. We restrict the message length to multiples of 64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into blocks of 4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets reduced to a 256 bit hash, such that distinct messages are highly unlikely to have the same hash. Then for each value of the counter, we use a distinct random function (really a random permutation) from the space of hashes to the space of random outputs. ## Lazy computation; In practice, we don't need to save entire messages in memory. Instead we compute the hash on the fly. The new type is ",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:5508,Security,hash,hash,5508,"b0-4618-4f6c-b92a-0dfbf5da7fe2.png"">. Many MAC constructions must process blocks sequentially. As we'll see below, this would add significant overhead to random number generation. PMAC has the property that blocks of the message can be processed in any order. In our case, we use a modification of the PMAC1 construction in [4]. We restrict the message length to multiples of 64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into blocks of 4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets reduced to a 256 bit hash, such that distinct messages are highly unlikely to have the same hash. Then for each value of the counter, we use a distinct random function (really a random permutation) from the space of hashes to the space of random outputs. ## Lazy computation; In practice, we don't need to save entire messages in memory. Instead we compute the hash on the fly. The new type is `RNGState`. A value of this type consists of the data:; * `runningSum: IndexedSeq[Long]`: the xor of the encrypted contents of all full blocks; * `lastDynBlock: IndexedSeq[Long]`: the partial contents of the last block. The length of the sequence",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:5594,Security,encrypt,encrypt,5594,"andom number generation. PMAC has the property that blocks of the message can be processed in any order. In our case, we use a modification of the PMAC1 construction in [4]. We restrict the message length to multiples of 64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into blocks of 4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets reduced to a 256 bit hash, such that distinct messages are highly unlikely to have the same hash. Then for each value of the counter, we use a distinct random function (really a random permutation) from the space of hashes to the space of random outputs. ## Lazy computation; In practice, we don't need to save entire messages in memory. Instead we compute the hash on the fly. The new type is `RNGState`. A value of this type consists of the data:; * `runningSum: IndexedSeq[Long]`: the xor of the encrypted contents of all full blocks; * `lastDynBlock: IndexedSeq[Long]`: the partial contents of the last block. The length of the sequence is `numWordsInLastDynBlock`; * `numWordsInLastDynBlock: Int`: the number of words (longs), in the range `[0, 4)`, currently contained in `lastDynBlock`; *",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:5647,Security,encrypt,encrypt,5647,"andom number generation. PMAC has the property that blocks of the message can be processed in any order. In our case, we use a modification of the PMAC1 construction in [4]. We restrict the message length to multiples of 64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into blocks of 4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets reduced to a 256 bit hash, such that distinct messages are highly unlikely to have the same hash. Then for each value of the counter, we use a distinct random function (really a random permutation) from the space of hashes to the space of random outputs. ## Lazy computation; In practice, we don't need to save entire messages in memory. Instead we compute the hash on the fly. The new type is `RNGState`. A value of this type consists of the data:; * `runningSum: IndexedSeq[Long]`: the xor of the encrypted contents of all full blocks; * `lastDynBlock: IndexedSeq[Long]`: the partial contents of the last block. The length of the sequence is `numWordsInLastDynBlock`; * `numWordsInLastDynBlock: Int`: the number of words (longs), in the range `[0, 4)`, currently contained in `lastDynBlock`; *",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:5890,Security,hash,hash,5890,"64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into blocks of 4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets reduced to a 256 bit hash, such that distinct messages are highly unlikely to have the same hash. Then for each value of the counter, we use a distinct random function (really a random permutation) from the space of hashes to the space of random outputs. ## Lazy computation; In practice, we don't need to save entire messages in memory. Instead we compute the hash on the fly. The new type is `RNGState`. A value of this type consists of the data:; * `runningSum: IndexedSeq[Long]`: the xor of the encrypted contents of all full blocks; * `lastDynBlock: IndexedSeq[Long]`: the partial contents of the last block. The length of the sequence is `numWordsInLastDynBlock`; * `numWordsInLastDynBlock: Int`: the number of words (longs), in the range `[0, 4)`, currently contained in `lastDynBlock`; * `hasStaticSplit: Boolean`: whether the static block has been incorporated into `runningSum`; * `numDynBlocks: Int`: the number of completed blocks, not including the static block. This system is implemented using three ",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:5961,Security,hash,hash,5961,"64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into blocks of 4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets reduced to a 256 bit hash, such that distinct messages are highly unlikely to have the same hash. Then for each value of the counter, we use a distinct random function (really a random permutation) from the space of hashes to the space of random outputs. ## Lazy computation; In practice, we don't need to save entire messages in memory. Instead we compute the hash on the fly. The new type is `RNGState`. A value of this type consists of the data:; * `runningSum: IndexedSeq[Long]`: the xor of the encrypted contents of all full blocks; * `lastDynBlock: IndexedSeq[Long]`: the partial contents of the last block. The length of the sequence is `numWordsInLastDynBlock`; * `numWordsInLastDynBlock: Int`: the number of words (longs), in the range `[0, 4)`, currently contained in `lastDynBlock`; * `hasStaticSplit: Boolean`: whether the static block has been incorporated into `runningSum`; * `numDynBlocks: Int`: the number of completed blocks, not including the static block. This system is implemented using three ",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:6085,Security,hash,hashes,6085,"implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into blocks of 4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets reduced to a 256 bit hash, such that distinct messages are highly unlikely to have the same hash. Then for each value of the counter, we use a distinct random function (really a random permutation) from the space of hashes to the space of random outputs. ## Lazy computation; In practice, we don't need to save entire messages in memory. Instead we compute the hash on the fly. The new type is `RNGState`. A value of this type consists of the data:; * `runningSum: IndexedSeq[Long]`: the xor of the encrypted contents of all full blocks; * `lastDynBlock: IndexedSeq[Long]`: the partial contents of the last block. The length of the sequence is `numWordsInLastDynBlock`; * `numWordsInLastDynBlock: Int`: the number of words (longs), in the range `[0, 4)`, currently contained in `lastDynBlock`; * `hasStaticSplit: Boolean`: whether the static block has been incorporated into `runningSum`; * `numDynBlocks: Int`: the number of completed blocks, not including the static block. This system is implemented using three IR nodes:; * `RNGStateLiteral` - creates an `RNGState` representing the empty message; * `RNGSplit(state: RNGState, dynBitstring: ?): RNGState` - appends to `las",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:6230,Security,hash,hash,6230," shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets reduced to a 256 bit hash, such that distinct messages are highly unlikely to have the same hash. Then for each value of the counter, we use a distinct random function (really a random permutation) from the space of hashes to the space of random outputs. ## Lazy computation; In practice, we don't need to save entire messages in memory. Instead we compute the hash on the fly. The new type is `RNGState`. A value of this type consists of the data:; * `runningSum: IndexedSeq[Long]`: the xor of the encrypted contents of all full blocks; * `lastDynBlock: IndexedSeq[Long]`: the partial contents of the last block. The length of the sequence is `numWordsInLastDynBlock`; * `numWordsInLastDynBlock: Int`: the number of words (longs), in the range `[0, 4)`, currently contained in `lastDynBlock`; * `hasStaticSplit: Boolean`: whether the static block has been incorporated into `runningSum`; * `numDynBlocks: Int`: the number of completed blocks, not including the static block. This system is implemented using three IR nodes:; * `RNGStateLiteral` - creates an `RNGState` representing the empty message; * `RNGSplit(state: RNGState, dynBitstring: ?): RNGState` - appends to `lastDynBlock`. When the last block is full, encrypt it (using `numDynBlocks` for the tweak), and xor it into `runningSum`. Here `?` is either a single long, or an arbitrary sized tuple",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:6368,Security,encrypt,encrypted,6368,"k is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets reduced to a 256 bit hash, such that distinct messages are highly unlikely to have the same hash. Then for each value of the counter, we use a distinct random function (really a random permutation) from the space of hashes to the space of random outputs. ## Lazy computation; In practice, we don't need to save entire messages in memory. Instead we compute the hash on the fly. The new type is `RNGState`. A value of this type consists of the data:; * `runningSum: IndexedSeq[Long]`: the xor of the encrypted contents of all full blocks; * `lastDynBlock: IndexedSeq[Long]`: the partial contents of the last block. The length of the sequence is `numWordsInLastDynBlock`; * `numWordsInLastDynBlock: Int`: the number of words (longs), in the range `[0, 4)`, currently contained in `lastDynBlock`; * `hasStaticSplit: Boolean`: whether the static block has been incorporated into `runningSum`; * `numDynBlocks: Int`: the number of completed blocks, not including the static block. This system is implemented using three IR nodes:; * `RNGStateLiteral` - creates an `RNGState` representing the empty message; * `RNGSplit(state: RNGState, dynBitstring: ?): RNGState` - appends to `lastDynBlock`. When the last block is full, encrypt it (using `numDynBlocks` for the tweak), and xor it into `runningSum`. Here `?` is either a single long, or an arbitrary sized tuple of longs.; * `ApplySeeded(..., rngState: RNGState, staticUID: Long)`; * Statically, forms the static block `[nonce, staticUID, 0L, 0L]`, encrypts it, a",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:7086,Security,encrypt,encrypt,7086,"ndom outputs. ## Lazy computation; In practice, we don't need to save entire messages in memory. Instead we compute the hash on the fly. The new type is `RNGState`. A value of this type consists of the data:; * `runningSum: IndexedSeq[Long]`: the xor of the encrypted contents of all full blocks; * `lastDynBlock: IndexedSeq[Long]`: the partial contents of the last block. The length of the sequence is `numWordsInLastDynBlock`; * `numWordsInLastDynBlock: Int`: the number of words (longs), in the range `[0, 4)`, currently contained in `lastDynBlock`; * `hasStaticSplit: Boolean`: whether the static block has been incorporated into `runningSum`; * `numDynBlocks: Int`: the number of completed blocks, not including the static block. This system is implemented using three IR nodes:; * `RNGStateLiteral` - creates an `RNGState` representing the empty message; * `RNGSplit(state: RNGState, dynBitstring: ?): RNGState` - appends to `lastDynBlock`. When the last block is full, encrypt it (using `numDynBlocks` for the tweak), and xor it into `runningSum`. Here `?` is either a single long, or an arbitrary sized tuple of longs.; * `ApplySeeded(..., rngState: RNGState, staticUID: Long)`; * Statically, forms the static block `[nonce, staticUID, 0L, 0L]`, encrypts it, and embeds the result as a literal in the code.; * At runtime, only needs to xor into the `runningSum` the encryped static block and the (possibly padded) `lastDynBlock`, and encrypt the result. Hence each `ApplySeeded` call only needs one invocation of the block cipher at runtime (more precisely, one invocation per 256 random bits needed by the random function). This minimizes the overhead of random number generation in inner loops, and is the reason for choose PMAC. # UIDs; To use the above PMAC scheme, we need to assign a ""message"" to every random function invocation in the program. As long as each invocation gets a distinct message, the PMAC random function generates approximately independent randomness for each invocat",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:7364,Security,encrypt,encrypts,7364,"dSeq[Long]`: the xor of the encrypted contents of all full blocks; * `lastDynBlock: IndexedSeq[Long]`: the partial contents of the last block. The length of the sequence is `numWordsInLastDynBlock`; * `numWordsInLastDynBlock: Int`: the number of words (longs), in the range `[0, 4)`, currently contained in `lastDynBlock`; * `hasStaticSplit: Boolean`: whether the static block has been incorporated into `runningSum`; * `numDynBlocks: Int`: the number of completed blocks, not including the static block. This system is implemented using three IR nodes:; * `RNGStateLiteral` - creates an `RNGState` representing the empty message; * `RNGSplit(state: RNGState, dynBitstring: ?): RNGState` - appends to `lastDynBlock`. When the last block is full, encrypt it (using `numDynBlocks` for the tweak), and xor it into `runningSum`. Here `?` is either a single long, or an arbitrary sized tuple of longs.; * `ApplySeeded(..., rngState: RNGState, staticUID: Long)`; * Statically, forms the static block `[nonce, staticUID, 0L, 0L]`, encrypts it, and embeds the result as a literal in the code.; * At runtime, only needs to xor into the `runningSum` the encryped static block and the (possibly padded) `lastDynBlock`, and encrypt the result. Hence each `ApplySeeded` call only needs one invocation of the block cipher at runtime (more precisely, one invocation per 256 random bits needed by the random function). This minimizes the overhead of random number generation in inner loops, and is the reason for choose PMAC. # UIDs; To use the above PMAC scheme, we need to assign a ""message"" to every random function invocation in the program. As long as each invocation gets a distinct message, the PMAC random function generates approximately independent randomness for each invocation. We fix a key for the block cipher once and for all. It was generated randomly, and is hard coded in the compiler. This saves us from issues of users specifying ""bad"" keys. Instead, we reserve a part of the message to encode a",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:7552,Security,encrypt,encrypt,7552,"the sequence is `numWordsInLastDynBlock`; * `numWordsInLastDynBlock: Int`: the number of words (longs), in the range `[0, 4)`, currently contained in `lastDynBlock`; * `hasStaticSplit: Boolean`: whether the static block has been incorporated into `runningSum`; * `numDynBlocks: Int`: the number of completed blocks, not including the static block. This system is implemented using three IR nodes:; * `RNGStateLiteral` - creates an `RNGState` representing the empty message; * `RNGSplit(state: RNGState, dynBitstring: ?): RNGState` - appends to `lastDynBlock`. When the last block is full, encrypt it (using `numDynBlocks` for the tweak), and xor it into `runningSum`. Here `?` is either a single long, or an arbitrary sized tuple of longs.; * `ApplySeeded(..., rngState: RNGState, staticUID: Long)`; * Statically, forms the static block `[nonce, staticUID, 0L, 0L]`, encrypts it, and embeds the result as a literal in the code.; * At runtime, only needs to xor into the `runningSum` the encryped static block and the (possibly padded) `lastDynBlock`, and encrypt the result. Hence each `ApplySeeded` call only needs one invocation of the block cipher at runtime (more precisely, one invocation per 256 random bits needed by the random function). This minimizes the overhead of random number generation in inner loops, and is the reason for choose PMAC. # UIDs; To use the above PMAC scheme, we need to assign a ""message"" to every random function invocation in the program. As long as each invocation gets a distinct message, the PMAC random function generates approximately independent randomness for each invocation. We fix a key for the block cipher once and for all. It was generated randomly, and is hard coded in the compiler. This saves us from issues of users specifying ""bad"" keys. Instead, we reserve a part of the message to encode a session scoped uid. By changing that uid between sessions, we allow running identical code repeatedly with independent randomness. ## Static UIDs; We split t",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:11972,Security,encrypt,encrypted,11972,"MakeTuple(!3, !4) [(0 1)]; %6 = RNGSplit(!2, !5) // append row and col uids to message; !c0 = I32 [0]; !c10 = I32 [10]; !c1 = I32 [1]; !s = StreamRange(!c0, !c10, !c1) [1, False]; !s2 = StreamMap(!s) { (%elt) =>; !7 = Cast(%elt) [Int64]; MakeTuple(!7, %elt) [(0 1)] // map to stream of (uid, elt) pairs; }; !s3 = StreamMap(!s2) { (%elt2) =>; !8 = GetTupleElement(%elt2) [0]; %9 = RNGSplit(%6, !8) // append stream element uid to message; !c100 = I32 [100]; // call random function with current message/RNGState %9 and static uid 0; ApplySeeded(!c100, %9) [rand_int32, 0, Int32]; }; !10 = ToArray(!s3); InsertFields !entry (a: !10); }; ```; Note that because only 3 longs are added to the message, none of the `RNGSplit` calls generate any runtime code. They simply encode statically that the last block of the message at the time of the `ApplySeeded` call consists of the locals `[!3, !4, !8]`. Then the `ApplySeeded` just needs to pad the last block, xor it with the running sum (which is the encrypted static block, embedded as a constant in the code), and call the Threefry `encrypt` function just once. # Security; Cryptogrophers have developed a very pragmatic theory of what makes for ""good"" pseudorandomness. One of the benefits of using cryptographic primitives (even while weakening some of the components for performance, as we do with Threefish) is that we can use this framework to evaluate how well users can trust the outputs of the RNG. Using this theory for a quick sanity check, consider a pipeline with a 1e7 row by 1e7 column matrixtable, with 1e4 random function invocations per entry, running for a year on 1e23 cores. Let `b` be any boolean output of this pipeline. Let `P_1` and `P_2` be the probabilities that `b=1` in the scenarios where random functions are truly random, and using the above scheme, respectively. Then `abs(P_1 - P_2) < 3e-40`. The only assumption in this bound is that Threefry is a secure block cipher, i.e. that the best attack against it is a brute forc",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:12056,Security,encrypt,encrypt,12056,"MakeTuple(!3, !4) [(0 1)]; %6 = RNGSplit(!2, !5) // append row and col uids to message; !c0 = I32 [0]; !c10 = I32 [10]; !c1 = I32 [1]; !s = StreamRange(!c0, !c10, !c1) [1, False]; !s2 = StreamMap(!s) { (%elt) =>; !7 = Cast(%elt) [Int64]; MakeTuple(!7, %elt) [(0 1)] // map to stream of (uid, elt) pairs; }; !s3 = StreamMap(!s2) { (%elt2) =>; !8 = GetTupleElement(%elt2) [0]; %9 = RNGSplit(%6, !8) // append stream element uid to message; !c100 = I32 [100]; // call random function with current message/RNGState %9 and static uid 0; ApplySeeded(!c100, %9) [rand_int32, 0, Int32]; }; !10 = ToArray(!s3); InsertFields !entry (a: !10); }; ```; Note that because only 3 longs are added to the message, none of the `RNGSplit` calls generate any runtime code. They simply encode statically that the last block of the message at the time of the `ApplySeeded` call consists of the locals `[!3, !4, !8]`. Then the `ApplySeeded` just needs to pad the last block, xor it with the running sum (which is the encrypted static block, embedded as a constant in the code), and call the Threefry `encrypt` function just once. # Security; Cryptogrophers have developed a very pragmatic theory of what makes for ""good"" pseudorandomness. One of the benefits of using cryptographic primitives (even while weakening some of the components for performance, as we do with Threefish) is that we can use this framework to evaluate how well users can trust the outputs of the RNG. Using this theory for a quick sanity check, consider a pipeline with a 1e7 row by 1e7 column matrixtable, with 1e4 random function invocations per entry, running for a year on 1e23 cores. Let `b` be any boolean output of this pipeline. Let `P_1` and `P_2` be the probabilities that `b=1` in the scenarios where random functions are truly random, and using the above scheme, respectively. Then `abs(P_1 - P_2) < 3e-40`. The only assumption in this bound is that Threefry is a secure block cipher, i.e. that the best attack against it is a brute forc",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:12905,Security,secur,secure,12905,"Seeded` just needs to pad the last block, xor it with the running sum (which is the encrypted static block, embedded as a constant in the code), and call the Threefry `encrypt` function just once. # Security; Cryptogrophers have developed a very pragmatic theory of what makes for ""good"" pseudorandomness. One of the benefits of using cryptographic primitives (even while weakening some of the components for performance, as we do with Threefish) is that we can use this framework to evaluate how well users can trust the outputs of the RNG. Using this theory for a quick sanity check, consider a pipeline with a 1e7 row by 1e7 column matrixtable, with 1e4 random function invocations per entry, running for a year on 1e23 cores. Let `b` be any boolean output of this pipeline. Let `P_1` and `P_2` be the probabilities that `b=1` in the scenarios where random functions are truly random, and using the above scheme, respectively. Then `abs(P_1 - P_2) < 3e-40`. The only assumption in this bound is that Threefry is a secure block cipher, i.e. that the best attack against it is a brute force search of the space of all keys. The time bound comes from limiting how much of the key space the program is able to search. Clearly this will never be the weak link, and we can focus on how many random numbers are generated. This is a very practically reasurring result. It says that users can really trust that their results--interpreted under a model of true randomness--are not skewed by our implementation of pseudorandomness. # User interface; For the most part, users should not need to interact directly with the randomness infrastructure. If they don't, the default semantics are:; * Evaluating a hail expression multiple times in the same session always produces the same results; * Rebuilding an identical hail expression (e.g. `x = hl.rand_unif()` and `y = hl.rand_unif()`) evaluates with independent randomness.; * Running the same pipeline in multiple hail sessions uses independend randomness ",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:12945,Security,attack,attack,12945," encrypted static block, embedded as a constant in the code), and call the Threefry `encrypt` function just once. # Security; Cryptogrophers have developed a very pragmatic theory of what makes for ""good"" pseudorandomness. One of the benefits of using cryptographic primitives (even while weakening some of the components for performance, as we do with Threefish) is that we can use this framework to evaluate how well users can trust the outputs of the RNG. Using this theory for a quick sanity check, consider a pipeline with a 1e7 row by 1e7 column matrixtable, with 1e4 random function invocations per entry, running for a year on 1e23 cores. Let `b` be any boolean output of this pipeline. Let `P_1` and `P_2` be the probabilities that `b=1` in the scenarios where random functions are truly random, and using the above scheme, respectively. Then `abs(P_1 - P_2) < 3e-40`. The only assumption in this bound is that Threefry is a secure block cipher, i.e. that the best attack against it is a brute force search of the space of all keys. The time bound comes from limiting how much of the key space the program is able to search. Clearly this will never be the weak link, and we can focus on how many random numbers are generated. This is a very practically reasurring result. It says that users can really trust that their results--interpreted under a model of true randomness--are not skewed by our implementation of pseudorandomness. # User interface; For the most part, users should not need to interact directly with the randomness infrastructure. If they don't, the default semantics are:; * Evaluating a hail expression multiple times in the same session always produces the same results; * Rebuilding an identical hail expression (e.g. `x = hl.rand_unif()` and `y = hl.rand_unif()`) evaluates with independent randomness.; * Running the same pipeline in multiple hail sessions uses independend randomness each time. The last two can be overridden if needed:; * To build identical expressi",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:14491,Security,hash,hashing,14491,"obabilities that `b=1` in the scenarios where random functions are truly random, and using the above scheme, respectively. Then `abs(P_1 - P_2) < 3e-40`. The only assumption in this bound is that Threefry is a secure block cipher, i.e. that the best attack against it is a brute force search of the space of all keys. The time bound comes from limiting how much of the key space the program is able to search. Clearly this will never be the weak link, and we can focus on how many random numbers are generated. This is a very practically reasurring result. It says that users can really trust that their results--interpreted under a model of true randomness--are not skewed by our implementation of pseudorandomness. # User interface; For the most part, users should not need to interact directly with the randomness infrastructure. If they don't, the default semantics are:; * Evaluating a hail expression multiple times in the same session always produces the same results; * Rebuilding an identical hail expression (e.g. `x = hl.rand_unif()` and `y = hl.rand_unif()`) evaluates with independent randomness.; * Running the same pipeline in multiple hail sessions uses independend randomness each time. The last two can be overridden if needed:; * To build identical expressions using the same randomness, manually specify ""seeds"" (should we rename this?) on each random function call. E.g. `x = hl.rand_unif(seed=0)`. This overrides using the global counter to populate the static uid. It is guaranteed that user specified static uids never clash with automatically generated ones.; * To run the same pipeline in multiple sessions with the same randomness, manually specify the ""global seed"" on init: `hl.init(global_seed=0)`. [1] ""Splittable pseudorandom number generators using cryptographic hashing""; [2] ""The Skein Hash Function Family""; [3] ""Parallel random numbers: as easy as 1, 2, 3""; [4] Rogaway, ""Efficient Instantiations of Tweakable Blockciphers and Refinements to Modes OCB and PMAC""; ",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:1952,Usability,simpl,simpler,1952,"fferent practical constructions. We use the PMAC message authentication code, which depends on a tweakable block cipher, for which we use a reduced-round Threefish. Either or both of these pieces could be replaced with little effort, e.g. to improve performance. # Threefish/Threefry:; We use the Threefish [2] block cipher, modified to use 20 rounds for efficiency (the full Threefish4x64 uses 72 rounds), as suggested by [3] (although we make use of the Threefish tweak). Reference implementation is `Threefry.encrypt`. `threefish4x64` takes:; * key `K = (k_0, ..., k_3)`: 4 words; * tweak `T = (t_0, t_1)`: 2 words; * plaintext `P = (p_0, ..., p_3)`: 4 words. Intutively, this is a function taking a key and tweak as input, and returning a permutation on the space of all 256-bit blocks. The security claim is that if the key is chosen randomly, then for any choice of tweak, the resulting permutation ""looks like"" a uniformly chosen random permutation. Like most (all?) block ciphers, it is constructed as a sequence of simpler permutations. Think of shuffling a deck of cards: each shuffle isn't that random (is easily distinguishable from a completely random permutation), but a sequence of seven shuffles is indistinguishable from a random permutation. The simple permutations are called ""rounds"". Each round consists of applying a function ""Mix"" to pairs of 64-bit words, which is a bit-level permutation, followed by a permutation of the four words. <img width=""440"" alt=""threefish"" src=""https://user-images.githubusercontent.com/3430459/197852931-59bb6734-917b-4c4a-b3a2-7e2e9302a09f.png"">. ## key schedule; The key schedule turns the key and tweak into 6 subkeys, each 4 words. Subkey `s` is denoted `(k_{s,0}, ..., k_{s,3})`. First compute two additional words `k_4 = C ^ k_0 ^ k_1 ^ k_2 ^ k_3` and `t_2 = t0 ^ t_1`, where `C = 0x1BD11BDAA9FC1A22`. Then; ```; k_{s,0} = k_{s mod 5}; k_{s,1} = k_{s+1 mod 5} + t_{s mod 3}; k_{s,2} = k_{s+2 mod 5} + t_{s+1 mod 3}; k_{s,3} = k_{s+3 mod 5} + ",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:2192,Usability,simpl,simple,2192,"ry:; We use the Threefish [2] block cipher, modified to use 20 rounds for efficiency (the full Threefish4x64 uses 72 rounds), as suggested by [3] (although we make use of the Threefish tweak). Reference implementation is `Threefry.encrypt`. `threefish4x64` takes:; * key `K = (k_0, ..., k_3)`: 4 words; * tweak `T = (t_0, t_1)`: 2 words; * plaintext `P = (p_0, ..., p_3)`: 4 words. Intutively, this is a function taking a key and tweak as input, and returning a permutation on the space of all 256-bit blocks. The security claim is that if the key is chosen randomly, then for any choice of tweak, the resulting permutation ""looks like"" a uniformly chosen random permutation. Like most (all?) block ciphers, it is constructed as a sequence of simpler permutations. Think of shuffling a deck of cards: each shuffle isn't that random (is easily distinguishable from a completely random permutation), but a sequence of seven shuffles is indistinguishable from a random permutation. The simple permutations are called ""rounds"". Each round consists of applying a function ""Mix"" to pairs of 64-bit words, which is a bit-level permutation, followed by a permutation of the four words. <img width=""440"" alt=""threefish"" src=""https://user-images.githubusercontent.com/3430459/197852931-59bb6734-917b-4c4a-b3a2-7e2e9302a09f.png"">. ## key schedule; The key schedule turns the key and tweak into 6 subkeys, each 4 words. Subkey `s` is denoted `(k_{s,0}, ..., k_{s,3})`. First compute two additional words `k_4 = C ^ k_0 ^ k_1 ^ k_2 ^ k_3` and `t_2 = t0 ^ t_1`, where `C = 0x1BD11BDAA9FC1A22`. Then; ```; k_{s,0} = k_{s mod 5}; k_{s,1} = k_{s+1 mod 5} + t_{s mod 3}; k_{s,2} = k_{s+2 mod 5} + t_{s+1 mod 3}; k_{s,3} = k_{s+3 mod 5} + s; ```. ## an encryption round; Encryption is performed over 20 rounds. Let `v_i` be the `i`th word of the encryption state, initialized; ```; v_i = p_i; ```; Before round `d` if `d mod 4 = 0`, add subkey `s = d/4`; ```; v_i += k_{s,i}; ```; Then apply the `mix` function to adjace",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:4896,Usability,simpl,simplicity,4896,"sage tags (really a very large finite length). The security claim is that if the block cipher used ""looks like"" a random permutation, then the MAC ""looks like"" a random function. In particular, for each message `m`, `pmac(m)` looks like a stream of random bits, and for distinct messages `m1` and `m2`, `pmac(m1)` and `pmac(m2)` look like completely independent streams of random bits. Yet this is a deterministic function, so computing `pmac` on the same message always produces the same stream of bits. <img width=""654"" alt=""Screen Shot 2022-10-25 at 11 53 13 AM"" src=""https://user-images.githubusercontent.com/3430459/197853175-d96ea5b0-4618-4f6c-b92a-0dfbf5da7fe2.png"">. Many MAC constructions must process blocks sequentially. As we'll see below, this would add significant overhead to random number generation. PMAC has the property that blocks of the message can be processed in any order. In our case, we use a modification of the PMAC1 construction in [4]. We restrict the message length to multiples of 64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into blocks of 4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets red",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:5807,Usability,intuit,intuition,5807,"64-bits for simplicity. Our modified PMAC is a function `pmac(nonce: Long, staticID: Long, message: Array[Long], counter: Long)`, defined as follows (reference implementation `Threefry.pmac`):; * Form a block `M[-1] = [nonce, staticID, 0L, 0L]`.; * Split `message` into blocks of 4 longs each, `M[0], ..., M[d]`, allowing the last block to be shorter.; * Let `E[i] = encrypt(key, [i, 0L], M[i])`, for `i=-1..d-1`, *all but the last block*; * Let `E` be the xor of all `E[i]`; * If the last block is not full, let `B` be `M[d]` padded by a single `1L` followed by `0L`s, to 4 longs. Otherwise, let `B = M[d]`.; * Compute the hash `H = E ^ B`.; * If the last block was full, compute the final MAC tag as; * `T = encrypt(key, [-2, counter], H)`; * otherwise; * `T = encrypt(key, [-3, counter], H)`; ; The counter is used to enable generating long streams of random numbers for each message, not just a single 256 bit tag. The intuition is that each message (plus nonce and staticID) gets reduced to a 256 bit hash, such that distinct messages are highly unlikely to have the same hash. Then for each value of the counter, we use a distinct random function (really a random permutation) from the space of hashes to the space of random outputs. ## Lazy computation; In practice, we don't need to save entire messages in memory. Instead we compute the hash on the fly. The new type is `RNGState`. A value of this type consists of the data:; * `runningSum: IndexedSeq[Long]`: the xor of the encrypted contents of all full blocks; * `lastDynBlock: IndexedSeq[Long]`: the partial contents of the last block. The length of the sequence is `numWordsInLastDynBlock`; * `numWordsInLastDynBlock: Int`: the number of words (longs), in the range `[0, 4)`, currently contained in `lastDynBlock`; * `hasStaticSplit: Boolean`: whether the static block has been incorporated into `runningSum`; * `numDynBlocks: Int`: the number of completed blocks, not including the static block. This system is implemented using three ",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:8873,Usability,simpl,simply,8873,"n in the program. As long as each invocation gets a distinct message, the PMAC random function generates approximately independent randomness for each invocation. We fix a key for the block cipher once and for all. It was generated randomly, and is hard coded in the compiler. This saves us from issues of users specifying ""bad"" keys. Instead, we reserve a part of the message to encode a session scoped uid. By changing that uid between sessions, we allow running identical code repeatedly with independent randomness. ## Static UIDs; We split the message into static and dynamic components. The static component consists of two longs. The first, called the ""rng nonce"", is a hail session constant. It replaces the old ""global seed"", allowing the same pipeline to run with independent randomness each session, unless determinism is specifically requested. The second component is stored in the `ApplySeeded` IR node. We simply maintain a global counter, and increment it each time an `ApplySeeded` node is constructed, ensuring that each node in a pipeline has a distinct static uid. The dynamic component is needed to distinguish between different invocations of a single `ApplySeeded` node inside a looping construct. It is an arbitrary length message (though it will typically be quite small, probably less than 10 longs). It is constructed as follows:. ## Dynamic UIDs; Every stream, table, or matrix table pipeline is transformed to explicitly generate a unique uid per stream entry, table row, and matrix table row/column. These uids are explicit in the IR as ordinary values/fields, so the compiler automatically preserves the RNG determinism. ## Putting it all together; Consider the example pipeline; ```; mt = hl.utils.range_matrix_table(10, 10); mt = mt.annotate_entries(a=hl.range(10).map(lambda i: hl.rand_int32(100))); ```; Before elaborating UIDs in the IR in python, the IR looks like this (after a little tidying):; ```; !1 = MatrixRead [DropRowColUIDs, ...] // don't add uid fields;",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md:11736,Usability,simpl,simply,11736,"row, %entry) =>; !2 = RNGStateLiteral // RNGState corresponding to empty message; !3 = GetField(%row) [__row_uid] // get row and col uids; !4 = GetField(%col) [__col_uid]; !5 = MakeTuple(!3, !4) [(0 1)]; %6 = RNGSplit(!2, !5) // append row and col uids to message; !c0 = I32 [0]; !c10 = I32 [10]; !c1 = I32 [1]; !s = StreamRange(!c0, !c10, !c1) [1, False]; !s2 = StreamMap(!s) { (%elt) =>; !7 = Cast(%elt) [Int64]; MakeTuple(!7, %elt) [(0 1)] // map to stream of (uid, elt) pairs; }; !s3 = StreamMap(!s2) { (%elt2) =>; !8 = GetTupleElement(%elt2) [0]; %9 = RNGSplit(%6, !8) // append stream element uid to message; !c100 = I32 [100]; // call random function with current message/RNGState %9 and static uid 0; ApplySeeded(!c100, %9) [rand_int32, 0, Int32]; }; !10 = ToArray(!s3); InsertFields !entry (a: !10); }; ```; Note that because only 3 longs are added to the message, none of the `RNGSplit` calls generate any runtime code. They simply encode statically that the last block of the message at the time of the `ApplySeeded` call consists of the locals `[!3, !4, !8]`. Then the `ApplySeeded` just needs to pad the last block, xor it with the running sum (which is the encrypted static block, embedded as a constant in the code), and call the Threefry `encrypt` function just once. # Security; Cryptogrophers have developed a very pragmatic theory of what makes for ""good"" pseudorandomness. One of the benefits of using cryptographic primitives (even while weakening some of the components for performance, as we do with Threefish) is that we can use this framework to evaluate how well users can trust the outputs of the RNG. Using this theory for a quick sanity check, consider a pipeline with a 1e7 row by 1e7 column matrixtable, with 1e4 random function invocations per entry, running for a year on 1e23 cores. Let `b` be any boolean output of this pipeline. Let `P_1` and `P_2` be the probabilities that `b=1` in the scenarios where random functions are truly random, and using the above schem",MatchSource.DOCS,dev-docs/hail-query/randomness.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/randomness.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md:2136,Deployability,configurat,configuration,2136,"` to the list; of ""Authorized redirect URIs"". # Programmatic manipulation of OAuth 2.0 Client IDs. There is a [GitHub; issue](https://github.com/hashicorp/terraform-provider-google/issues/6074); explaining that Google does not provide a public API to manipulate OAuth 2.0; Client ID redirect URIs, much to everyone's chagrin. Google marked as fixed [an issue to create an; API](https://issuetracker.google.com/issues/116182848) for modifying OAuth 2.0; Client IDs even though all they did was provide a very limited API:; - https://cloud.google.com/iap/docs/reference/rest#rest-resource:-v1.projects.brands; - https://cloud.google.com/iap/docs/programmatic-oauth-clients. The `gcloud alpha iap oauth-clients list` command does not list our OAuth 2.0; Client ID. Presumably the type of client id that supports redirect URIs is; special and completely unsupported by this API. # Google IAM and Kubernetes Roles. In order to access the Kubernetes cluster, the extant developer should do the; following:. - Navigate in the GCP Console to IAM and grant the IAM `Kubernetes Engine Cluster Viewer` role.; This will allow the new developer's google account to authenticate with the; cluster using `kubectl` but it will not grant the new developer access to k8s resources.; - To grant the new developer access to their developer namespace, the extant; developer should apply the following configuration through `kubectl apply`. ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: dev-admin; namespace: <DEV_USERNAME>; rules:; - apiGroups: [""""]; resources: [""*""]; verbs: [""*""]; - apiGroups: [""apps""]; resources: [""*""]; verbs: [""*""]; - apiGroups: [""rbac.authorization.k8s.io""]; resources: [""*""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: <DEV_USERNAME>-dev-admin-binding; namespace: <DEV_USERNAME>; subjects:; - kind: User; name: <DEV_EMAIL>; namespace: <DEV_USERNAME>; roleRef:; kind: Role; name: dev-admin; apiGroup: """"; ```; ",MatchSource.DOCS,dev-docs/services/creating-a-developer-account.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md:2136,Modifiability,config,configuration,2136,"` to the list; of ""Authorized redirect URIs"". # Programmatic manipulation of OAuth 2.0 Client IDs. There is a [GitHub; issue](https://github.com/hashicorp/terraform-provider-google/issues/6074); explaining that Google does not provide a public API to manipulate OAuth 2.0; Client ID redirect URIs, much to everyone's chagrin. Google marked as fixed [an issue to create an; API](https://issuetracker.google.com/issues/116182848) for modifying OAuth 2.0; Client IDs even though all they did was provide a very limited API:; - https://cloud.google.com/iap/docs/reference/rest#rest-resource:-v1.projects.brands; - https://cloud.google.com/iap/docs/programmatic-oauth-clients. The `gcloud alpha iap oauth-clients list` command does not list our OAuth 2.0; Client ID. Presumably the type of client id that supports redirect URIs is; special and completely unsupported by this API. # Google IAM and Kubernetes Roles. In order to access the Kubernetes cluster, the extant developer should do the; following:. - Navigate in the GCP Console to IAM and grant the IAM `Kubernetes Engine Cluster Viewer` role.; This will allow the new developer's google account to authenticate with the; cluster using `kubectl` but it will not grant the new developer access to k8s resources.; - To grant the new developer access to their developer namespace, the extant; developer should apply the following configuration through `kubectl apply`. ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: dev-admin; namespace: <DEV_USERNAME>; rules:; - apiGroups: [""""]; resources: [""*""]; verbs: [""*""]; - apiGroups: [""apps""]; resources: [""*""]; verbs: [""*""]; - apiGroups: [""rbac.authorization.k8s.io""]; resources: [""*""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: <DEV_USERNAME>-dev-admin-binding; namespace: <DEV_USERNAME>; subjects:; - kind: User; name: <DEV_EMAIL>; namespace: <DEV_USERNAME>; roleRef:; kind: Role; name: dev-admin; apiGroup: """"; ```; ",MatchSource.DOCS,dev-docs/services/creating-a-developer-account.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md:901,Security,hash,hashicorp,901,"# Creating a Developer Account. Do *not* sign up for a developer account. The ""sign up"" link on ""auth.hail.is""; can only produce non-developer accounts. Instead, an extant developer must:. - Navigate to https://auth.hail.is/users and create the new user. Use their; Broad Institute email and username. Make sure you check the ""Developer""; checkbox and not the ""Service Account"" checkbox.; - Unfortunately it is impossible to automatically register an OAuth 2.0 redirect; URI for the new namespace. Instead:; - Navigate to https://console.developers.google.com/apis/credentials/?project=broad-ctsa; (nb: the `broad-ctsa` project, not `hail-vdc`).; - Click ""auth"" under ""OAuth 2.0 Client IDs"".; - Add `https://internal.hail.is/${USERNAME}/auth/oauth2callback` to the list; of ""Authorized redirect URIs"". # Programmatic manipulation of OAuth 2.0 Client IDs. There is a [GitHub; issue](https://github.com/hashicorp/terraform-provider-google/issues/6074); explaining that Google does not provide a public API to manipulate OAuth 2.0; Client ID redirect URIs, much to everyone's chagrin. Google marked as fixed [an issue to create an; API](https://issuetracker.google.com/issues/116182848) for modifying OAuth 2.0; Client IDs even though all they did was provide a very limited API:; - https://cloud.google.com/iap/docs/reference/rest#rest-resource:-v1.projects.brands; - https://cloud.google.com/iap/docs/programmatic-oauth-clients. The `gcloud alpha iap oauth-clients list` command does not list our OAuth 2.0; Client ID. Presumably the type of client id that supports redirect URIs is; special and completely unsupported by this API. # Google IAM and Kubernetes Roles. In order to access the Kubernetes cluster, the extant developer should do the; following:. - Navigate in the GCP Console to IAM and grant the IAM `Kubernetes Engine Cluster Viewer` role.; This will allow the new developer's google account to authenticate with the; cluster using `kubectl` but it will not grant the new developer access",MatchSource.DOCS,dev-docs/services/creating-a-developer-account.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md:1678,Security,access,access,1678,"ternal.hail.is/${USERNAME}/auth/oauth2callback` to the list; of ""Authorized redirect URIs"". # Programmatic manipulation of OAuth 2.0 Client IDs. There is a [GitHub; issue](https://github.com/hashicorp/terraform-provider-google/issues/6074); explaining that Google does not provide a public API to manipulate OAuth 2.0; Client ID redirect URIs, much to everyone's chagrin. Google marked as fixed [an issue to create an; API](https://issuetracker.google.com/issues/116182848) for modifying OAuth 2.0; Client IDs even though all they did was provide a very limited API:; - https://cloud.google.com/iap/docs/reference/rest#rest-resource:-v1.projects.brands; - https://cloud.google.com/iap/docs/programmatic-oauth-clients. The `gcloud alpha iap oauth-clients list` command does not list our OAuth 2.0; Client ID. Presumably the type of client id that supports redirect URIs is; special and completely unsupported by this API. # Google IAM and Kubernetes Roles. In order to access the Kubernetes cluster, the extant developer should do the; following:. - Navigate in the GCP Console to IAM and grant the IAM `Kubernetes Engine Cluster Viewer` role.; This will allow the new developer's google account to authenticate with the; cluster using `kubectl` but it will not grant the new developer access to k8s resources.; - To grant the new developer access to their developer namespace, the extant; developer should apply the following configuration through `kubectl apply`. ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: dev-admin; namespace: <DEV_USERNAME>; rules:; - apiGroups: [""""]; resources: [""*""]; verbs: [""*""]; - apiGroups: [""apps""]; resources: [""*""]; verbs: [""*""]; - apiGroups: [""rbac.authorization.k8s.io""]; resources: [""*""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: <DEV_USERNAME>-dev-admin-binding; namespace: <DEV_USERNAME>; subjects:; - kind: User; name: <DEV_EMAIL>; namespace: <DEV_USERNAME>; roleRef:; kin",MatchSource.DOCS,dev-docs/services/creating-a-developer-account.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md:1908,Security,authenticat,authenticate,1908,"` to the list; of ""Authorized redirect URIs"". # Programmatic manipulation of OAuth 2.0 Client IDs. There is a [GitHub; issue](https://github.com/hashicorp/terraform-provider-google/issues/6074); explaining that Google does not provide a public API to manipulate OAuth 2.0; Client ID redirect URIs, much to everyone's chagrin. Google marked as fixed [an issue to create an; API](https://issuetracker.google.com/issues/116182848) for modifying OAuth 2.0; Client IDs even though all they did was provide a very limited API:; - https://cloud.google.com/iap/docs/reference/rest#rest-resource:-v1.projects.brands; - https://cloud.google.com/iap/docs/programmatic-oauth-clients. The `gcloud alpha iap oauth-clients list` command does not list our OAuth 2.0; Client ID. Presumably the type of client id that supports redirect URIs is; special and completely unsupported by this API. # Google IAM and Kubernetes Roles. In order to access the Kubernetes cluster, the extant developer should do the; following:. - Navigate in the GCP Console to IAM and grant the IAM `Kubernetes Engine Cluster Viewer` role.; This will allow the new developer's google account to authenticate with the; cluster using `kubectl` but it will not grant the new developer access to k8s resources.; - To grant the new developer access to their developer namespace, the extant; developer should apply the following configuration through `kubectl apply`. ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: dev-admin; namespace: <DEV_USERNAME>; rules:; - apiGroups: [""""]; resources: [""*""]; verbs: [""*""]; - apiGroups: [""apps""]; resources: [""*""]; verbs: [""*""]; - apiGroups: [""rbac.authorization.k8s.io""]; resources: [""*""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: <DEV_USERNAME>-dev-admin-binding; namespace: <DEV_USERNAME>; subjects:; - kind: User; name: <DEV_EMAIL>; namespace: <DEV_USERNAME>; roleRef:; kind: Role; name: dev-admin; apiGroup: """"; ```; ",MatchSource.DOCS,dev-docs/services/creating-a-developer-account.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md:1995,Security,access,access,1995,"` to the list; of ""Authorized redirect URIs"". # Programmatic manipulation of OAuth 2.0 Client IDs. There is a [GitHub; issue](https://github.com/hashicorp/terraform-provider-google/issues/6074); explaining that Google does not provide a public API to manipulate OAuth 2.0; Client ID redirect URIs, much to everyone's chagrin. Google marked as fixed [an issue to create an; API](https://issuetracker.google.com/issues/116182848) for modifying OAuth 2.0; Client IDs even though all they did was provide a very limited API:; - https://cloud.google.com/iap/docs/reference/rest#rest-resource:-v1.projects.brands; - https://cloud.google.com/iap/docs/programmatic-oauth-clients. The `gcloud alpha iap oauth-clients list` command does not list our OAuth 2.0; Client ID. Presumably the type of client id that supports redirect URIs is; special and completely unsupported by this API. # Google IAM and Kubernetes Roles. In order to access the Kubernetes cluster, the extant developer should do the; following:. - Navigate in the GCP Console to IAM and grant the IAM `Kubernetes Engine Cluster Viewer` role.; This will allow the new developer's google account to authenticate with the; cluster using `kubectl` but it will not grant the new developer access to k8s resources.; - To grant the new developer access to their developer namespace, the extant; developer should apply the following configuration through `kubectl apply`. ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: dev-admin; namespace: <DEV_USERNAME>; rules:; - apiGroups: [""""]; resources: [""*""]; verbs: [""*""]; - apiGroups: [""apps""]; resources: [""*""]; verbs: [""*""]; - apiGroups: [""rbac.authorization.k8s.io""]; resources: [""*""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: <DEV_USERNAME>-dev-admin-binding; namespace: <DEV_USERNAME>; subjects:; - kind: User; name: <DEV_EMAIL>; namespace: <DEV_USERNAME>; roleRef:; kind: Role; name: dev-admin; apiGroup: """"; ```; ",MatchSource.DOCS,dev-docs/services/creating-a-developer-account.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md:2050,Security,access,access,2050,"` to the list; of ""Authorized redirect URIs"". # Programmatic manipulation of OAuth 2.0 Client IDs. There is a [GitHub; issue](https://github.com/hashicorp/terraform-provider-google/issues/6074); explaining that Google does not provide a public API to manipulate OAuth 2.0; Client ID redirect URIs, much to everyone's chagrin. Google marked as fixed [an issue to create an; API](https://issuetracker.google.com/issues/116182848) for modifying OAuth 2.0; Client IDs even though all they did was provide a very limited API:; - https://cloud.google.com/iap/docs/reference/rest#rest-resource:-v1.projects.brands; - https://cloud.google.com/iap/docs/programmatic-oauth-clients. The `gcloud alpha iap oauth-clients list` command does not list our OAuth 2.0; Client ID. Presumably the type of client id that supports redirect URIs is; special and completely unsupported by this API. # Google IAM and Kubernetes Roles. In order to access the Kubernetes cluster, the extant developer should do the; following:. - Navigate in the GCP Console to IAM and grant the IAM `Kubernetes Engine Cluster Viewer` role.; This will allow the new developer's google account to authenticate with the; cluster using `kubectl` but it will not grant the new developer access to k8s resources.; - To grant the new developer access to their developer namespace, the extant; developer should apply the following configuration through `kubectl apply`. ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: dev-admin; namespace: <DEV_USERNAME>; rules:; - apiGroups: [""""]; resources: [""*""]; verbs: [""*""]; - apiGroups: [""apps""]; resources: [""*""]; verbs: [""*""]; - apiGroups: [""rbac.authorization.k8s.io""]; resources: [""*""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: <DEV_USERNAME>-dev-admin-binding; namespace: <DEV_USERNAME>; subjects:; - kind: User; name: <DEV_EMAIL>; namespace: <DEV_USERNAME>; roleRef:; kind: Role; name: dev-admin; apiGroup: """"; ```; ",MatchSource.DOCS,dev-docs/services/creating-a-developer-account.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md:2213,Security,authoriz,authorization,2213,"` to the list; of ""Authorized redirect URIs"". # Programmatic manipulation of OAuth 2.0 Client IDs. There is a [GitHub; issue](https://github.com/hashicorp/terraform-provider-google/issues/6074); explaining that Google does not provide a public API to manipulate OAuth 2.0; Client ID redirect URIs, much to everyone's chagrin. Google marked as fixed [an issue to create an; API](https://issuetracker.google.com/issues/116182848) for modifying OAuth 2.0; Client IDs even though all they did was provide a very limited API:; - https://cloud.google.com/iap/docs/reference/rest#rest-resource:-v1.projects.brands; - https://cloud.google.com/iap/docs/programmatic-oauth-clients. The `gcloud alpha iap oauth-clients list` command does not list our OAuth 2.0; Client ID. Presumably the type of client id that supports redirect URIs is; special and completely unsupported by this API. # Google IAM and Kubernetes Roles. In order to access the Kubernetes cluster, the extant developer should do the; following:. - Navigate in the GCP Console to IAM and grant the IAM `Kubernetes Engine Cluster Viewer` role.; This will allow the new developer's google account to authenticate with the; cluster using `kubectl` but it will not grant the new developer access to k8s resources.; - To grant the new developer access to their developer namespace, the extant; developer should apply the following configuration through `kubectl apply`. ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: dev-admin; namespace: <DEV_USERNAME>; rules:; - apiGroups: [""""]; resources: [""*""]; verbs: [""*""]; - apiGroups: [""apps""]; resources: [""*""]; verbs: [""*""]; - apiGroups: [""rbac.authorization.k8s.io""]; resources: [""*""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: <DEV_USERNAME>-dev-admin-binding; namespace: <DEV_USERNAME>; subjects:; - kind: User; name: <DEV_EMAIL>; namespace: <DEV_USERNAME>; roleRef:; kind: Role; name: dev-admin; apiGroup: """"; ```; ",MatchSource.DOCS,dev-docs/services/creating-a-developer-account.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md:2427,Security,authoriz,authorization,2427,"` to the list; of ""Authorized redirect URIs"". # Programmatic manipulation of OAuth 2.0 Client IDs. There is a [GitHub; issue](https://github.com/hashicorp/terraform-provider-google/issues/6074); explaining that Google does not provide a public API to manipulate OAuth 2.0; Client ID redirect URIs, much to everyone's chagrin. Google marked as fixed [an issue to create an; API](https://issuetracker.google.com/issues/116182848) for modifying OAuth 2.0; Client IDs even though all they did was provide a very limited API:; - https://cloud.google.com/iap/docs/reference/rest#rest-resource:-v1.projects.brands; - https://cloud.google.com/iap/docs/programmatic-oauth-clients. The `gcloud alpha iap oauth-clients list` command does not list our OAuth 2.0; Client ID. Presumably the type of client id that supports redirect URIs is; special and completely unsupported by this API. # Google IAM and Kubernetes Roles. In order to access the Kubernetes cluster, the extant developer should do the; following:. - Navigate in the GCP Console to IAM and grant the IAM `Kubernetes Engine Cluster Viewer` role.; This will allow the new developer's google account to authenticate with the; cluster using `kubectl` but it will not grant the new developer access to k8s resources.; - To grant the new developer access to their developer namespace, the extant; developer should apply the following configuration through `kubectl apply`. ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: dev-admin; namespace: <DEV_USERNAME>; rules:; - apiGroups: [""""]; resources: [""*""]; verbs: [""*""]; - apiGroups: [""apps""]; resources: [""*""]; verbs: [""*""]; - apiGroups: [""rbac.authorization.k8s.io""]; resources: [""*""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: <DEV_USERNAME>-dev-admin-binding; namespace: <DEV_USERNAME>; subjects:; - kind: User; name: <DEV_EMAIL>; namespace: <DEV_USERNAME>; roleRef:; kind: Role; name: dev-admin; apiGroup: """"; ```; ",MatchSource.DOCS,dev-docs/services/creating-a-developer-account.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md:2524,Security,authoriz,authorization,2524,"` to the list; of ""Authorized redirect URIs"". # Programmatic manipulation of OAuth 2.0 Client IDs. There is a [GitHub; issue](https://github.com/hashicorp/terraform-provider-google/issues/6074); explaining that Google does not provide a public API to manipulate OAuth 2.0; Client ID redirect URIs, much to everyone's chagrin. Google marked as fixed [an issue to create an; API](https://issuetracker.google.com/issues/116182848) for modifying OAuth 2.0; Client IDs even though all they did was provide a very limited API:; - https://cloud.google.com/iap/docs/reference/rest#rest-resource:-v1.projects.brands; - https://cloud.google.com/iap/docs/programmatic-oauth-clients. The `gcloud alpha iap oauth-clients list` command does not list our OAuth 2.0; Client ID. Presumably the type of client id that supports redirect URIs is; special and completely unsupported by this API. # Google IAM and Kubernetes Roles. In order to access the Kubernetes cluster, the extant developer should do the; following:. - Navigate in the GCP Console to IAM and grant the IAM `Kubernetes Engine Cluster Viewer` role.; This will allow the new developer's google account to authenticate with the; cluster using `kubectl` but it will not grant the new developer access to k8s resources.; - To grant the new developer access to their developer namespace, the extant; developer should apply the following configuration through `kubectl apply`. ```yaml; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: dev-admin; namespace: <DEV_USERNAME>; rules:; - apiGroups: [""""]; resources: [""*""]; verbs: [""*""]; - apiGroups: [""apps""]; resources: [""*""]; verbs: [""*""]; - apiGroups: [""rbac.authorization.k8s.io""]; resources: [""*""]; verbs: [""*""]; ---; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: <DEV_USERNAME>-dev-admin-binding; namespace: <DEV_USERNAME>; subjects:; - kind: User; name: <DEV_EMAIL>; namespace: <DEV_USERNAME>; roleRef:; kind: Role; name: dev-admin; apiGroup: """"; ```; ",MatchSource.DOCS,dev-docs/services/creating-a-developer-account.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/creating-a-developer-account.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md:1717,Deployability,update,updates,1717,"tch workers to the services in Kubernetes. These reverse proxies/load balancers handle traffic routing to the appropriate; namespace/service, manage TLS, perform additional authorization checks for non-prod; namespaces, and enforce rate limits.; Our reverse proxy of choice is [Envoy](https://www.envoyproxy.io/). The general routing rules for the gateways are as follows (Kubernetes DNS provides addresses; for `Service`s in the form of `<service>.<namespace>.svc.cluster.local`):. ### Gateway; - `<service>.hail.is/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail.is/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`[^1]. [^1]: At time of writing, developers cannot currently sign in to PR namespaces through the; browser because they are not assigned a callback for GCP/Azure OAuth flows. ### Internal Gateway; - `<service>.hail/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`. For Envoy to properly pool connections to K8s services, it needs to know; which ""clusters"" (services) exist at any point in time. This list is static for; production services, but PR namespaces are ephemeral and are; created/destroyed by CI many times per day. In order to notify the gateways; of new namespaces/services, CI tracks which namespaces are active and periodically; updates a K8s `ConfigMap` with fresh Envoy configuration. The gateways, using the; [Envoy xDS API](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/dynamic_configuration#xds-configuration-api-overview); can dynamically load this new configuration as it changes without dropping existing traffic.; You can see CI's current view of the cluster's namespaces/services at ci.hail.is/namespaces; and can inspect the current Envoy config at ci.hail.is/envoy-config/gateway and; ci.hail.is/envoy-config/internal-gateway.; ",MatchSource.DOCS,dev-docs/services/gateways.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md:1760,Deployability,configurat,configuration,1760,"tch workers to the services in Kubernetes. These reverse proxies/load balancers handle traffic routing to the appropriate; namespace/service, manage TLS, perform additional authorization checks for non-prod; namespaces, and enforce rate limits.; Our reverse proxy of choice is [Envoy](https://www.envoyproxy.io/). The general routing rules for the gateways are as follows (Kubernetes DNS provides addresses; for `Service`s in the form of `<service>.<namespace>.svc.cluster.local`):. ### Gateway; - `<service>.hail.is/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail.is/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`[^1]. [^1]: At time of writing, developers cannot currently sign in to PR namespaces through the; browser because they are not assigned a callback for GCP/Azure OAuth flows. ### Internal Gateway; - `<service>.hail/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`. For Envoy to properly pool connections to K8s services, it needs to know; which ""clusters"" (services) exist at any point in time. This list is static for; production services, but PR namespaces are ephemeral and are; created/destroyed by CI many times per day. In order to notify the gateways; of new namespaces/services, CI tracks which namespaces are active and periodically; updates a K8s `ConfigMap` with fresh Envoy configuration. The gateways, using the; [Envoy xDS API](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/dynamic_configuration#xds-configuration-api-overview); can dynamically load this new configuration as it changes without dropping existing traffic.; You can see CI's current view of the cluster's namespaces/services at ci.hail.is/namespaces; and can inspect the current Envoy config at ci.hail.is/envoy-config/gateway and; ci.hail.is/envoy-config/internal-gateway.; ",MatchSource.DOCS,dev-docs/services/gateways.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md:1917,Deployability,configurat,configuration-api-overview,1917,"tch workers to the services in Kubernetes. These reverse proxies/load balancers handle traffic routing to the appropriate; namespace/service, manage TLS, perform additional authorization checks for non-prod; namespaces, and enforce rate limits.; Our reverse proxy of choice is [Envoy](https://www.envoyproxy.io/). The general routing rules for the gateways are as follows (Kubernetes DNS provides addresses; for `Service`s in the form of `<service>.<namespace>.svc.cluster.local`):. ### Gateway; - `<service>.hail.is/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail.is/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`[^1]. [^1]: At time of writing, developers cannot currently sign in to PR namespaces through the; browser because they are not assigned a callback for GCP/Azure OAuth flows. ### Internal Gateway; - `<service>.hail/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`. For Envoy to properly pool connections to K8s services, it needs to know; which ""clusters"" (services) exist at any point in time. This list is static for; production services, but PR namespaces are ephemeral and are; created/destroyed by CI many times per day. In order to notify the gateways; of new namespaces/services, CI tracks which namespaces are active and periodically; updates a K8s `ConfigMap` with fresh Envoy configuration. The gateways, using the; [Envoy xDS API](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/dynamic_configuration#xds-configuration-api-overview); can dynamically load this new configuration as it changes without dropping existing traffic.; You can see CI's current view of the cluster's namespaces/services at ci.hail.is/namespaces; and can inspect the current Envoy config at ci.hail.is/envoy-config/gateway and; ci.hail.is/envoy-config/internal-gateway.; ",MatchSource.DOCS,dev-docs/services/gateways.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md:1976,Deployability,configurat,configuration,1976,"tch workers to the services in Kubernetes. These reverse proxies/load balancers handle traffic routing to the appropriate; namespace/service, manage TLS, perform additional authorization checks for non-prod; namespaces, and enforce rate limits.; Our reverse proxy of choice is [Envoy](https://www.envoyproxy.io/). The general routing rules for the gateways are as follows (Kubernetes DNS provides addresses; for `Service`s in the form of `<service>.<namespace>.svc.cluster.local`):. ### Gateway; - `<service>.hail.is/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail.is/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`[^1]. [^1]: At time of writing, developers cannot currently sign in to PR namespaces through the; browser because they are not assigned a callback for GCP/Azure OAuth flows. ### Internal Gateway; - `<service>.hail/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`. For Envoy to properly pool connections to K8s services, it needs to know; which ""clusters"" (services) exist at any point in time. This list is static for; production services, but PR namespaces are ephemeral and are; created/destroyed by CI many times per day. In order to notify the gateways; of new namespaces/services, CI tracks which namespaces are active and periodically; updates a K8s `ConfigMap` with fresh Envoy configuration. The gateways, using the; [Envoy xDS API](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/dynamic_configuration#xds-configuration-api-overview); can dynamically load this new configuration as it changes without dropping existing traffic.; You can see CI's current view of the cluster's namespaces/services at ci.hail.is/namespaces; and can inspect the current Envoy config at ci.hail.is/envoy-config/gateway and; ci.hail.is/envoy-config/internal-gateway.; ",MatchSource.DOCS,dev-docs/services/gateways.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md:352,Integrability,rout,routing,352,"# Overview of the Batch Control Plane External and Internal Load Balancers. Traffic flows into the Kubernetes cluster through two points of ingress: `gateway`,; which receives traffic from the internet, and `internal-gateway`, which manages traffic; from batch workers to the services in Kubernetes. These reverse proxies/load balancers handle traffic routing to the appropriate; namespace/service, manage TLS, perform additional authorization checks for non-prod; namespaces, and enforce rate limits.; Our reverse proxy of choice is [Envoy](https://www.envoyproxy.io/). The general routing rules for the gateways are as follows (Kubernetes DNS provides addresses; for `Service`s in the form of `<service>.<namespace>.svc.cluster.local`):. ### Gateway; - `<service>.hail.is/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail.is/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`[^1]. [^1]: At time of writing, developers cannot currently sign in to PR namespaces through the; browser because they are not assigned a callback for GCP/Azure OAuth flows. ### Internal Gateway; - `<service>.hail/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`. For Envoy to properly pool connections to K8s services, it needs to know; which ""clusters"" (services) exist at any point in time. This list is static for; production services, but PR namespaces are ephemeral and are; created/destroyed by CI many times per day. In order to notify the gateways; of new namespaces/services, CI tracks which namespaces are active and periodically; updates a K8s `ConfigMap` with fresh Envoy configuration. The gateways, using the; [Envoy xDS API](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/dynamic_configuration#xds-configuration-api-overview); can dynamically load this new configuration as it chang",MatchSource.DOCS,dev-docs/services/gateways.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md:583,Integrability,rout,routing,583,"# Overview of the Batch Control Plane External and Internal Load Balancers. Traffic flows into the Kubernetes cluster through two points of ingress: `gateway`,; which receives traffic from the internet, and `internal-gateway`, which manages traffic; from batch workers to the services in Kubernetes. These reverse proxies/load balancers handle traffic routing to the appropriate; namespace/service, manage TLS, perform additional authorization checks for non-prod; namespaces, and enforce rate limits.; Our reverse proxy of choice is [Envoy](https://www.envoyproxy.io/). The general routing rules for the gateways are as follows (Kubernetes DNS provides addresses; for `Service`s in the form of `<service>.<namespace>.svc.cluster.local`):. ### Gateway; - `<service>.hail.is/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail.is/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`[^1]. [^1]: At time of writing, developers cannot currently sign in to PR namespaces through the; browser because they are not assigned a callback for GCP/Azure OAuth flows. ### Internal Gateway; - `<service>.hail/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`. For Envoy to properly pool connections to K8s services, it needs to know; which ""clusters"" (services) exist at any point in time. This list is static for; production services, but PR namespaces are ephemeral and are; created/destroyed by CI many times per day. In order to notify the gateways; of new namespaces/services, CI tracks which namespaces are active and periodically; updates a K8s `ConfigMap` with fresh Envoy configuration. The gateways, using the; [Envoy xDS API](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/dynamic_configuration#xds-configuration-api-overview); can dynamically load this new configuration as it chang",MatchSource.DOCS,dev-docs/services/gateways.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md:1760,Modifiability,config,configuration,1760,"tch workers to the services in Kubernetes. These reverse proxies/load balancers handle traffic routing to the appropriate; namespace/service, manage TLS, perform additional authorization checks for non-prod; namespaces, and enforce rate limits.; Our reverse proxy of choice is [Envoy](https://www.envoyproxy.io/). The general routing rules for the gateways are as follows (Kubernetes DNS provides addresses; for `Service`s in the form of `<service>.<namespace>.svc.cluster.local`):. ### Gateway; - `<service>.hail.is/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail.is/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`[^1]. [^1]: At time of writing, developers cannot currently sign in to PR namespaces through the; browser because they are not assigned a callback for GCP/Azure OAuth flows. ### Internal Gateway; - `<service>.hail/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`. For Envoy to properly pool connections to K8s services, it needs to know; which ""clusters"" (services) exist at any point in time. This list is static for; production services, but PR namespaces are ephemeral and are; created/destroyed by CI many times per day. In order to notify the gateways; of new namespaces/services, CI tracks which namespaces are active and periodically; updates a K8s `ConfigMap` with fresh Envoy configuration. The gateways, using the; [Envoy xDS API](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/dynamic_configuration#xds-configuration-api-overview); can dynamically load this new configuration as it changes without dropping existing traffic.; You can see CI's current view of the cluster's namespaces/services at ci.hail.is/namespaces; and can inspect the current Envoy config at ci.hail.is/envoy-config/gateway and; ci.hail.is/envoy-config/internal-gateway.; ",MatchSource.DOCS,dev-docs/services/gateways.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md:1917,Modifiability,config,configuration-api-overview,1917,"tch workers to the services in Kubernetes. These reverse proxies/load balancers handle traffic routing to the appropriate; namespace/service, manage TLS, perform additional authorization checks for non-prod; namespaces, and enforce rate limits.; Our reverse proxy of choice is [Envoy](https://www.envoyproxy.io/). The general routing rules for the gateways are as follows (Kubernetes DNS provides addresses; for `Service`s in the form of `<service>.<namespace>.svc.cluster.local`):. ### Gateway; - `<service>.hail.is/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail.is/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`[^1]. [^1]: At time of writing, developers cannot currently sign in to PR namespaces through the; browser because they are not assigned a callback for GCP/Azure OAuth flows. ### Internal Gateway; - `<service>.hail/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`. For Envoy to properly pool connections to K8s services, it needs to know; which ""clusters"" (services) exist at any point in time. This list is static for; production services, but PR namespaces are ephemeral and are; created/destroyed by CI many times per day. In order to notify the gateways; of new namespaces/services, CI tracks which namespaces are active and periodically; updates a K8s `ConfigMap` with fresh Envoy configuration. The gateways, using the; [Envoy xDS API](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/dynamic_configuration#xds-configuration-api-overview); can dynamically load this new configuration as it changes without dropping existing traffic.; You can see CI's current view of the cluster's namespaces/services at ci.hail.is/namespaces; and can inspect the current Envoy config at ci.hail.is/envoy-config/gateway and; ci.hail.is/envoy-config/internal-gateway.; ",MatchSource.DOCS,dev-docs/services/gateways.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md:1976,Modifiability,config,configuration,1976,"tch workers to the services in Kubernetes. These reverse proxies/load balancers handle traffic routing to the appropriate; namespace/service, manage TLS, perform additional authorization checks for non-prod; namespaces, and enforce rate limits.; Our reverse proxy of choice is [Envoy](https://www.envoyproxy.io/). The general routing rules for the gateways are as follows (Kubernetes DNS provides addresses; for `Service`s in the form of `<service>.<namespace>.svc.cluster.local`):. ### Gateway; - `<service>.hail.is/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail.is/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`[^1]. [^1]: At time of writing, developers cannot currently sign in to PR namespaces through the; browser because they are not assigned a callback for GCP/Azure OAuth flows. ### Internal Gateway; - `<service>.hail/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`. For Envoy to properly pool connections to K8s services, it needs to know; which ""clusters"" (services) exist at any point in time. This list is static for; production services, but PR namespaces are ephemeral and are; created/destroyed by CI many times per day. In order to notify the gateways; of new namespaces/services, CI tracks which namespaces are active and periodically; updates a K8s `ConfigMap` with fresh Envoy configuration. The gateways, using the; [Envoy xDS API](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/dynamic_configuration#xds-configuration-api-overview); can dynamically load this new configuration as it changes without dropping existing traffic.; You can see CI's current view of the cluster's namespaces/services at ci.hail.is/namespaces; and can inspect the current Envoy config at ci.hail.is/envoy-config/gateway and; ci.hail.is/envoy-config/internal-gateway.; ",MatchSource.DOCS,dev-docs/services/gateways.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md:2167,Modifiability,config,config,2167,"tch workers to the services in Kubernetes. These reverse proxies/load balancers handle traffic routing to the appropriate; namespace/service, manage TLS, perform additional authorization checks for non-prod; namespaces, and enforce rate limits.; Our reverse proxy of choice is [Envoy](https://www.envoyproxy.io/). The general routing rules for the gateways are as follows (Kubernetes DNS provides addresses; for `Service`s in the form of `<service>.<namespace>.svc.cluster.local`):. ### Gateway; - `<service>.hail.is/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail.is/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`[^1]. [^1]: At time of writing, developers cannot currently sign in to PR namespaces through the; browser because they are not assigned a callback for GCP/Azure OAuth flows. ### Internal Gateway; - `<service>.hail/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`. For Envoy to properly pool connections to K8s services, it needs to know; which ""clusters"" (services) exist at any point in time. This list is static for; production services, but PR namespaces are ephemeral and are; created/destroyed by CI many times per day. In order to notify the gateways; of new namespaces/services, CI tracks which namespaces are active and periodically; updates a K8s `ConfigMap` with fresh Envoy configuration. The gateways, using the; [Envoy xDS API](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/dynamic_configuration#xds-configuration-api-overview); can dynamically load this new configuration as it changes without dropping existing traffic.; You can see CI's current view of the cluster's namespaces/services at ci.hail.is/namespaces; and can inspect the current Envoy config at ci.hail.is/envoy-config/gateway and; ci.hail.is/envoy-config/internal-gateway.; ",MatchSource.DOCS,dev-docs/services/gateways.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md:2194,Modifiability,config,config,2194,"tch workers to the services in Kubernetes. These reverse proxies/load balancers handle traffic routing to the appropriate; namespace/service, manage TLS, perform additional authorization checks for non-prod; namespaces, and enforce rate limits.; Our reverse proxy of choice is [Envoy](https://www.envoyproxy.io/). The general routing rules for the gateways are as follows (Kubernetes DNS provides addresses; for `Service`s in the form of `<service>.<namespace>.svc.cluster.local`):. ### Gateway; - `<service>.hail.is/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail.is/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`[^1]. [^1]: At time of writing, developers cannot currently sign in to PR namespaces through the; browser because they are not assigned a callback for GCP/Azure OAuth flows. ### Internal Gateway; - `<service>.hail/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`. For Envoy to properly pool connections to K8s services, it needs to know; which ""clusters"" (services) exist at any point in time. This list is static for; production services, but PR namespaces are ephemeral and are; created/destroyed by CI many times per day. In order to notify the gateways; of new namespaces/services, CI tracks which namespaces are active and periodically; updates a K8s `ConfigMap` with fresh Envoy configuration. The gateways, using the; [Envoy xDS API](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/dynamic_configuration#xds-configuration-api-overview); can dynamically load this new configuration as it changes without dropping existing traffic.; You can see CI's current view of the cluster's namespaces/services at ci.hail.is/namespaces; and can inspect the current Envoy config at ci.hail.is/envoy-config/gateway and; ci.hail.is/envoy-config/internal-gateway.; ",MatchSource.DOCS,dev-docs/services/gateways.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md:2231,Modifiability,config,config,2231,"tch workers to the services in Kubernetes. These reverse proxies/load balancers handle traffic routing to the appropriate; namespace/service, manage TLS, perform additional authorization checks for non-prod; namespaces, and enforce rate limits.; Our reverse proxy of choice is [Envoy](https://www.envoyproxy.io/). The general routing rules for the gateways are as follows (Kubernetes DNS provides addresses; for `Service`s in the form of `<service>.<namespace>.svc.cluster.local`):. ### Gateway; - `<service>.hail.is/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail.is/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`[^1]. [^1]: At time of writing, developers cannot currently sign in to PR namespaces through the; browser because they are not assigned a callback for GCP/Azure OAuth flows. ### Internal Gateway; - `<service>.hail/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`. For Envoy to properly pool connections to K8s services, it needs to know; which ""clusters"" (services) exist at any point in time. This list is static for; production services, but PR namespaces are ephemeral and are; created/destroyed by CI many times per day. In order to notify the gateways; of new namespaces/services, CI tracks which namespaces are active and periodically; updates a K8s `ConfigMap` with fresh Envoy configuration. The gateways, using the; [Envoy xDS API](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/dynamic_configuration#xds-configuration-api-overview); can dynamically load this new configuration as it changes without dropping existing traffic.; You can see CI's current view of the cluster's namespaces/services at ci.hail.is/namespaces; and can inspect the current Envoy config at ci.hail.is/envoy-config/gateway and; ci.hail.is/envoy-config/internal-gateway.; ",MatchSource.DOCS,dev-docs/services/gateways.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md:322,Performance,load,load,322,"# Overview of the Batch Control Plane External and Internal Load Balancers. Traffic flows into the Kubernetes cluster through two points of ingress: `gateway`,; which receives traffic from the internet, and `internal-gateway`, which manages traffic; from batch workers to the services in Kubernetes. These reverse proxies/load balancers handle traffic routing to the appropriate; namespace/service, manage TLS, perform additional authorization checks for non-prod; namespaces, and enforce rate limits.; Our reverse proxy of choice is [Envoy](https://www.envoyproxy.io/). The general routing rules for the gateways are as follows (Kubernetes DNS provides addresses; for `Service`s in the form of `<service>.<namespace>.svc.cluster.local`):. ### Gateway; - `<service>.hail.is/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail.is/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`[^1]. [^1]: At time of writing, developers cannot currently sign in to PR namespaces through the; browser because they are not assigned a callback for GCP/Azure OAuth flows. ### Internal Gateway; - `<service>.hail/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`. For Envoy to properly pool connections to K8s services, it needs to know; which ""clusters"" (services) exist at any point in time. This list is static for; production services, but PR namespaces are ephemeral and are; created/destroyed by CI many times per day. In order to notify the gateways; of new namespaces/services, CI tracks which namespaces are active and periodically; updates a K8s `ConfigMap` with fresh Envoy configuration. The gateways, using the; [Envoy xDS API](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/dynamic_configuration#xds-configuration-api-overview); can dynamically load this new configuration as it chang",MatchSource.DOCS,dev-docs/services/gateways.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md:411,Performance,perform,perform,411,"# Overview of the Batch Control Plane External and Internal Load Balancers. Traffic flows into the Kubernetes cluster through two points of ingress: `gateway`,; which receives traffic from the internet, and `internal-gateway`, which manages traffic; from batch workers to the services in Kubernetes. These reverse proxies/load balancers handle traffic routing to the appropriate; namespace/service, manage TLS, perform additional authorization checks for non-prod; namespaces, and enforce rate limits.; Our reverse proxy of choice is [Envoy](https://www.envoyproxy.io/). The general routing rules for the gateways are as follows (Kubernetes DNS provides addresses; for `Service`s in the form of `<service>.<namespace>.svc.cluster.local`):. ### Gateway; - `<service>.hail.is/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail.is/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`[^1]. [^1]: At time of writing, developers cannot currently sign in to PR namespaces through the; browser because they are not assigned a callback for GCP/Azure OAuth flows. ### Internal Gateway; - `<service>.hail/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`. For Envoy to properly pool connections to K8s services, it needs to know; which ""clusters"" (services) exist at any point in time. This list is static for; production services, but PR namespaces are ephemeral and are; created/destroyed by CI many times per day. In order to notify the gateways; of new namespaces/services, CI tracks which namespaces are active and periodically; updates a K8s `ConfigMap` with fresh Envoy configuration. The gateways, using the; [Envoy xDS API](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/dynamic_configuration#xds-configuration-api-overview); can dynamically load this new configuration as it chang",MatchSource.DOCS,dev-docs/services/gateways.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md:1962,Performance,load,load,1962,"tch workers to the services in Kubernetes. These reverse proxies/load balancers handle traffic routing to the appropriate; namespace/service, manage TLS, perform additional authorization checks for non-prod; namespaces, and enforce rate limits.; Our reverse proxy of choice is [Envoy](https://www.envoyproxy.io/). The general routing rules for the gateways are as follows (Kubernetes DNS provides addresses; for `Service`s in the form of `<service>.<namespace>.svc.cluster.local`):. ### Gateway; - `<service>.hail.is/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail.is/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`[^1]. [^1]: At time of writing, developers cannot currently sign in to PR namespaces through the; browser because they are not assigned a callback for GCP/Azure OAuth flows. ### Internal Gateway; - `<service>.hail/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`. For Envoy to properly pool connections to K8s services, it needs to know; which ""clusters"" (services) exist at any point in time. This list is static for; production services, but PR namespaces are ephemeral and are; created/destroyed by CI many times per day. In order to notify the gateways; of new namespaces/services, CI tracks which namespaces are active and periodically; updates a K8s `ConfigMap` with fresh Envoy configuration. The gateways, using the; [Envoy xDS API](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/dynamic_configuration#xds-configuration-api-overview); can dynamically load this new configuration as it changes without dropping existing traffic.; You can see CI's current view of the cluster's namespaces/services at ci.hail.is/namespaces; and can inspect the current Envoy config at ci.hail.is/envoy-config/gateway and; ci.hail.is/envoy-config/internal-gateway.; ",MatchSource.DOCS,dev-docs/services/gateways.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md:430,Security,authoriz,authorization,430,"# Overview of the Batch Control Plane External and Internal Load Balancers. Traffic flows into the Kubernetes cluster through two points of ingress: `gateway`,; which receives traffic from the internet, and `internal-gateway`, which manages traffic; from batch workers to the services in Kubernetes. These reverse proxies/load balancers handle traffic routing to the appropriate; namespace/service, manage TLS, perform additional authorization checks for non-prod; namespaces, and enforce rate limits.; Our reverse proxy of choice is [Envoy](https://www.envoyproxy.io/). The general routing rules for the gateways are as follows (Kubernetes DNS provides addresses; for `Service`s in the form of `<service>.<namespace>.svc.cluster.local`):. ### Gateway; - `<service>.hail.is/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail.is/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`[^1]. [^1]: At time of writing, developers cannot currently sign in to PR namespaces through the; browser because they are not assigned a callback for GCP/Azure OAuth flows. ### Internal Gateway; - `<service>.hail/<path> => <service>.default.svc.cluster.local/<path>`; - `internal.hail/<dev-or-pr>/<service>/<path> => <service>.<dev-or-pr>.svc.cluster.local/<developer>/<service>/<path>`. For Envoy to properly pool connections to K8s services, it needs to know; which ""clusters"" (services) exist at any point in time. This list is static for; production services, but PR namespaces are ephemeral and are; created/destroyed by CI many times per day. In order to notify the gateways; of new namespaces/services, CI tracks which namespaces are active and periodically; updates a K8s `ConfigMap` with fresh Envoy configuration. The gateways, using the; [Envoy xDS API](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/operations/dynamic_configuration#xds-configuration-api-overview); can dynamically load this new configuration as it chang",MatchSource.DOCS,dev-docs/services/gateways.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/gateways.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:1875,Availability,redundant,redundant,1875," 1 ensures that all keys stored in k8s secrets are no more than two months old,; while Step 2 ensures that any key that is not in use is deleted.; Step 2 does **not** act on service accounts that just underwent Step 1 because; the old keys might still be in use by active pods/jobs. We assume that no; pod/job will run for longer than 30 days. We consider the ""active"" key for a service account to be the key stored in a; k8s secret. If no secret is associated with a service account, we take the active; key to be the most recently created user-managed[^1] key. We then consider; service accounts to be in one of four states:. - Expired: The active key in Kubernetes is older than 90 days and should be; rotated immediately.; - In Progress: The active key was created in the past 30 days and there exist; old keys that may still be in use.; - Ready for Delete: The active key was created more than 30 days ago and there; exist old keys that can be safely deleted.; - Up to Date: The active key is valid and there are no redundant keys. ## Rotating keys. Make sure that you are first authenticated with the correct GCP project by; running `gcpsetcluster <PROJECT>` (you need to have sourced; `$HAIL/devbin/functions.sh` to run this command). Then run. ```; python3 $HAIL/devbin/rotate_keys.py <PROJECT>; ```. Don't worry, this won't do anything scary on its own!. The script's initial output shows the following:; - Which GSA key secrets in Kubernetes belong to which Google service accounts.; - Which secrets have no corresponding service account. This likely means; the original service account was deleted without deleting its secret.; If this occurs, the secret should probably be deleted, but check to ensure that; nothing still depends on it first.; - Which service accounts have no corresponding key secret in Kubernetes. This; can be OK, such as with the terraform service account. However, any user; accounts should not be in this category, and it might indicate that they; weren't properly ",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:3363,Availability,redundant,redundant,3363,"responding service account. This likely means; the original service account was deleted without deleting its secret.; If this occurs, the secret should probably be deleted, but check to ensure that; nothing still depends on it first.; - Which service accounts have no corresponding key secret in Kubernetes. This; can be OK, such as with the terraform service account. However, any user; accounts should not be in this category, and it might indicate that they; weren't properly deleted. Each service account is listed with its rotation state. NOTE: `rotate_keys.py` *only* checks secrets with a `key.json` field. This; is an invariant of `auth` and `terraform`-created accounts, but might not catch; legacy secrets. Additionally, the script will warn of any service accounts with more than 1; corresponding key secret per namespace. This should not happen with the; exception of the test user, which is used in dev namespaces instead of other; robot service accounts. There should be no redundant secrets in the default; namespace. ### Updating keys. The `update` flow steps through each service account and lists its GSA keys,; sorted in reverse order of their date of creation.; Any keys that are found in Kubernetes will list the name and namespace; of the corresponding secret at the end of the row. Developer and test accounts; might have secrets in multiple namespaces (default and dev / test), but user; accounts should only have secrets present in `default`. The script will prompt; for each service account if you would like to create a new key. Enter `yes`; and the script will create a new GSA key and update any relevant secrets. Any; other input will do nothing. The output should then show the updated key at; the top of the list and all Kubernetes secrets pointing to the new key.; The script refreshes its Kubernetes secrets after updating them so this should; accurately reflect the state of the cluster.; Continue until all service/user keys have been updated. ### Deleting keys. R",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:683,Deployability,update,update,683,"# GSA Key Rotation. Every identity in batch, user or robot, has a corresponding Google Service; Account (GSA). A service or user job can authenticate with Google as; a specific service account with a Service Account Key. When a new user is; created, `auth` creates a service account, requests a key for that account, and; stores the key as a Kubernetes secret with the name `<username>-gsa-key`. Service account keys are confidential and should be rotated at least every 90; days to mitigate the danger of attack if a key is leaked. The key rotation; strategy consists of two parts:. 1. For each Google service account whose newest key is at least 60 days old,; create a new key and update the Kubernetes secret to reflect the new value.; 2. For each Google service account whose newest key is older than 30 days old,; delete all but the newest key. Step 1 ensures that all keys stored in k8s secrets are no more than two months old,; while Step 2 ensures that any key that is not in use is deleted.; Step 2 does **not** act on service accounts that just underwent Step 1 because; the old keys might still be in use by active pods/jobs. We assume that no; pod/job will run for longer than 30 days. We consider the ""active"" key for a service account to be the key stored in a; k8s secret. If no secret is associated with a service account, we take the active; key to be the most recently created user-managed[^1] key. We then consider; service accounts to be in one of four states:. - Expired: The active key in Kubernetes is older than 90 days and should be; rotated immediately.; - In Progress: The active key was created in the past 30 days and there exist; old keys that may still be in use.; - Ready for Delete: The active key was created more than 30 days ago and there; exist old keys that can be safely deleted.; - Up to Date: The active key is valid and there are no redundant keys. ## Rotating keys. Make sure that you are first authenticated with the correct GCP project by; running `gcpsetc",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:3432,Deployability,update,update,3432,".; If this occurs, the secret should probably be deleted, but check to ensure that; nothing still depends on it first.; - Which service accounts have no corresponding key secret in Kubernetes. This; can be OK, such as with the terraform service account. However, any user; accounts should not be in this category, and it might indicate that they; weren't properly deleted. Each service account is listed with its rotation state. NOTE: `rotate_keys.py` *only* checks secrets with a `key.json` field. This; is an invariant of `auth` and `terraform`-created accounts, but might not catch; legacy secrets. Additionally, the script will warn of any service accounts with more than 1; corresponding key secret per namespace. This should not happen with the; exception of the test user, which is used in dev namespaces instead of other; robot service accounts. There should be no redundant secrets in the default; namespace. ### Updating keys. The `update` flow steps through each service account and lists its GSA keys,; sorted in reverse order of their date of creation.; Any keys that are found in Kubernetes will list the name and namespace; of the corresponding secret at the end of the row. Developer and test accounts; might have secrets in multiple namespaces (default and dev / test), but user; accounts should only have secrets present in `default`. The script will prompt; for each service account if you would like to create a new key. Enter `yes`; and the script will create a new GSA key and update any relevant secrets. Any; other input will do nothing. The output should then show the updated key at; the top of the list and all Kubernetes secrets pointing to the new key.; The script refreshes its Kubernetes secrets after updating them so this should; accurately reflect the state of the cluster.; Continue until all service/user keys have been updated. ### Deleting keys. Remember to complete this step no sooner than 2 days after updating keys. The `delete` flow steps through each servic",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:3989,Deployability,update,update,3989,"ey.json` field. This; is an invariant of `auth` and `terraform`-created accounts, but might not catch; legacy secrets. Additionally, the script will warn of any service accounts with more than 1; corresponding key secret per namespace. This should not happen with the; exception of the test user, which is used in dev namespaces instead of other; robot service accounts. There should be no redundant secrets in the default; namespace. ### Updating keys. The `update` flow steps through each service account and lists its GSA keys,; sorted in reverse order of their date of creation.; Any keys that are found in Kubernetes will list the name and namespace; of the corresponding secret at the end of the row. Developer and test accounts; might have secrets in multiple namespaces (default and dev / test), but user; accounts should only have secrets present in `default`. The script will prompt; for each service account if you would like to create a new key. Enter `yes`; and the script will create a new GSA key and update any relevant secrets. Any; other input will do nothing. The output should then show the updated key at; the top of the list and all Kubernetes secrets pointing to the new key.; The script refreshes its Kubernetes secrets after updating them so this should; accurately reflect the state of the cluster.; Continue until all service/user keys have been updated. ### Deleting keys. Remember to complete this step no sooner than 2 days after updating keys. The `delete` flow steps through each service account similarly to the update; flow. Entering `yes` for a particular service account will delete all; user-managed keys belonging to the service account except for the most; recently created key. The script will allow deletion of old keys for service; accounts in the Ready for Delete or In Progress state, though the latter requires; additional confirmation to prove you are confident that old keys are not still; in use.; Continue until all old service/user keys have been dele",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:4084,Deployability,update,updated,4084,"cript will warn of any service accounts with more than 1; corresponding key secret per namespace. This should not happen with the; exception of the test user, which is used in dev namespaces instead of other; robot service accounts. There should be no redundant secrets in the default; namespace. ### Updating keys. The `update` flow steps through each service account and lists its GSA keys,; sorted in reverse order of their date of creation.; Any keys that are found in Kubernetes will list the name and namespace; of the corresponding secret at the end of the row. Developer and test accounts; might have secrets in multiple namespaces (default and dev / test), but user; accounts should only have secrets present in `default`. The script will prompt; for each service account if you would like to create a new key. Enter `yes`; and the script will create a new GSA key and update any relevant secrets. Any; other input will do nothing. The output should then show the updated key at; the top of the list and all Kubernetes secrets pointing to the new key.; The script refreshes its Kubernetes secrets after updating them so this should; accurately reflect the state of the cluster.; Continue until all service/user keys have been updated. ### Deleting keys. Remember to complete this step no sooner than 2 days after updating keys. The `delete` flow steps through each service account similarly to the update; flow. Entering `yes` for a particular service account will delete all; user-managed keys belonging to the service account except for the most; recently created key. The script will allow deletion of old keys for service; accounts in the Ready for Delete or In Progress state, though the latter requires; additional confirmation to prove you are confident that old keys are not still; in use.; Continue until all old service/user keys have been deleted. [^1]: All GSA keys that we create are considered ""user-managed"". We are responsible; for creating, deleting, rotating and revoking t",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:4346,Deployability,update,updated,4346," service accounts. There should be no redundant secrets in the default; namespace. ### Updating keys. The `update` flow steps through each service account and lists its GSA keys,; sorted in reverse order of their date of creation.; Any keys that are found in Kubernetes will list the name and namespace; of the corresponding secret at the end of the row. Developer and test accounts; might have secrets in multiple namespaces (default and dev / test), but user; accounts should only have secrets present in `default`. The script will prompt; for each service account if you would like to create a new key. Enter `yes`; and the script will create a new GSA key and update any relevant secrets. Any; other input will do nothing. The output should then show the updated key at; the top of the list and all Kubernetes secrets pointing to the new key.; The script refreshes its Kubernetes secrets after updating them so this should; accurately reflect the state of the cluster.; Continue until all service/user keys have been updated. ### Deleting keys. Remember to complete this step no sooner than 2 days after updating keys. The `delete` flow steps through each service account similarly to the update; flow. Entering `yes` for a particular service account will delete all; user-managed keys belonging to the service account except for the most; recently created key. The script will allow deletion of old keys for service; accounts in the Ready for Delete or In Progress state, though the latter requires; additional confirmation to prove you are confident that old keys are not still; in use.; Continue until all old service/user keys have been deleted. [^1]: All GSA keys that we create are considered ""user-managed"". We are responsible; for creating, deleting, rotating and revoking them. In addition to user-managed; keys, Google has system-managed keys. These keys are managed by Google and thus; we cannot delete them. An attempt to delete system managed keys will output a; warning and the keys ",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:4518,Deployability,update,update,4518," its GSA keys,; sorted in reverse order of their date of creation.; Any keys that are found in Kubernetes will list the name and namespace; of the corresponding secret at the end of the row. Developer and test accounts; might have secrets in multiple namespaces (default and dev / test), but user; accounts should only have secrets present in `default`. The script will prompt; for each service account if you would like to create a new key. Enter `yes`; and the script will create a new GSA key and update any relevant secrets. Any; other input will do nothing. The output should then show the updated key at; the top of the list and all Kubernetes secrets pointing to the new key.; The script refreshes its Kubernetes secrets after updating them so this should; accurately reflect the state of the cluster.; Continue until all service/user keys have been updated. ### Deleting keys. Remember to complete this step no sooner than 2 days after updating keys. The `delete` flow steps through each service account similarly to the update; flow. Entering `yes` for a particular service account will delete all; user-managed keys belonging to the service account except for the most; recently created key. The script will allow deletion of old keys for service; accounts in the Ready for Delete or In Progress state, though the latter requires; additional confirmation to prove you are confident that old keys are not still; in use.; Continue until all old service/user keys have been deleted. [^1]: All GSA keys that we create are considered ""user-managed"". We are responsible; for creating, deleting, rotating and revoking them. In addition to user-managed; keys, Google has system-managed keys. These keys are managed by Google and thus; we cannot delete them. An attempt to delete system managed keys will output a; warning and the keys will remain in the key list for the service account. If the; only remaining keys after a delete system managed + the most recent user-managed; key, then everything",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:2588,Integrability,depend,depends,2588,"hould be; rotated immediately.; - In Progress: The active key was created in the past 30 days and there exist; old keys that may still be in use.; - Ready for Delete: The active key was created more than 30 days ago and there; exist old keys that can be safely deleted.; - Up to Date: The active key is valid and there are no redundant keys. ## Rotating keys. Make sure that you are first authenticated with the correct GCP project by; running `gcpsetcluster <PROJECT>` (you need to have sourced; `$HAIL/devbin/functions.sh` to run this command). Then run. ```; python3 $HAIL/devbin/rotate_keys.py <PROJECT>; ```. Don't worry, this won't do anything scary on its own!. The script's initial output shows the following:; - Which GSA key secrets in Kubernetes belong to which Google service accounts.; - Which secrets have no corresponding service account. This likely means; the original service account was deleted without deleting its secret.; If this occurs, the secret should probably be deleted, but check to ensure that; nothing still depends on it first.; - Which service accounts have no corresponding key secret in Kubernetes. This; can be OK, such as with the terraform service account. However, any user; accounts should not be in this category, and it might indicate that they; weren't properly deleted. Each service account is listed with its rotation state. NOTE: `rotate_keys.py` *only* checks secrets with a `key.json` field. This; is an invariant of `auth` and `terraform`-created accounts, but might not catch; legacy secrets. Additionally, the script will warn of any service accounts with more than 1; corresponding key secret per namespace. This should not happen with the; exception of the test user, which is used in dev namespaces instead of other; robot service accounts. There should be no redundant secrets in the default; namespace. ### Updating keys. The `update` flow steps through each service account and lists its GSA keys,; sorted in reverse order of their date of crea",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:1803,Safety,safe,safely,1803,"e service account whose newest key is older than 30 days old,; delete all but the newest key. Step 1 ensures that all keys stored in k8s secrets are no more than two months old,; while Step 2 ensures that any key that is not in use is deleted.; Step 2 does **not** act on service accounts that just underwent Step 1 because; the old keys might still be in use by active pods/jobs. We assume that no; pod/job will run for longer than 30 days. We consider the ""active"" key for a service account to be the key stored in a; k8s secret. If no secret is associated with a service account, we take the active; key to be the most recently created user-managed[^1] key. We then consider; service accounts to be in one of four states:. - Expired: The active key in Kubernetes is older than 90 days and should be; rotated immediately.; - In Progress: The active key was created in the past 30 days and there exist; old keys that may still be in use.; - Ready for Delete: The active key was created more than 30 days ago and there; exist old keys that can be safely deleted.; - Up to Date: The active key is valid and there are no redundant keys. ## Rotating keys. Make sure that you are first authenticated with the correct GCP project by; running `gcpsetcluster <PROJECT>` (you need to have sourced; `$HAIL/devbin/functions.sh` to run this command). Then run. ```; python3 $HAIL/devbin/rotate_keys.py <PROJECT>; ```. Don't worry, this won't do anything scary on its own!. The script's initial output shows the following:; - Which GSA key secrets in Kubernetes belong to which Google service accounts.; - Which secrets have no corresponding service account. This likely means; the original service account was deleted without deleting its secret.; If this occurs, the secret should probably be deleted, but check to ensure that; nothing still depends on it first.; - Which service accounts have no corresponding key secret in Kubernetes. This; can be OK, such as with the terraform service account. However, any",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:1875,Safety,redund,redundant,1875," 1 ensures that all keys stored in k8s secrets are no more than two months old,; while Step 2 ensures that any key that is not in use is deleted.; Step 2 does **not** act on service accounts that just underwent Step 1 because; the old keys might still be in use by active pods/jobs. We assume that no; pod/job will run for longer than 30 days. We consider the ""active"" key for a service account to be the key stored in a; k8s secret. If no secret is associated with a service account, we take the active; key to be the most recently created user-managed[^1] key. We then consider; service accounts to be in one of four states:. - Expired: The active key in Kubernetes is older than 90 days and should be; rotated immediately.; - In Progress: The active key was created in the past 30 days and there exist; old keys that may still be in use.; - Ready for Delete: The active key was created more than 30 days ago and there; exist old keys that can be safely deleted.; - Up to Date: The active key is valid and there are no redundant keys. ## Rotating keys. Make sure that you are first authenticated with the correct GCP project by; running `gcpsetcluster <PROJECT>` (you need to have sourced; `$HAIL/devbin/functions.sh` to run this command). Then run. ```; python3 $HAIL/devbin/rotate_keys.py <PROJECT>; ```. Don't worry, this won't do anything scary on its own!. The script's initial output shows the following:; - Which GSA key secrets in Kubernetes belong to which Google service accounts.; - Which secrets have no corresponding service account. This likely means; the original service account was deleted without deleting its secret.; If this occurs, the secret should probably be deleted, but check to ensure that; nothing still depends on it first.; - Which service accounts have no corresponding key secret in Kubernetes. This; can be OK, such as with the terraform service account. However, any user; accounts should not be in this category, and it might indicate that they; weren't properly ",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:3363,Safety,redund,redundant,3363,"responding service account. This likely means; the original service account was deleted without deleting its secret.; If this occurs, the secret should probably be deleted, but check to ensure that; nothing still depends on it first.; - Which service accounts have no corresponding key secret in Kubernetes. This; can be OK, such as with the terraform service account. However, any user; accounts should not be in this category, and it might indicate that they; weren't properly deleted. Each service account is listed with its rotation state. NOTE: `rotate_keys.py` *only* checks secrets with a `key.json` field. This; is an invariant of `auth` and `terraform`-created accounts, but might not catch; legacy secrets. Additionally, the script will warn of any service accounts with more than 1; corresponding key secret per namespace. This should not happen with the; exception of the test user, which is used in dev namespaces instead of other; robot service accounts. There should be no redundant secrets in the default; namespace. ### Updating keys. The `update` flow steps through each service account and lists its GSA keys,; sorted in reverse order of their date of creation.; Any keys that are found in Kubernetes will list the name and namespace; of the corresponding secret at the end of the row. Developer and test accounts; might have secrets in multiple namespaces (default and dev / test), but user; accounts should only have secrets present in `default`. The script will prompt; for each service account if you would like to create a new key. Enter `yes`; and the script will create a new GSA key and update any relevant secrets. Any; other input will do nothing. The output should then show the updated key at; the top of the list and all Kubernetes secrets pointing to the new key.; The script refreshes its Kubernetes secrets after updating them so this should; accurately reflect the state of the cluster.; Continue until all service/user keys have been updated. ### Deleting keys. R",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:137,Security,authenticat,authenticate,137,"# GSA Key Rotation. Every identity in batch, user or robot, has a corresponding Google Service; Account (GSA). A service or user job can authenticate with Google as; a specific service account with a Service Account Key. When a new user is; created, `auth` creates a service account, requests a key for that account, and; stores the key as a Kubernetes secret with the name `<username>-gsa-key`. Service account keys are confidential and should be rotated at least every 90; days to mitigate the danger of attack if a key is leaked. The key rotation; strategy consists of two parts:. 1. For each Google service account whose newest key is at least 60 days old,; create a new key and update the Kubernetes secret to reflect the new value.; 2. For each Google service account whose newest key is older than 30 days old,; delete all but the newest key. Step 1 ensures that all keys stored in k8s secrets are no more than two months old,; while Step 2 ensures that any key that is not in use is deleted.; Step 2 does **not** act on service accounts that just underwent Step 1 because; the old keys might still be in use by active pods/jobs. We assume that no; pod/job will run for longer than 30 days. We consider the ""active"" key for a service account to be the key stored in a; k8s secret. If no secret is associated with a service account, we take the active; key to be the most recently created user-managed[^1] key. We then consider; service accounts to be in one of four states:. - Expired: The active key in Kubernetes is older than 90 days and should be; rotated immediately.; - In Progress: The active key was created in the past 30 days and there exist; old keys that may still be in use.; - Ready for Delete: The active key was created more than 30 days ago and there; exist old keys that can be safely deleted.; - Up to Date: The active key is valid and there are no redundant keys. ## Rotating keys. Make sure that you are first authenticated with the correct GCP project by; running `gcpsetc",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:421,Security,confidential,confidential,421,"# GSA Key Rotation. Every identity in batch, user or robot, has a corresponding Google Service; Account (GSA). A service or user job can authenticate with Google as; a specific service account with a Service Account Key. When a new user is; created, `auth` creates a service account, requests a key for that account, and; stores the key as a Kubernetes secret with the name `<username>-gsa-key`. Service account keys are confidential and should be rotated at least every 90; days to mitigate the danger of attack if a key is leaked. The key rotation; strategy consists of two parts:. 1. For each Google service account whose newest key is at least 60 days old,; create a new key and update the Kubernetes secret to reflect the new value.; 2. For each Google service account whose newest key is older than 30 days old,; delete all but the newest key. Step 1 ensures that all keys stored in k8s secrets are no more than two months old,; while Step 2 ensures that any key that is not in use is deleted.; Step 2 does **not** act on service accounts that just underwent Step 1 because; the old keys might still be in use by active pods/jobs. We assume that no; pod/job will run for longer than 30 days. We consider the ""active"" key for a service account to be the key stored in a; k8s secret. If no secret is associated with a service account, we take the active; key to be the most recently created user-managed[^1] key. We then consider; service accounts to be in one of four states:. - Expired: The active key in Kubernetes is older than 90 days and should be; rotated immediately.; - In Progress: The active key was created in the past 30 days and there exist; old keys that may still be in use.; - Ready for Delete: The active key was created more than 30 days ago and there; exist old keys that can be safely deleted.; - Up to Date: The active key is valid and there are no redundant keys. ## Rotating keys. Make sure that you are first authenticated with the correct GCP project by; running `gcpsetc",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:506,Security,attack,attack,506,"# GSA Key Rotation. Every identity in batch, user or robot, has a corresponding Google Service; Account (GSA). A service or user job can authenticate with Google as; a specific service account with a Service Account Key. When a new user is; created, `auth` creates a service account, requests a key for that account, and; stores the key as a Kubernetes secret with the name `<username>-gsa-key`. Service account keys are confidential and should be rotated at least every 90; days to mitigate the danger of attack if a key is leaked. The key rotation; strategy consists of two parts:. 1. For each Google service account whose newest key is at least 60 days old,; create a new key and update the Kubernetes secret to reflect the new value.; 2. For each Google service account whose newest key is older than 30 days old,; delete all but the newest key. Step 1 ensures that all keys stored in k8s secrets are no more than two months old,; while Step 2 ensures that any key that is not in use is deleted.; Step 2 does **not** act on service accounts that just underwent Step 1 because; the old keys might still be in use by active pods/jobs. We assume that no; pod/job will run for longer than 30 days. We consider the ""active"" key for a service account to be the key stored in a; k8s secret. If no secret is associated with a service account, we take the active; key to be the most recently created user-managed[^1] key. We then consider; service accounts to be in one of four states:. - Expired: The active key in Kubernetes is older than 90 days and should be; rotated immediately.; - In Progress: The active key was created in the past 30 days and there exist; old keys that may still be in use.; - Ready for Delete: The active key was created more than 30 days ago and there; exist old keys that can be safely deleted.; - Up to Date: The active key is valid and there are no redundant keys. ## Rotating keys. Make sure that you are first authenticated with the correct GCP project by; running `gcpsetc",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:1938,Security,authenticat,authenticated,1938,"s deleted.; Step 2 does **not** act on service accounts that just underwent Step 1 because; the old keys might still be in use by active pods/jobs. We assume that no; pod/job will run for longer than 30 days. We consider the ""active"" key for a service account to be the key stored in a; k8s secret. If no secret is associated with a service account, we take the active; key to be the most recently created user-managed[^1] key. We then consider; service accounts to be in one of four states:. - Expired: The active key in Kubernetes is older than 90 days and should be; rotated immediately.; - In Progress: The active key was created in the past 30 days and there exist; old keys that may still be in use.; - Ready for Delete: The active key was created more than 30 days ago and there; exist old keys that can be safely deleted.; - Up to Date: The active key is valid and there are no redundant keys. ## Rotating keys. Make sure that you are first authenticated with the correct GCP project by; running `gcpsetcluster <PROJECT>` (you need to have sourced; `$HAIL/devbin/functions.sh` to run this command). Then run. ```; python3 $HAIL/devbin/rotate_keys.py <PROJECT>; ```. Don't worry, this won't do anything scary on its own!. The script's initial output shows the following:; - Which GSA key secrets in Kubernetes belong to which Google service accounts.; - Which secrets have no corresponding service account. This likely means; the original service account was deleted without deleting its secret.; If this occurs, the secret should probably be deleted, but check to ensure that; nothing still depends on it first.; - Which service accounts have no corresponding key secret in Kubernetes. This; can be OK, such as with the terraform service account. However, any user; accounts should not be in this category, and it might indicate that they; weren't properly deleted. Each service account is listed with its rotation state. NOTE: `rotate_keys.py` *only* checks secrets with a `key.json` field. ",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:3259,Testability,test,test,3259," GSA key secrets in Kubernetes belong to which Google service accounts.; - Which secrets have no corresponding service account. This likely means; the original service account was deleted without deleting its secret.; If this occurs, the secret should probably be deleted, but check to ensure that; nothing still depends on it first.; - Which service accounts have no corresponding key secret in Kubernetes. This; can be OK, such as with the terraform service account. However, any user; accounts should not be in this category, and it might indicate that they; weren't properly deleted. Each service account is listed with its rotation state. NOTE: `rotate_keys.py` *only* checks secrets with a `key.json` field. This; is an invariant of `auth` and `terraform`-created accounts, but might not catch; legacy secrets. Additionally, the script will warn of any service accounts with more than 1; corresponding key secret per namespace. This should not happen with the; exception of the test user, which is used in dev namespaces instead of other; robot service accounts. There should be no redundant secrets in the default; namespace. ### Updating keys. The `update` flow steps through each service account and lists its GSA keys,; sorted in reverse order of their date of creation.; Any keys that are found in Kubernetes will list the name and namespace; of the corresponding secret at the end of the row. Developer and test accounts; might have secrets in multiple namespaces (default and dev / test), but user; accounts should only have secrets present in `default`. The script will prompt; for each service account if you would like to create a new key. Enter `yes`; and the script will create a new GSA key and update any relevant secrets. Any; other input will do nothing. The output should then show the updated key at; the top of the list and all Kubernetes secrets pointing to the new key.; The script refreshes its Kubernetes secrets after updating them so this should; accurately reflect the ",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:3694,Testability,test,test,3694,"r; accounts should not be in this category, and it might indicate that they; weren't properly deleted. Each service account is listed with its rotation state. NOTE: `rotate_keys.py` *only* checks secrets with a `key.json` field. This; is an invariant of `auth` and `terraform`-created accounts, but might not catch; legacy secrets. Additionally, the script will warn of any service accounts with more than 1; corresponding key secret per namespace. This should not happen with the; exception of the test user, which is used in dev namespaces instead of other; robot service accounts. There should be no redundant secrets in the default; namespace. ### Updating keys. The `update` flow steps through each service account and lists its GSA keys,; sorted in reverse order of their date of creation.; Any keys that are found in Kubernetes will list the name and namespace; of the corresponding secret at the end of the row. Developer and test accounts; might have secrets in multiple namespaces (default and dev / test), but user; accounts should only have secrets present in `default`. The script will prompt; for each service account if you would like to create a new key. Enter `yes`; and the script will create a new GSA key and update any relevant secrets. Any; other input will do nothing. The output should then show the updated key at; the top of the list and all Kubernetes secrets pointing to the new key.; The script refreshes its Kubernetes secrets after updating them so this should; accurately reflect the state of the cluster.; Continue until all service/user keys have been updated. ### Deleting keys. Remember to complete this step no sooner than 2 days after updating keys. The `delete` flow steps through each service account similarly to the update; flow. Entering `yes` for a particular service account will delete all; user-managed keys belonging to the service account except for the most; recently created key. The script will allow deletion of old keys for service; accounts in th",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md:3770,Testability,test,test,3770,"r; accounts should not be in this category, and it might indicate that they; weren't properly deleted. Each service account is listed with its rotation state. NOTE: `rotate_keys.py` *only* checks secrets with a `key.json` field. This; is an invariant of `auth` and `terraform`-created accounts, but might not catch; legacy secrets. Additionally, the script will warn of any service accounts with more than 1; corresponding key secret per namespace. This should not happen with the; exception of the test user, which is used in dev namespaces instead of other; robot service accounts. There should be no redundant secrets in the default; namespace. ### Updating keys. The `update` flow steps through each service account and lists its GSA keys,; sorted in reverse order of their date of creation.; Any keys that are found in Kubernetes will list the name and namespace; of the corresponding secret at the end of the row. Developer and test accounts; might have secrets in multiple namespaces (default and dev / test), but user; accounts should only have secrets present in `default`. The script will prompt; for each service account if you would like to create a new key. Enter `yes`; and the script will create a new GSA key and update any relevant secrets. Any; other input will do nothing. The output should then show the updated key at; the top of the list and all Kubernetes secrets pointing to the new key.; The script refreshes its Kubernetes secrets after updating them so this should; accurately reflect the state of the cluster.; Continue until all service/user keys have been updated. ### Deleting keys. Remember to complete this step no sooner than 2 days after updating keys. The `delete` flow steps through each service account similarly to the update; flow. Entering `yes` for a particular service account will delete all; user-managed keys belonging to the service account except for the most; recently created key. The script will allow deletion of old keys for service; accounts in th",MatchSource.DOCS,dev-docs/services/key-rotation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/key-rotation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md:3894,Deployability,install,installations,3894,"he node pools themselves, which should adhere to the naming; conventions outlined above and specified as terraform variables. To complete step 1, copy the existing node pool resource; under a new name, `vdc_preemptible_pool_2`, make the desired changes to the new; resource and apply the terraform. This should not alter existing node pools. Once draining is complete, take the following steps to remove the old node pool; and restore a clean terraform state:; 1. Delete the resource `vdc_preemptible_pool` and apply. This should delete the old node pool.; 2. Move the state of the new resource into the old one. For example, if in Azure, run. ```; terraform state mv \; module.vdc.azurerm_kubernetes_cluster_node_pool.vdc_preemptible_pool_2 \; module.vdc.azurerm_kubernetes_cluster_node_pool.vdc_preemptible_pool; ```. 3. Rename `vdc_preemptible_pool_2` to `vdc_preemptible_pool`. If you try; to `terraform apply`, there should be no planned changes and the git history; should be clean. ## Troubleshooting. ### Terraform Kubernetes provider dialing localhost; Occasionally, the `kubernetes` provider can initialize before fetching necessary; state (as the credentials are themselves terraform resources) and fall back to; dialing localhost. This can occur if you are switching between Hail installations; and the local mirror of the terraform state needs to be sync'd from remote storage; at the start of `terraform apply`. As of writing, this; [remains an issue](https://github.com/hashicorp/terraform-provider-kubernetes/issues/1028); with the kubernetes provider. A workaround to fully initialize the state is instead; of just running `terraform apply` for the entire module, to instead target just; the resources that generate the kubernetes configuration but do not themselves; rely on the kubernetes provider. Run `terraform apply -var-file=global.tfvars -target=module.vdc`; to correctly sync local terraform state, and subsequent invocations of `terraform apply`; should work as expected.; ",MatchSource.DOCS,dev-docs/services/kubernetes-operations.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md:4350,Deployability,configurat,configuration,4350,"he node pools themselves, which should adhere to the naming; conventions outlined above and specified as terraform variables. To complete step 1, copy the existing node pool resource; under a new name, `vdc_preemptible_pool_2`, make the desired changes to the new; resource and apply the terraform. This should not alter existing node pools. Once draining is complete, take the following steps to remove the old node pool; and restore a clean terraform state:; 1. Delete the resource `vdc_preemptible_pool` and apply. This should delete the old node pool.; 2. Move the state of the new resource into the old one. For example, if in Azure, run. ```; terraform state mv \; module.vdc.azurerm_kubernetes_cluster_node_pool.vdc_preemptible_pool_2 \; module.vdc.azurerm_kubernetes_cluster_node_pool.vdc_preemptible_pool; ```. 3. Rename `vdc_preemptible_pool_2` to `vdc_preemptible_pool`. If you try; to `terraform apply`, there should be no planned changes and the git history; should be clean. ## Troubleshooting. ### Terraform Kubernetes provider dialing localhost; Occasionally, the `kubernetes` provider can initialize before fetching necessary; state (as the credentials are themselves terraform resources) and fall back to; dialing localhost. This can occur if you are switching between Hail installations; and the local mirror of the terraform state needs to be sync'd from remote storage; at the start of `terraform apply`. As of writing, this; [remains an issue](https://github.com/hashicorp/terraform-provider-kubernetes/issues/1028); with the kubernetes provider. A workaround to fully initialize the state is instead; of just running `terraform apply` for the entire module, to instead target just; the resources that generate the kubernetes configuration but do not themselves; rely on the kubernetes provider. Run `terraform apply -var-file=global.tfvars -target=module.vdc`; to correctly sync local terraform state, and subsequent invocations of `terraform apply`; should work as expected.; ",MatchSource.DOCS,dev-docs/services/kubernetes-operations.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md:192,Energy Efficiency,drain,drain,192,"# Kubernetes Operations. ## Altering a Node Pool. ### When managing node pools manually. We will have the old node pool and the new node pool active simultaneously. We will use `cordon` and; `drain` to move all load from the old node pool to the new node pool. Then we will delete the old; node pool. 1. Add a new node pool to the cluster. You can use the UI or `gcloud`. We have two kinds of node; pools: non-preemptible and preemptible, their names should always be non-preemptible-pool-N and; preemptible-pool-N, respectively. When you re-create the nodepool, increment the number by; one. Take care to copy the taints and tags correctly. 2. Wait for the new nodepool to be ready. 3. Disable auto-scaling on the old nodepool. 4. Cordon all nodes in the old nodepool. This prevents pods from being newly scheduled on these; nodes. ```; kubectl cordon --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. 5. Drain the nodes in the old nodepool. This moves pods from the old nodepool to the new node pool. ```; kubectl drain --ignore-daemonsets --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. This will likely fail because the metrics-server uses some local disk to store metrics. If that is; the *only* pod listed, then you can re-run the command with `--delete-emptydir-data`. You may lose a; short period of k8s metrics. This will also impair the HorizontalPodAutoscaler. Other pods, such as grafana and memory may also be listed here. You can use `--delete-emptydir-data`; to force them to be deleted as well. Deleting memory will cause a loss of cache for Hail Query on; Batch jobs using memory. Neither of these are catastrophic to delete. ```; kubectl drain --delete-emptydir-data --ignore-daemonsets --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. 6. The old node pool will still have nodes present. The autoscaler will, in all likelihood, not; remove the nodes because they contain certain unmoveable kube-system pods. Instead, you can; verify all r",MatchSource.DOCS,dev-docs/services/kubernetes-operations.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md:806,Energy Efficiency,schedul,scheduled,806,"# Kubernetes Operations. ## Altering a Node Pool. ### When managing node pools manually. We will have the old node pool and the new node pool active simultaneously. We will use `cordon` and; `drain` to move all load from the old node pool to the new node pool. Then we will delete the old; node pool. 1. Add a new node pool to the cluster. You can use the UI or `gcloud`. We have two kinds of node; pools: non-preemptible and preemptible, their names should always be non-preemptible-pool-N and; preemptible-pool-N, respectively. When you re-create the nodepool, increment the number by; one. Take care to copy the taints and tags correctly. 2. Wait for the new nodepool to be ready. 3. Disable auto-scaling on the old nodepool. 4. Cordon all nodes in the old nodepool. This prevents pods from being newly scheduled on these; nodes. ```; kubectl cordon --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. 5. Drain the nodes in the old nodepool. This moves pods from the old nodepool to the new node pool. ```; kubectl drain --ignore-daemonsets --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. This will likely fail because the metrics-server uses some local disk to store metrics. If that is; the *only* pod listed, then you can re-run the command with `--delete-emptydir-data`. You may lose a; short period of k8s metrics. This will also impair the HorizontalPodAutoscaler. Other pods, such as grafana and memory may also be listed here. You can use `--delete-emptydir-data`; to force them to be deleted as well. Deleting memory will cause a loss of cache for Hail Query on; Batch jobs using memory. Neither of these are catastrophic to delete. ```; kubectl drain --delete-emptydir-data --ignore-daemonsets --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. 6. The old node pool will still have nodes present. The autoscaler will, in all likelihood, not; remove the nodes because they contain certain unmoveable kube-system pods. Instead, you can; verify all r",MatchSource.DOCS,dev-docs/services/kubernetes-operations.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md:1030,Energy Efficiency,drain,drain,1030,"ool. ### When managing node pools manually. We will have the old node pool and the new node pool active simultaneously. We will use `cordon` and; `drain` to move all load from the old node pool to the new node pool. Then we will delete the old; node pool. 1. Add a new node pool to the cluster. You can use the UI or `gcloud`. We have two kinds of node; pools: non-preemptible and preemptible, their names should always be non-preemptible-pool-N and; preemptible-pool-N, respectively. When you re-create the nodepool, increment the number by; one. Take care to copy the taints and tags correctly. 2. Wait for the new nodepool to be ready. 3. Disable auto-scaling on the old nodepool. 4. Cordon all nodes in the old nodepool. This prevents pods from being newly scheduled on these; nodes. ```; kubectl cordon --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. 5. Drain the nodes in the old nodepool. This moves pods from the old nodepool to the new node pool. ```; kubectl drain --ignore-daemonsets --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. This will likely fail because the metrics-server uses some local disk to store metrics. If that is; the *only* pod listed, then you can re-run the command with `--delete-emptydir-data`. You may lose a; short period of k8s metrics. This will also impair the HorizontalPodAutoscaler. Other pods, such as grafana and memory may also be listed here. You can use `--delete-emptydir-data`; to force them to be deleted as well. Deleting memory will cause a loss of cache for Hail Query on; Batch jobs using memory. Neither of these are catastrophic to delete. ```; kubectl drain --delete-emptydir-data --ignore-daemonsets --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. 6. The old node pool will still have nodes present. The autoscaler will, in all likelihood, not; remove the nodes because they contain certain unmoveable kube-system pods. Instead, you can; verify all relevant pods have moved by running the drain",MatchSource.DOCS,dev-docs/services/kubernetes-operations.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md:1686,Energy Efficiency,drain,drain,1686,"he old nodepool. 4. Cordon all nodes in the old nodepool. This prevents pods from being newly scheduled on these; nodes. ```; kubectl cordon --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. 5. Drain the nodes in the old nodepool. This moves pods from the old nodepool to the new node pool. ```; kubectl drain --ignore-daemonsets --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. This will likely fail because the metrics-server uses some local disk to store metrics. If that is; the *only* pod listed, then you can re-run the command with `--delete-emptydir-data`. You may lose a; short period of k8s metrics. This will also impair the HorizontalPodAutoscaler. Other pods, such as grafana and memory may also be listed here. You can use `--delete-emptydir-data`; to force them to be deleted as well. Deleting memory will cause a loss of cache for Hail Query on; Batch jobs using memory. Neither of these are catastrophic to delete. ```; kubectl drain --delete-emptydir-data --ignore-daemonsets --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. 6. The old node pool will still have nodes present. The autoscaler will, in all likelihood, not; remove the nodes because they contain certain unmoveable kube-system pods. Instead, you can; verify all relevant pods have moved by running the drain command again. You should see no pod; names printed except for kube-system daemon sets. You will see all the nodes printed with the; message ""drained"". 7. Delete the old node pool. ```; gcloud container node-pools delete $OLD_POOL_NAME --cluster $CLUSTER_NAME; ```. ### When using terraform; If using terraform to manage the node pools, we use terraform to create and delete; the pools. Assume we are replacing a pool whose terraform resource name is; `vdc_preemptible_pool`. NOTE: the following names apply to the *terraform resource*,; not the names of the node pools themselves, which should adhere to the naming; conventions outlined above and specified as terraf",MatchSource.DOCS,dev-docs/services/kubernetes-operations.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md:2040,Energy Efficiency,drain,drain,2040,". ```; kubectl drain --ignore-daemonsets --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. This will likely fail because the metrics-server uses some local disk to store metrics. If that is; the *only* pod listed, then you can re-run the command with `--delete-emptydir-data`. You may lose a; short period of k8s metrics. This will also impair the HorizontalPodAutoscaler. Other pods, such as grafana and memory may also be listed here. You can use `--delete-emptydir-data`; to force them to be deleted as well. Deleting memory will cause a loss of cache for Hail Query on; Batch jobs using memory. Neither of these are catastrophic to delete. ```; kubectl drain --delete-emptydir-data --ignore-daemonsets --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. 6. The old node pool will still have nodes present. The autoscaler will, in all likelihood, not; remove the nodes because they contain certain unmoveable kube-system pods. Instead, you can; verify all relevant pods have moved by running the drain command again. You should see no pod; names printed except for kube-system daemon sets. You will see all the nodes printed with the; message ""drained"". 7. Delete the old node pool. ```; gcloud container node-pools delete $OLD_POOL_NAME --cluster $CLUSTER_NAME; ```. ### When using terraform; If using terraform to manage the node pools, we use terraform to create and delete; the pools. Assume we are replacing a pool whose terraform resource name is; `vdc_preemptible_pool`. NOTE: the following names apply to the *terraform resource*,; not the names of the node pools themselves, which should adhere to the naming; conventions outlined above and specified as terraform variables. To complete step 1, copy the existing node pool resource; under a new name, `vdc_preemptible_pool_2`, make the desired changes to the new; resource and apply the terraform. This should not alter existing node pools. Once draining is complete, take the following steps to remove the old n",MatchSource.DOCS,dev-docs/services/kubernetes-operations.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md:2188,Energy Efficiency,drain,drained,2188,"ver uses some local disk to store metrics. If that is; the *only* pod listed, then you can re-run the command with `--delete-emptydir-data`. You may lose a; short period of k8s metrics. This will also impair the HorizontalPodAutoscaler. Other pods, such as grafana and memory may also be listed here. You can use `--delete-emptydir-data`; to force them to be deleted as well. Deleting memory will cause a loss of cache for Hail Query on; Batch jobs using memory. Neither of these are catastrophic to delete. ```; kubectl drain --delete-emptydir-data --ignore-daemonsets --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. 6. The old node pool will still have nodes present. The autoscaler will, in all likelihood, not; remove the nodes because they contain certain unmoveable kube-system pods. Instead, you can; verify all relevant pods have moved by running the drain command again. You should see no pod; names printed except for kube-system daemon sets. You will see all the nodes printed with the; message ""drained"". 7. Delete the old node pool. ```; gcloud container node-pools delete $OLD_POOL_NAME --cluster $CLUSTER_NAME; ```. ### When using terraform; If using terraform to manage the node pools, we use terraform to create and delete; the pools. Assume we are replacing a pool whose terraform resource name is; `vdc_preemptible_pool`. NOTE: the following names apply to the *terraform resource*,; not the names of the node pools themselves, which should adhere to the naming; conventions outlined above and specified as terraform variables. To complete step 1, copy the existing node pool resource; under a new name, `vdc_preemptible_pool_2`, make the desired changes to the new; resource and apply the terraform. This should not alter existing node pools. Once draining is complete, take the following steps to remove the old node pool; and restore a clean terraform state:; 1. Delete the resource `vdc_preemptible_pool` and apply. This should delete the old node pool.; 2. Mov",MatchSource.DOCS,dev-docs/services/kubernetes-operations.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md:2949,Energy Efficiency,drain,draining,2949,"vant pods have moved by running the drain command again. You should see no pod; names printed except for kube-system daemon sets. You will see all the nodes printed with the; message ""drained"". 7. Delete the old node pool. ```; gcloud container node-pools delete $OLD_POOL_NAME --cluster $CLUSTER_NAME; ```. ### When using terraform; If using terraform to manage the node pools, we use terraform to create and delete; the pools. Assume we are replacing a pool whose terraform resource name is; `vdc_preemptible_pool`. NOTE: the following names apply to the *terraform resource*,; not the names of the node pools themselves, which should adhere to the naming; conventions outlined above and specified as terraform variables. To complete step 1, copy the existing node pool resource; under a new name, `vdc_preemptible_pool_2`, make the desired changes to the new; resource and apply the terraform. This should not alter existing node pools. Once draining is complete, take the following steps to remove the old node pool; and restore a clean terraform state:; 1. Delete the resource `vdc_preemptible_pool` and apply. This should delete the old node pool.; 2. Move the state of the new resource into the old one. For example, if in Azure, run. ```; terraform state mv \; module.vdc.azurerm_kubernetes_cluster_node_pool.vdc_preemptible_pool_2 \; module.vdc.azurerm_kubernetes_cluster_node_pool.vdc_preemptible_pool; ```. 3. Rename `vdc_preemptible_pool_2` to `vdc_preemptible_pool`. If you try; to `terraform apply`, there should be no planned changes and the git history; should be clean. ## Troubleshooting. ### Terraform Kubernetes provider dialing localhost; Occasionally, the `kubernetes` provider can initialize before fetching necessary; state (as the credentials are themselves terraform resources) and fall back to; dialing localhost. This can occur if you are switching between Hail installations; and the local mirror of the terraform state needs to be sync'd from remote storage; at the star",MatchSource.DOCS,dev-docs/services/kubernetes-operations.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md:2179,Integrability,message,message,2179,"ver uses some local disk to store metrics. If that is; the *only* pod listed, then you can re-run the command with `--delete-emptydir-data`. You may lose a; short period of k8s metrics. This will also impair the HorizontalPodAutoscaler. Other pods, such as grafana and memory may also be listed here. You can use `--delete-emptydir-data`; to force them to be deleted as well. Deleting memory will cause a loss of cache for Hail Query on; Batch jobs using memory. Neither of these are catastrophic to delete. ```; kubectl drain --delete-emptydir-data --ignore-daemonsets --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. 6. The old node pool will still have nodes present. The autoscaler will, in all likelihood, not; remove the nodes because they contain certain unmoveable kube-system pods. Instead, you can; verify all relevant pods have moved by running the drain command again. You should see no pod; names printed except for kube-system daemon sets. You will see all the nodes printed with the; message ""drained"". 7. Delete the old node pool. ```; gcloud container node-pools delete $OLD_POOL_NAME --cluster $CLUSTER_NAME; ```. ### When using terraform; If using terraform to manage the node pools, we use terraform to create and delete; the pools. Assume we are replacing a pool whose terraform resource name is; `vdc_preemptible_pool`. NOTE: the following names apply to the *terraform resource*,; not the names of the node pools themselves, which should adhere to the naming; conventions outlined above and specified as terraform variables. To complete step 1, copy the existing node pool resource; under a new name, `vdc_preemptible_pool_2`, make the desired changes to the new; resource and apply the terraform. This should not alter existing node pools. Once draining is complete, take the following steps to remove the old node pool; and restore a clean terraform state:; 1. Delete the resource `vdc_preemptible_pool` and apply. This should delete the old node pool.; 2. Mov",MatchSource.DOCS,dev-docs/services/kubernetes-operations.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md:2717,Modifiability,variab,variables,2717,"ry. Neither of these are catastrophic to delete. ```; kubectl drain --delete-emptydir-data --ignore-daemonsets --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. 6. The old node pool will still have nodes present. The autoscaler will, in all likelihood, not; remove the nodes because they contain certain unmoveable kube-system pods. Instead, you can; verify all relevant pods have moved by running the drain command again. You should see no pod; names printed except for kube-system daemon sets. You will see all the nodes printed with the; message ""drained"". 7. Delete the old node pool. ```; gcloud container node-pools delete $OLD_POOL_NAME --cluster $CLUSTER_NAME; ```. ### When using terraform; If using terraform to manage the node pools, we use terraform to create and delete; the pools. Assume we are replacing a pool whose terraform resource name is; `vdc_preemptible_pool`. NOTE: the following names apply to the *terraform resource*,; not the names of the node pools themselves, which should adhere to the naming; conventions outlined above and specified as terraform variables. To complete step 1, copy the existing node pool resource; under a new name, `vdc_preemptible_pool_2`, make the desired changes to the new; resource and apply the terraform. This should not alter existing node pools. Once draining is complete, take the following steps to remove the old node pool; and restore a clean terraform state:; 1. Delete the resource `vdc_preemptible_pool` and apply. This should delete the old node pool.; 2. Move the state of the new resource into the old one. For example, if in Azure, run. ```; terraform state mv \; module.vdc.azurerm_kubernetes_cluster_node_pool.vdc_preemptible_pool_2 \; module.vdc.azurerm_kubernetes_cluster_node_pool.vdc_preemptible_pool; ```. 3. Rename `vdc_preemptible_pool_2` to `vdc_preemptible_pool`. If you try; to `terraform apply`, there should be no planned changes and the git history; should be clean. ## Troubleshooting. ### Terraform",MatchSource.DOCS,dev-docs/services/kubernetes-operations.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md:4350,Modifiability,config,configuration,4350,"he node pools themselves, which should adhere to the naming; conventions outlined above and specified as terraform variables. To complete step 1, copy the existing node pool resource; under a new name, `vdc_preemptible_pool_2`, make the desired changes to the new; resource and apply the terraform. This should not alter existing node pools. Once draining is complete, take the following steps to remove the old node pool; and restore a clean terraform state:; 1. Delete the resource `vdc_preemptible_pool` and apply. This should delete the old node pool.; 2. Move the state of the new resource into the old one. For example, if in Azure, run. ```; terraform state mv \; module.vdc.azurerm_kubernetes_cluster_node_pool.vdc_preemptible_pool_2 \; module.vdc.azurerm_kubernetes_cluster_node_pool.vdc_preemptible_pool; ```. 3. Rename `vdc_preemptible_pool_2` to `vdc_preemptible_pool`. If you try; to `terraform apply`, there should be no planned changes and the git history; should be clean. ## Troubleshooting. ### Terraform Kubernetes provider dialing localhost; Occasionally, the `kubernetes` provider can initialize before fetching necessary; state (as the credentials are themselves terraform resources) and fall back to; dialing localhost. This can occur if you are switching between Hail installations; and the local mirror of the terraform state needs to be sync'd from remote storage; at the start of `terraform apply`. As of writing, this; [remains an issue](https://github.com/hashicorp/terraform-provider-kubernetes/issues/1028); with the kubernetes provider. A workaround to fully initialize the state is instead; of just running `terraform apply` for the entire module, to instead target just; the resources that generate the kubernetes configuration but do not themselves; rely on the kubernetes provider. Run `terraform apply -var-file=global.tfvars -target=module.vdc`; to correctly sync local terraform state, and subsequent invocations of `terraform apply`; should work as expected.; ",MatchSource.DOCS,dev-docs/services/kubernetes-operations.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md:211,Performance,load,load,211,"# Kubernetes Operations. ## Altering a Node Pool. ### When managing node pools manually. We will have the old node pool and the new node pool active simultaneously. We will use `cordon` and; `drain` to move all load from the old node pool to the new node pool. Then we will delete the old; node pool. 1. Add a new node pool to the cluster. You can use the UI or `gcloud`. We have two kinds of node; pools: non-preemptible and preemptible, their names should always be non-preemptible-pool-N and; preemptible-pool-N, respectively. When you re-create the nodepool, increment the number by; one. Take care to copy the taints and tags correctly. 2. Wait for the new nodepool to be ready. 3. Disable auto-scaling on the old nodepool. 4. Cordon all nodes in the old nodepool. This prevents pods from being newly scheduled on these; nodes. ```; kubectl cordon --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. 5. Drain the nodes in the old nodepool. This moves pods from the old nodepool to the new node pool. ```; kubectl drain --ignore-daemonsets --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. This will likely fail because the metrics-server uses some local disk to store metrics. If that is; the *only* pod listed, then you can re-run the command with `--delete-emptydir-data`. You may lose a; short period of k8s metrics. This will also impair the HorizontalPodAutoscaler. Other pods, such as grafana and memory may also be listed here. You can use `--delete-emptydir-data`; to force them to be deleted as well. Deleting memory will cause a loss of cache for Hail Query on; Batch jobs using memory. Neither of these are catastrophic to delete. ```; kubectl drain --delete-emptydir-data --ignore-daemonsets --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. 6. The old node pool will still have nodes present. The autoscaler will, in all likelihood, not; remove the nodes because they contain certain unmoveable kube-system pods. Instead, you can; verify all r",MatchSource.DOCS,dev-docs/services/kubernetes-operations.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md:1578,Performance,cache,cache,1578," by; one. Take care to copy the taints and tags correctly. 2. Wait for the new nodepool to be ready. 3. Disable auto-scaling on the old nodepool. 4. Cordon all nodes in the old nodepool. This prevents pods from being newly scheduled on these; nodes. ```; kubectl cordon --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. 5. Drain the nodes in the old nodepool. This moves pods from the old nodepool to the new node pool. ```; kubectl drain --ignore-daemonsets --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. This will likely fail because the metrics-server uses some local disk to store metrics. If that is; the *only* pod listed, then you can re-run the command with `--delete-emptydir-data`. You may lose a; short period of k8s metrics. This will also impair the HorizontalPodAutoscaler. Other pods, such as grafana and memory may also be listed here. You can use `--delete-emptydir-data`; to force them to be deleted as well. Deleting memory will cause a loss of cache for Hail Query on; Batch jobs using memory. Neither of these are catastrophic to delete. ```; kubectl drain --delete-emptydir-data --ignore-daemonsets --selector=""cloud.google.com/gke-nodepool=$OLD_POOL_NAME""; ```. 6. The old node pool will still have nodes present. The autoscaler will, in all likelihood, not; remove the nodes because they contain certain unmoveable kube-system pods. Instead, you can; verify all relevant pods have moved by running the drain command again. You should see no pod; names printed except for kube-system daemon sets. You will see all the nodes printed with the; message ""drained"". 7. Delete the old node pool. ```; gcloud container node-pools delete $OLD_POOL_NAME --cluster $CLUSTER_NAME; ```. ### When using terraform; If using terraform to manage the node pools, we use terraform to create and delete; the pools. Assume we are replacing a pool whose terraform resource name is; `vdc_preemptible_pool`. NOTE: the following names apply to the *terraform resource*,; ",MatchSource.DOCS,dev-docs/services/kubernetes-operations.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md:4087,Security,hash,hashicorp,4087,"he node pools themselves, which should adhere to the naming; conventions outlined above and specified as terraform variables. To complete step 1, copy the existing node pool resource; under a new name, `vdc_preemptible_pool_2`, make the desired changes to the new; resource and apply the terraform. This should not alter existing node pools. Once draining is complete, take the following steps to remove the old node pool; and restore a clean terraform state:; 1. Delete the resource `vdc_preemptible_pool` and apply. This should delete the old node pool.; 2. Move the state of the new resource into the old one. For example, if in Azure, run. ```; terraform state mv \; module.vdc.azurerm_kubernetes_cluster_node_pool.vdc_preemptible_pool_2 \; module.vdc.azurerm_kubernetes_cluster_node_pool.vdc_preemptible_pool; ```. 3. Rename `vdc_preemptible_pool_2` to `vdc_preemptible_pool`. If you try; to `terraform apply`, there should be no planned changes and the git history; should be clean. ## Troubleshooting. ### Terraform Kubernetes provider dialing localhost; Occasionally, the `kubernetes` provider can initialize before fetching necessary; state (as the credentials are themselves terraform resources) and fall back to; dialing localhost. This can occur if you are switching between Hail installations; and the local mirror of the terraform state needs to be sync'd from remote storage; at the start of `terraform apply`. As of writing, this; [remains an issue](https://github.com/hashicorp/terraform-provider-kubernetes/issues/1028); with the kubernetes provider. A workaround to fully initialize the state is instead; of just running `terraform apply` for the entire module, to instead target just; the resources that generate the kubernetes configuration but do not themselves; rely on the kubernetes provider. Run `terraform apply -var-file=global.tfvars -target=module.vdc`; to correctly sync local terraform state, and subsequent invocations of `terraform apply`; should work as expected.; ",MatchSource.DOCS,dev-docs/services/kubernetes-operations.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/kubernetes-operations.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md:213,Availability,downtime,downtime,213,"# Let's Encrypt. Hail uses Let's Encrypt certificates for the gateway pod. ## Refreshing Certificates. Update the `letsencrypt-config` secret:. ```; make -C letsencrypt run; ```. Restart your gateway pods without downtime. When the restart they load the new certificate:. ```; kubectl rollout restart deployment gateway-deployment; ```. ## Revoking Certificates. First, gather a list of the crt.sh IDs for the certificates you want to revoke from; https://crt.sh/?q=YOUR_DOMAIN_HERE . You will notice there is always a precertificate and a leaf; certificate. Both have the same serial number so revoking one revokes the other. In the next step,; the command will fail if you specify an already revoked certificate, so you should only specify one; of each precertificate and leaf certificate pair. To get list of IDs run the following:. ```; $ CERT_IDS_TO_REVOKE=$(curl https://crt.sh/?q=YOUR_DOMAIN_HERE | pup 'td.outer a json{}' | jq '.[].text' | egrep -o '[0-9]{10}'); $ CERT_IDS_TO_REVOKE=$(echo \'$CERT_IDS_TO_REVOKE\'); $ echo $CERT_IDS_TO_REVOKE; '6503198927 6503193970 6128196502'; ```. ```; make -C letsencrypt revoke CERT_IDS_TO_REVOKE='6503198927 6503193970 6128196502'; ```; ",MatchSource.DOCS,dev-docs/services/letsencrypt.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md:994,Availability,echo,echo,994,"# Let's Encrypt. Hail uses Let's Encrypt certificates for the gateway pod. ## Refreshing Certificates. Update the `letsencrypt-config` secret:. ```; make -C letsencrypt run; ```. Restart your gateway pods without downtime. When the restart they load the new certificate:. ```; kubectl rollout restart deployment gateway-deployment; ```. ## Revoking Certificates. First, gather a list of the crt.sh IDs for the certificates you want to revoke from; https://crt.sh/?q=YOUR_DOMAIN_HERE . You will notice there is always a precertificate and a leaf; certificate. Both have the same serial number so revoking one revokes the other. In the next step,; the command will fail if you specify an already revoked certificate, so you should only specify one; of each precertificate and leaf certificate pair. To get list of IDs run the following:. ```; $ CERT_IDS_TO_REVOKE=$(curl https://crt.sh/?q=YOUR_DOMAIN_HERE | pup 'td.outer a json{}' | jq '.[].text' | egrep -o '[0-9]{10}'); $ CERT_IDS_TO_REVOKE=$(echo \'$CERT_IDS_TO_REVOKE\'); $ echo $CERT_IDS_TO_REVOKE; '6503198927 6503193970 6128196502'; ```. ```; make -C letsencrypt revoke CERT_IDS_TO_REVOKE='6503198927 6503193970 6128196502'; ```; ",MatchSource.DOCS,dev-docs/services/letsencrypt.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md:1027,Availability,echo,echo,1027,"# Let's Encrypt. Hail uses Let's Encrypt certificates for the gateway pod. ## Refreshing Certificates. Update the `letsencrypt-config` secret:. ```; make -C letsencrypt run; ```. Restart your gateway pods without downtime. When the restart they load the new certificate:. ```; kubectl rollout restart deployment gateway-deployment; ```. ## Revoking Certificates. First, gather a list of the crt.sh IDs for the certificates you want to revoke from; https://crt.sh/?q=YOUR_DOMAIN_HERE . You will notice there is always a precertificate and a leaf; certificate. Both have the same serial number so revoking one revokes the other. In the next step,; the command will fail if you specify an already revoked certificate, so you should only specify one; of each precertificate and leaf certificate pair. To get list of IDs run the following:. ```; $ CERT_IDS_TO_REVOKE=$(curl https://crt.sh/?q=YOUR_DOMAIN_HERE | pup 'td.outer a json{}' | jq '.[].text' | egrep -o '[0-9]{10}'); $ CERT_IDS_TO_REVOKE=$(echo \'$CERT_IDS_TO_REVOKE\'); $ echo $CERT_IDS_TO_REVOKE; '6503198927 6503193970 6128196502'; ```. ```; make -C letsencrypt revoke CERT_IDS_TO_REVOKE='6503198927 6503193970 6128196502'; ```; ",MatchSource.DOCS,dev-docs/services/letsencrypt.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md:285,Deployability,rollout,rollout,285,"# Let's Encrypt. Hail uses Let's Encrypt certificates for the gateway pod. ## Refreshing Certificates. Update the `letsencrypt-config` secret:. ```; make -C letsencrypt run; ```. Restart your gateway pods without downtime. When the restart they load the new certificate:. ```; kubectl rollout restart deployment gateway-deployment; ```. ## Revoking Certificates. First, gather a list of the crt.sh IDs for the certificates you want to revoke from; https://crt.sh/?q=YOUR_DOMAIN_HERE . You will notice there is always a precertificate and a leaf; certificate. Both have the same serial number so revoking one revokes the other. In the next step,; the command will fail if you specify an already revoked certificate, so you should only specify one; of each precertificate and leaf certificate pair. To get list of IDs run the following:. ```; $ CERT_IDS_TO_REVOKE=$(curl https://crt.sh/?q=YOUR_DOMAIN_HERE | pup 'td.outer a json{}' | jq '.[].text' | egrep -o '[0-9]{10}'); $ CERT_IDS_TO_REVOKE=$(echo \'$CERT_IDS_TO_REVOKE\'); $ echo $CERT_IDS_TO_REVOKE; '6503198927 6503193970 6128196502'; ```. ```; make -C letsencrypt revoke CERT_IDS_TO_REVOKE='6503198927 6503193970 6128196502'; ```; ",MatchSource.DOCS,dev-docs/services/letsencrypt.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md:301,Deployability,deploy,deployment,301,"# Let's Encrypt. Hail uses Let's Encrypt certificates for the gateway pod. ## Refreshing Certificates. Update the `letsencrypt-config` secret:. ```; make -C letsencrypt run; ```. Restart your gateway pods without downtime. When the restart they load the new certificate:. ```; kubectl rollout restart deployment gateway-deployment; ```. ## Revoking Certificates. First, gather a list of the crt.sh IDs for the certificates you want to revoke from; https://crt.sh/?q=YOUR_DOMAIN_HERE . You will notice there is always a precertificate and a leaf; certificate. Both have the same serial number so revoking one revokes the other. In the next step,; the command will fail if you specify an already revoked certificate, so you should only specify one; of each precertificate and leaf certificate pair. To get list of IDs run the following:. ```; $ CERT_IDS_TO_REVOKE=$(curl https://crt.sh/?q=YOUR_DOMAIN_HERE | pup 'td.outer a json{}' | jq '.[].text' | egrep -o '[0-9]{10}'); $ CERT_IDS_TO_REVOKE=$(echo \'$CERT_IDS_TO_REVOKE\'); $ echo $CERT_IDS_TO_REVOKE; '6503198927 6503193970 6128196502'; ```. ```; make -C letsencrypt revoke CERT_IDS_TO_REVOKE='6503198927 6503193970 6128196502'; ```; ",MatchSource.DOCS,dev-docs/services/letsencrypt.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md:320,Deployability,deploy,deployment,320,"# Let's Encrypt. Hail uses Let's Encrypt certificates for the gateway pod. ## Refreshing Certificates. Update the `letsencrypt-config` secret:. ```; make -C letsencrypt run; ```. Restart your gateway pods without downtime. When the restart they load the new certificate:. ```; kubectl rollout restart deployment gateway-deployment; ```. ## Revoking Certificates. First, gather a list of the crt.sh IDs for the certificates you want to revoke from; https://crt.sh/?q=YOUR_DOMAIN_HERE . You will notice there is always a precertificate and a leaf; certificate. Both have the same serial number so revoking one revokes the other. In the next step,; the command will fail if you specify an already revoked certificate, so you should only specify one; of each precertificate and leaf certificate pair. To get list of IDs run the following:. ```; $ CERT_IDS_TO_REVOKE=$(curl https://crt.sh/?q=YOUR_DOMAIN_HERE | pup 'td.outer a json{}' | jq '.[].text' | egrep -o '[0-9]{10}'); $ CERT_IDS_TO_REVOKE=$(echo \'$CERT_IDS_TO_REVOKE\'); $ echo $CERT_IDS_TO_REVOKE; '6503198927 6503193970 6128196502'; ```. ```; make -C letsencrypt revoke CERT_IDS_TO_REVOKE='6503198927 6503193970 6128196502'; ```; ",MatchSource.DOCS,dev-docs/services/letsencrypt.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md:127,Modifiability,config,config,127,"# Let's Encrypt. Hail uses Let's Encrypt certificates for the gateway pod. ## Refreshing Certificates. Update the `letsencrypt-config` secret:. ```; make -C letsencrypt run; ```. Restart your gateway pods without downtime. When the restart they load the new certificate:. ```; kubectl rollout restart deployment gateway-deployment; ```. ## Revoking Certificates. First, gather a list of the crt.sh IDs for the certificates you want to revoke from; https://crt.sh/?q=YOUR_DOMAIN_HERE . You will notice there is always a precertificate and a leaf; certificate. Both have the same serial number so revoking one revokes the other. In the next step,; the command will fail if you specify an already revoked certificate, so you should only specify one; of each precertificate and leaf certificate pair. To get list of IDs run the following:. ```; $ CERT_IDS_TO_REVOKE=$(curl https://crt.sh/?q=YOUR_DOMAIN_HERE | pup 'td.outer a json{}' | jq '.[].text' | egrep -o '[0-9]{10}'); $ CERT_IDS_TO_REVOKE=$(echo \'$CERT_IDS_TO_REVOKE\'); $ echo $CERT_IDS_TO_REVOKE; '6503198927 6503193970 6128196502'; ```. ```; make -C letsencrypt revoke CERT_IDS_TO_REVOKE='6503198927 6503193970 6128196502'; ```; ",MatchSource.DOCS,dev-docs/services/letsencrypt.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md:245,Performance,load,load,245,"# Let's Encrypt. Hail uses Let's Encrypt certificates for the gateway pod. ## Refreshing Certificates. Update the `letsencrypt-config` secret:. ```; make -C letsencrypt run; ```. Restart your gateway pods without downtime. When the restart they load the new certificate:. ```; kubectl rollout restart deployment gateway-deployment; ```. ## Revoking Certificates. First, gather a list of the crt.sh IDs for the certificates you want to revoke from; https://crt.sh/?q=YOUR_DOMAIN_HERE . You will notice there is always a precertificate and a leaf; certificate. Both have the same serial number so revoking one revokes the other. In the next step,; the command will fail if you specify an already revoked certificate, so you should only specify one; of each precertificate and leaf certificate pair. To get list of IDs run the following:. ```; $ CERT_IDS_TO_REVOKE=$(curl https://crt.sh/?q=YOUR_DOMAIN_HERE | pup 'td.outer a json{}' | jq '.[].text' | egrep -o '[0-9]{10}'); $ CERT_IDS_TO_REVOKE=$(echo \'$CERT_IDS_TO_REVOKE\'); $ echo $CERT_IDS_TO_REVOKE; '6503198927 6503193970 6128196502'; ```. ```; make -C letsencrypt revoke CERT_IDS_TO_REVOKE='6503198927 6503193970 6128196502'; ```; ",MatchSource.DOCS,dev-docs/services/letsencrypt.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md:41,Security,certificate,certificates,41,"# Let's Encrypt. Hail uses Let's Encrypt certificates for the gateway pod. ## Refreshing Certificates. Update the `letsencrypt-config` secret:. ```; make -C letsencrypt run; ```. Restart your gateway pods without downtime. When the restart they load the new certificate:. ```; kubectl rollout restart deployment gateway-deployment; ```. ## Revoking Certificates. First, gather a list of the crt.sh IDs for the certificates you want to revoke from; https://crt.sh/?q=YOUR_DOMAIN_HERE . You will notice there is always a precertificate and a leaf; certificate. Both have the same serial number so revoking one revokes the other. In the next step,; the command will fail if you specify an already revoked certificate, so you should only specify one; of each precertificate and leaf certificate pair. To get list of IDs run the following:. ```; $ CERT_IDS_TO_REVOKE=$(curl https://crt.sh/?q=YOUR_DOMAIN_HERE | pup 'td.outer a json{}' | jq '.[].text' | egrep -o '[0-9]{10}'); $ CERT_IDS_TO_REVOKE=$(echo \'$CERT_IDS_TO_REVOKE\'); $ echo $CERT_IDS_TO_REVOKE; '6503198927 6503193970 6128196502'; ```. ```; make -C letsencrypt revoke CERT_IDS_TO_REVOKE='6503198927 6503193970 6128196502'; ```; ",MatchSource.DOCS,dev-docs/services/letsencrypt.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md:258,Security,certificate,certificate,258,"# Let's Encrypt. Hail uses Let's Encrypt certificates for the gateway pod. ## Refreshing Certificates. Update the `letsencrypt-config` secret:. ```; make -C letsencrypt run; ```. Restart your gateway pods without downtime. When the restart they load the new certificate:. ```; kubectl rollout restart deployment gateway-deployment; ```. ## Revoking Certificates. First, gather a list of the crt.sh IDs for the certificates you want to revoke from; https://crt.sh/?q=YOUR_DOMAIN_HERE . You will notice there is always a precertificate and a leaf; certificate. Both have the same serial number so revoking one revokes the other. In the next step,; the command will fail if you specify an already revoked certificate, so you should only specify one; of each precertificate and leaf certificate pair. To get list of IDs run the following:. ```; $ CERT_IDS_TO_REVOKE=$(curl https://crt.sh/?q=YOUR_DOMAIN_HERE | pup 'td.outer a json{}' | jq '.[].text' | egrep -o '[0-9]{10}'); $ CERT_IDS_TO_REVOKE=$(echo \'$CERT_IDS_TO_REVOKE\'); $ echo $CERT_IDS_TO_REVOKE; '6503198927 6503193970 6128196502'; ```. ```; make -C letsencrypt revoke CERT_IDS_TO_REVOKE='6503198927 6503193970 6128196502'; ```; ",MatchSource.DOCS,dev-docs/services/letsencrypt.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md:410,Security,certificate,certificates,410,"# Let's Encrypt. Hail uses Let's Encrypt certificates for the gateway pod. ## Refreshing Certificates. Update the `letsencrypt-config` secret:. ```; make -C letsencrypt run; ```. Restart your gateway pods without downtime. When the restart they load the new certificate:. ```; kubectl rollout restart deployment gateway-deployment; ```. ## Revoking Certificates. First, gather a list of the crt.sh IDs for the certificates you want to revoke from; https://crt.sh/?q=YOUR_DOMAIN_HERE . You will notice there is always a precertificate and a leaf; certificate. Both have the same serial number so revoking one revokes the other. In the next step,; the command will fail if you specify an already revoked certificate, so you should only specify one; of each precertificate and leaf certificate pair. To get list of IDs run the following:. ```; $ CERT_IDS_TO_REVOKE=$(curl https://crt.sh/?q=YOUR_DOMAIN_HERE | pup 'td.outer a json{}' | jq '.[].text' | egrep -o '[0-9]{10}'); $ CERT_IDS_TO_REVOKE=$(echo \'$CERT_IDS_TO_REVOKE\'); $ echo $CERT_IDS_TO_REVOKE; '6503198927 6503193970 6128196502'; ```. ```; make -C letsencrypt revoke CERT_IDS_TO_REVOKE='6503198927 6503193970 6128196502'; ```; ",MatchSource.DOCS,dev-docs/services/letsencrypt.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md:546,Security,certificate,certificate,546,"# Let's Encrypt. Hail uses Let's Encrypt certificates for the gateway pod. ## Refreshing Certificates. Update the `letsencrypt-config` secret:. ```; make -C letsencrypt run; ```. Restart your gateway pods without downtime. When the restart they load the new certificate:. ```; kubectl rollout restart deployment gateway-deployment; ```. ## Revoking Certificates. First, gather a list of the crt.sh IDs for the certificates you want to revoke from; https://crt.sh/?q=YOUR_DOMAIN_HERE . You will notice there is always a precertificate and a leaf; certificate. Both have the same serial number so revoking one revokes the other. In the next step,; the command will fail if you specify an already revoked certificate, so you should only specify one; of each precertificate and leaf certificate pair. To get list of IDs run the following:. ```; $ CERT_IDS_TO_REVOKE=$(curl https://crt.sh/?q=YOUR_DOMAIN_HERE | pup 'td.outer a json{}' | jq '.[].text' | egrep -o '[0-9]{10}'); $ CERT_IDS_TO_REVOKE=$(echo \'$CERT_IDS_TO_REVOKE\'); $ echo $CERT_IDS_TO_REVOKE; '6503198927 6503193970 6128196502'; ```. ```; make -C letsencrypt revoke CERT_IDS_TO_REVOKE='6503198927 6503193970 6128196502'; ```; ",MatchSource.DOCS,dev-docs/services/letsencrypt.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md:702,Security,certificate,certificate,702,"# Let's Encrypt. Hail uses Let's Encrypt certificates for the gateway pod. ## Refreshing Certificates. Update the `letsencrypt-config` secret:. ```; make -C letsencrypt run; ```. Restart your gateway pods without downtime. When the restart they load the new certificate:. ```; kubectl rollout restart deployment gateway-deployment; ```. ## Revoking Certificates. First, gather a list of the crt.sh IDs for the certificates you want to revoke from; https://crt.sh/?q=YOUR_DOMAIN_HERE . You will notice there is always a precertificate and a leaf; certificate. Both have the same serial number so revoking one revokes the other. In the next step,; the command will fail if you specify an already revoked certificate, so you should only specify one; of each precertificate and leaf certificate pair. To get list of IDs run the following:. ```; $ CERT_IDS_TO_REVOKE=$(curl https://crt.sh/?q=YOUR_DOMAIN_HERE | pup 'td.outer a json{}' | jq '.[].text' | egrep -o '[0-9]{10}'); $ CERT_IDS_TO_REVOKE=$(echo \'$CERT_IDS_TO_REVOKE\'); $ echo $CERT_IDS_TO_REVOKE; '6503198927 6503193970 6128196502'; ```. ```; make -C letsencrypt revoke CERT_IDS_TO_REVOKE='6503198927 6503193970 6128196502'; ```; ",MatchSource.DOCS,dev-docs/services/letsencrypt.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md:779,Security,certificate,certificate,779,"# Let's Encrypt. Hail uses Let's Encrypt certificates for the gateway pod. ## Refreshing Certificates. Update the `letsencrypt-config` secret:. ```; make -C letsencrypt run; ```. Restart your gateway pods without downtime. When the restart they load the new certificate:. ```; kubectl rollout restart deployment gateway-deployment; ```. ## Revoking Certificates. First, gather a list of the crt.sh IDs for the certificates you want to revoke from; https://crt.sh/?q=YOUR_DOMAIN_HERE . You will notice there is always a precertificate and a leaf; certificate. Both have the same serial number so revoking one revokes the other. In the next step,; the command will fail if you specify an already revoked certificate, so you should only specify one; of each precertificate and leaf certificate pair. To get list of IDs run the following:. ```; $ CERT_IDS_TO_REVOKE=$(curl https://crt.sh/?q=YOUR_DOMAIN_HERE | pup 'td.outer a json{}' | jq '.[].text' | egrep -o '[0-9]{10}'); $ CERT_IDS_TO_REVOKE=$(echo \'$CERT_IDS_TO_REVOKE\'); $ echo $CERT_IDS_TO_REVOKE; '6503198927 6503193970 6128196502'; ```. ```; make -C letsencrypt revoke CERT_IDS_TO_REVOKE='6503198927 6503193970 6128196502'; ```; ",MatchSource.DOCS,dev-docs/services/letsencrypt.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/letsencrypt.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md:2188,Availability,down,down,2188,"sends data requests to your dev namespace service; - Note the use of the 'x-hail-return-jinja-context' header which instructs the remote server to return json data instead of rendering the page.; - This header can also be used elsewhere to see what data the jinja templates are working with; - The dev proxy renders the data using your local template, and that is what you see. #### I messed up the Batch database in my dev namespace. How do I start fresh?. If you only want to delete the Batch database and leave the other databases alone,; you can submit a dev deploy using the `delete_batch_tables` job. The following; will create a dev deploy that removes your current Batch database and redeploys; Batch with a fresh one:. ```bash; hailctl dev deploy -b <github_username>/hail:<your branch> -s delete_batch_tables,deploy_batch; ```. If you want to start totally clean, another option is to delete your dev namespace's; database completely by deleting the underlying Kubernetes resources.; The database is a Kubernetes `StatefulSet`, with the data stored in a; persistent disk owned by a `PersistentVolumeClaim`. Deleting the `StatefulSet` will; delete the MySQL pod, but not the underlying claim/data.; So to get a totally clean slate, you must delete both resources:. ```bash; kubectl -n <my_namespace> delete statefulset db; # When that's done...; kubectl -n <my_namespace> delete pvc mysql-persistent-storage-db-0; ```. The next dev deploy will set up a new database:. ```bash; hailctl dev deploy -b <github_username>/hail:<your branch> -s deploy_batch,add_developers; ```. #### My namespace scaled down overnight. How do I get them back?. There is a Kubernetes `CronJob` that runs in the evenings that scales down; development namespaces. To scale back up, you need to use `kubectl scale`,; or you can use the devbin function `kscale`, like. ```bash; kscale <your_namespace> up; ```. If you want to manually scale down your namespace when not using it, run; `kscale <your_namespace> down`.; ",MatchSource.DOCS,dev-docs/services/services-development-faq.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md:2299,Availability,down,down,2299,"sends data requests to your dev namespace service; - Note the use of the 'x-hail-return-jinja-context' header which instructs the remote server to return json data instead of rendering the page.; - This header can also be used elsewhere to see what data the jinja templates are working with; - The dev proxy renders the data using your local template, and that is what you see. #### I messed up the Batch database in my dev namespace. How do I start fresh?. If you only want to delete the Batch database and leave the other databases alone,; you can submit a dev deploy using the `delete_batch_tables` job. The following; will create a dev deploy that removes your current Batch database and redeploys; Batch with a fresh one:. ```bash; hailctl dev deploy -b <github_username>/hail:<your branch> -s delete_batch_tables,deploy_batch; ```. If you want to start totally clean, another option is to delete your dev namespace's; database completely by deleting the underlying Kubernetes resources.; The database is a Kubernetes `StatefulSet`, with the data stored in a; persistent disk owned by a `PersistentVolumeClaim`. Deleting the `StatefulSet` will; delete the MySQL pod, but not the underlying claim/data.; So to get a totally clean slate, you must delete both resources:. ```bash; kubectl -n <my_namespace> delete statefulset db; # When that's done...; kubectl -n <my_namespace> delete pvc mysql-persistent-storage-db-0; ```. The next dev deploy will set up a new database:. ```bash; hailctl dev deploy -b <github_username>/hail:<your branch> -s deploy_batch,add_developers; ```. #### My namespace scaled down overnight. How do I get them back?. There is a Kubernetes `CronJob` that runs in the evenings that scales down; development namespaces. To scale back up, you need to use `kubectl scale`,; or you can use the devbin function `kscale`, like. ```bash; kscale <your_namespace> up; ```. If you want to manually scale down your namespace when not using it, run; `kscale <your_namespace> down`.; ",MatchSource.DOCS,dev-docs/services/services-development-faq.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md:2504,Availability,down,down,2504,"sends data requests to your dev namespace service; - Note the use of the 'x-hail-return-jinja-context' header which instructs the remote server to return json data instead of rendering the page.; - This header can also be used elsewhere to see what data the jinja templates are working with; - The dev proxy renders the data using your local template, and that is what you see. #### I messed up the Batch database in my dev namespace. How do I start fresh?. If you only want to delete the Batch database and leave the other databases alone,; you can submit a dev deploy using the `delete_batch_tables` job. The following; will create a dev deploy that removes your current Batch database and redeploys; Batch with a fresh one:. ```bash; hailctl dev deploy -b <github_username>/hail:<your branch> -s delete_batch_tables,deploy_batch; ```. If you want to start totally clean, another option is to delete your dev namespace's; database completely by deleting the underlying Kubernetes resources.; The database is a Kubernetes `StatefulSet`, with the data stored in a; persistent disk owned by a `PersistentVolumeClaim`. Deleting the `StatefulSet` will; delete the MySQL pod, but not the underlying claim/data.; So to get a totally clean slate, you must delete both resources:. ```bash; kubectl -n <my_namespace> delete statefulset db; # When that's done...; kubectl -n <my_namespace> delete pvc mysql-persistent-storage-db-0; ```. The next dev deploy will set up a new database:. ```bash; hailctl dev deploy -b <github_username>/hail:<your branch> -s deploy_batch,add_developers; ```. #### My namespace scaled down overnight. How do I get them back?. There is a Kubernetes `CronJob` that runs in the evenings that scales down; development namespaces. To scale back up, you need to use `kubectl scale`,; or you can use the devbin function `kscale`, like. ```bash; kscale <your_namespace> up; ```. If you want to manually scale down your namespace when not using it, run; `kscale <your_namespace> down`.; ",MatchSource.DOCS,dev-docs/services/services-development-faq.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md:2573,Availability,down,down,2573,"sends data requests to your dev namespace service; - Note the use of the 'x-hail-return-jinja-context' header which instructs the remote server to return json data instead of rendering the page.; - This header can also be used elsewhere to see what data the jinja templates are working with; - The dev proxy renders the data using your local template, and that is what you see. #### I messed up the Batch database in my dev namespace. How do I start fresh?. If you only want to delete the Batch database and leave the other databases alone,; you can submit a dev deploy using the `delete_batch_tables` job. The following; will create a dev deploy that removes your current Batch database and redeploys; Batch with a fresh one:. ```bash; hailctl dev deploy -b <github_username>/hail:<your branch> -s delete_batch_tables,deploy_batch; ```. If you want to start totally clean, another option is to delete your dev namespace's; database completely by deleting the underlying Kubernetes resources.; The database is a Kubernetes `StatefulSet`, with the data stored in a; persistent disk owned by a `PersistentVolumeClaim`. Deleting the `StatefulSet` will; delete the MySQL pod, but not the underlying claim/data.; So to get a totally clean slate, you must delete both resources:. ```bash; kubectl -n <my_namespace> delete statefulset db; # When that's done...; kubectl -n <my_namespace> delete pvc mysql-persistent-storage-db-0; ```. The next dev deploy will set up a new database:. ```bash; hailctl dev deploy -b <github_username>/hail:<your branch> -s deploy_batch,add_developers; ```. #### My namespace scaled down overnight. How do I get them back?. There is a Kubernetes `CronJob` that runs in the evenings that scales down; development namespaces. To scale back up, you need to use `kubectl scale`,; or you can use the devbin function `kscale`, like. ```bash; kscale <your_namespace> up; ```. If you want to manually scale down your namespace when not using it, run; `kscale <your_namespace> down`.; ",MatchSource.DOCS,dev-docs/services/services-development-faq.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md:136,Deployability,install,install,136,"# FAQ when developing Hail Batch. #### How do I bring up the local UI dev service?. Eg for `batch`:; ```bash; # First time only:; $ pip install -e web_common -e batch -e gear. #Then:; $ SERVICE=batch make devserver; ```. This starts running a local instance of the batch UI for faster turnaround developing; UI features. Note that:; - HTML (template) changes will be reflected instantly when you refresh the page; - Python code changes will not be reflected in the devserver:; - Your UI requests to localhost get sent to the [dev proxy](../../devbin/dev_proxy.py); - The dev proxy sends data requests to your dev namespace service; - Note the use of the 'x-hail-return-jinja-context' header which instructs the remote server to return json data instead of rendering the page.; - This header can also be used elsewhere to see what data the jinja templates are working with; - The dev proxy renders the data using your local template, and that is what you see. #### I messed up the Batch database in my dev namespace. How do I start fresh?. If you only want to delete the Batch database and leave the other databases alone,; you can submit a dev deploy using the `delete_batch_tables` job. The following; will create a dev deploy that removes your current Batch database and redeploys; Batch with a fresh one:. ```bash; hailctl dev deploy -b <github_username>/hail:<your branch> -s delete_batch_tables,deploy_batch; ```. If you want to start totally clean, another option is to delete your dev namespace's; database completely by deleting the underlying Kubernetes resources.; The database is a Kubernetes `StatefulSet`, with the data stored in a; persistent disk owned by a `PersistentVolumeClaim`. Deleting the `StatefulSet` will; delete the MySQL pod, but not the underlying claim/data.; So to get a totally clean slate, you must delete both resources:. ```bash; kubectl -n <my_namespace> delete statefulset db; # When that's done...; kubectl -n <my_namespace> delete pvc mysql-persistent-storage-db-",MatchSource.DOCS,dev-docs/services/services-development-faq.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md:1144,Deployability,deploy,deploy,1144,"First time only:; $ pip install -e web_common -e batch -e gear. #Then:; $ SERVICE=batch make devserver; ```. This starts running a local instance of the batch UI for faster turnaround developing; UI features. Note that:; - HTML (template) changes will be reflected instantly when you refresh the page; - Python code changes will not be reflected in the devserver:; - Your UI requests to localhost get sent to the [dev proxy](../../devbin/dev_proxy.py); - The dev proxy sends data requests to your dev namespace service; - Note the use of the 'x-hail-return-jinja-context' header which instructs the remote server to return json data instead of rendering the page.; - This header can also be used elsewhere to see what data the jinja templates are working with; - The dev proxy renders the data using your local template, and that is what you see. #### I messed up the Batch database in my dev namespace. How do I start fresh?. If you only want to delete the Batch database and leave the other databases alone,; you can submit a dev deploy using the `delete_batch_tables` job. The following; will create a dev deploy that removes your current Batch database and redeploys; Batch with a fresh one:. ```bash; hailctl dev deploy -b <github_username>/hail:<your branch> -s delete_batch_tables,deploy_batch; ```. If you want to start totally clean, another option is to delete your dev namespace's; database completely by deleting the underlying Kubernetes resources.; The database is a Kubernetes `StatefulSet`, with the data stored in a; persistent disk owned by a `PersistentVolumeClaim`. Deleting the `StatefulSet` will; delete the MySQL pod, but not the underlying claim/data.; So to get a totally clean slate, you must delete both resources:. ```bash; kubectl -n <my_namespace> delete statefulset db; # When that's done...; kubectl -n <my_namespace> delete pvc mysql-persistent-storage-db-0; ```. The next dev deploy will set up a new database:. ```bash; hailctl dev deploy -b <github_username>/hail:<",MatchSource.DOCS,dev-docs/services/services-development-faq.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md:1221,Deployability,deploy,deploy,1221,"l instance of the batch UI for faster turnaround developing; UI features. Note that:; - HTML (template) changes will be reflected instantly when you refresh the page; - Python code changes will not be reflected in the devserver:; - Your UI requests to localhost get sent to the [dev proxy](../../devbin/dev_proxy.py); - The dev proxy sends data requests to your dev namespace service; - Note the use of the 'x-hail-return-jinja-context' header which instructs the remote server to return json data instead of rendering the page.; - This header can also be used elsewhere to see what data the jinja templates are working with; - The dev proxy renders the data using your local template, and that is what you see. #### I messed up the Batch database in my dev namespace. How do I start fresh?. If you only want to delete the Batch database and leave the other databases alone,; you can submit a dev deploy using the `delete_batch_tables` job. The following; will create a dev deploy that removes your current Batch database and redeploys; Batch with a fresh one:. ```bash; hailctl dev deploy -b <github_username>/hail:<your branch> -s delete_batch_tables,deploy_batch; ```. If you want to start totally clean, another option is to delete your dev namespace's; database completely by deleting the underlying Kubernetes resources.; The database is a Kubernetes `StatefulSet`, with the data stored in a; persistent disk owned by a `PersistentVolumeClaim`. Deleting the `StatefulSet` will; delete the MySQL pod, but not the underlying claim/data.; So to get a totally clean slate, you must delete both resources:. ```bash; kubectl -n <my_namespace> delete statefulset db; # When that's done...; kubectl -n <my_namespace> delete pvc mysql-persistent-storage-db-0; ```. The next dev deploy will set up a new database:. ```bash; hailctl dev deploy -b <github_username>/hail:<your branch> -s deploy_batch,add_developers; ```. #### My namespace scaled down overnight. How do I get them back?. There is a Kubernet",MatchSource.DOCS,dev-docs/services/services-development-faq.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md:1330,Deployability,deploy,deploy,1330," be reflected instantly when you refresh the page; - Python code changes will not be reflected in the devserver:; - Your UI requests to localhost get sent to the [dev proxy](../../devbin/dev_proxy.py); - The dev proxy sends data requests to your dev namespace service; - Note the use of the 'x-hail-return-jinja-context' header which instructs the remote server to return json data instead of rendering the page.; - This header can also be used elsewhere to see what data the jinja templates are working with; - The dev proxy renders the data using your local template, and that is what you see. #### I messed up the Batch database in my dev namespace. How do I start fresh?. If you only want to delete the Batch database and leave the other databases alone,; you can submit a dev deploy using the `delete_batch_tables` job. The following; will create a dev deploy that removes your current Batch database and redeploys; Batch with a fresh one:. ```bash; hailctl dev deploy -b <github_username>/hail:<your branch> -s delete_batch_tables,deploy_batch; ```. If you want to start totally clean, another option is to delete your dev namespace's; database completely by deleting the underlying Kubernetes resources.; The database is a Kubernetes `StatefulSet`, with the data stored in a; persistent disk owned by a `PersistentVolumeClaim`. Deleting the `StatefulSet` will; delete the MySQL pod, but not the underlying claim/data.; So to get a totally clean slate, you must delete both resources:. ```bash; kubectl -n <my_namespace> delete statefulset db; # When that's done...; kubectl -n <my_namespace> delete pvc mysql-persistent-storage-db-0; ```. The next dev deploy will set up a new database:. ```bash; hailctl dev deploy -b <github_username>/hail:<your branch> -s deploy_batch,add_developers; ```. #### My namespace scaled down overnight. How do I get them back?. There is a Kubernetes `CronJob` that runs in the evenings that scales down; development namespaces. To scale back up, you need to use ",MatchSource.DOCS,dev-docs/services/services-development-faq.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md:2022,Deployability,deploy,deploy,2022,"sends data requests to your dev namespace service; - Note the use of the 'x-hail-return-jinja-context' header which instructs the remote server to return json data instead of rendering the page.; - This header can also be used elsewhere to see what data the jinja templates are working with; - The dev proxy renders the data using your local template, and that is what you see. #### I messed up the Batch database in my dev namespace. How do I start fresh?. If you only want to delete the Batch database and leave the other databases alone,; you can submit a dev deploy using the `delete_batch_tables` job. The following; will create a dev deploy that removes your current Batch database and redeploys; Batch with a fresh one:. ```bash; hailctl dev deploy -b <github_username>/hail:<your branch> -s delete_batch_tables,deploy_batch; ```. If you want to start totally clean, another option is to delete your dev namespace's; database completely by deleting the underlying Kubernetes resources.; The database is a Kubernetes `StatefulSet`, with the data stored in a; persistent disk owned by a `PersistentVolumeClaim`. Deleting the `StatefulSet` will; delete the MySQL pod, but not the underlying claim/data.; So to get a totally clean slate, you must delete both resources:. ```bash; kubectl -n <my_namespace> delete statefulset db; # When that's done...; kubectl -n <my_namespace> delete pvc mysql-persistent-storage-db-0; ```. The next dev deploy will set up a new database:. ```bash; hailctl dev deploy -b <github_username>/hail:<your branch> -s deploy_batch,add_developers; ```. #### My namespace scaled down overnight. How do I get them back?. There is a Kubernetes `CronJob` that runs in the evenings that scales down; development namespaces. To scale back up, you need to use `kubectl scale`,; or you can use the devbin function `kscale`, like. ```bash; kscale <your_namespace> up; ```. If you want to manually scale down your namespace when not using it, run; `kscale <your_namespace> down`.; ",MatchSource.DOCS,dev-docs/services/services-development-faq.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md:2079,Deployability,deploy,deploy,2079,"sends data requests to your dev namespace service; - Note the use of the 'x-hail-return-jinja-context' header which instructs the remote server to return json data instead of rendering the page.; - This header can also be used elsewhere to see what data the jinja templates are working with; - The dev proxy renders the data using your local template, and that is what you see. #### I messed up the Batch database in my dev namespace. How do I start fresh?. If you only want to delete the Batch database and leave the other databases alone,; you can submit a dev deploy using the `delete_batch_tables` job. The following; will create a dev deploy that removes your current Batch database and redeploys; Batch with a fresh one:. ```bash; hailctl dev deploy -b <github_username>/hail:<your branch> -s delete_batch_tables,deploy_batch; ```. If you want to start totally clean, another option is to delete your dev namespace's; database completely by deleting the underlying Kubernetes resources.; The database is a Kubernetes `StatefulSet`, with the data stored in a; persistent disk owned by a `PersistentVolumeClaim`. Deleting the `StatefulSet` will; delete the MySQL pod, but not the underlying claim/data.; So to get a totally clean slate, you must delete both resources:. ```bash; kubectl -n <my_namespace> delete statefulset db; # When that's done...; kubectl -n <my_namespace> delete pvc mysql-persistent-storage-db-0; ```. The next dev deploy will set up a new database:. ```bash; hailctl dev deploy -b <github_username>/hail:<your branch> -s deploy_batch,add_developers; ```. #### My namespace scaled down overnight. How do I get them back?. There is a Kubernetes `CronJob` that runs in the evenings that scales down; development namespaces. To scale back up, you need to use `kubectl scale`,; or you can use the devbin function `kscale`, like. ```bash; kscale <your_namespace> up; ```. If you want to manually scale down your namespace when not using it, run; `kscale <your_namespace> down`.; ",MatchSource.DOCS,dev-docs/services/services-development-faq.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/services-development-faq.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:1766,Availability,failure,failure,1766,"reating instances; of a Java `KeyStore`. It is not possible to [elide a; password](https://stackoverflow.com/questions/27497723/export-a-pkcs12-file-without-an-export-password).; Using the empty string as a password is not recommended because many tools do; not properly support it. ```; openssl pkcs12 -export \; -out server-keystore.p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though it is not itself expired. Download the Hail root; certificate to a local file and inspect the start and end dates. ```; kubectl get secrets ssl-config-hail-root -o json \; | jq -r '.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. ",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:2394,Availability,down,download,2394,"e textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though it is not itself expired. Download the Hail root; certificate to a local file and inspect the start and end dates. ```; kubectl get secrets ssl-config-hail-root -o json \; | jq -r '.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. Once all services have restarted, there should be no; downtime. 1. Regenerate the root certificate (from your laptop):. ```; openssl req -new -x509 \; -subj /CN=hail-root \; -nodes \; -newkey rsa:4096 \; -keyout hail-root-key.pem \; -out hail-root-cert.pem \; -days 365 \; -sha256; ```. 2. Update kubernetes with the new root certificate:. ```; kubectl create secret generic \; -n default ssl-config-hail-root \; --from-file=hail-root-key.pem \; --from-file=hail-root-cert.pem \; --save-config \; --dry-run=client \; -o yaml \; | kubectl apply ",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:2763,Availability,downtime,downtime,2763,"ways; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though it is not itself expired. Download the Hail root; certificate to a local file and inspect the start and end dates. ```; kubectl get secrets ssl-config-hail-root -o json \; | jq -r '.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. Once all services have restarted, there should be no; downtime. 1. Regenerate the root certificate (from your laptop):. ```; openssl req -new -x509 \; -subj /CN=hail-root \; -nodes \; -newkey rsa:4096 \; -keyout hail-root-key.pem \; -out hail-root-cert.pem \; -days 365 \; -sha256; ```. 2. Update kubernetes with the new root certificate:. ```; kubectl create secret generic \; -n default ssl-config-hail-root \; --from-file=hail-root-key.pem \; --from-file=hail-root-cert.pem \; --save-config \; --dry-run=client \; -o yaml \; | kubectl apply -f -; ```. 3. Update all the service certificates:. ```; make -C $HAIL/hail python/hailtop/hail_version. PYTHONPATH=$HAIL/hail/python \; python3 $HAIL/tls/create_certs.py \; default \; $HAIL/tls/config.yaml \; hail-root-key.pem \; hail-root-cert.pem; ```. 4. Get a list of all the services for that need to be restarted (some of these are; not actually services, but including them in the nex",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:2916,Availability,downtime,downtime,2916,"-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though it is not itself expired. Download the Hail root; certificate to a local file and inspect the start and end dates. ```; kubectl get secrets ssl-config-hail-root -o json \; | jq -r '.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. Once all services have restarted, there should be no; downtime. 1. Regenerate the root certificate (from your laptop):. ```; openssl req -new -x509 \; -subj /CN=hail-root \; -nodes \; -newkey rsa:4096 \; -keyout hail-root-key.pem \; -out hail-root-cert.pem \; -days 365 \; -sha256; ```. 2. Update kubernetes with the new root certificate:. ```; kubectl create secret generic \; -n default ssl-config-hail-root \; --from-file=hail-root-key.pem \; --from-file=hail-root-cert.pem \; --save-config \; --dry-run=client \; -o yaml \; | kubectl apply -f -; ```. 3. Update all the service certificates:. ```; make -C $HAIL/hail python/hailtop/hail_version. PYTHONPATH=$HAIL/hail/python \; python3 $HAIL/tls/create_certs.py \; default \; $HAIL/tls/config.yaml \; hail-root-key.pem \; hail-root-cert.pem; ```. 4. Get a list of all the services for that need to be restarted (some of these are; not actually services, but including them in the next step is OK). ```; SERVICES_TO_RESTART=$(python3 -c 'import os; import yaml; hail_dir = os.get",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:4099,Deployability,deploy,deployments,4099,"'.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. Once all services have restarted, there should be no; downtime. 1. Regenerate the root certificate (from your laptop):. ```; openssl req -new -x509 \; -subj /CN=hail-root \; -nodes \; -newkey rsa:4096 \; -keyout hail-root-key.pem \; -out hail-root-cert.pem \; -days 365 \; -sha256; ```. 2. Update kubernetes with the new root certificate:. ```; kubectl create secret generic \; -n default ssl-config-hail-root \; --from-file=hail-root-key.pem \; --from-file=hail-root-cert.pem \; --save-config \; --dry-run=client \; -o yaml \; | kubectl apply -f -; ```. 3. Update all the service certificates:. ```; make -C $HAIL/hail python/hailtop/hail_version. PYTHONPATH=$HAIL/hail/python \; python3 $HAIL/tls/create_certs.py \; default \; $HAIL/tls/config.yaml \; hail-root-key.pem \; hail-root-cert.pem; ```. 4. Get a list of all the services for that need to be restarted (some of these are; not actually services, but including them in the next step is OK). ```; SERVICES_TO_RESTART=$(python3 -c 'import os; import yaml; hail_dir = os.getenv(""HAIL""); x = yaml.safe_load(open(f""{hail_dir}/tls/config.yaml""))[""principals""]; print("","".join(x[""name""] for x in x))'); ```. 5. Restart all the services by deleting the pods (but, critically, not the; deployments):. ```; kubectl delete pods -l ""app in ($SERVICES_TO_RESTART)""; ```; ",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:2144,Modifiability,config,config-hail-root,2144,"``. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though it is not itself expired. Download the Hail root; certificate to a local file and inspect the start and end dates. ```; kubectl get secrets ssl-config-hail-root -o json \; | jq -r '.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. Once all services have restarted, there should be no; downtime. 1. Regenerate the root certificate (from your laptop):. ```; openssl req -new -x509 \; -subj /CN=hail-root \; -nodes \; -newkey rsa:4096 \; -keyout hail-root-key.pem \; -out hail-root-cert.pem \; -days 365 \; -sha256; ```.",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:2462,Modifiability,config,config-batch,2462,"x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though it is not itself expired. Download the Hail root; certificate to a local file and inspect the start and end dates. ```; kubectl get secrets ssl-config-hail-root -o json \; | jq -r '.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. Once all services have restarted, there should be no; downtime. 1. Regenerate the root certificate (from your laptop):. ```; openssl req -new -x509 \; -subj /CN=hail-root \; -nodes \; -newkey rsa:4096 \; -keyout hail-root-key.pem \; -out hail-root-cert.pem \; -days 365 \; -sha256; ```. 2. Update kubernetes with the new root certificate:. ```; kubectl create secret generic \; -n default ssl-config-hail-root \; --from-file=hail-root-key.pem \; --from-file=hail-root-cert.pem \; --save-config \; --dry-run=client \; -o yaml \; | kubectl apply -f -; ```. 3. Update all the service certificates:. ```; m",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:3255,Modifiability,config,config-hail-root,3255,"'.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. Once all services have restarted, there should be no; downtime. 1. Regenerate the root certificate (from your laptop):. ```; openssl req -new -x509 \; -subj /CN=hail-root \; -nodes \; -newkey rsa:4096 \; -keyout hail-root-key.pem \; -out hail-root-cert.pem \; -days 365 \; -sha256; ```. 2. Update kubernetes with the new root certificate:. ```; kubectl create secret generic \; -n default ssl-config-hail-root \; --from-file=hail-root-key.pem \; --from-file=hail-root-cert.pem \; --save-config \; --dry-run=client \; -o yaml \; | kubectl apply -f -; ```. 3. Update all the service certificates:. ```; make -C $HAIL/hail python/hailtop/hail_version. PYTHONPATH=$HAIL/hail/python \; python3 $HAIL/tls/create_certs.py \; default \; $HAIL/tls/config.yaml \; hail-root-key.pem \; hail-root-cert.pem; ```. 4. Get a list of all the services for that need to be restarted (some of these are; not actually services, but including them in the next step is OK). ```; SERVICES_TO_RESTART=$(python3 -c 'import os; import yaml; hail_dir = os.getenv(""HAIL""); x = yaml.safe_load(open(f""{hail_dir}/tls/config.yaml""))[""principals""]; print("","".join(x[""name""] for x in x))'); ```. 5. Restart all the services by deleting the pods (but, critically, not the; deployments):. ```; kubectl delete pods -l ""app in ($SERVICES_TO_RESTART)""; ```; ",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:3349,Modifiability,config,config,3349,"'.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. Once all services have restarted, there should be no; downtime. 1. Regenerate the root certificate (from your laptop):. ```; openssl req -new -x509 \; -subj /CN=hail-root \; -nodes \; -newkey rsa:4096 \; -keyout hail-root-key.pem \; -out hail-root-cert.pem \; -days 365 \; -sha256; ```. 2. Update kubernetes with the new root certificate:. ```; kubectl create secret generic \; -n default ssl-config-hail-root \; --from-file=hail-root-key.pem \; --from-file=hail-root-cert.pem \; --save-config \; --dry-run=client \; -o yaml \; | kubectl apply -f -; ```. 3. Update all the service certificates:. ```; make -C $HAIL/hail python/hailtop/hail_version. PYTHONPATH=$HAIL/hail/python \; python3 $HAIL/tls/create_certs.py \; default \; $HAIL/tls/config.yaml \; hail-root-key.pem \; hail-root-cert.pem; ```. 4. Get a list of all the services for that need to be restarted (some of these are; not actually services, but including them in the next step is OK). ```; SERVICES_TO_RESTART=$(python3 -c 'import os; import yaml; hail_dir = os.getenv(""HAIL""); x = yaml.safe_load(open(f""{hail_dir}/tls/config.yaml""))[""principals""]; print("","".join(x[""name""] for x in x))'); ```. 5. Restart all the services by deleting the pods (but, critically, not the; deployments):. ```; kubectl delete pods -l ""app in ($SERVICES_TO_RESTART)""; ```; ",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:3601,Modifiability,config,config,3601,"'.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. Once all services have restarted, there should be no; downtime. 1. Regenerate the root certificate (from your laptop):. ```; openssl req -new -x509 \; -subj /CN=hail-root \; -nodes \; -newkey rsa:4096 \; -keyout hail-root-key.pem \; -out hail-root-cert.pem \; -days 365 \; -sha256; ```. 2. Update kubernetes with the new root certificate:. ```; kubectl create secret generic \; -n default ssl-config-hail-root \; --from-file=hail-root-key.pem \; --from-file=hail-root-cert.pem \; --save-config \; --dry-run=client \; -o yaml \; | kubectl apply -f -; ```. 3. Update all the service certificates:. ```; make -C $HAIL/hail python/hailtop/hail_version. PYTHONPATH=$HAIL/hail/python \; python3 $HAIL/tls/create_certs.py \; default \; $HAIL/tls/config.yaml \; hail-root-key.pem \; hail-root-cert.pem; ```. 4. Get a list of all the services for that need to be restarted (some of these are; not actually services, but including them in the next step is OK). ```; SERVICES_TO_RESTART=$(python3 -c 'import os; import yaml; hail_dir = os.getenv(""HAIL""); x = yaml.safe_load(open(f""{hail_dir}/tls/config.yaml""))[""principals""]; print("","".join(x[""name""] for x in x))'); ```. 5. Restart all the services by deleting the pods (but, critically, not the; deployments):. ```; kubectl delete pods -l ""app in ($SERVICES_TO_RESTART)""; ```; ",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:3947,Modifiability,config,config,3947,"'.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. Once all services have restarted, there should be no; downtime. 1. Regenerate the root certificate (from your laptop):. ```; openssl req -new -x509 \; -subj /CN=hail-root \; -nodes \; -newkey rsa:4096 \; -keyout hail-root-key.pem \; -out hail-root-cert.pem \; -days 365 \; -sha256; ```. 2. Update kubernetes with the new root certificate:. ```; kubectl create secret generic \; -n default ssl-config-hail-root \; --from-file=hail-root-key.pem \; --from-file=hail-root-cert.pem \; --save-config \; --dry-run=client \; -o yaml \; | kubectl apply -f -; ```. 3. Update all the service certificates:. ```; make -C $HAIL/hail python/hailtop/hail_version. PYTHONPATH=$HAIL/hail/python \; python3 $HAIL/tls/create_certs.py \; default \; $HAIL/tls/config.yaml \; hail-root-key.pem \; hail-root-cert.pem; ```. 4. Get a list of all the services for that need to be restarted (some of these are; not actually services, but including them in the next step is OK). ```; SERVICES_TO_RESTART=$(python3 -c 'import os; import yaml; hail_dir = os.getenv(""HAIL""); x = yaml.safe_load(open(f""{hail_dir}/tls/config.yaml""))[""principals""]; print("","".join(x[""name""] for x in x))'); ```. 5. Restart all the services by deleting the pods (but, critically, not the; deployments):. ```; kubectl delete pods -l ""app in ($SERVICES_TO_RESTART)""; ```; ",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:88,Security,certificate,certificate,88,"# TLS Cookbook. ## Create a Self-Signed x509 Certificate in PEM Format. Produce an x509 certificate. The key is a 4096-bit RSA key. The cert is valid; from today until 365 days from today. The server must be accessed via the domain; name `localhost`. A client that verifies hostnames will reject this certificate; if the server is accessed via an IP (like `127.0.0.1`) or other names (like; `wm06b-953`). The certificate is not password protected due to `-nodes`. ```; openssl req -x509 \; -newkey rsa:4096 \; -keyout server-key.pem \; -out server-cert.pem \; -days 365 \; -subj '/CN=localhost' \; -nodes \; -sha256; ```. ## Bundle a Key and Certificate into a PKCS12 File. Create a PKCS12 file. PKCS12 files are primarily useful for creating instances; of a Java `KeyStore`. It is not possible to [elide a; password](https://stackoverflow.com/questions/27497723/export-a-pkcs12-file-without-an-export-password).; Using the empty string as a password is not recommended because many tools do; not properly support it. ```; openssl pkcs12 -export \; -out server-keystore.p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though i",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:208,Security,access,accessed,208,"# TLS Cookbook. ## Create a Self-Signed x509 Certificate in PEM Format. Produce an x509 certificate. The key is a 4096-bit RSA key. The cert is valid; from today until 365 days from today. The server must be accessed via the domain; name `localhost`. A client that verifies hostnames will reject this certificate; if the server is accessed via an IP (like `127.0.0.1`) or other names (like; `wm06b-953`). The certificate is not password protected due to `-nodes`. ```; openssl req -x509 \; -newkey rsa:4096 \; -keyout server-key.pem \; -out server-cert.pem \; -days 365 \; -subj '/CN=localhost' \; -nodes \; -sha256; ```. ## Bundle a Key and Certificate into a PKCS12 File. Create a PKCS12 file. PKCS12 files are primarily useful for creating instances; of a Java `KeyStore`. It is not possible to [elide a; password](https://stackoverflow.com/questions/27497723/export-a-pkcs12-file-without-an-export-password).; Using the empty string as a password is not recommended because many tools do; not properly support it. ```; openssl pkcs12 -export \; -out server-keystore.p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though i",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:301,Security,certificate,certificate,301,"# TLS Cookbook. ## Create a Self-Signed x509 Certificate in PEM Format. Produce an x509 certificate. The key is a 4096-bit RSA key. The cert is valid; from today until 365 days from today. The server must be accessed via the domain; name `localhost`. A client that verifies hostnames will reject this certificate; if the server is accessed via an IP (like `127.0.0.1`) or other names (like; `wm06b-953`). The certificate is not password protected due to `-nodes`. ```; openssl req -x509 \; -newkey rsa:4096 \; -keyout server-key.pem \; -out server-cert.pem \; -days 365 \; -subj '/CN=localhost' \; -nodes \; -sha256; ```. ## Bundle a Key and Certificate into a PKCS12 File. Create a PKCS12 file. PKCS12 files are primarily useful for creating instances; of a Java `KeyStore`. It is not possible to [elide a; password](https://stackoverflow.com/questions/27497723/export-a-pkcs12-file-without-an-export-password).; Using the empty string as a password is not recommended because many tools do; not properly support it. ```; openssl pkcs12 -export \; -out server-keystore.p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though i",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:331,Security,access,accessed,331,"# TLS Cookbook. ## Create a Self-Signed x509 Certificate in PEM Format. Produce an x509 certificate. The key is a 4096-bit RSA key. The cert is valid; from today until 365 days from today. The server must be accessed via the domain; name `localhost`. A client that verifies hostnames will reject this certificate; if the server is accessed via an IP (like `127.0.0.1`) or other names (like; `wm06b-953`). The certificate is not password protected due to `-nodes`. ```; openssl req -x509 \; -newkey rsa:4096 \; -keyout server-key.pem \; -out server-cert.pem \; -days 365 \; -subj '/CN=localhost' \; -nodes \; -sha256; ```. ## Bundle a Key and Certificate into a PKCS12 File. Create a PKCS12 file. PKCS12 files are primarily useful for creating instances; of a Java `KeyStore`. It is not possible to [elide a; password](https://stackoverflow.com/questions/27497723/export-a-pkcs12-file-without-an-export-password).; Using the empty string as a password is not recommended because many tools do; not properly support it. ```; openssl pkcs12 -export \; -out server-keystore.p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though i",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:409,Security,certificate,certificate,409,"# TLS Cookbook. ## Create a Self-Signed x509 Certificate in PEM Format. Produce an x509 certificate. The key is a 4096-bit RSA key. The cert is valid; from today until 365 days from today. The server must be accessed via the domain; name `localhost`. A client that verifies hostnames will reject this certificate; if the server is accessed via an IP (like `127.0.0.1`) or other names (like; `wm06b-953`). The certificate is not password protected due to `-nodes`. ```; openssl req -x509 \; -newkey rsa:4096 \; -keyout server-key.pem \; -out server-cert.pem \; -days 365 \; -subj '/CN=localhost' \; -nodes \; -sha256; ```. ## Bundle a Key and Certificate into a PKCS12 File. Create a PKCS12 file. PKCS12 files are primarily useful for creating instances; of a Java `KeyStore`. It is not possible to [elide a; password](https://stackoverflow.com/questions/27497723/export-a-pkcs12-file-without-an-export-password).; Using the empty string as a password is not recommended because many tools do; not properly support it. ```; openssl pkcs12 -export \; -out server-keystore.p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though i",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:428,Security,password,password,428,"# TLS Cookbook. ## Create a Self-Signed x509 Certificate in PEM Format. Produce an x509 certificate. The key is a 4096-bit RSA key. The cert is valid; from today until 365 days from today. The server must be accessed via the domain; name `localhost`. A client that verifies hostnames will reject this certificate; if the server is accessed via an IP (like `127.0.0.1`) or other names (like; `wm06b-953`). The certificate is not password protected due to `-nodes`. ```; openssl req -x509 \; -newkey rsa:4096 \; -keyout server-key.pem \; -out server-cert.pem \; -days 365 \; -subj '/CN=localhost' \; -nodes \; -sha256; ```. ## Bundle a Key and Certificate into a PKCS12 File. Create a PKCS12 file. PKCS12 files are primarily useful for creating instances; of a Java `KeyStore`. It is not possible to [elide a; password](https://stackoverflow.com/questions/27497723/export-a-pkcs12-file-without-an-export-password).; Using the empty string as a password is not recommended because many tools do; not properly support it. ```; openssl pkcs12 -export \; -out server-keystore.p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though i",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:808,Security,password,password,808,"# TLS Cookbook. ## Create a Self-Signed x509 Certificate in PEM Format. Produce an x509 certificate. The key is a 4096-bit RSA key. The cert is valid; from today until 365 days from today. The server must be accessed via the domain; name `localhost`. A client that verifies hostnames will reject this certificate; if the server is accessed via an IP (like `127.0.0.1`) or other names (like; `wm06b-953`). The certificate is not password protected due to `-nodes`. ```; openssl req -x509 \; -newkey rsa:4096 \; -keyout server-key.pem \; -out server-cert.pem \; -days 365 \; -subj '/CN=localhost' \; -nodes \; -sha256; ```. ## Bundle a Key and Certificate into a PKCS12 File. Create a PKCS12 file. PKCS12 files are primarily useful for creating instances; of a Java `KeyStore`. It is not possible to [elide a; password](https://stackoverflow.com/questions/27497723/export-a-pkcs12-file-without-an-export-password).; Using the empty string as a password is not recommended because many tools do; not properly support it. ```; openssl pkcs12 -export \; -out server-keystore.p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though i",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:902,Security,password,password,902,"# TLS Cookbook. ## Create a Self-Signed x509 Certificate in PEM Format. Produce an x509 certificate. The key is a 4096-bit RSA key. The cert is valid; from today until 365 days from today. The server must be accessed via the domain; name `localhost`. A client that verifies hostnames will reject this certificate; if the server is accessed via an IP (like `127.0.0.1`) or other names (like; `wm06b-953`). The certificate is not password protected due to `-nodes`. ```; openssl req -x509 \; -newkey rsa:4096 \; -keyout server-key.pem \; -out server-cert.pem \; -days 365 \; -subj '/CN=localhost' \; -nodes \; -sha256; ```. ## Bundle a Key and Certificate into a PKCS12 File. Create a PKCS12 file. PKCS12 files are primarily useful for creating instances; of a Java `KeyStore`. It is not possible to [elide a; password](https://stackoverflow.com/questions/27497723/export-a-pkcs12-file-without-an-export-password).; Using the empty string as a password is not recommended because many tools do; not properly support it. ```; openssl pkcs12 -export \; -out server-keystore.p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though i",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:942,Security,password,password,942,"# TLS Cookbook. ## Create a Self-Signed x509 Certificate in PEM Format. Produce an x509 certificate. The key is a 4096-bit RSA key. The cert is valid; from today until 365 days from today. The server must be accessed via the domain; name `localhost`. A client that verifies hostnames will reject this certificate; if the server is accessed via an IP (like `127.0.0.1`) or other names (like; `wm06b-953`). The certificate is not password protected due to `-nodes`. ```; openssl req -x509 \; -newkey rsa:4096 \; -keyout server-key.pem \; -out server-cert.pem \; -days 365 \; -subj '/CN=localhost' \; -nodes \; -sha256; ```. ## Bundle a Key and Certificate into a PKCS12 File. Create a PKCS12 file. PKCS12 files are primarily useful for creating instances; of a Java `KeyStore`. It is not possible to [elide a; password](https://stackoverflow.com/questions/27497723/export-a-pkcs12-file-without-an-export-password).; Using the empty string as a password is not recommended because many tools do; not properly support it. ```; openssl pkcs12 -export \; -out server-keystore.p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though i",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:1220,Security,certificate,certificate,1220," be accessed via the domain; name `localhost`. A client that verifies hostnames will reject this certificate; if the server is accessed via an IP (like `127.0.0.1`) or other names (like; `wm06b-953`). The certificate is not password protected due to `-nodes`. ```; openssl req -x509 \; -newkey rsa:4096 \; -keyout server-key.pem \; -out server-cert.pem \; -days 365 \; -subj '/CN=localhost' \; -nodes \; -sha256; ```. ## Bundle a Key and Certificate into a PKCS12 File. Create a PKCS12 file. PKCS12 files are primarily useful for creating instances; of a Java `KeyStore`. It is not possible to [elide a; password](https://stackoverflow.com/questions/27497723/export-a-pkcs12-file-without-an-export-password).; Using the empty string as a password is not recommended because many tools do; not properly support it. ```; openssl pkcs12 -export \; -out server-keystore.p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though it is not itself expired. Download the Hail root; certificate to a local file and inspect the start and end dates. ```; kubectl get secrets ssl-config-hail-root -o json \; | jq -r '.data[""hail-root-cert.pe",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:1245,Security,certificate,certificate,1245,"ames will reject this certificate; if the server is accessed via an IP (like `127.0.0.1`) or other names (like; `wm06b-953`). The certificate is not password protected due to `-nodes`. ```; openssl req -x509 \; -newkey rsa:4096 \; -keyout server-key.pem \; -out server-cert.pem \; -days 365 \; -subj '/CN=localhost' \; -nodes \; -sha256; ```. ## Bundle a Key and Certificate into a PKCS12 File. Create a PKCS12 file. PKCS12 files are primarily useful for creating instances; of a Java `KeyStore`. It is not possible to [elide a; password](https://stackoverflow.com/questions/27497723/export-a-pkcs12-file-without-an-export-password).; Using the empty string as a password is not recommended because many tools do; not properly support it. ```; openssl pkcs12 -export \; -out server-keystore.p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though it is not itself expired. Download the Hail root; certificate to a local file and inspect the start and end dates. ```; kubectl get secrets ssl-config-hail-root -o json \; | jq -r '.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate s",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:1438,Security,certificate,certificate,1438," is not password protected due to `-nodes`. ```; openssl req -x509 \; -newkey rsa:4096 \; -keyout server-key.pem \; -out server-cert.pem \; -days 365 \; -subj '/CN=localhost' \; -nodes \; -sha256; ```. ## Bundle a Key and Certificate into a PKCS12 File. Create a PKCS12 file. PKCS12 files are primarily useful for creating instances; of a Java `KeyStore`. It is not possible to [elide a; password](https://stackoverflow.com/questions/27497723/export-a-pkcs12-file-without-an-export-password).; Using the empty string as a password is not recommended because many tools do; not properly support it. ```; openssl pkcs12 -export \; -out server-keystore.p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though it is not itself expired. Download the Hail root; certificate to a local file and inspect the start and end dates. ```; kubectl get secrets ssl-config-hail-root -o json \; | jq -r '.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` inco",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:1754,Security,certificate,certificate,1754,"reating instances; of a Java `KeyStore`. It is not possible to [elide a; password](https://stackoverflow.com/questions/27497723/export-a-pkcs12-file-without-an-export-password).; Using the empty string as a password is not recommended because many tools do; not properly support it. ```; openssl pkcs12 -export \; -out server-keystore.p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though it is not itself expired. Download the Hail root; certificate to a local file and inspect the start and end dates. ```; kubectl get secrets ssl-config-hail-root -o json \; | jq -r '.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. ",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:1836,Security,certificate,certificates,1836,"export-a-pkcs12-file-without-an-export-password).; Using the empty string as a password is not recommended because many tools do; not properly support it. ```; openssl pkcs12 -export \; -out server-keystore.p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though it is not itself expired. Download the Hail root; certificate to a local file and inspect the start and end dates. ```; kubectl get secrets ssl-config-hail-root -o json \; | jq -r '.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. O",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:1870,Security,certificate,certificate,1870,"export-a-pkcs12-file-without-an-export-password).; Using the empty string as a password is not recommended because many tools do; not properly support it. ```; openssl pkcs12 -export \; -out server-keystore.p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though it is not itself expired. Download the Hail root; certificate to a local file and inspect the start and end dates. ```; kubectl get secrets ssl-config-hail-root -o json \; | jq -r '.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. O",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:1910,Security,certificate,certificate,1910,"mmended because many tools do; not properly support it. ```; openssl pkcs12 -export \; -out server-keystore.p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though it is not itself expired. Download the Hail root; certificate to a local file and inspect the start and end dates. ```; kubectl get secrets ssl-config-hail-root -o json \; | jq -r '.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. Once all services have restarted, there should be no; downtime. 1. Regenerate the root certificate (f",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:1945,Security,certificate,certificate,1945,"mmended because many tools do; not properly support it. ```; openssl pkcs12 -export \; -out server-keystore.p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though it is not itself expired. Download the Hail root; certificate to a local file and inspect the start and end dates. ```; kubectl get secrets ssl-config-hail-root -o json \; | jq -r '.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. Once all services have restarted, there should be no; downtime. 1. Regenerate the root certificate (f",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:2050,Security,certificate,certificate,2050,".p12 \; -inkey server-key.pem \; -in server-cert.pem \; -passout pass:foobar; ```. ## Inspect a Certificate. Print the start and end dates for a given certificate. Note that a certificate; whose start date is in the future is called ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though it is not itself expired. Download the Hail root; certificate to a local file and inspect the start and end dates. ```; kubectl get secrets ssl-config-hail-root -o json \; | jq -r '.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. Once all services have restarted, there should be no; downtime. 1. Regenerate the root certificate (from your laptop):. ```; openssl req -new -x509 \; -subj /CN=hail-root \; -nodes \; -newkey rsa:4096 \; -key",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:2266,Security,certificate,certificate,2266,"lled ""expired"" by many tools. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Print a complete textual representation of the certificate. ```; openssl x509 -text -noout -in cert.pem; ```. ## Determine the Cause of Certificate Expiration. Check the start and end dates. ```; openssl x509 -startdate -enddate -noout -in cert.pem; ```. Is the start date in the future? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though it is not itself expired. Download the Hail root; certificate to a local file and inspect the start and end dates. ```; kubectl get secrets ssl-config-hail-root -o json \; | jq -r '.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. Once all services have restarted, there should be no; downtime. 1. Regenerate the root certificate (from your laptop):. ```; openssl req -new -x509 \; -subj /CN=hail-root \; -nodes \; -newkey rsa:4096 \; -keyout hail-root-key.pem \; -out hail-root-cert.pem \; -days 365 \; -sha256; ```. 2. Update kubernetes with the new root certificate:. ```; kubectl create secret generic \; -n default ssl-config-hail-root \; --from-file=hail-root-k",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:2722,Security,certificate,certificates,2722,"uture? Is the end date in the past? What is the time; on the machine on which the certificate failure occurred? Remember to always; compare times in UTC!. All Hail certificates are signed by a root certificate named `hail-root`. If; this certificate is expired, the signed certificate will also be called; ""expired"" even though it is not itself expired. Download the Hail root; certificate to a local file and inspect the start and end dates. ```; kubectl get secrets ssl-config-hail-root -o json \; | jq -r '.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. Once all services have restarted, there should be no; downtime. 1. Regenerate the root certificate (from your laptop):. ```; openssl req -new -x509 \; -subj /CN=hail-root \; -nodes \; -newkey rsa:4096 \; -keyout hail-root-key.pem \; -out hail-root-cert.pem \; -days 365 \; -sha256; ```. 2. Update kubernetes with the new root certificate:. ```; kubectl create secret generic \; -n default ssl-config-hail-root \; --from-file=hail-root-key.pem \; --from-file=hail-root-cert.pem \; --save-config \; --dry-run=client \; -o yaml \; | kubectl apply -f -; ```. 3. Update all the service certificates:. ```; make -C $HAIL/hail python/hailtop/hail_version. PYTHONPATH=$HAIL/hail/python \; python3 $HAIL/tls/create_certs.py \; default \; $HAIL/tls/config.yaml \; hail-root-key.pem \; hail-root-cert.pem; ```. 4. Get a li",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:2949,Security,certificate,certificate,2949,"te will also be called; ""expired"" even though it is not itself expired. Download the Hail root; certificate to a local file and inspect the start and end dates. ```; kubectl get secrets ssl-config-hail-root -o json \; | jq -r '.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. Once all services have restarted, there should be no; downtime. 1. Regenerate the root certificate (from your laptop):. ```; openssl req -new -x509 \; -subj /CN=hail-root \; -nodes \; -newkey rsa:4096 \; -keyout hail-root-key.pem \; -out hail-root-cert.pem \; -days 365 \; -sha256; ```. 2. Update kubernetes with the new root certificate:. ```; kubectl create secret generic \; -n default ssl-config-hail-root \; --from-file=hail-root-key.pem \; --from-file=hail-root-cert.pem \; --save-config \; --dry-run=client \; -o yaml \; | kubectl apply -f -; ```. 3. Update all the service certificates:. ```; make -C $HAIL/hail python/hailtop/hail_version. PYTHONPATH=$HAIL/hail/python \; python3 $HAIL/tls/create_certs.py \; default \; $HAIL/tls/config.yaml \; hail-root-key.pem \; hail-root-cert.pem; ```. 4. Get a list of all the services for that need to be restarted (some of these are; not actually services, but including them in the next step is OK). ```; SERVICES_TO_RESTART=$(python3 -c 'import os; import yaml; hail_dir = os.getenv(""HAIL""); x = yaml.safe_load(open(f""{hail_dir}/tls/config.y",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:3188,Security,certificate,certificate,3188," -r '.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. Once all services have restarted, there should be no; downtime. 1. Regenerate the root certificate (from your laptop):. ```; openssl req -new -x509 \; -subj /CN=hail-root \; -nodes \; -newkey rsa:4096 \; -keyout hail-root-key.pem \; -out hail-root-cert.pem \; -days 365 \; -sha256; ```. 2. Update kubernetes with the new root certificate:. ```; kubectl create secret generic \; -n default ssl-config-hail-root \; --from-file=hail-root-key.pem \; --from-file=hail-root-cert.pem \; --save-config \; --dry-run=client \; -o yaml \; | kubectl apply -f -; ```. 3. Update all the service certificates:. ```; make -C $HAIL/hail python/hailtop/hail_version. PYTHONPATH=$HAIL/hail/python \; python3 $HAIL/tls/create_certs.py \; default \; $HAIL/tls/config.yaml \; hail-root-key.pem \; hail-root-cert.pem; ```. 4. Get a list of all the services for that need to be restarted (some of these are; not actually services, but including them in the next step is OK). ```; SERVICES_TO_RESTART=$(python3 -c 'import os; import yaml; hail_dir = os.getenv(""HAIL""); x = yaml.safe_load(open(f""{hail_dir}/tls/config.yaml""))[""principals""]; print("","".join(x[""name""] for x in x))'); ```. 5. Restart all the services by deleting the pods (but, critically, not the; deployments):. ```; kubectl delete pods -l ""app in ($SERVICES_TO_RESTART)""; `",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md:3443,Security,certificate,certificates,3443,"'.data[""hail-root-cert.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. This certificate should be bit-for-bit identical to the `SERVICE-incoming.pem`; trust file, but you should verify that. For example, download the; `batch` incoming trust:. ```; kubectl get secrets ssl-config-batch -o json \; | jq -r '.data[""batch-incoming.pem""]' \; | base64 --decode \; > hail-root-cert.pem; ```. ## Regenerate All the Certificates. If something has gone wrong, a relatively straightforward way to get back to; working is to regenerate all the certificates. This procedure will cause; downtime: services using the old certs will not trust servers using the new; certs and vice-versa. Once all services have restarted, there should be no; downtime. 1. Regenerate the root certificate (from your laptop):. ```; openssl req -new -x509 \; -subj /CN=hail-root \; -nodes \; -newkey rsa:4096 \; -keyout hail-root-key.pem \; -out hail-root-cert.pem \; -days 365 \; -sha256; ```. 2. Update kubernetes with the new root certificate:. ```; kubectl create secret generic \; -n default ssl-config-hail-root \; --from-file=hail-root-key.pem \; --from-file=hail-root-cert.pem \; --save-config \; --dry-run=client \; -o yaml \; | kubectl apply -f -; ```. 3. Update all the service certificates:. ```; make -C $HAIL/hail python/hailtop/hail_version. PYTHONPATH=$HAIL/hail/python \; python3 $HAIL/tls/create_certs.py \; default \; $HAIL/tls/config.yaml \; hail-root-key.pem \; hail-root-cert.pem; ```. 4. Get a list of all the services for that need to be restarted (some of these are; not actually services, but including them in the next step is OK). ```; SERVICES_TO_RESTART=$(python3 -c 'import os; import yaml; hail_dir = os.getenv(""HAIL""); x = yaml.safe_load(open(f""{hail_dir}/tls/config.yaml""))[""principals""]; print("","".join(x[""name""] for x in x))'); ```. 5. Restart all the services by deleting the pods (but, critically, not the; deployments):. ```; kubectl delete pods -l ""app in ($SERVICES_TO_RESTART)""; ```; ",MatchSource.DOCS,dev-docs/services/tls-cookbook.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls-cookbook.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:6431,Availability,avail,available,6431,"yption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `sit",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:9934,Availability,error,error,9934,"eputy problem: it issues a callback in; response to a batch finishing. That callback is issued from within the cluster; and therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:7148,Deployability,configurat,configuration,7148,"Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:7188,Deployability,configurat,configuration,7188,"Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:7347,Deployability,configurat,configuration,7347,"ust agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former i",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:7448,Deployability,configurat,configuration,7448,"ers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:279,Energy Efficiency,adapt,adaptation,279,"# TLS. TLS stands for Transport Layer Security. We use TLS to encrypt all traffic; between all services in our kubernetes (k8s) cluster. All traffic in k8s is intended to be encrypted. [PR; 8561](https://github.com/hail-is/hail/pull/8561) started this work. This; document is an adaptation of the explanation given there. Traffic only enters; our cluster via the gateway service. This document describes the use of; encryption within the cluster, but the gateway service also uses TLS to encrypt; communications between it and the outside world. Currently, all traffic in our cluster should be encrypted except for:; - from the batch-driver to the batch workers; - from the batch workers to the internal-gateway; - to ukbb-rg; - from the notebook service to the notebook workers; - to letsencrypt (oh the irony). Known issues:; - We have not implemented mutual TLS (mTLS): servers do not verify they trust; their clients.; - We do not rotate certificates.; - All certificates are signed by one root certificate which everyone trusts. We; intend for each client and server to have an explicit incoming and outgoing; trusted clients list. Such lists would prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:1563,Integrability,message,message,1563,"ter should be encrypted except for:; - from the batch-driver to the batch workers; - from the batch workers to the internal-gateway; - to ukbb-rg; - from the notebook service to the notebook workers; - to letsencrypt (oh the irony). Known issues:; - We have not implemented mutual TLS (mTLS): servers do not verify they trust; their clients.; - We do not rotate certificates.; - All certificates are signed by one root certificate which everyone trusts. We; intend for each client and server to have an explicit incoming and outgoing; trusted clients list. Such lists would prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:1632,Integrability,protocol,protocol,1632,"er to the batch workers; - from the batch workers to the internal-gateway; - to ukbb-rg; - from the notebook service to the notebook workers; - to letsencrypt (oh the irony). Known issues:; - We have not implemented mutual TLS (mTLS): servers do not verify they trust; their clients.; - We do not rotate certificates.; - All certificates are signed by one root certificate which everyone trusts. We; intend for each client and server to have an explicit incoming and outgoing; trusted clients list. Such lists would prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file ",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:1989,Integrability,interface,interface,1989,"; - All certificates are signed by one root certificate which everyone trusts. We; intend for each client and server to have an explicit incoming and outgoing; trusted clients list. Such lists would prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired;",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:2019,Integrability,protocol,protocol,2019,"rver to have an explicit incoming and outgoing; trusted clients list. Such lists would prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticat",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:2073,Integrability,message,messages,2073,"rver to have an explicit incoming and outgoing; trusted clients list. Such lists would prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticat",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:2129,Integrability,protocol,protocol,2129,"ould prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique ",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:5861,Integrability,message,messages,5861,"certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principals trust the root certificate and thus trust; every other principal. In the future, there will be no root certificate. Instead, each principal will; have a self-signed certificate and a list of trusted certificates. Limiting the; trusted principals is one of many ways in which we limit the damage done by a; misbehaving or compromised service. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who i",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:9711,Integrability,message,message,9711,"hat the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. Before the TLS PR, Batch had a confused deputy problem: it issues a callback in; response to a batch finishing. That callback is issued from within the cluster; and therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Footnotes. [1] ",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:11152,Integrability,message,message,11152," includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages.; ",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:11295,Integrability,message,messages,11295," includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages.; ",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:11459,Integrability,message,messages,11459," includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages.; ",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:11780,Integrability,message,messages,11780," includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages.; ",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:279,Modifiability,adapt,adaptation,279,"# TLS. TLS stands for Transport Layer Security. We use TLS to encrypt all traffic; between all services in our kubernetes (k8s) cluster. All traffic in k8s is intended to be encrypted. [PR; 8561](https://github.com/hail-is/hail/pull/8561) started this work. This; document is an adaptation of the explanation given there. Traffic only enters; our cluster via the gateway service. This document describes the use of; encryption within the cluster, but the gateway service also uses TLS to encrypt; communications between it and the outside world. Currently, all traffic in our cluster should be encrypted except for:; - from the batch-driver to the batch workers; - from the batch workers to the internal-gateway; - to ukbb-rg; - from the notebook service to the notebook workers; - to letsencrypt (oh the irony). Known issues:; - We have not implemented mutual TLS (mTLS): servers do not verify they trust; their clients.; - We do not rotate certificates.; - All certificates are signed by one root certificate which everyone trusts. We; intend for each client and server to have an explicit incoming and outgoing; trusted clients list. Such lists would prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:2546,Modifiability,enhance,enhanced,2546,"sage is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The firs",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:4951,Modifiability,config,config-hail-root,4951,"ose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principals trust the root certificate and thus trust; every other principal. In the future, there will be no root certificate. Instead, each principal will; have a self-signed certificate and a list of trusted certificates. Limiting the; trusted principals is one of many ways in which we limit the damage done by a; misbehaving or compromised service. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:6713,Modifiability,config,config-NAME,6713,"recy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of cert",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:6895,Modifiability,config,config,6895,"nt over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Jav",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:6944,Modifiability,config,config,6944,"es who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certi",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:7148,Modifiability,config,configuration,7148,"Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:7188,Modifiability,config,configuration,7188,"Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:7270,Modifiability,config,config-site,7270,"e; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, ",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:7319,Modifiability,config,config-http,7319,"s quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:7347,Modifiability,config,configuration,7347,"ust agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former i",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:7371,Modifiability,config,configures,7371,"ust agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former i",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:7419,Modifiability,config,config-proxy,7419,"re ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the a",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:7448,Modifiability,config,configuration,7448,"ers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:7472,Modifiability,config,configures,7472,"ers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:9366,Safety,safe,safe,9366,"vices. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. Before the TLS PR, Batch had a confused deputy problem: it issues a callback in; response to a batch finishing. That callback is issued from within the cluster; and therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](k",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:62,Security,encrypt,encrypt,62,"# TLS. TLS stands for Transport Layer Security. We use TLS to encrypt all traffic; between all services in our kubernetes (k8s) cluster. All traffic in k8s is intended to be encrypted. [PR; 8561](https://github.com/hail-is/hail/pull/8561) started this work. This; document is an adaptation of the explanation given there. Traffic only enters; our cluster via the gateway service. This document describes the use of; encryption within the cluster, but the gateway service also uses TLS to encrypt; communications between it and the outside world. Currently, all traffic in our cluster should be encrypted except for:; - from the batch-driver to the batch workers; - from the batch workers to the internal-gateway; - to ukbb-rg; - from the notebook service to the notebook workers; - to letsencrypt (oh the irony). Known issues:; - We have not implemented mutual TLS (mTLS): servers do not verify they trust; their clients.; - We do not rotate certificates.; - All certificates are signed by one root certificate which everyone trusts. We; intend for each client and server to have an explicit incoming and outgoing; trusted clients list. Such lists would prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:174,Security,encrypt,encrypted,174,"# TLS. TLS stands for Transport Layer Security. We use TLS to encrypt all traffic; between all services in our kubernetes (k8s) cluster. All traffic in k8s is intended to be encrypted. [PR; 8561](https://github.com/hail-is/hail/pull/8561) started this work. This; document is an adaptation of the explanation given there. Traffic only enters; our cluster via the gateway service. This document describes the use of; encryption within the cluster, but the gateway service also uses TLS to encrypt; communications between it and the outside world. Currently, all traffic in our cluster should be encrypted except for:; - from the batch-driver to the batch workers; - from the batch workers to the internal-gateway; - to ukbb-rg; - from the notebook service to the notebook workers; - to letsencrypt (oh the irony). Known issues:; - We have not implemented mutual TLS (mTLS): servers do not verify they trust; their clients.; - We do not rotate certificates.; - All certificates are signed by one root certificate which everyone trusts. We; intend for each client and server to have an explicit incoming and outgoing; trusted clients list. Such lists would prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:416,Security,encrypt,encryption,416,"# TLS. TLS stands for Transport Layer Security. We use TLS to encrypt all traffic; between all services in our kubernetes (k8s) cluster. All traffic in k8s is intended to be encrypted. [PR; 8561](https://github.com/hail-is/hail/pull/8561) started this work. This; document is an adaptation of the explanation given there. Traffic only enters; our cluster via the gateway service. This document describes the use of; encryption within the cluster, but the gateway service also uses TLS to encrypt; communications between it and the outside world. Currently, all traffic in our cluster should be encrypted except for:; - from the batch-driver to the batch workers; - from the batch workers to the internal-gateway; - to ukbb-rg; - from the notebook service to the notebook workers; - to letsencrypt (oh the irony). Known issues:; - We have not implemented mutual TLS (mTLS): servers do not verify they trust; their clients.; - We do not rotate certificates.; - All certificates are signed by one root certificate which everyone trusts. We; intend for each client and server to have an explicit incoming and outgoing; trusted clients list. Such lists would prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:488,Security,encrypt,encrypt,488,"# TLS. TLS stands for Transport Layer Security. We use TLS to encrypt all traffic; between all services in our kubernetes (k8s) cluster. All traffic in k8s is intended to be encrypted. [PR; 8561](https://github.com/hail-is/hail/pull/8561) started this work. This; document is an adaptation of the explanation given there. Traffic only enters; our cluster via the gateway service. This document describes the use of; encryption within the cluster, but the gateway service also uses TLS to encrypt; communications between it and the outside world. Currently, all traffic in our cluster should be encrypted except for:; - from the batch-driver to the batch workers; - from the batch workers to the internal-gateway; - to ukbb-rg; - from the notebook service to the notebook workers; - to letsencrypt (oh the irony). Known issues:; - We have not implemented mutual TLS (mTLS): servers do not verify they trust; their clients.; - We do not rotate certificates.; - All certificates are signed by one root certificate which everyone trusts. We; intend for each client and server to have an explicit incoming and outgoing; trusted clients list. Such lists would prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:594,Security,encrypt,encrypted,594,"# TLS. TLS stands for Transport Layer Security. We use TLS to encrypt all traffic; between all services in our kubernetes (k8s) cluster. All traffic in k8s is intended to be encrypted. [PR; 8561](https://github.com/hail-is/hail/pull/8561) started this work. This; document is an adaptation of the explanation given there. Traffic only enters; our cluster via the gateway service. This document describes the use of; encryption within the cluster, but the gateway service also uses TLS to encrypt; communications between it and the outside world. Currently, all traffic in our cluster should be encrypted except for:; - from the batch-driver to the batch workers; - from the batch workers to the internal-gateway; - to ukbb-rg; - from the notebook service to the notebook workers; - to letsencrypt (oh the irony). Known issues:; - We have not implemented mutual TLS (mTLS): servers do not verify they trust; their clients.; - We do not rotate certificates.; - All certificates are signed by one root certificate which everyone trusts. We; intend for each client and server to have an explicit incoming and outgoing; trusted clients list. Such lists would prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:942,Security,certificate,certificates,942,"# TLS. TLS stands for Transport Layer Security. We use TLS to encrypt all traffic; between all services in our kubernetes (k8s) cluster. All traffic in k8s is intended to be encrypted. [PR; 8561](https://github.com/hail-is/hail/pull/8561) started this work. This; document is an adaptation of the explanation given there. Traffic only enters; our cluster via the gateway service. This document describes the use of; encryption within the cluster, but the gateway service also uses TLS to encrypt; communications between it and the outside world. Currently, all traffic in our cluster should be encrypted except for:; - from the batch-driver to the batch workers; - from the batch workers to the internal-gateway; - to ukbb-rg; - from the notebook service to the notebook workers; - to letsencrypt (oh the irony). Known issues:; - We have not implemented mutual TLS (mTLS): servers do not verify they trust; their clients.; - We do not rotate certificates.; - All certificates are signed by one root certificate which everyone trusts. We; intend for each client and server to have an explicit incoming and outgoing; trusted clients list. Such lists would prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:963,Security,certificate,certificates,963,"# TLS. TLS stands for Transport Layer Security. We use TLS to encrypt all traffic; between all services in our kubernetes (k8s) cluster. All traffic in k8s is intended to be encrypted. [PR; 8561](https://github.com/hail-is/hail/pull/8561) started this work. This; document is an adaptation of the explanation given there. Traffic only enters; our cluster via the gateway service. This document describes the use of; encryption within the cluster, but the gateway service also uses TLS to encrypt; communications between it and the outside world. Currently, all traffic in our cluster should be encrypted except for:; - from the batch-driver to the batch workers; - from the batch workers to the internal-gateway; - to ukbb-rg; - from the notebook service to the notebook workers; - to letsencrypt (oh the irony). Known issues:; - We have not implemented mutual TLS (mTLS): servers do not verify they trust; their clients.; - We do not rotate certificates.; - All certificates are signed by one root certificate which everyone trusts. We; intend for each client and server to have an explicit incoming and outgoing; trusted clients list. Such lists would prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:999,Security,certificate,certificate,999,"# TLS. TLS stands for Transport Layer Security. We use TLS to encrypt all traffic; between all services in our kubernetes (k8s) cluster. All traffic in k8s is intended to be encrypted. [PR; 8561](https://github.com/hail-is/hail/pull/8561) started this work. This; document is an adaptation of the explanation given there. Traffic only enters; our cluster via the gateway service. This document describes the use of; encryption within the cluster, but the gateway service also uses TLS to encrypt; communications between it and the outside world. Currently, all traffic in our cluster should be encrypted except for:; - from the batch-driver to the batch workers; - from the batch workers to the internal-gateway; - to ukbb-rg; - from the notebook service to the notebook workers; - to letsencrypt (oh the irony). Known issues:; - We have not implemented mutual TLS (mTLS): servers do not verify they trust; their clients.; - We do not rotate certificates.; - All certificates are signed by one root certificate which everyone trusts. We; intend for each client and server to have an explicit incoming and outgoing; trusted clients list. Such lists would prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:1574,Security,encrypt,encrypted,1574,"ter should be encrypted except for:; - from the batch-driver to the batch workers; - from the batch workers to the internal-gateway; - to ukbb-rg; - from the notebook service to the notebook workers; - to letsencrypt (oh the irony). Known issues:; - We have not implemented mutual TLS (mTLS): servers do not verify they trust; their clients.; - We do not rotate certificates.; - All certificates are signed by one root certificate which everyone trusts. We; intend for each client and server to have an explicit incoming and outgoing; trusted clients list. Such lists would prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:2059,Security,encrypt,encryption,2059,"rver to have an explicit incoming and outgoing; trusted clients list. Such lists would prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticat",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:2097,Security,authenticat,authentication,2097,"rver to have an explicit incoming and outgoing; trusted clients list. Such lists would prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticat",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:2146,Security,authenticat,authentication,2146,"ould prevent a compromised server or client; from being used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique ",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:2216,Security,authenticat,authentication,2216,"ng used to communicate with arbitrary services.; - Our servers should reject all insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both ou",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:2265,Security,authenticat,authenticate,2265,"l insecure cipher suites and reject TLS versions; other than 1.3. ## HTTPS and TLS. HTTP is implemented on TCP/IP. HTTPS is also implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `t",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:2410,Security,authenticat,authentication,2410,"so implemented on TCP/IP and differs; very mildly. After the socket is opened, a TLS [1] connection is established; over the socket. Thereafter, every HTTP message is encrypted and transported by; the TLS machinery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:3011,Security,certificate,certificate,3011,"y the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private RSA key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""ho",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:3032,Security,secur,security,3032,"on of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private RSA key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject ",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:3056,Security,authenticat,authenticatable,3056,"on of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private RSA key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject ",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:3318,Security,certificate,certificate,3318,"tication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private RSA key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal h",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:3835,Security,encrypt,encrypted,3835," two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private RSA key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; t",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:3861,Security,password,password,3861," two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private RSA key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; t",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:3904,Security,certificate,certificate,3904,". It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private RSA key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is similar. We have a root certi",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:4072,Security,certificate,certificate,4072,"icatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private RSA key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principal",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:4144,Security,certificate,certificate,4144,"icatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private RSA key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principal",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:4562,Security,certificate,certificate,4562,"anatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private RSA key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principals trust the root certificate and thus trust; every other principal. In the future, there will be no root certificate. Instead, each principal will; have a self-signed certificate and a list of trusted certificates. Limiting the; trusted principals is one of many ways in which we limit the damage done by a; misbehaving or compromised service. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret sc",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:4643,Security,certificate,certificate,4643,"anatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private RSA key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principals trust the root certificate and thus trust; every other principal. In the future, there will be no root certificate. Instead, each principal will; have a self-signed certificate and a list of trusted certificates. Limiting the; trusted principals is one of many ways in which we limit the damage done by a; misbehaving or compromised service. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret sc",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:4749,Security,certificate,certificates,4749,"`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principals trust the root certificate and thus trust; every other principal. In the future, there will be no root certificate. Instead, each principal will; have a self-signed certificate and a list of trusted certificates. Limiting the; trusted principals is one of many ways in which we limit the damage done by a; misbehaving or compromised service. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our serv",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:4848,Security,certificate,certificate,4848," using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principals trust the root certificate and thus trust; every other principal. In the future, there will be no root certificate. Instead, each principal will; have a self-signed certificate and a list of trusted certificates. Limiting the; trusted principals is one of many ways in which we limit the damage done by a; misbehaving or compromised service. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to enc",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:4876,Security,certificate,certificate,4876," using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principals trust the root certificate and thus trust; every other principal. In the future, there will be no root certificate. Instead, each principal will; have a self-signed certificate and a list of trusted certificates. Limiting the; trusted principals is one of many ways in which we limit the damage done by a; misbehaving or compromised service. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to enc",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:4927,Security,certificate,certificate,4927,"ose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principals trust the root certificate and thus trust; every other principal. In the future, there will be no root certificate. Instead, each principal will; have a self-signed certificate and a list of trusted certificates. Limiting the; trusted principals is one of many ways in which we limit the damage done by a; misbehaving or compromised service. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:4991,Security,certificate,certificate,4991,"(web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principals trust the root certificate and thus trust; every other principal. In the future, there will be no root certificate. Instead, each principal will; have a self-signed certificate and a list of trusted certificates. Limiting the; trusted principals is one of many ways in which we limit the damage done by a; misbehaving or compromised service. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:5032,Security,certificate,certificate,5032,"(web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principals trust the root certificate and thus trust; every other principal. In the future, there will be no root certificate. Instead, each principal will; have a self-signed certificate and a list of trusted certificates. Limiting the; trusted principals is one of many ways in which we limit the damage done by a; misbehaving or compromised service. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:5081,Security,certificate,certificate,5081,"the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principals trust the root certificate and thus trust; every other principal. In the future, there will be no root certificate. Instead, each principal will; have a self-signed certificate and a list of trusted certificates. Limiting the; trusted principals is one of many ways in which we limit the damage done by a; misbehaving or compromised service. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:5169,Security,certificate,certificate,5169,"; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principals trust the root certificate and thus trust; every other principal. In the future, there will be no root certificate. Instead, each principal will; have a self-signed certificate and a list of trusted certificates. Limiting the; trusted principals is one of many ways in which we limit the damage done by a; misbehaving or compromised service. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution ",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:5231,Security,certificate,certificate,5231,"hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principals trust the root certificate and thus trust; every other principal. In the future, there will be no root certificate. Instead, each principal will; have a self-signed certificate and a list of trusted certificates. Limiting the; trusted principals is one of many ways in which we limit the damage done by a; misbehaving or compromised service. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.or",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:5265,Security,certificate,certificates,5265,"hostnames; for the aformentioned hostname check. #### Trust. An X.509 Certificate only proves that the principal has a private key. On the; world wide web, ownership of a particular domain, e.g. `example.com`, is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principals trust the root certificate and thus trust; every other principal. In the future, there will be no root certificate. Instead, each principal will; have a self-signed certificate and a list of trusted certificates. Limiting the; trusted principals is one of many ways in which we limit the damage done by a; misbehaving or compromised service. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.or",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:5437,Security,encrypt,encryption,5437,"is; guaranteed by a Certificate Authority (CA). A Certificate Authority (like; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principals trust the root certificate and thus trust; every other principal. In the future, there will be no root certificate. Instead, each principal will; have a self-signed certificate and a list of trusted certificates. Limiting the; trusted principals is one of many ways in which we limit the damage done by a; misbehaving or compromised service. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:5473,Security,encrypt,encryption,5473,"ke; letsencypt or VeriSign) digitally signs the certificate of a user that has; proven they own the domain name mentioned in the certificate (see above; discussion of `CN` and `subjectAltName`). Web browsers have a file of the ""root""; certificates of the public Certificate Authorities. They establish a ""chain of; trust"" from a root certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principals trust the root certificate and thus trust; every other principal. In the future, there will be no root certificate. Instead, each principal will; have a self-signed certificate and a list of trusted certificates. Limiting the; trusted principals is one of many ways in which we limit the damage done by a; misbehaving or compromised service. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure cipher",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:5841,Security,encrypt,encrypt,5841,"certificate to the server's certificate. Our system is similar. We have a root certificate named; `ssl-config-hail-root`. Each principal has a certificate which is signed by the; root certificate. All other principals trust the root certificate and thus trust; every other principal. In the future, there will be no root certificate. Instead, each principal will; have a self-signed certificate and a list of trusted certificates. Limiting the; trusted principals is one of many ways in which we limit the damage done by a; misbehaving or compromised service. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who i",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:6541,Security,secur,secure,6541,"ble *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a p",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:7644,Security,certificate,certificate,7644,"ew Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the att",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:7691,Security,certificate,certificates,7691,"e secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy ",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:7774,Security,certificate,certificate,7774,"e secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy ",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:7852,Security,certificate,certificates,7852,"llowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. Befor",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:7925,Security,certificate,certificates,7925,"s:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. Before the TLS PR, Batch had a confused deputy problem: it issues a callback in; response to a batch finishing. That cal",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:8010,Security,certificate,certificate,8010,"s:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. Before the TLS PR, Batch had a confused deputy problem: it issues a callback in; response to a batch finishing. That cal",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:8088,Security,certificate,certificates,8088,"`site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. Before the TLS PR, Batch had a confused deputy problem: it issues a callback in; response to a batch finishing. That callback is issued from within the cluster; and therefore can name many of our services which are not exposed to the; intern",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:8204,Security,certificate,certificate,8204,"ration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. Before the TLS PR, Batch had a confused deputy problem: it issues a callback in; response to a batch finishing. That callback is issued from within the cluster; and therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will n",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:8234,Security,certificate,certificate,8234,"ration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains five files:. - `site-config-http.conf`: an NGINX configuration file that configures TLS for; incoming requests.; - `site-config-proxy.conf`: an NGINX configuration file that configures TLS for; outgoing (proxy_pass) requests.; - `site-key.pem`: a private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. Before the TLS PR, Batch had a confused deputy problem: it issues a callback in; response to a batch finishing. That callback is issued from within the cluster; and therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will n",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:8570,Security,secur,security,8570," private key.; - `site-key-store.p12`: the same private key in PKCS12 format.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. Before the TLS PR, Batch had a confused deputy problem: it issues a callback in; response to a batch finishing. That callback is issued from within the cluster; and therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `ai",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:8643,Security,attack,attacker,8643,"ormat.; - `site-cert.pem`: a certificate.; - `site-incoming.pem`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. Before the TLS PR, Batch had a confused deputy problem: it issues a callback in; response to a batch finishing. That callback is issued from within the cluster; and therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:8687,Security,attack,attacker,8687,"`: a list of certificates trusted for incoming requests; (this currently only contains the root certificate).; - `site-incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. Before the TLS PR, Batch had a confused deputy problem: it issues a callback in; response to a batch finishing. That callback is issued from within the cluster; and therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does no",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:8808,Security,attack,attacker,8808,"incoming-store.jks`: the same list of trusted incoming certificates in; Java KeyStore format.; - `site-outgoing.pem`: a list of certificates expected from outgoing requests; (this currently only contains the root certificate).; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. Before the TLS PR, Batch had a confused deputy problem: it issues a callback in; response to a batch finishing. That callback is issued from within the cluster; and therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; inclu",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:9065,Security,expose,exposed,9065,"; - `site-outgoing-store.jks`: the same list of trusted outgoing certificates in; Java KeyStore format. If site makes an HTTP request to a server and that server does not return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. Before the TLS PR, Batch had a confused deputy problem: it issues a callback in; response to a batch finishing. That callback is issued from within the cluster; and therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; doe",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:9220,Security,certificate,certificate,9220," return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. Before the TLS PR, Batch had a confused deputy problem: it issues a callback in; response to a batch finishing. That callback is issued from within the cluster; and therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. Th",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:9264,Security,certificate,certificates,9264," return a; certificate in or signed by a certificate in `site-outgoing.pem`, it will; immediately halt the connection. There are two other kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. Before the TLS PR, Batch had a confused deputy problem: it issues a callback in; response to a batch finishing. That callback is issued from within the cluster; and therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. Th",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:9319,Security,certificate,certificate,9319,"ther kinds: `json` and `curl`. The former is for Hail Python; services. The later is for the admin-pod. ### Batch Confused Deputy. The [confused deputy; problem](https://en.wikipedia.org/wiki/Confused_deputy_problem) is a classic; in computer security. It refers to a situation with two parties: the deputy and; the attacker. The deputy has authority that the attacker does not. For example,; the deputy can arrest people. A *confused deputy* is one which has been tricked; by the attacker into misusing its authority. Before the TLS PR, Batch had a confused deputy problem: it issues a callback in; response to a batch finishing. That callback is issued from within the cluster; and therefore can name many of our services which are not exposed to the; internet. With the introduction of TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not supp",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:10145,Security,certificate,certificates,10145,"f TLS everywhere, a confused deputy callback; will fail because the victim will not receive a valid Batch certificate (batch; purposely does not send certificates with the callbacks). Batch only uses its; certificate to send a callback for CI. This is safe because we control CI and; ensure it is not compromised. In the long run, I want to fix batch to use an entirely different network for; callbacks. ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryp",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:10465,Security,certificate,certificate,10465,". ### Notes of Annoyance. `aiohttp` silently ignores most invalid TLS requests, this makes debugging a TLS; issue difficult. `aiohttp`'s `raise_for_status` does not include the HTTP body in the; message. NGINX sometimes returns 400 in response to TLS issues with a proxy. It; includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a sh",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:11108,Security,encrypt,encryption,11108," includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages.; ",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:11635,Security,secur,securely,11635," includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages.; ",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:3281,Testability,test,test-batch,3281,"lients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the rest:. - `-newkey rsa:4096`. TLS supports many different kinds of private keys. This; generates a 4096-bit private RSA key.; - `-nodes`. This should be read as ""no DES"". It means that the private key is not; itself encrypted using DES and a password.; - `-subj /CN=example.com`. This certificate is valid for a server whose DNS name; is `example.com`. If ""hostname checking"" is enabled (web browsers always; enable it), then the client will reject the certificate if the hostname used; to open the socket does not match the certificate.; - `-addext subjectAltName = ...`. This specifies additional acceptable hostnames; for the aformentioned hostname check. #### Trust. An ",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:2623,Usability,simpl,simple,2623,"ery. The HTTP protocol is unchanged. The default port for HTTP is; 80 and the default port for HTTPS 443, however any port may be used. There are currently four versions of TLS, the latest is TLS 1.3. All versions of; SSL are considered insecure. The OpenSSL library implements TLS. There are other implementations, such as; LibreSSL, but they implement roughly the same interface as OpenSSL. The TLS protocol defines both a scheme for the *encryption of messages* and for; the *authentication of parties*. The protocol defines authentication as; optional. In practice, at least one party presents authentication. For example,; public web servers authenticate themselves to clients but clients do not; reciprocate. ### Authentication; #### X.509 Certificates. TLS uses X.509 Certificates for authentication. X.509 is a rather complicated; standard. X.509 Certificates can be serialized in a variety of ways. We use the; Privacy-enhanced Electronic Mail (PEM) file format for serialization. PEM is; really simple. A file may contain multiple base64-encoded blobs each with a; header and footer of the form:. ```; -----BEGIN LABEL-----; ...; -----END LABEL-----; ```. where `LABEL` describes the data. We only use two labels: `CERTIFICATE` and; `PRIVATE KEY`. An X.509 Certificate is an unforgeable proof of identity. It usually is paired; with a private key that was used to digitally sign the certificate. In the; security literature, an authenticatable entity is usually called a; *principal*. Each principal should have a unique private key. In our system the; principals are both our services (e.g. `batch`, `batch-driver`) and any; non-serving clients (e.g. `test-batch`, `admin-pod`). A key and certificate are; generated ad nihilum by `openssl req -new`:. ```; openssl req -new \; -x509; -keyout key_file; -out cert_file; -newkey rsa:4096; -nodes; -subj /CN=example.com; -addext subjectAltName = DNS:www.example.com,DNS:foo.com; ```. The first three arguments are self-explanatory. I explain the",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:6328,Usability,clear,clear,6328," principals is one of many ways in which we limit the damage done by a; misbehaving or compromised service. ### Encryption. TLS has many encryption schemes. I will focus on encryption using a *symmetric*; key because asymmetric schemes do not enable *forward secrecy* [2]. Under; forward secret schemes, the two parties share a private key unknown to all; adversaries. TLS 1.3 makes forward secrecy mandatory. I intend to eventually; require all our services to refuse to speak anything other than TLS 1.3. The shared private key is used to encrypt and decrypt messages sent over a; socket. This poses a problem: how do two parties who have never met each other; agree on a private key without revealing the key to the public? This is a; classic cryptography problem called [key; exchange](https://en.wikipedia.org/wiki/Key_exchange). The classic solution to; this problem is [Diffie-Hellman key; exchange](https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange). The; Wikipedia article has ""General overview"" which is quite clear. In addition to a key, the parties must agree on a cipher. There are many old,; insecure ciphers available. In the future I intend all our servers to refuse to; use insecure ciphers. Mozilla; [has a list of secure cipher suites](https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations). ## New Hail Concepts. Every principal in our system has a secret: `ssl-config-NAME`. These secrets are; automatically created for a particular namespace by `tls/create_certs.py`. Who; trusts who (i.e. who is allowed to talk to whom) is defined by; `tls/config.yaml`. For example, `site` is defined in `config.yaml` as follows:. ```; - name: site; domain: site; kind: nginx; ```. A principal named ""site"" exists. Site's domain names are `site`,; `site.NAMESPACE`, `site.NAMESPACE.svc.cluster.local`. Site's configuration files; should be in NGINX configuration file format. `create_certs.py` will create a; new secret named `ssl-config-site` which contains fi",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md:11214,Usability,learn,learned,11214," includes crucial details in the HTTP body. I usually debug these issues by; sshing to the client pod and using curl to manually reproduce the error. Readiness and liveness probes cannot use HTTP. Although k8s supports HTTPS, it; does not support so-called ""mTLS"" or ""mutual TLS."" This is fancy verbiage for; servers that require clients to send trusted certificates. I will eventually require; this. There is a lot of information in GitHub issues and the Istio web pages; about this, but at the end of the day, kubernetes does not support this. TCP; probes are the best we can do. There [was a; PR](kubernetes/kubernetes#61231) to allow httpGet probes; to send the kubelet certificate, but it was closed because, apparently, the; [httpGet probes can be targeted at arbitrary IP; addresses](kubernetes/kubernetes#61231 (review)) (what; the hell?), ergo Confused Deputy. ### Footnotes. [1] TLS: Transport Layer Security. Preceded by Secure Sockets Layer (SSL) which; is not obsolete and insecure. After SSL version 3, a new version of SSL was; proposed in RFC 2246. This new version was backwards-incompatible and was; thus given a new name: Transport Layer Security. In common discussion, SSL; and TLS are used interchangeable. Indeed, the python TLS library is called; `ssl`. [2] Forward secrecy is a property of an encryption system. Forward secrecy means; a message cannot be decrypted in the future by an adversary who learned one; of the private keys. For example, imagine you are sending sensitive messages; to another individual. If that individual is later coerced into revealing; their secret key, forward secrecy would prevent the coercer from reading; your messages. Forward secrecy is achieved by negotiating a shared private; key between the two parties that is only used for a ""session"" and then; discarded. If the session key is securely discarded and neither key can; recreate it without cooperation from the other key, then *one* leaked key is; insufficient to reveal the messages.; ",MatchSource.DOCS,dev-docs/services/tls.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/tls.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/linalg/block-matrix-lowering.md:1771,Availability,down,downstream,1771,"orming the block context into the matrix value of each block. ## Global values. If a value is needed in the body of the computation, such as with a relational ; Let, it should generally be included as a global value. Global values are named for ease of reference; all names should be unique. Since global values are stored as an ordered array, previously-defined global ; values can be referenced in the definition of new global values. The current ; structure of the BlockMatrixIR means that it is unlikely to be necessary, though. When creating a BlockMatrixStage from other BlockMatrixStages, broadcast values; should be preserved from all child BlockMatrixStages. ### Referencing global values from contexts and body. Global values can be referenced from both block contexts and the body of the ; computation using `Ref(name, value.typ)`. ## Contexts. Each block in a BlockMatrixIR node will need to define its own context as ; a value IR node. Since any given block context could be used multiple times in downstream lowering ; transformations (e.g. multiply), the actual IR constructed for any given context; should be minimal. If a context requires non-trivial computation, such as in `ValueToBlockMatrix`; which creates an arbitrary NDArray, the computation itself should be stored as ; a global value and a reference to that value used in the context IR itself. (It ; will not be broadcast unless needed in distributed computation.). # lowering value IRs with BlockMatrixStages. BlockMatrixStage generates a value IR node which executes all block computations and; returns the results as a (sparse) array. The primary way for doing this is with the; function:. ```; toIR(bodyTranform: IR => IR, ordering: Option[Array[(Int, Int)]]): IR; ```. The bodyTransform function should return the (minimal) value from each block ; necessary to evaluate that node; a `BlockMatrixCollect`, for example, will need ; the entire NDArray, but a `BlockMatrixWrite` operation might only need to know ; the fil",MatchSource.DOCS,dev-docs/hail-query/linalg/block-matrix-lowering.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/hail-query/linalg/block-matrix-lowering.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:439,Integrability,depend,depend,439,"# Hail Identity Management. Every application in the Hail System assumes an identity from the; Identity Provider (IdP) of the backing cloud environment.; In GCP, these identities are human Google accounts; or Google Service Accounts. In Azure, they are human users,; Service Principals, and Managed Identities in Microsoft Entra ID; (aka Active Directory). ## Identity Assignment. Which identity is assumed by an application can sometimes depend; on where the application is running. These are separated into the; following environments. ### User's Computer. Code running in this environment acts under the user's human identity. Authorization; to act on the user's behalf is obtained through an OAuth2 flow, either through; auth.hail.is/login or `hailctl auth login`. NOTE: The credentials that are obtained through `hailctl auth login` are purely; to interact with the Hail Service's APIs and are narrowly scoped.; Requests directly to the cloud use the credentials obtained through the clouds'; CLIs, `gcloud` and `az`. For example, you can use the hail FS by running; `gcloud auth application-default login` or setting `GOOGLE_APPLICATION_CREDENTIALS`; without having an account with the Hail Service. ### User Code in the Cloud. Hail Batch jobs run processes on the user's behalf, and therefore need to assume an; identity that represents the user. Since we cannot (and do not want to) have access; to a user's human credentials or OAuth flow on every job, the Hail System creates and; maintains robot identities to represent the user. In GCP, these are Google Service; Accounts (GSAs) and in Azure they are Service Principals. The Auth service; is responsible for the lifecycle of these robot identities, and the Batch service is; responsible for securing credentials for these identities and delivering them to; Batch Workers. Service operators are responsible for manually rotating these credentials; on a regular cadence using the `devbin/rotate_keys.py` script. ### Hail Services. Services l",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:2210,Performance,perform,perform,2210,"ith the Hail Service. ### User Code in the Cloud. Hail Batch jobs run processes on the user's behalf, and therefore need to assume an; identity that represents the user. Since we cannot (and do not want to) have access; to a user's human credentials or OAuth flow on every job, the Hail System creates and; maintains robot identities to represent the user. In GCP, these are Google Service; Accounts (GSAs) and in Azure they are Service Principals. The Auth service; is responsible for the lifecycle of these robot identities, and the Batch service is; responsible for securing credentials for these identities and delivering them to; Batch Workers. Service operators are responsible for manually rotating these credentials; on a regular cadence using the `devbin/rotate_keys.py` script. ### Hail Services. Services like `auth`, `batch` and `ci` have their own robot identities just like user; robot accounts. Unlike user robot accounts, these identities are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and authorizes requests in the system through OAuth2; access tokens from the underlying IdP. See [this RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0001-oauth-access-tokens.rst); for details on how this is implemented. When an application needs to use a Hail API, it creates an access token either through; a metadata server or on-disk credentials. See [the keyless job RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst); for details on how this works for Batch Workers which are multi-tenant. The Auth service; inspects the access token to discover the identity behind the request and validates the; `aud` claim to ensure the token is intended for the Hail API. It is up to the service hosting the; ",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:1396,Security,access,access,1396," on where the application is running. These are separated into the; following environments. ### User's Computer. Code running in this environment acts under the user's human identity. Authorization; to act on the user's behalf is obtained through an OAuth2 flow, either through; auth.hail.is/login or `hailctl auth login`. NOTE: The credentials that are obtained through `hailctl auth login` are purely; to interact with the Hail Service's APIs and are narrowly scoped.; Requests directly to the cloud use the credentials obtained through the clouds'; CLIs, `gcloud` and `az`. For example, you can use the hail FS by running; `gcloud auth application-default login` or setting `GOOGLE_APPLICATION_CREDENTIALS`; without having an account with the Hail Service. ### User Code in the Cloud. Hail Batch jobs run processes on the user's behalf, and therefore need to assume an; identity that represents the user. Since we cannot (and do not want to) have access; to a user's human credentials or OAuth flow on every job, the Hail System creates and; maintains robot identities to represent the user. In GCP, these are Google Service; Accounts (GSAs) and in Azure they are Service Principals. The Auth service; is responsible for the lifecycle of these robot identities, and the Batch service is; responsible for securing credentials for these identities and delivering them to; Batch Workers. Service operators are responsible for manually rotating these credentials; on a regular cadence using the `devbin/rotate_keys.py` script. ### Hail Services. Services like `auth`, `batch` and `ci` have their own robot identities just like user; robot accounts. Unlike user robot accounts, these identities are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and a",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:1753,Security,secur,securing,1753,"il.is/login or `hailctl auth login`. NOTE: The credentials that are obtained through `hailctl auth login` are purely; to interact with the Hail Service's APIs and are narrowly scoped.; Requests directly to the cloud use the credentials obtained through the clouds'; CLIs, `gcloud` and `az`. For example, you can use the hail FS by running; `gcloud auth application-default login` or setting `GOOGLE_APPLICATION_CREDENTIALS`; without having an account with the Hail Service. ### User Code in the Cloud. Hail Batch jobs run processes on the user's behalf, and therefore need to assume an; identity that represents the user. Since we cannot (and do not want to) have access; to a user's human credentials or OAuth flow on every job, the Hail System creates and; maintains robot identities to represent the user. In GCP, these are Google Service; Accounts (GSAs) and in Azure they are Service Principals. The Auth service; is responsible for the lifecycle of these robot identities, and the Batch service is; responsible for securing credentials for these identities and delivering them to; Batch Workers. Service operators are responsible for manually rotating these credentials; on a regular cadence using the `devbin/rotate_keys.py` script. ### Hail Services. Services like `auth`, `batch` and `ci` have their own robot identities just like user; robot accounts. Unlike user robot accounts, these identities are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and authorizes requests in the system through OAuth2; access tokens from the underlying IdP. See [this RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0001-oauth-access-tokens.rst); for details on how this is implemented. When an application needs to use a Hail API, it creates an ac",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:2428,Security,authenticat,authenticates,2428,"ystem creates and; maintains robot identities to represent the user. In GCP, these are Google Service; Accounts (GSAs) and in Azure they are Service Principals. The Auth service; is responsible for the lifecycle of these robot identities, and the Batch service is; responsible for securing credentials for these identities and delivering them to; Batch Workers. Service operators are responsible for manually rotating these credentials; on a regular cadence using the `devbin/rotate_keys.py` script. ### Hail Services. Services like `auth`, `batch` and `ci` have their own robot identities just like user; robot accounts. Unlike user robot accounts, these identities are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and authorizes requests in the system through OAuth2; access tokens from the underlying IdP. See [this RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0001-oauth-access-tokens.rst); for details on how this is implemented. When an application needs to use a Hail API, it creates an access token either through; a metadata server or on-disk credentials. See [the keyless job RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst); for details on how this works for Batch Workers which are multi-tenant. The Auth service; inspects the access token to discover the identity behind the request and validates the; `aud` claim to ensure the token is intended for the Hail API. It is up to the service hosting the; targeted API endpoint to validate that the authenticated user is authorized to use the endpoint. ## Legacy authentication methods. Prior to using access tokens, the Auth service would issue long-lived API keys; that would be persisted on users' computers and mounted in Batch job containe",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:2446,Security,authoriz,authorizes,2446,"ystem creates and; maintains robot identities to represent the user. In GCP, these are Google Service; Accounts (GSAs) and in Azure they are Service Principals. The Auth service; is responsible for the lifecycle of these robot identities, and the Batch service is; responsible for securing credentials for these identities and delivering them to; Batch Workers. Service operators are responsible for manually rotating these credentials; on a regular cadence using the `devbin/rotate_keys.py` script. ### Hail Services. Services like `auth`, `batch` and `ci` have their own robot identities just like user; robot accounts. Unlike user robot accounts, these identities are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and authorizes requests in the system through OAuth2; access tokens from the underlying IdP. See [this RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0001-oauth-access-tokens.rst); for details on how this is implemented. When an application needs to use a Hail API, it creates an access token either through; a metadata server or on-disk credentials. See [the keyless job RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst); for details on how this works for Batch Workers which are multi-tenant. The Auth service; inspects the access token to discover the identity behind the request and validates the; `aud` claim to ensure the token is intended for the Hail API. It is up to the service hosting the; targeted API endpoint to validate that the authenticated user is authorized to use the endpoint. ## Legacy authentication methods. Prior to using access tokens, the Auth service would issue long-lived API keys; that would be persisted on users' computers and mounted in Batch job containe",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:2496,Security,access,access,2496,"ystem creates and; maintains robot identities to represent the user. In GCP, these are Google Service; Accounts (GSAs) and in Azure they are Service Principals. The Auth service; is responsible for the lifecycle of these robot identities, and the Batch service is; responsible for securing credentials for these identities and delivering them to; Batch Workers. Service operators are responsible for manually rotating these credentials; on a regular cadence using the `devbin/rotate_keys.py` script. ### Hail Services. Services like `auth`, `batch` and `ci` have their own robot identities just like user; robot accounts. Unlike user robot accounts, these identities are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and authorizes requests in the system through OAuth2; access tokens from the underlying IdP. See [this RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0001-oauth-access-tokens.rst); for details on how this is implemented. When an application needs to use a Hail API, it creates an access token either through; a metadata server or on-disk credentials. See [the keyless job RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst); for details on how this works for Batch Workers which are multi-tenant. The Auth service; inspects the access token to discover the identity behind the request and validates the; `aud` claim to ensure the token is intended for the Hail API. It is up to the service hosting the; targeted API endpoint to validate that the authenticated user is authorized to use the endpoint. ## Legacy authentication methods. Prior to using access tokens, the Auth service would issue long-lived API keys; that would be persisted on users' computers and mounted in Batch job containe",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:2612,Security,access,access-tokens,2612,"in Azure they are Service Principals. The Auth service; is responsible for the lifecycle of these robot identities, and the Batch service is; responsible for securing credentials for these identities and delivering them to; Batch Workers. Service operators are responsible for manually rotating these credentials; on a regular cadence using the `devbin/rotate_keys.py` script. ### Hail Services. Services like `auth`, `batch` and `ci` have their own robot identities just like user; robot accounts. Unlike user robot accounts, these identities are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and authorizes requests in the system through OAuth2; access tokens from the underlying IdP. See [this RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0001-oauth-access-tokens.rst); for details on how this is implemented. When an application needs to use a Hail API, it creates an access token either through; a metadata server or on-disk credentials. See [the keyless job RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst); for details on how this works for Batch Workers which are multi-tenant. The Auth service; inspects the access token to discover the identity behind the request and validates the; `aud` claim to ensure the token is intended for the Hail API. It is up to the service hosting the; targeted API endpoint to validate that the authenticated user is authorized to use the endpoint. ## Legacy authentication methods. Prior to using access tokens, the Auth service would issue long-lived API keys; that would be persisted on users' computers and mounted in Batch job containers.; At time of writing, these tokens are no longer used in Hail clients but support; has yet to be officially dropped. In ",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:2731,Security,access,access,2731," responsible for securing credentials for these identities and delivering them to; Batch Workers. Service operators are responsible for manually rotating these credentials; on a regular cadence using the `devbin/rotate_keys.py` script. ### Hail Services. Services like `auth`, `batch` and `ci` have their own robot identities just like user; robot accounts. Unlike user robot accounts, these identities are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and authorizes requests in the system through OAuth2; access tokens from the underlying IdP. See [this RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0001-oauth-access-tokens.rst); for details on how this is implemented. When an application needs to use a Hail API, it creates an access token either through; a metadata server or on-disk credentials. See [the keyless job RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst); for details on how this works for Batch Workers which are multi-tenant. The Auth service; inspects the access token to discover the identity behind the request and validates the; `aud` claim to ensure the token is intended for the Hail API. It is up to the service hosting the; targeted API endpoint to validate that the authenticated user is authorized to use the endpoint. ## Legacy authentication methods. Prior to using access tokens, the Auth service would issue long-lived API keys; that would be persisted on users' computers and mounted in Batch job containers.; At time of writing, these tokens are no longer used in Hail clients but support; has yet to be officially dropped. In addition to long-lived API keys, the Auth service also supports ""Copy-Paste tokens""; which are short-lived, on-demand tokens that users can ",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:3010,Security,access,access,3010,"just like user; robot accounts. Unlike user robot accounts, these identities are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and authorizes requests in the system through OAuth2; access tokens from the underlying IdP. See [this RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0001-oauth-access-tokens.rst); for details on how this is implemented. When an application needs to use a Hail API, it creates an access token either through; a metadata server or on-disk credentials. See [the keyless job RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst); for details on how this works for Batch Workers which are multi-tenant. The Auth service; inspects the access token to discover the identity behind the request and validates the; `aud` claim to ensure the token is intended for the Hail API. It is up to the service hosting the; targeted API endpoint to validate that the authenticated user is authorized to use the endpoint. ## Legacy authentication methods. Prior to using access tokens, the Auth service would issue long-lived API keys; that would be persisted on users' computers and mounted in Batch job containers.; At time of writing, these tokens are no longer used in Hail clients but support; has yet to be officially dropped. In addition to long-lived API keys, the Auth service also supports ""Copy-Paste tokens""; which are short-lived, on-demand tokens that users can obtain to then grant temporary; access to another user-controlled computer where they cannot conduct an OAuth flow.; The primary motivating use case here is to access Hail Batch from a Terra Jupyer notebook.; This authentication mechanism was originally implemented as a workaround for environments with; limited identity contr",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:3071,Security,validat,validates,3071,"just like user; robot accounts. Unlike user robot accounts, these identities are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and authorizes requests in the system through OAuth2; access tokens from the underlying IdP. See [this RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0001-oauth-access-tokens.rst); for details on how this is implemented. When an application needs to use a Hail API, it creates an access token either through; a metadata server or on-disk credentials. See [the keyless job RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst); for details on how this works for Batch Workers which are multi-tenant. The Auth service; inspects the access token to discover the identity behind the request and validates the; `aud` claim to ensure the token is intended for the Hail API. It is up to the service hosting the; targeted API endpoint to validate that the authenticated user is authorized to use the endpoint. ## Legacy authentication methods. Prior to using access tokens, the Auth service would issue long-lived API keys; that would be persisted on users' computers and mounted in Batch job containers.; At time of writing, these tokens are no longer used in Hail clients but support; has yet to be officially dropped. In addition to long-lived API keys, the Auth service also supports ""Copy-Paste tokens""; which are short-lived, on-demand tokens that users can obtain to then grant temporary; access to another user-controlled computer where they cannot conduct an OAuth flow.; The primary motivating use case here is to access Hail Batch from a Terra Jupyer notebook.; This authentication mechanism was originally implemented as a workaround for environments with; limited identity contr",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:3210,Security,validat,validate,3210,"are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and authorizes requests in the system through OAuth2; access tokens from the underlying IdP. See [this RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0001-oauth-access-tokens.rst); for details on how this is implemented. When an application needs to use a Hail API, it creates an access token either through; a metadata server or on-disk credentials. See [the keyless job RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst); for details on how this works for Batch Workers which are multi-tenant. The Auth service; inspects the access token to discover the identity behind the request and validates the; `aud` claim to ensure the token is intended for the Hail API. It is up to the service hosting the; targeted API endpoint to validate that the authenticated user is authorized to use the endpoint. ## Legacy authentication methods. Prior to using access tokens, the Auth service would issue long-lived API keys; that would be persisted on users' computers and mounted in Batch job containers.; At time of writing, these tokens are no longer used in Hail clients but support; has yet to be officially dropped. In addition to long-lived API keys, the Auth service also supports ""Copy-Paste tokens""; which are short-lived, on-demand tokens that users can obtain to then grant temporary; access to another user-controlled computer where they cannot conduct an OAuth flow.; The primary motivating use case here is to access Hail Batch from a Terra Jupyer notebook.; This authentication mechanism was originally implemented as a workaround for environments with; limited identity control and not a long-term solution for authenticating from such environments.; ",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:3228,Security,authenticat,authenticated,3228,"are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and authorizes requests in the system through OAuth2; access tokens from the underlying IdP. See [this RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0001-oauth-access-tokens.rst); for details on how this is implemented. When an application needs to use a Hail API, it creates an access token either through; a metadata server or on-disk credentials. See [the keyless job RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst); for details on how this works for Batch Workers which are multi-tenant. The Auth service; inspects the access token to discover the identity behind the request and validates the; `aud` claim to ensure the token is intended for the Hail API. It is up to the service hosting the; targeted API endpoint to validate that the authenticated user is authorized to use the endpoint. ## Legacy authentication methods. Prior to using access tokens, the Auth service would issue long-lived API keys; that would be persisted on users' computers and mounted in Batch job containers.; At time of writing, these tokens are no longer used in Hail clients but support; has yet to be officially dropped. In addition to long-lived API keys, the Auth service also supports ""Copy-Paste tokens""; which are short-lived, on-demand tokens that users can obtain to then grant temporary; access to another user-controlled computer where they cannot conduct an OAuth flow.; The primary motivating use case here is to access Hail Batch from a Terra Jupyer notebook.; This authentication mechanism was originally implemented as a workaround for environments with; limited identity control and not a long-term solution for authenticating from such environments.; ",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:3250,Security,authoriz,authorized,3250,"are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and authorizes requests in the system through OAuth2; access tokens from the underlying IdP. See [this RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0001-oauth-access-tokens.rst); for details on how this is implemented. When an application needs to use a Hail API, it creates an access token either through; a metadata server or on-disk credentials. See [the keyless job RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst); for details on how this works for Batch Workers which are multi-tenant. The Auth service; inspects the access token to discover the identity behind the request and validates the; `aud` claim to ensure the token is intended for the Hail API. It is up to the service hosting the; targeted API endpoint to validate that the authenticated user is authorized to use the endpoint. ## Legacy authentication methods. Prior to using access tokens, the Auth service would issue long-lived API keys; that would be persisted on users' computers and mounted in Batch job containers.; At time of writing, these tokens are no longer used in Hail clients but support; has yet to be officially dropped. In addition to long-lived API keys, the Auth service also supports ""Copy-Paste tokens""; which are short-lived, on-demand tokens that users can obtain to then grant temporary; access to another user-controlled computer where they cannot conduct an OAuth flow.; The primary motivating use case here is to access Hail Batch from a Terra Jupyer notebook.; This authentication mechanism was originally implemented as a workaround for environments with; limited identity control and not a long-term solution for authenticating from such environments.; ",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:3292,Security,authenticat,authentication,3292,"are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and authorizes requests in the system through OAuth2; access tokens from the underlying IdP. See [this RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0001-oauth-access-tokens.rst); for details on how this is implemented. When an application needs to use a Hail API, it creates an access token either through; a metadata server or on-disk credentials. See [the keyless job RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst); for details on how this works for Batch Workers which are multi-tenant. The Auth service; inspects the access token to discover the identity behind the request and validates the; `aud` claim to ensure the token is intended for the Hail API. It is up to the service hosting the; targeted API endpoint to validate that the authenticated user is authorized to use the endpoint. ## Legacy authentication methods. Prior to using access tokens, the Auth service would issue long-lived API keys; that would be persisted on users' computers and mounted in Batch job containers.; At time of writing, these tokens are no longer used in Hail clients but support; has yet to be officially dropped. In addition to long-lived API keys, the Auth service also supports ""Copy-Paste tokens""; which are short-lived, on-demand tokens that users can obtain to then grant temporary; access to another user-controlled computer where they cannot conduct an OAuth flow.; The primary motivating use case here is to access Hail Batch from a Terra Jupyer notebook.; This authentication mechanism was originally implemented as a workaround for environments with; limited identity control and not a long-term solution for authenticating from such environments.; ",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:3331,Security,access,access,3331,"are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and authorizes requests in the system through OAuth2; access tokens from the underlying IdP. See [this RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0001-oauth-access-tokens.rst); for details on how this is implemented. When an application needs to use a Hail API, it creates an access token either through; a metadata server or on-disk credentials. See [the keyless job RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst); for details on how this works for Batch Workers which are multi-tenant. The Auth service; inspects the access token to discover the identity behind the request and validates the; `aud` claim to ensure the token is intended for the Hail API. It is up to the service hosting the; targeted API endpoint to validate that the authenticated user is authorized to use the endpoint. ## Legacy authentication methods. Prior to using access tokens, the Auth service would issue long-lived API keys; that would be persisted on users' computers and mounted in Batch job containers.; At time of writing, these tokens are no longer used in Hail clients but support; has yet to be officially dropped. In addition to long-lived API keys, the Auth service also supports ""Copy-Paste tokens""; which are short-lived, on-demand tokens that users can obtain to then grant temporary; access to another user-controlled computer where they cannot conduct an OAuth flow.; The primary motivating use case here is to access Hail Batch from a Terra Jupyer notebook.; This authentication mechanism was originally implemented as a workaround for environments with; limited identity control and not a long-term solution for authenticating from such environments.; ",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:3768,Security,access,access,3768,"are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and authorizes requests in the system through OAuth2; access tokens from the underlying IdP. See [this RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0001-oauth-access-tokens.rst); for details on how this is implemented. When an application needs to use a Hail API, it creates an access token either through; a metadata server or on-disk credentials. See [the keyless job RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst); for details on how this works for Batch Workers which are multi-tenant. The Auth service; inspects the access token to discover the identity behind the request and validates the; `aud` claim to ensure the token is intended for the Hail API. It is up to the service hosting the; targeted API endpoint to validate that the authenticated user is authorized to use the endpoint. ## Legacy authentication methods. Prior to using access tokens, the Auth service would issue long-lived API keys; that would be persisted on users' computers and mounted in Batch job containers.; At time of writing, these tokens are no longer used in Hail clients but support; has yet to be officially dropped. In addition to long-lived API keys, the Auth service also supports ""Copy-Paste tokens""; which are short-lived, on-demand tokens that users can obtain to then grant temporary; access to another user-controlled computer where they cannot conduct an OAuth flow.; The primary motivating use case here is to access Hail Batch from a Terra Jupyer notebook.; This authentication mechanism was originally implemented as a workaround for environments with; limited identity control and not a long-term solution for authenticating from such environments.; ",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:3896,Security,access,access,3896,"are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and authorizes requests in the system through OAuth2; access tokens from the underlying IdP. See [this RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0001-oauth-access-tokens.rst); for details on how this is implemented. When an application needs to use a Hail API, it creates an access token either through; a metadata server or on-disk credentials. See [the keyless job RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst); for details on how this works for Batch Workers which are multi-tenant. The Auth service; inspects the access token to discover the identity behind the request and validates the; `aud` claim to ensure the token is intended for the Hail API. It is up to the service hosting the; targeted API endpoint to validate that the authenticated user is authorized to use the endpoint. ## Legacy authentication methods. Prior to using access tokens, the Auth service would issue long-lived API keys; that would be persisted on users' computers and mounted in Batch job containers.; At time of writing, these tokens are no longer used in Hail clients but support; has yet to be officially dropped. In addition to long-lived API keys, the Auth service also supports ""Copy-Paste tokens""; which are short-lived, on-demand tokens that users can obtain to then grant temporary; access to another user-controlled computer where they cannot conduct an OAuth flow.; The primary motivating use case here is to access Hail Batch from a Terra Jupyer notebook.; This authentication mechanism was originally implemented as a workaround for environments with; limited identity control and not a long-term solution for authenticating from such environments.; ",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:3950,Security,authenticat,authentication,3950,"are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and authorizes requests in the system through OAuth2; access tokens from the underlying IdP. See [this RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0001-oauth-access-tokens.rst); for details on how this is implemented. When an application needs to use a Hail API, it creates an access token either through; a metadata server or on-disk credentials. See [the keyless job RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst); for details on how this works for Batch Workers which are multi-tenant. The Auth service; inspects the access token to discover the identity behind the request and validates the; `aud` claim to ensure the token is intended for the Hail API. It is up to the service hosting the; targeted API endpoint to validate that the authenticated user is authorized to use the endpoint. ## Legacy authentication methods. Prior to using access tokens, the Auth service would issue long-lived API keys; that would be persisted on users' computers and mounted in Batch job containers.; At time of writing, these tokens are no longer used in Hail clients but support; has yet to be officially dropped. In addition to long-lived API keys, the Auth service also supports ""Copy-Paste tokens""; which are short-lived, on-demand tokens that users can obtain to then grant temporary; access to another user-controlled computer where they cannot conduct an OAuth flow.; The primary motivating use case here is to access Hail Batch from a Terra Jupyer notebook.; This authentication mechanism was originally implemented as a workaround for environments with; limited identity control and not a long-term solution for authenticating from such environments.; ",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:4099,Security,authenticat,authenticating,4099,"are granted certain roles; in the cloud environment that allow them to perform their functions like creating VMs; and writing to buckets. These roles should be tightly restricted to just the permissions; needed by the specific service. ## Authentication and Authorization. The Hail System authenticates and authorizes requests in the system through OAuth2; access tokens from the underlying IdP. See [this RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0001-oauth-access-tokens.rst); for details on how this is implemented. When an application needs to use a Hail API, it creates an access token either through; a metadata server or on-disk credentials. See [the keyless job RFC](https://github.com/hail-is/hail-rfcs/blob/main/rfc/0012-keyless-job-auth.rst); for details on how this works for Batch Workers which are multi-tenant. The Auth service; inspects the access token to discover the identity behind the request and validates the; `aud` claim to ensure the token is intended for the Hail API. It is up to the service hosting the; targeted API endpoint to validate that the authenticated user is authorized to use the endpoint. ## Legacy authentication methods. Prior to using access tokens, the Auth service would issue long-lived API keys; that would be persisted on users' computers and mounted in Batch job containers.; At time of writing, these tokens are no longer used in Hail clients but support; has yet to be officially dropped. In addition to long-lived API keys, the Auth service also supports ""Copy-Paste tokens""; which are short-lived, on-demand tokens that users can obtain to then grant temporary; access to another user-controlled computer where they cannot conduct an OAuth flow.; The primary motivating use case here is to access Hail Batch from a Terra Jupyer notebook.; This authentication mechanism was originally implemented as a workaround for environments with; limited identity control and not a long-term solution for authenticating from such environments.; ",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:738,Testability,log,login,738,"# Hail Identity Management. Every application in the Hail System assumes an identity from the; Identity Provider (IdP) of the backing cloud environment.; In GCP, these identities are human Google accounts; or Google Service Accounts. In Azure, they are human users,; Service Principals, and Managed Identities in Microsoft Entra ID; (aka Active Directory). ## Identity Assignment. Which identity is assumed by an application can sometimes depend; on where the application is running. These are separated into the; following environments. ### User's Computer. Code running in this environment acts under the user's human identity. Authorization; to act on the user's behalf is obtained through an OAuth2 flow, either through; auth.hail.is/login or `hailctl auth login`. NOTE: The credentials that are obtained through `hailctl auth login` are purely; to interact with the Hail Service's APIs and are narrowly scoped.; Requests directly to the cloud use the credentials obtained through the clouds'; CLIs, `gcloud` and `az`. For example, you can use the hail FS by running; `gcloud auth application-default login` or setting `GOOGLE_APPLICATION_CREDENTIALS`; without having an account with the Hail Service. ### User Code in the Cloud. Hail Batch jobs run processes on the user's behalf, and therefore need to assume an; identity that represents the user. Since we cannot (and do not want to) have access; to a user's human credentials or OAuth flow on every job, the Hail System creates and; maintains robot identities to represent the user. In GCP, these are Google Service; Accounts (GSAs) and in Azure they are Service Principals. The Auth service; is responsible for the lifecycle of these robot identities, and the Batch service is; responsible for securing credentials for these identities and delivering them to; Batch Workers. Service operators are responsible for manually rotating these credentials; on a regular cadence using the `devbin/rotate_keys.py` script. ### Hail Services. Services l",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:761,Testability,log,login,761,"# Hail Identity Management. Every application in the Hail System assumes an identity from the; Identity Provider (IdP) of the backing cloud environment.; In GCP, these identities are human Google accounts; or Google Service Accounts. In Azure, they are human users,; Service Principals, and Managed Identities in Microsoft Entra ID; (aka Active Directory). ## Identity Assignment. Which identity is assumed by an application can sometimes depend; on where the application is running. These are separated into the; following environments. ### User's Computer. Code running in this environment acts under the user's human identity. Authorization; to act on the user's behalf is obtained through an OAuth2 flow, either through; auth.hail.is/login or `hailctl auth login`. NOTE: The credentials that are obtained through `hailctl auth login` are purely; to interact with the Hail Service's APIs and are narrowly scoped.; Requests directly to the cloud use the credentials obtained through the clouds'; CLIs, `gcloud` and `az`. For example, you can use the hail FS by running; `gcloud auth application-default login` or setting `GOOGLE_APPLICATION_CREDENTIALS`; without having an account with the Hail Service. ### User Code in the Cloud. Hail Batch jobs run processes on the user's behalf, and therefore need to assume an; identity that represents the user. Since we cannot (and do not want to) have access; to a user's human credentials or OAuth flow on every job, the Hail System creates and; maintains robot identities to represent the user. In GCP, these are Google Service; Accounts (GSAs) and in Azure they are Service Principals. The Auth service; is responsible for the lifecycle of these robot identities, and the Batch service is; responsible for securing credentials for these identities and delivering them to; Batch Workers. Service operators are responsible for manually rotating these credentials; on a regular cadence using the `devbin/rotate_keys.py` script. ### Hail Services. Services l",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:831,Testability,log,login,831,"# Hail Identity Management. Every application in the Hail System assumes an identity from the; Identity Provider (IdP) of the backing cloud environment.; In GCP, these identities are human Google accounts; or Google Service Accounts. In Azure, they are human users,; Service Principals, and Managed Identities in Microsoft Entra ID; (aka Active Directory). ## Identity Assignment. Which identity is assumed by an application can sometimes depend; on where the application is running. These are separated into the; following environments. ### User's Computer. Code running in this environment acts under the user's human identity. Authorization; to act on the user's behalf is obtained through an OAuth2 flow, either through; auth.hail.is/login or `hailctl auth login`. NOTE: The credentials that are obtained through `hailctl auth login` are purely; to interact with the Hail Service's APIs and are narrowly scoped.; Requests directly to the cloud use the credentials obtained through the clouds'; CLIs, `gcloud` and `az`. For example, you can use the hail FS by running; `gcloud auth application-default login` or setting `GOOGLE_APPLICATION_CREDENTIALS`; without having an account with the Hail Service. ### User Code in the Cloud. Hail Batch jobs run processes on the user's behalf, and therefore need to assume an; identity that represents the user. Since we cannot (and do not want to) have access; to a user's human credentials or OAuth flow on every job, the Hail System creates and; maintains robot identities to represent the user. In GCP, these are Google Service; Accounts (GSAs) and in Azure they are Service Principals. The Auth service; is responsible for the lifecycle of these robot identities, and the Batch service is; responsible for securing credentials for these identities and delivering them to; Batch Workers. Service operators are responsible for manually rotating these credentials; on a regular cadence using the `devbin/rotate_keys.py` script. ### Hail Services. Services l",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md:1105,Testability,log,login,1105,"(IdP) of the backing cloud environment.; In GCP, these identities are human Google accounts; or Google Service Accounts. In Azure, they are human users,; Service Principals, and Managed Identities in Microsoft Entra ID; (aka Active Directory). ## Identity Assignment. Which identity is assumed by an application can sometimes depend; on where the application is running. These are separated into the; following environments. ### User's Computer. Code running in this environment acts under the user's human identity. Authorization; to act on the user's behalf is obtained through an OAuth2 flow, either through; auth.hail.is/login or `hailctl auth login`. NOTE: The credentials that are obtained through `hailctl auth login` are purely; to interact with the Hail Service's APIs and are narrowly scoped.; Requests directly to the cloud use the credentials obtained through the clouds'; CLIs, `gcloud` and `az`. For example, you can use the hail FS by running; `gcloud auth application-default login` or setting `GOOGLE_APPLICATION_CREDENTIALS`; without having an account with the Hail Service. ### User Code in the Cloud. Hail Batch jobs run processes on the user's behalf, and therefore need to assume an; identity that represents the user. Since we cannot (and do not want to) have access; to a user's human credentials or OAuth flow on every job, the Hail System creates and; maintains robot identities to represent the user. In GCP, these are Google Service; Accounts (GSAs) and in Azure they are Service Principals. The Auth service; is responsible for the lifecycle of these robot identities, and the Batch service is; responsible for securing credentials for these identities and delivering them to; Batch Workers. Service operators are responsible for manually rotating these credentials; on a regular cadence using the `devbin/rotate_keys.py` script. ### Hail Services. Services like `auth`, `batch` and `ci` have their own robot identities just like user; robot accounts. Unlike user robot ac",MatchSource.DOCS,dev-docs/services/auth/identity-management.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/auth/identity-management.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md:328,Deployability,configurat,configuration,328,"# Container runtime. Containers in batch are run using the [crun](https://github.com/containers/crun) container runtime.; `crun` is a low-level container runtime like `runc` (what Docker uses) which implements the; Open Container Initiative (OCI) specification for running containers given an image's filesystem and a; [runtime configuration](https://github.com/opencontainers/runtime-spec/blob/master/config.md). The JSON; configuration specifies, among other things, the linux namespaces and cgroups under which to run the container; and the user command to run. All images run on a worker are preprocessed by extracting their root filesystem into `/host/rootfs/` and; storing any additional image configuration like environment variables and users in memory in the worker; process. These root filesystems are immutable and job containers cannot write to them. All directories and; files relating to a user's job except for the underlying rootfs are stored under the job's scratch directory.; The scratch directory contains directories for each container in the job (input, main, output) and an `io`; directory that is mounted into each container. Each container directory contains; - The upper, merged, and work directories for the overlay filesystem used in the container. For; a great explanation of how overlayfs works, see; [here](https://jvns.ca/blog/2019/11/18/how-containers-work--overlayfs/).; - Any volumes specified in the user's image that are mounted into the container; - The container's `config.json` that the worker creates and passes to `crun`. Batch uses [xfs_quota](https://man7.org/linux/man-pages/man8/xfs_quota.8.html) to enforce storage; limits for jobs. Each job receives its own XFS project rooted at the scratch directory. Any writes from; the main, input and output containers into their root filesystems contribute to the overall job storage quota.; Storage in `/io` is subject to the user's quota *unless* `/io` is mounted from an external disk. Below is the layout of j",MatchSource.DOCS,dev-docs/services/batch/job-container-design.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md:424,Deployability,configurat,configuration,424,"# Container runtime. Containers in batch are run using the [crun](https://github.com/containers/crun) container runtime.; `crun` is a low-level container runtime like `runc` (what Docker uses) which implements the; Open Container Initiative (OCI) specification for running containers given an image's filesystem and a; [runtime configuration](https://github.com/opencontainers/runtime-spec/blob/master/config.md). The JSON; configuration specifies, among other things, the linux namespaces and cgroups under which to run the container; and the user command to run. All images run on a worker are preprocessed by extracting their root filesystem into `/host/rootfs/` and; storing any additional image configuration like environment variables and users in memory in the worker; process. These root filesystems are immutable and job containers cannot write to them. All directories and; files relating to a user's job except for the underlying rootfs are stored under the job's scratch directory.; The scratch directory contains directories for each container in the job (input, main, output) and an `io`; directory that is mounted into each container. Each container directory contains; - The upper, merged, and work directories for the overlay filesystem used in the container. For; a great explanation of how overlayfs works, see; [here](https://jvns.ca/blog/2019/11/18/how-containers-work--overlayfs/).; - Any volumes specified in the user's image that are mounted into the container; - The container's `config.json` that the worker creates and passes to `crun`. Batch uses [xfs_quota](https://man7.org/linux/man-pages/man8/xfs_quota.8.html) to enforce storage; limits for jobs. Each job receives its own XFS project rooted at the scratch directory. Any writes from; the main, input and output containers into their root filesystems contribute to the overall job storage quota.; Storage in `/io` is subject to the user's quota *unless* `/io` is mounted from an external disk. Below is the layout of j",MatchSource.DOCS,dev-docs/services/batch/job-container-design.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md:700,Deployability,configurat,configuration,700,"# Container runtime. Containers in batch are run using the [crun](https://github.com/containers/crun) container runtime.; `crun` is a low-level container runtime like `runc` (what Docker uses) which implements the; Open Container Initiative (OCI) specification for running containers given an image's filesystem and a; [runtime configuration](https://github.com/opencontainers/runtime-spec/blob/master/config.md). The JSON; configuration specifies, among other things, the linux namespaces and cgroups under which to run the container; and the user command to run. All images run on a worker are preprocessed by extracting their root filesystem into `/host/rootfs/` and; storing any additional image configuration like environment variables and users in memory in the worker; process. These root filesystems are immutable and job containers cannot write to them. All directories and; files relating to a user's job except for the underlying rootfs are stored under the job's scratch directory.; The scratch directory contains directories for each container in the job (input, main, output) and an `io`; directory that is mounted into each container. Each container directory contains; - The upper, merged, and work directories for the overlay filesystem used in the container. For; a great explanation of how overlayfs works, see; [here](https://jvns.ca/blog/2019/11/18/how-containers-work--overlayfs/).; - Any volumes specified in the user's image that are mounted into the container; - The container's `config.json` that the worker creates and passes to `crun`. Batch uses [xfs_quota](https://man7.org/linux/man-pages/man8/xfs_quota.8.html) to enforce storage; limits for jobs. Each job receives its own XFS project rooted at the scratch directory. Any writes from; the main, input and output containers into their root filesystems contribute to the overall job storage quota.; Storage in `/io` is subject to the user's quota *unless* `/io` is mounted from an external disk. Below is the layout of j",MatchSource.DOCS,dev-docs/services/batch/job-container-design.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md:328,Modifiability,config,configuration,328,"# Container runtime. Containers in batch are run using the [crun](https://github.com/containers/crun) container runtime.; `crun` is a low-level container runtime like `runc` (what Docker uses) which implements the; Open Container Initiative (OCI) specification for running containers given an image's filesystem and a; [runtime configuration](https://github.com/opencontainers/runtime-spec/blob/master/config.md). The JSON; configuration specifies, among other things, the linux namespaces and cgroups under which to run the container; and the user command to run. All images run on a worker are preprocessed by extracting their root filesystem into `/host/rootfs/` and; storing any additional image configuration like environment variables and users in memory in the worker; process. These root filesystems are immutable and job containers cannot write to them. All directories and; files relating to a user's job except for the underlying rootfs are stored under the job's scratch directory.; The scratch directory contains directories for each container in the job (input, main, output) and an `io`; directory that is mounted into each container. Each container directory contains; - The upper, merged, and work directories for the overlay filesystem used in the container. For; a great explanation of how overlayfs works, see; [here](https://jvns.ca/blog/2019/11/18/how-containers-work--overlayfs/).; - Any volumes specified in the user's image that are mounted into the container; - The container's `config.json` that the worker creates and passes to `crun`. Batch uses [xfs_quota](https://man7.org/linux/man-pages/man8/xfs_quota.8.html) to enforce storage; limits for jobs. Each job receives its own XFS project rooted at the scratch directory. Any writes from; the main, input and output containers into their root filesystems contribute to the overall job storage quota.; Storage in `/io` is subject to the user's quota *unless* `/io` is mounted from an external disk. Below is the layout of j",MatchSource.DOCS,dev-docs/services/batch/job-container-design.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md:402,Modifiability,config,config,402,"# Container runtime. Containers in batch are run using the [crun](https://github.com/containers/crun) container runtime.; `crun` is a low-level container runtime like `runc` (what Docker uses) which implements the; Open Container Initiative (OCI) specification for running containers given an image's filesystem and a; [runtime configuration](https://github.com/opencontainers/runtime-spec/blob/master/config.md). The JSON; configuration specifies, among other things, the linux namespaces and cgroups under which to run the container; and the user command to run. All images run on a worker are preprocessed by extracting their root filesystem into `/host/rootfs/` and; storing any additional image configuration like environment variables and users in memory in the worker; process. These root filesystems are immutable and job containers cannot write to them. All directories and; files relating to a user's job except for the underlying rootfs are stored under the job's scratch directory.; The scratch directory contains directories for each container in the job (input, main, output) and an `io`; directory that is mounted into each container. Each container directory contains; - The upper, merged, and work directories for the overlay filesystem used in the container. For; a great explanation of how overlayfs works, see; [here](https://jvns.ca/blog/2019/11/18/how-containers-work--overlayfs/).; - Any volumes specified in the user's image that are mounted into the container; - The container's `config.json` that the worker creates and passes to `crun`. Batch uses [xfs_quota](https://man7.org/linux/man-pages/man8/xfs_quota.8.html) to enforce storage; limits for jobs. Each job receives its own XFS project rooted at the scratch directory. Any writes from; the main, input and output containers into their root filesystems contribute to the overall job storage quota.; Storage in `/io` is subject to the user's quota *unless* `/io` is mounted from an external disk. Below is the layout of j",MatchSource.DOCS,dev-docs/services/batch/job-container-design.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md:424,Modifiability,config,configuration,424,"# Container runtime. Containers in batch are run using the [crun](https://github.com/containers/crun) container runtime.; `crun` is a low-level container runtime like `runc` (what Docker uses) which implements the; Open Container Initiative (OCI) specification for running containers given an image's filesystem and a; [runtime configuration](https://github.com/opencontainers/runtime-spec/blob/master/config.md). The JSON; configuration specifies, among other things, the linux namespaces and cgroups under which to run the container; and the user command to run. All images run on a worker are preprocessed by extracting their root filesystem into `/host/rootfs/` and; storing any additional image configuration like environment variables and users in memory in the worker; process. These root filesystems are immutable and job containers cannot write to them. All directories and; files relating to a user's job except for the underlying rootfs are stored under the job's scratch directory.; The scratch directory contains directories for each container in the job (input, main, output) and an `io`; directory that is mounted into each container. Each container directory contains; - The upper, merged, and work directories for the overlay filesystem used in the container. For; a great explanation of how overlayfs works, see; [here](https://jvns.ca/blog/2019/11/18/how-containers-work--overlayfs/).; - Any volumes specified in the user's image that are mounted into the container; - The container's `config.json` that the worker creates and passes to `crun`. Batch uses [xfs_quota](https://man7.org/linux/man-pages/man8/xfs_quota.8.html) to enforce storage; limits for jobs. Each job receives its own XFS project rooted at the scratch directory. Any writes from; the main, input and output containers into their root filesystems contribute to the overall job storage quota.; Storage in `/io` is subject to the user's quota *unless* `/io` is mounted from an external disk. Below is the layout of j",MatchSource.DOCS,dev-docs/services/batch/job-container-design.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md:700,Modifiability,config,configuration,700,"# Container runtime. Containers in batch are run using the [crun](https://github.com/containers/crun) container runtime.; `crun` is a low-level container runtime like `runc` (what Docker uses) which implements the; Open Container Initiative (OCI) specification for running containers given an image's filesystem and a; [runtime configuration](https://github.com/opencontainers/runtime-spec/blob/master/config.md). The JSON; configuration specifies, among other things, the linux namespaces and cgroups under which to run the container; and the user command to run. All images run on a worker are preprocessed by extracting their root filesystem into `/host/rootfs/` and; storing any additional image configuration like environment variables and users in memory in the worker; process. These root filesystems are immutable and job containers cannot write to them. All directories and; files relating to a user's job except for the underlying rootfs are stored under the job's scratch directory.; The scratch directory contains directories for each container in the job (input, main, output) and an `io`; directory that is mounted into each container. Each container directory contains; - The upper, merged, and work directories for the overlay filesystem used in the container. For; a great explanation of how overlayfs works, see; [here](https://jvns.ca/blog/2019/11/18/how-containers-work--overlayfs/).; - Any volumes specified in the user's image that are mounted into the container; - The container's `config.json` that the worker creates and passes to `crun`. Batch uses [xfs_quota](https://man7.org/linux/man-pages/man8/xfs_quota.8.html) to enforce storage; limits for jobs. Each job receives its own XFS project rooted at the scratch directory. Any writes from; the main, input and output containers into their root filesystems contribute to the overall job storage quota.; Storage in `/io` is subject to the user's quota *unless* `/io` is mounted from an external disk. Below is the layout of j",MatchSource.DOCS,dev-docs/services/batch/job-container-design.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md:731,Modifiability,variab,variables,731,"# Container runtime. Containers in batch are run using the [crun](https://github.com/containers/crun) container runtime.; `crun` is a low-level container runtime like `runc` (what Docker uses) which implements the; Open Container Initiative (OCI) specification for running containers given an image's filesystem and a; [runtime configuration](https://github.com/opencontainers/runtime-spec/blob/master/config.md). The JSON; configuration specifies, among other things, the linux namespaces and cgroups under which to run the container; and the user command to run. All images run on a worker are preprocessed by extracting their root filesystem into `/host/rootfs/` and; storing any additional image configuration like environment variables and users in memory in the worker; process. These root filesystems are immutable and job containers cannot write to them. All directories and; files relating to a user's job except for the underlying rootfs are stored under the job's scratch directory.; The scratch directory contains directories for each container in the job (input, main, output) and an `io`; directory that is mounted into each container. Each container directory contains; - The upper, merged, and work directories for the overlay filesystem used in the container. For; a great explanation of how overlayfs works, see; [here](https://jvns.ca/blog/2019/11/18/how-containers-work--overlayfs/).; - Any volumes specified in the user's image that are mounted into the container; - The container's `config.json` that the worker creates and passes to `crun`. Batch uses [xfs_quota](https://man7.org/linux/man-pages/man8/xfs_quota.8.html) to enforce storage; limits for jobs. Each job receives its own XFS project rooted at the scratch directory. Any writes from; the main, input and output containers into their root filesystems contribute to the overall job storage quota.; Storage in `/io` is subject to the user's quota *unless* `/io` is mounted from an external disk. Below is the layout of j",MatchSource.DOCS,dev-docs/services/batch/job-container-design.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md:1505,Modifiability,config,config,1505,"her things, the linux namespaces and cgroups under which to run the container; and the user command to run. All images run on a worker are preprocessed by extracting their root filesystem into `/host/rootfs/` and; storing any additional image configuration like environment variables and users in memory in the worker; process. These root filesystems are immutable and job containers cannot write to them. All directories and; files relating to a user's job except for the underlying rootfs are stored under the job's scratch directory.; The scratch directory contains directories for each container in the job (input, main, output) and an `io`; directory that is mounted into each container. Each container directory contains; - The upper, merged, and work directories for the overlay filesystem used in the container. For; a great explanation of how overlayfs works, see; [here](https://jvns.ca/blog/2019/11/18/how-containers-work--overlayfs/).; - Any volumes specified in the user's image that are mounted into the container; - The container's `config.json` that the worker creates and passes to `crun`. Batch uses [xfs_quota](https://man7.org/linux/man-pages/man8/xfs_quota.8.html) to enforce storage; limits for jobs. Each job receives its own XFS project rooted at the scratch directory. Any writes from; the main, input and output containers into their root filesystems contribute to the overall job storage quota.; Storage in `/io` is subject to the user's quota *unless* `/io` is mounted from an external disk. Below is the layout of job's scratch directory on the worker. NOTE: Since the underlying image/root filesystem; is not stored per-job, it does not contribute toward a job's storage quota. ```; scratch/; ├─ io/ (potentially mounted from an external disk); ├─ input/; │ ├─ rootfs_overlay/; │ │ ├─ upperdir/ (writeable layer); │ │ ├─ merged/ (what the container sees as its root); │ │ │ ├─ bin/ (from the overlay's lowerdir [the image's rootfs]); │ │ │ ├─ etc/ (from the overlay's low",MatchSource.DOCS,dev-docs/services/batch/job-container-design.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md:2565,Modifiability,config,config,2565,"ontainer in the job (input, main, output) and an `io`; directory that is mounted into each container. Each container directory contains; - The upper, merged, and work directories for the overlay filesystem used in the container. For; a great explanation of how overlayfs works, see; [here](https://jvns.ca/blog/2019/11/18/how-containers-work--overlayfs/).; - Any volumes specified in the user's image that are mounted into the container; - The container's `config.json` that the worker creates and passes to `crun`. Batch uses [xfs_quota](https://man7.org/linux/man-pages/man8/xfs_quota.8.html) to enforce storage; limits for jobs. Each job receives its own XFS project rooted at the scratch directory. Any writes from; the main, input and output containers into their root filesystems contribute to the overall job storage quota.; Storage in `/io` is subject to the user's quota *unless* `/io` is mounted from an external disk. Below is the layout of job's scratch directory on the worker. NOTE: Since the underlying image/root filesystem; is not stored per-job, it does not contribute toward a job's storage quota. ```; scratch/; ├─ io/ (potentially mounted from an external disk); ├─ input/; │ ├─ rootfs_overlay/; │ │ ├─ upperdir/ (writeable layer); │ │ ├─ merged/ (what the container sees as its root); │ │ │ ├─ bin/ (from the overlay's lowerdir [the image's rootfs]); │ │ │ ├─ etc/ (from the overlay's lowerdir [the image's rootfs]); │ │ │ ├─ ...; │ │ │ ├─ io/ (bind mount); │ │ ├─ workdir/; │ ├─ volumes/; │ ├─ config.json; ├─ main/; │ ├─ rootfs_overlay/; │ │ ├─ upperdir/ (writeable layer); │ │ ├─ merged/ (what crun/the container sees as its root); │ │ │ ├─ bin/ (from the overlay's lowerdir [the image's rootfs]); │ │ │ ├─ etc/ (from the overlay's lowerdir [the image's rootfs]); │ │ │ ├─ ...; │ │ │ ├─ io/ (bind mount); │ │ │ ├─ image/specified/volume/ (bind mount from volumes/); │ │ ├─ workdir/; │ ├─ volumes/; │ │ ├─ image/specified/volume/; │ ├─ config.json; ├─ output/; │ ├─ ...; ```; ",MatchSource.DOCS,dev-docs/services/batch/job-container-design.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md:3008,Modifiability,config,config,3008,"ontainer in the job (input, main, output) and an `io`; directory that is mounted into each container. Each container directory contains; - The upper, merged, and work directories for the overlay filesystem used in the container. For; a great explanation of how overlayfs works, see; [here](https://jvns.ca/blog/2019/11/18/how-containers-work--overlayfs/).; - Any volumes specified in the user's image that are mounted into the container; - The container's `config.json` that the worker creates and passes to `crun`. Batch uses [xfs_quota](https://man7.org/linux/man-pages/man8/xfs_quota.8.html) to enforce storage; limits for jobs. Each job receives its own XFS project rooted at the scratch directory. Any writes from; the main, input and output containers into their root filesystems contribute to the overall job storage quota.; Storage in `/io` is subject to the user's quota *unless* `/io` is mounted from an external disk. Below is the layout of job's scratch directory on the worker. NOTE: Since the underlying image/root filesystem; is not stored per-job, it does not contribute toward a job's storage quota. ```; scratch/; ├─ io/ (potentially mounted from an external disk); ├─ input/; │ ├─ rootfs_overlay/; │ │ ├─ upperdir/ (writeable layer); │ │ ├─ merged/ (what the container sees as its root); │ │ │ ├─ bin/ (from the overlay's lowerdir [the image's rootfs]); │ │ │ ├─ etc/ (from the overlay's lowerdir [the image's rootfs]); │ │ │ ├─ ...; │ │ │ ├─ io/ (bind mount); │ │ ├─ workdir/; │ ├─ volumes/; │ ├─ config.json; ├─ main/; │ ├─ rootfs_overlay/; │ │ ├─ upperdir/ (writeable layer); │ │ ├─ merged/ (what crun/the container sees as its root); │ │ │ ├─ bin/ (from the overlay's lowerdir [the image's rootfs]); │ │ │ ├─ etc/ (from the overlay's lowerdir [the image's rootfs]); │ │ │ ├─ ...; │ │ │ ├─ io/ (bind mount); │ │ │ ├─ image/specified/volume/ (bind mount from volumes/); │ │ ├─ workdir/; │ ├─ volumes/; │ │ ├─ image/specified/volume/; │ ├─ config.json; ├─ output/; │ ├─ ...; ```; ",MatchSource.DOCS,dev-docs/services/batch/job-container-design.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/job-container-design.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:247,Availability,error,errors,247,"# Large-scale Batch Operation. Operating Batch at or near its limit is semi-manual because Batch currently is unable to regulate the amount of incoming work it accepts. Alerts in the #grafana Zulip stream fire when the Batch Driver or workers log errors. You can follow these alerts to the; corresponding metric at [grafana.hail.is](grafana.hail.is). The upper-left corner of the panels for the alerts should have a link to; the logs in Google Logging. Other than error logs, CPU usage is the best measure of Batch's remaining capacity. CPU usage for the Batch Driver and the database; are displayed on the [performance panel](https://grafana.hail.is/d/TVytxvb7z/performance) of Grafana[^1]. [^1]: CPU utilization is listed as a percentage of its *limit*, and at time of writing the CPU limit for the Batch Driver is set; to `1.5vCPU`. As such, 60% utilization is using almost a full core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:464,Availability,error,error,464,"# Large-scale Batch Operation. Operating Batch at or near its limit is semi-manual because Batch currently is unable to regulate the amount of incoming work it accepts. Alerts in the #grafana Zulip stream fire when the Batch Driver or workers log errors. You can follow these alerts to the; corresponding metric at [grafana.hail.is](grafana.hail.is). The upper-left corner of the panels for the alerts should have a link to; the logs in Google Logging. Other than error logs, CPU usage is the best measure of Batch's remaining capacity. CPU usage for the Batch Driver and the database; are displayed on the [performance panel](https://grafana.hail.is/d/TVytxvb7z/performance) of Grafana[^1]. [^1]: CPU utilization is listed as a percentage of its *limit*, and at time of writing the CPU limit for the Batch Driver is set; to `1.5vCPU`. As such, 60% utilization is using almost a full core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:2051,Deployability,deploy,deploy,2051,"helmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py#L176); and manually redeploy CI with `make -C ci deploy NAMESPACE=default`.[^2] Note that this may need to be; paired with reducing the cluster size as rate-limited instances are cost centers that we do not charge; users for. [^2]: This can be undone if CI merges a new commit to `main` and redeploys itself. Other indicators of service health include API request latency (endpoints on the Batch Driver should last <1s),; and the size of the DB Connection Queue. Queued database transactions imply that `internal-gateway` is not applying; sufficient back-pressure to protect the Driver. The rate limit should be tuned so that Batch runs at ~95% CPU at the maximum request rate.; Changes to Batch will change the maximum acceptable request rate, so this setting should be revisited periodically,; esp. when running a new large-scale workload. To determine if the cluster size is at the maximum, check the CPU and rate limiting when the cluster is not growing,; but just replacing preempted nodes. The CPU should not be pegged, and `inter",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:1249,Energy Efficiency,schedul,scheduling,1249,"og errors. You can follow these alerts to the; corresponding metric at [grafana.hail.is](grafana.hail.is). The upper-left corner of the panels for the alerts should have a link to; the logs in Google Logging. Other than error logs, CPU usage is the best measure of Batch's remaining capacity. CPU usage for the Batch Driver and the database; are displayed on the [performance panel](https://grafana.hail.is/d/TVytxvb7z/performance) of Grafana[^1]. [^1]: CPU utilization is listed as a percentage of its *limit*, and at time of writing the CPU limit for the Batch Driver is set; to `1.5vCPU`. As such, 60% utilization is using almost a full core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py#L176); and manually redeploy CI with `make -C ci deploy NAMESPACE=default`.[^2] Note that this may need to be; paired with reducing the cluster size as rate-limited instances are cost centers that we do not charge; users for. [^2]: This can b",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:1515,Energy Efficiency,reduce,reduce,1515,". CPU usage for the Batch Driver and the database; are displayed on the [performance panel](https://grafana.hail.is/d/TVytxvb7z/performance) of Grafana[^1]. [^1]: CPU utilization is listed as a percentage of its *limit*, and at time of writing the CPU limit for the Batch Driver is set; to `1.5vCPU`. As such, 60% utilization is using almost a full core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py#L176); and manually redeploy CI with `make -C ci deploy NAMESPACE=default`.[^2] Note that this may need to be; paired with reducing the cluster size as rate-limited instances are cost centers that we do not charge; users for. [^2]: This can be undone if CI merges a new commit to `main` and redeploys itself. Other indicators of service health include API request latency (endpoints on the Batch Driver should last <1s),; and the size of the DB Connection Queue. Queued database transactions imply that `internal-gateway` is not appl",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:1880,Energy Efficiency,reduce,reduce,1880,"core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py#L176); and manually redeploy CI with `make -C ci deploy NAMESPACE=default`.[^2] Note that this may need to be; paired with reducing the cluster size as rate-limited instances are cost centers that we do not charge; users for. [^2]: This can be undone if CI merges a new commit to `main` and redeploys itself. Other indicators of service health include API request latency (endpoints on the Batch Driver should last <1s),; and the size of the DB Connection Queue. Queued database transactions imply that `internal-gateway` is not applying; sufficient back-pressure to protect the Driver. The rate limit should be tuned so that Batch runs at ~95% CPU at the maximum request rate.; Changes to Batch will change the maximum acceptable request rate, so this setting should be revisited periodically,; esp. when running a new large-scale workload. To determine if the cluster size is at th",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:2209,Energy Efficiency,charge,charge,2209,"t time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py#L176); and manually redeploy CI with `make -C ci deploy NAMESPACE=default`.[^2] Note that this may need to be; paired with reducing the cluster size as rate-limited instances are cost centers that we do not charge; users for. [^2]: This can be undone if CI merges a new commit to `main` and redeploys itself. Other indicators of service health include API request latency (endpoints on the Batch Driver should last <1s),; and the size of the DB Connection Queue. Queued database transactions imply that `internal-gateway` is not applying; sufficient back-pressure to protect the Driver. The rate limit should be tuned so that Batch runs at ~95% CPU at the maximum request rate.; Changes to Batch will change the maximum acceptable request rate, so this setting should be revisited periodically,; esp. when running a new large-scale workload. To determine if the cluster size is at the maximum, check the CPU and rate limiting when the cluster is not growing,; but just replacing preempted nodes. The CPU should not be pegged, and `internal-gateway` should reject requests at most in transient bursts.; In general, the load will be much lower at equil",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:1121,Integrability,inject,inject,1121," work it accepts. Alerts in the #grafana Zulip stream fire when the Batch Driver or workers log errors. You can follow these alerts to the; corresponding metric at [grafana.hail.is](grafana.hail.is). The upper-left corner of the panels for the alerts should have a link to; the logs in Google Logging. Other than error logs, CPU usage is the best measure of Batch's remaining capacity. CPU usage for the Batch Driver and the database; are displayed on the [performance panel](https://grafana.hail.is/d/TVytxvb7z/performance) of Grafana[^1]. [^1]: CPU utilization is listed as a percentage of its *limit*, and at time of writing the CPU limit for the Batch Driver is set; to `1.5vCPU`. As such, 60% utilization is using almost a full core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py#L176); and manually redeploy CI with `make -C ci deploy NAMESPACE=default`.[^2] Note that this may need to be; paired with reducing the cluster size ",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:608,Performance,perform,performance,608,"# Large-scale Batch Operation. Operating Batch at or near its limit is semi-manual because Batch currently is unable to regulate the amount of incoming work it accepts. Alerts in the #grafana Zulip stream fire when the Batch Driver or workers log errors. You can follow these alerts to the; corresponding metric at [grafana.hail.is](grafana.hail.is). The upper-left corner of the panels for the alerts should have a link to; the logs in Google Logging. Other than error logs, CPU usage is the best measure of Batch's remaining capacity. CPU usage for the Batch Driver and the database; are displayed on the [performance panel](https://grafana.hail.is/d/TVytxvb7z/performance) of Grafana[^1]. [^1]: CPU utilization is listed as a percentage of its *limit*, and at time of writing the CPU limit for the Batch Driver is set; to `1.5vCPU`. As such, 60% utilization is using almost a full core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:663,Performance,perform,performance,663,"# Large-scale Batch Operation. Operating Batch at or near its limit is semi-manual because Batch currently is unable to regulate the amount of incoming work it accepts. Alerts in the #grafana Zulip stream fire when the Batch Driver or workers log errors. You can follow these alerts to the; corresponding metric at [grafana.hail.is](grafana.hail.is). The upper-left corner of the panels for the alerts should have a link to; the logs in Google Logging. Other than error logs, CPU usage is the best measure of Batch's remaining capacity. CPU usage for the Batch Driver and the database; are displayed on the [performance panel](https://grafana.hail.is/d/TVytxvb7z/performance) of Grafana[^1]. [^1]: CPU utilization is listed as a percentage of its *limit*, and at time of writing the CPU limit for the Batch Driver is set; to `1.5vCPU`. As such, 60% utilization is using almost a full core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:1075,Performance,latency,latency,1075,"near its limit is semi-manual because Batch currently is unable to regulate the amount of incoming work it accepts. Alerts in the #grafana Zulip stream fire when the Batch Driver or workers log errors. You can follow these alerts to the; corresponding metric at [grafana.hail.is](grafana.hail.is). The upper-left corner of the panels for the alerts should have a link to; the logs in Google Logging. Other than error logs, CPU usage is the best measure of Batch's remaining capacity. CPU usage for the Batch Driver and the database; are displayed on the [performance panel](https://grafana.hail.is/d/TVytxvb7z/performance) of Grafana[^1]. [^1]: CPU utilization is listed as a percentage of its *limit*, and at time of writing the CPU limit for the Batch Driver is set; to `1.5vCPU`. As such, 60% utilization is using almost a full core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py#L176); and manually redeploy CI with `make -C ci de",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:1448,Performance,load,load,1448,"han error logs, CPU usage is the best measure of Batch's remaining capacity. CPU usage for the Batch Driver and the database; are displayed on the [performance panel](https://grafana.hail.is/d/TVytxvb7z/performance) of Grafana[^1]. [^1]: CPU utilization is listed as a percentage of its *limit*, and at time of writing the CPU limit for the Batch Driver is set; to `1.5vCPU`. As such, 60% utilization is using almost a full core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py#L176); and manually redeploy CI with `make -C ci deploy NAMESPACE=default`.[^2] Note that this may need to be; paired with reducing the cluster size as rate-limited instances are cost centers that we do not charge; users for. [^2]: This can be undone if CI merges a new commit to `main` and redeploys itself. Other indicators of service health include API request latency (endpoints on the Batch Driver should last <1s),; and the size of the DB Connection Qu",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:1495,Performance,throughput,throughput,1495,"han error logs, CPU usage is the best measure of Batch's remaining capacity. CPU usage for the Batch Driver and the database; are displayed on the [performance panel](https://grafana.hail.is/d/TVytxvb7z/performance) of Grafana[^1]. [^1]: CPU utilization is listed as a percentage of its *limit*, and at time of writing the CPU limit for the Batch Driver is set; to `1.5vCPU`. As such, 60% utilization is using almost a full core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py#L176); and manually redeploy CI with `make -C ci deploy NAMESPACE=default`.[^2] Note that this may need to be; paired with reducing the cluster size as rate-limited instances are cost centers that we do not charge; users for. [^2]: This can be undone if CI merges a new commit to `main` and redeploys itself. Other indicators of service health include API request latency (endpoints on the Batch Driver should last <1s),; and the size of the DB Connection Qu",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:1522,Performance,load,load,1522,". CPU usage for the Batch Driver and the database; are displayed on the [performance panel](https://grafana.hail.is/d/TVytxvb7z/performance) of Grafana[^1]. [^1]: CPU utilization is listed as a percentage of its *limit*, and at time of writing the CPU limit for the Batch Driver is set; to `1.5vCPU`. As such, 60% utilization is using almost a full core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py#L176); and manually redeploy CI with `make -C ci deploy NAMESPACE=default`.[^2] Note that this may need to be; paired with reducing the cluster size as rate-limited instances are cost centers that we do not charge; users for. [^2]: This can be undone if CI merges a new commit to `main` and redeploys itself. Other indicators of service health include API request latency (endpoints on the Batch Driver should last <1s),; and the size of the DB Connection Queue. Queued database transactions imply that `internal-gateway` is not appl",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:1573,Performance,load,load,1573,"e [performance panel](https://grafana.hail.is/d/TVytxvb7z/performance) of Grafana[^1]. [^1]: CPU utilization is listed as a percentage of its *limit*, and at time of writing the CPU limit for the Batch Driver is set; to `1.5vCPU`. As such, 60% utilization is using almost a full core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py#L176); and manually redeploy CI with `make -C ci deploy NAMESPACE=default`.[^2] Note that this may need to be; paired with reducing the cluster size as rate-limited instances are cost centers that we do not charge; users for. [^2]: This can be undone if CI merges a new commit to `main` and redeploys itself. Other indicators of service health include API request latency (endpoints on the Batch Driver should last <1s),; and the size of the DB Connection Queue. Queued database transactions imply that `internal-gateway` is not applying; sufficient back-pressure to protect the Driver. The rate limit s",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:1720,Performance,load,load,1720,"]: CPU utilization is listed as a percentage of its *limit*, and at time of writing the CPU limit for the Batch Driver is set; to `1.5vCPU`. As such, 60% utilization is using almost a full core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py#L176); and manually redeploy CI with `make -C ci deploy NAMESPACE=default`.[^2] Note that this may need to be; paired with reducing the cluster size as rate-limited instances are cost centers that we do not charge; users for. [^2]: This can be undone if CI merges a new commit to `main` and redeploys itself. Other indicators of service health include API request latency (endpoints on the Batch Driver should last <1s),; and the size of the DB Connection Queue. Queued database transactions imply that `internal-gateway` is not applying; sufficient back-pressure to protect the Driver. The rate limit should be tuned so that Batch runs at ~95% CPU at the maximum request rate.; Changes to Batc",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:2366,Performance,latency,latency,2366," in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py#L176); and manually redeploy CI with `make -C ci deploy NAMESPACE=default`.[^2] Note that this may need to be; paired with reducing the cluster size as rate-limited instances are cost centers that we do not charge; users for. [^2]: This can be undone if CI merges a new commit to `main` and redeploys itself. Other indicators of service health include API request latency (endpoints on the Batch Driver should last <1s),; and the size of the DB Connection Queue. Queued database transactions imply that `internal-gateway` is not applying; sufficient back-pressure to protect the Driver. The rate limit should be tuned so that Batch runs at ~95% CPU at the maximum request rate.; Changes to Batch will change the maximum acceptable request rate, so this setting should be revisited periodically,; esp. when running a new large-scale workload. To determine if the cluster size is at the maximum, check the CPU and rate limiting when the cluster is not growing,; but just replacing preempted nodes. The CPU should not be pegged, and `internal-gateway` should reject requests at most in transient bursts.; In general, the load will be much lower at equilibrium because filling an empty node requires many operations. ## Quotas. When using Local SSDs on preemptible machines there are only two quotas that matter: ""Preemptible; Local SSD (GB)"" (`PREEMPTIBLE_LOCAL_SSD_GB`) and ""Preemptible ",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:2614,Performance,tune,tuned,2614,"current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py#L176); and manually redeploy CI with `make -C ci deploy NAMESPACE=default`.[^2] Note that this may need to be; paired with reducing the cluster size as rate-limited instances are cost centers that we do not charge; users for. [^2]: This can be undone if CI merges a new commit to `main` and redeploys itself. Other indicators of service health include API request latency (endpoints on the Batch Driver should last <1s),; and the size of the DB Connection Queue. Queued database transactions imply that `internal-gateway` is not applying; sufficient back-pressure to protect the Driver. The rate limit should be tuned so that Batch runs at ~95% CPU at the maximum request rate.; Changes to Batch will change the maximum acceptable request rate, so this setting should be revisited periodically,; esp. when running a new large-scale workload. To determine if the cluster size is at the maximum, check the CPU and rate limiting when the cluster is not growing,; but just replacing preempted nodes. The CPU should not be pegged, and `internal-gateway` should reject requests at most in transient bursts.; In general, the load will be much lower at equilibrium because filling an empty node requires many operations. ## Quotas. When using Local SSDs on preemptible machines there are only two quotas that matter: ""Preemptible; Local SSD (GB)"" (`PREEMPTIBLE_LOCAL_SSD_GB`) and ""Preemptible CPUs"" (`PREEMPTIBLE_CPUS`). The former; is measured in GB, so you'll need 375 GB of quota for every machine with a Local SSD. The latter is; measured in cores. For example, if you are using a mix of n1 and n2 machines with 8 cores and 1; Local SSD",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:3120,Performance,load,load,3120,"-limited instances are cost centers that we do not charge; users for. [^2]: This can be undone if CI merges a new commit to `main` and redeploys itself. Other indicators of service health include API request latency (endpoints on the Batch Driver should last <1s),; and the size of the DB Connection Queue. Queued database transactions imply that `internal-gateway` is not applying; sufficient back-pressure to protect the Driver. The rate limit should be tuned so that Batch runs at ~95% CPU at the maximum request rate.; Changes to Batch will change the maximum acceptable request rate, so this setting should be revisited periodically,; esp. when running a new large-scale workload. To determine if the cluster size is at the maximum, check the CPU and rate limiting when the cluster is not growing,; but just replacing preempted nodes. The CPU should not be pegged, and `internal-gateway` should reject requests at most in transient bursts.; In general, the load will be much lower at equilibrium because filling an empty node requires many operations. ## Quotas. When using Local SSDs on preemptible machines there are only two quotas that matter: ""Preemptible; Local SSD (GB)"" (`PREEMPTIBLE_LOCAL_SSD_GB`) and ""Preemptible CPUs"" (`PREEMPTIBLE_CPUS`). The former; is measured in GB, so you'll need 375 GB of quota for every machine with a Local SSD. The latter is; measured in cores. For example, if you are using a mix of n1 and n2 machines with 8 cores and 1; Local SSD, a 5000 machine (40,000 core) cluster will need:. - 1,875,000 GB of Preemptible Local SSD quota, and. - 40,000 cores of Preemptible CPUs quota. In practice, we use Local SSD quota much faster than CPU quota. Google will freely gives us a 5,000; core quota in any given zone. We've also received quotas as high as 300,000 cores. Google is; hesitant to grant a quota of more than 400,000 GB in a zone. The largest Preemptible Local SSD quota; we have been granted in one zone is 640,000 GB. We recommend requesting double you",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:1121,Security,inject,inject,1121," work it accepts. Alerts in the #grafana Zulip stream fire when the Batch Driver or workers log errors. You can follow these alerts to the; corresponding metric at [grafana.hail.is](grafana.hail.is). The upper-left corner of the panels for the alerts should have a link to; the logs in Google Logging. Other than error logs, CPU usage is the best measure of Batch's remaining capacity. CPU usage for the Batch Driver and the database; are displayed on the [performance panel](https://grafana.hail.is/d/TVytxvb7z/performance) of Grafana[^1]. [^1]: CPU utilization is listed as a percentage of its *limit*, and at time of writing the CPU limit for the Batch Driver is set; to `1.5vCPU`. As such, 60% utilization is using almost a full core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py#L176); and manually redeploy CI with `make -C ci deploy NAMESPACE=default`.[^2] Note that this may need to be; paired with reducing the cluster size ",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:243,Testability,log,log,243,"# Large-scale Batch Operation. Operating Batch at or near its limit is semi-manual because Batch currently is unable to regulate the amount of incoming work it accepts. Alerts in the #grafana Zulip stream fire when the Batch Driver or workers log errors. You can follow these alerts to the; corresponding metric at [grafana.hail.is](grafana.hail.is). The upper-left corner of the panels for the alerts should have a link to; the logs in Google Logging. Other than error logs, CPU usage is the best measure of Batch's remaining capacity. CPU usage for the Batch Driver and the database; are displayed on the [performance panel](https://grafana.hail.is/d/TVytxvb7z/performance) of Grafana[^1]. [^1]: CPU utilization is listed as a percentage of its *limit*, and at time of writing the CPU limit for the Batch Driver is set; to `1.5vCPU`. As such, 60% utilization is using almost a full core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:429,Testability,log,logs,429,"# Large-scale Batch Operation. Operating Batch at or near its limit is semi-manual because Batch currently is unable to regulate the amount of incoming work it accepts. Alerts in the #grafana Zulip stream fire when the Batch Driver or workers log errors. You can follow these alerts to the; corresponding metric at [grafana.hail.is](grafana.hail.is). The upper-left corner of the panels for the alerts should have a link to; the logs in Google Logging. Other than error logs, CPU usage is the best measure of Batch's remaining capacity. CPU usage for the Batch Driver and the database; are displayed on the [performance panel](https://grafana.hail.is/d/TVytxvb7z/performance) of Grafana[^1]. [^1]: CPU utilization is listed as a percentage of its *limit*, and at time of writing the CPU limit for the Batch Driver is set; to `1.5vCPU`. As such, 60% utilization is using almost a full core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:470,Testability,log,logs,470,"# Large-scale Batch Operation. Operating Batch at or near its limit is semi-manual because Batch currently is unable to regulate the amount of incoming work it accepts. Alerts in the #grafana Zulip stream fire when the Batch Driver or workers log errors. You can follow these alerts to the; corresponding metric at [grafana.hail.is](grafana.hail.is). The upper-left corner of the panels for the alerts should have a link to; the logs in Google Logging. Other than error logs, CPU usage is the best measure of Batch's remaining capacity. CPU usage for the Batch Driver and the database; are displayed on the [performance panel](https://grafana.hail.is/d/TVytxvb7z/performance) of Grafana[^1]. [^1]: CPU utilization is listed as a percentage of its *limit*, and at time of writing the CPU limit for the Batch Driver is set; to `1.5vCPU`. As such, 60% utilization is using almost a full core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:1195,Usability,feedback,feedback,1195," work it accepts. Alerts in the #grafana Zulip stream fire when the Batch Driver or workers log errors. You can follow these alerts to the; corresponding metric at [grafana.hail.is](grafana.hail.is). The upper-left corner of the panels for the alerts should have a link to; the logs in Google Logging. Other than error logs, CPU usage is the best measure of Batch's remaining capacity. CPU usage for the Batch Driver and the database; are displayed on the [performance panel](https://grafana.hail.is/d/TVytxvb7z/performance) of Grafana[^1]. [^1]: CPU utilization is listed as a percentage of its *limit*, and at time of writing the CPU limit for the Batch Driver is set; to `1.5vCPU`. As such, 60% utilization is using almost a full core, which is all we can reasonably expect from a single python process. The goal is to operate the Batch Driver around 95% CPU. When Batch becomes overwhelmed, CPU is saturated and request latency increases.; New requests from workers inject work into the system, but time out and are retried, creating a bad feedback loop. Driver CPU usage is driven largely job scheduling and job completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py#L176); and manually redeploy CI with `make -C ci deploy NAMESPACE=default`.[^2] Note that this may need to be; paired with reducing the cluster size ",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md:2246,Usability,undo,undone,2246,"completion.; Floods of job completion, the main source of traffic from workers -> driver, are controlled by rate limits in the `internal-gateway`.; Altering the rate limit at full load is the most direct way to modulate system throughput. You can reduce load on the system in one of two ways:; 1. Gradual load shedding: Reduce the maximum cluster size to below its current size. As instances; die off naturally, the Driver will not replenish them, and load will ease over time. You can also; manually kill off `preemptible` instances if necessary to improve cluster health.; 2. Throttle Mark Job Complete (MJC): reduce the rate limit [here](https://github.com/hail-is/hail/blob/923cc552c9460527101139970d46364948e4f6a8/ci/ci/envoy.py#L176); and manually redeploy CI with `make -C ci deploy NAMESPACE=default`.[^2] Note that this may need to be; paired with reducing the cluster size as rate-limited instances are cost centers that we do not charge; users for. [^2]: This can be undone if CI merges a new commit to `main` and redeploys itself. Other indicators of service health include API request latency (endpoints on the Batch Driver should last <1s),; and the size of the DB Connection Queue. Queued database transactions imply that `internal-gateway` is not applying; sufficient back-pressure to protect the Driver. The rate limit should be tuned so that Batch runs at ~95% CPU at the maximum request rate.; Changes to Batch will change the maximum acceptable request rate, so this setting should be revisited periodically,; esp. when running a new large-scale workload. To determine if the cluster size is at the maximum, check the CPU and rate limiting when the cluster is not growing,; but just replacing preempted nodes. The CPU should not be pegged, and `internal-gateway` should reject requests at most in transient bursts.; In general, the load will be much lower at equilibrium because filling an empty node requires many operations. ## Quotas. When using Local SSDs on preemptible machin",MatchSource.DOCS,dev-docs/services/batch/operation.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/batch/operation.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:6617,Availability,failure,failure,6617,"d submit it against the production Hail Batch service using its own service credentials; - Tasks in the batch are defined in `build.yaml` in the top repo directory. CI generates jobs in the batch from the steps defined in there.; - The batch contains jobs which will:; - Clone the appropriate branch; - Squash and rebase onto `main`; - This is done in a way that means the resulting SHA will match the SHA which would be produced when the PR merges.; - Build new versions of all hail tools, packages and libraries; - Build a new set of docker images for hail query and the updated services; - Deploy the batch suite of k8s services into one of many CI-owned namespaces in the Hail Batch cluster; - These namespaces are named like `""pr-<PR_NUMBER>-<CI-NAMESPACE>-<RANDOM>""`; - Where `CI-NAMESPACE` is the namespace where CI is running (usually `default`). ; - Run a series of tests against the services; - Each test:; - Submits a specially crafted batch to the newly deployed batch namespace; - Checks that the results of running the batch are what we would expect; - The CI service polls the batch which it submits for overall success or failure.; - Once the batch has either succeeded or failed, CI uses that result to report status back to GitHub. Examples of CI test runs can be seen by searching through the production batch log, as long as you are a member; of the `ci` billing project. . > [!NOTE]; > CI test runs in hail.is can be seen [here](https://batch.hail.is/batches?q=user+%3D+ci%0D%0Atest+%3D+1).; > Note:; > - You will need to be a member of the `ci` billing project to view these logs.; > - Due to the numbers of batches involved, this page may take a while to load . #### CI Testing Timeline. The image below shows the CI testing timeline:. ```mermaid; sequenceDiagram; participant Github; box Kubernetes (PROD: 'default' namespace); participant CI as PROD CI Service (default namespace); participant PB as PROD Batch (default namespace); end; box GCE (PROD VMs); participant PVM as",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:11875,Availability,down,down,11875," Kubernetes (PROD: 'default' namespace); participant CI as PROD CI Service (default namespace); participant PB as PROD Batch (default namespace); end; box GCE (PROD VMs); participant PVM as PROD VMs; end. Github->>CI: Notify 'main' branch updated; CI->>PB: Submit CI deploy batch; activate PB; PB->>PVM: Submit build jobs; activate PVM; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit deploy jobs; activate PVM; PVM->>PB: Redeploy batch service; PVM->>CI: Redeploy CI service; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit test jobs; activate PVM; PVM->>PB: Submit test batches; PB->>PVM: Submit test batch jobs; activate PVM; PVM-->>PB: Validate results; deactivate PVM; PVM-->>PB: Done; deactivate PVM; PB-->>CI: Detects completion; ```. ## References. ### The update loop. The following reference diagrams show the actions taken during CI's main update loop. Note that these diagrams are code-up views of how CI works. They should be cross references against the; ""purpose down"" descriptions above, of how CI uses these flows to make the changes it wants to happen. ```mermaid; flowchart LR; UpdateLoop[Update Loop]; ; UpdateLoop --> UpdateGithub; subgraph UpdateGithub; GithubChanged{github_changed?} -->|yes| _update_github; end; ; UpdateGithub --> UpdateBatch; subgraph UpdateBatch; BatchChanged{batch_changed?} -->|yes| _update_batch; end. UpdateBatch --> UpdateState; subgraph UpdateState; StateChanged{state_changed?} -->|yes| _update_state; end. UpdateState --> Loop; ```. #### Update Github. ```mermaid; flowchart LR; UpdateLoop[Update Loop]; ; UpdateLoop -->|github_changed| UpdateGithub(Update Github); UpdateGithub --> UpdateSha(Update SHA for\nWatchedBranch); subgraph UpdatePRs; direction LR; UpdatePRList(Update list of PRs for\nWatchedBranch); UpdatePRList --> AssignReviewers(Assign Reviewers); AssignReviewers --> UpdatePrGithub(Update local PR state from github); UpdatePrGithub --> ChecksChanged{New check\nor review\nstate?}; ChecksChanged -->|yes| SetStateChanged(Set sta",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:14832,Availability,failure,failure,14832,e_changed). end. UpdateGithub --> UpdatePRs; ```. #### Update Batch. ```mermaid; flowchart LR; UpdateLoop[Update Loop]; ; UpdateLoop -->|batch_changed| UpdateBatch; UpdateBatch --> UpdateDeployable; UpdateBatch --> UpdatePRBatches ; subgraph UpdateDeployable; direction LR; Deployable{deployable?} --> UpdateDeployBatch(Update internal state of \ndeploy batch); end; subgraph UpdatePRBatches; direction LR; IterateOverPrs(Iterate over PRs) --> FindLatest(Find latest non-cancelled batch for SHA); FindLatest --> UpdateBatchState(Update local state of PR batch); UpdateBatchState --> BatchStateChanged{changed?}; BatchStateChanged -->|yes| SetStateChanged2(Set state_changed); end. ```. #### Update State. Updating the external state involves two steps: a 'heal' step and a 'try to merge' step. ```mermaid; flowchart LR; UpdateLoop[Update Loop]; ; UpdateLoop -->|state_changed| Heal; Heal --> TryToMerge(Try to merge); ```. ##### Heal. ```mermaid; flowchart LR; subgraph HealDeploy; direction LR; NewDeploy{new deployable SHA?} -->|yes| CreateDeploy(Create deploy\nbatch); end; HealDeploy --> DetermineMergeCandidate(Determine merge candidate); DetermineMergeCandidate --> HealPRs; subgraph HealPRs; direction LR; IterateOverPrs2(Iterate over PRs) --> PostGithubStatus(Post github status); PostGithubStatus --> StartTestBatch; subgraph StartTestBatch; direction LR; NoTestsYet{no test batch yet?} -->|yes| CreateTestBatch(Create test batch); CurrentMergeCandidate{current merge\ncandidate} -->|yes| TestBatchStale(previous test batch\noutdated); TestBatchStale -->|yes| CreateTestBatch; end; end; HealPRs --> CancelOrphanBuilds ; ```. ##### Try to Merge. ```mermaid; flowchart LR; IterateOverMergeable(Iterate over\nmergeable PRs in\nmerge priority order); IterateOverMergeable --> IsMergeable{is_mergeable?\n\n-No DO_NOT_MERGE flag\n-Reviewed\n-Checks passed\n-Up to date test run}; ; IsMergeable -->|yes| Merge(Merge the PR). Merge -->|failure| IterateOverMergeable; Merge -->|success| Return; ```; ,MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:227,Deployability,deploy,deployed,227,"# The CI Service. ## Documentation Scope. This documentation is trying to achieve two goals:. - How the CI service is structured and operates as a general purpose, customizable component; - How the CI service is configured and deployed by the Hail team:; - From the `hail-is/hail:main` branch of the hail repository; - To the `default` namespace of the Hail Batch cluster in GCP. The primary text of the document will focus on the first goal, documenting the general purpose component. When describing how CI is configured and deployed by the Hail team, this will be clearly annotated. ; Note that the Hail team's configuration is mostly stable, but should be treated with a grain of salt and validated ; against the actual configuration instead of being assumed to be true. ## Overview of the CI Service. The CI service has three functional purposes:. - Runs tests against pull requests which target designated 'watched branches'; - Merges PRs onto watched branches when they are ready; - Deploys services to the live infrastructure when the watched branch is updated. ## Structure and Operation. The CI service itself is deployed as a Kubernetes service in the Hail Batch cluster. See ; [architecture diagram](../Hail%20Batch%20Architectural%20Diagram.png). CI must be configured with an access token allowing it to operate on ; behalf of a github account . > [!NOTE] ; > This account is called `hail-ci-robot` in hail-is/hail. ### Watched Branches. One of the core concepts of the CI service is the ""watched branch"". A watched branch is a branch in a github repository; which the CI service is configured to monitor for changes. The CI service will run tests against PRs which target the; watched branch, will merge PRs against watched branches when they are ready, and will deploy services when the watched ; branch is updated. > [!NOTE] ; > In `hail-is/hail`, there is one watched branch: `hail-is/hail:main`. ### Tracked State. For each branch, CI will track three types of state which can be ma",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:527,Deployability,deploy,deployed,527,"# The CI Service. ## Documentation Scope. This documentation is trying to achieve two goals:. - How the CI service is structured and operates as a general purpose, customizable component; - How the CI service is configured and deployed by the Hail team:; - From the `hail-is/hail:main` branch of the hail repository; - To the `default` namespace of the Hail Batch cluster in GCP. The primary text of the document will focus on the first goal, documenting the general purpose component. When describing how CI is configured and deployed by the Hail team, this will be clearly annotated. ; Note that the Hail team's configuration is mostly stable, but should be treated with a grain of salt and validated ; against the actual configuration instead of being assumed to be true. ## Overview of the CI Service. The CI service has three functional purposes:. - Runs tests against pull requests which target designated 'watched branches'; - Merges PRs onto watched branches when they are ready; - Deploys services to the live infrastructure when the watched branch is updated. ## Structure and Operation. The CI service itself is deployed as a Kubernetes service in the Hail Batch cluster. See ; [architecture diagram](../Hail%20Batch%20Architectural%20Diagram.png). CI must be configured with an access token allowing it to operate on ; behalf of a github account . > [!NOTE] ; > This account is called `hail-ci-robot` in hail-is/hail. ### Watched Branches. One of the core concepts of the CI service is the ""watched branch"". A watched branch is a branch in a github repository; which the CI service is configured to monitor for changes. The CI service will run tests against PRs which target the; watched branch, will merge PRs against watched branches when they are ready, and will deploy services when the watched ; branch is updated. > [!NOTE] ; > In `hail-is/hail`, there is one watched branch: `hail-is/hail:main`. ### Tracked State. For each branch, CI will track three types of state which can be ma",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:614,Deployability,configurat,configuration,614,"# The CI Service. ## Documentation Scope. This documentation is trying to achieve two goals:. - How the CI service is structured and operates as a general purpose, customizable component; - How the CI service is configured and deployed by the Hail team:; - From the `hail-is/hail:main` branch of the hail repository; - To the `default` namespace of the Hail Batch cluster in GCP. The primary text of the document will focus on the first goal, documenting the general purpose component. When describing how CI is configured and deployed by the Hail team, this will be clearly annotated. ; Note that the Hail team's configuration is mostly stable, but should be treated with a grain of salt and validated ; against the actual configuration instead of being assumed to be true. ## Overview of the CI Service. The CI service has three functional purposes:. - Runs tests against pull requests which target designated 'watched branches'; - Merges PRs onto watched branches when they are ready; - Deploys services to the live infrastructure when the watched branch is updated. ## Structure and Operation. The CI service itself is deployed as a Kubernetes service in the Hail Batch cluster. See ; [architecture diagram](../Hail%20Batch%20Architectural%20Diagram.png). CI must be configured with an access token allowing it to operate on ; behalf of a github account . > [!NOTE] ; > This account is called `hail-ci-robot` in hail-is/hail. ### Watched Branches. One of the core concepts of the CI service is the ""watched branch"". A watched branch is a branch in a github repository; which the CI service is configured to monitor for changes. The CI service will run tests against PRs which target the; watched branch, will merge PRs against watched branches when they are ready, and will deploy services when the watched ; branch is updated. > [!NOTE] ; > In `hail-is/hail`, there is one watched branch: `hail-is/hail:main`. ### Tracked State. For each branch, CI will track three types of state which can be ma",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:724,Deployability,configurat,configuration,724,"# The CI Service. ## Documentation Scope. This documentation is trying to achieve two goals:. - How the CI service is structured and operates as a general purpose, customizable component; - How the CI service is configured and deployed by the Hail team:; - From the `hail-is/hail:main` branch of the hail repository; - To the `default` namespace of the Hail Batch cluster in GCP. The primary text of the document will focus on the first goal, documenting the general purpose component. When describing how CI is configured and deployed by the Hail team, this will be clearly annotated. ; Note that the Hail team's configuration is mostly stable, but should be treated with a grain of salt and validated ; against the actual configuration instead of being assumed to be true. ## Overview of the CI Service. The CI service has three functional purposes:. - Runs tests against pull requests which target designated 'watched branches'; - Merges PRs onto watched branches when they are ready; - Deploys services to the live infrastructure when the watched branch is updated. ## Structure and Operation. The CI service itself is deployed as a Kubernetes service in the Hail Batch cluster. See ; [architecture diagram](../Hail%20Batch%20Architectural%20Diagram.png). CI must be configured with an access token allowing it to operate on ; behalf of a github account . > [!NOTE] ; > This account is called `hail-ci-robot` in hail-is/hail. ### Watched Branches. One of the core concepts of the CI service is the ""watched branch"". A watched branch is a branch in a github repository; which the CI service is configured to monitor for changes. The CI service will run tests against PRs which target the; watched branch, will merge PRs against watched branches when they are ready, and will deploy services when the watched ; branch is updated. > [!NOTE] ; > In `hail-is/hail`, there is one watched branch: `hail-is/hail:main`. ### Tracked State. For each branch, CI will track three types of state which can be ma",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:1061,Deployability,update,updated,1061,"# The CI Service. ## Documentation Scope. This documentation is trying to achieve two goals:. - How the CI service is structured and operates as a general purpose, customizable component; - How the CI service is configured and deployed by the Hail team:; - From the `hail-is/hail:main` branch of the hail repository; - To the `default` namespace of the Hail Batch cluster in GCP. The primary text of the document will focus on the first goal, documenting the general purpose component. When describing how CI is configured and deployed by the Hail team, this will be clearly annotated. ; Note that the Hail team's configuration is mostly stable, but should be treated with a grain of salt and validated ; against the actual configuration instead of being assumed to be true. ## Overview of the CI Service. The CI service has three functional purposes:. - Runs tests against pull requests which target designated 'watched branches'; - Merges PRs onto watched branches when they are ready; - Deploys services to the live infrastructure when the watched branch is updated. ## Structure and Operation. The CI service itself is deployed as a Kubernetes service in the Hail Batch cluster. See ; [architecture diagram](../Hail%20Batch%20Architectural%20Diagram.png). CI must be configured with an access token allowing it to operate on ; behalf of a github account . > [!NOTE] ; > This account is called `hail-ci-robot` in hail-is/hail. ### Watched Branches. One of the core concepts of the CI service is the ""watched branch"". A watched branch is a branch in a github repository; which the CI service is configured to monitor for changes. The CI service will run tests against PRs which target the; watched branch, will merge PRs against watched branches when they are ready, and will deploy services when the watched ; branch is updated. > [!NOTE] ; > In `hail-is/hail`, there is one watched branch: `hail-is/hail:main`. ### Tracked State. For each branch, CI will track three types of state which can be ma",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:1123,Deployability,deploy,deployed,1123,"es as a general purpose, customizable component; - How the CI service is configured and deployed by the Hail team:; - From the `hail-is/hail:main` branch of the hail repository; - To the `default` namespace of the Hail Batch cluster in GCP. The primary text of the document will focus on the first goal, documenting the general purpose component. When describing how CI is configured and deployed by the Hail team, this will be clearly annotated. ; Note that the Hail team's configuration is mostly stable, but should be treated with a grain of salt and validated ; against the actual configuration instead of being assumed to be true. ## Overview of the CI Service. The CI service has three functional purposes:. - Runs tests against pull requests which target designated 'watched branches'; - Merges PRs onto watched branches when they are ready; - Deploys services to the live infrastructure when the watched branch is updated. ## Structure and Operation. The CI service itself is deployed as a Kubernetes service in the Hail Batch cluster. See ; [architecture diagram](../Hail%20Batch%20Architectural%20Diagram.png). CI must be configured with an access token allowing it to operate on ; behalf of a github account . > [!NOTE] ; > This account is called `hail-ci-robot` in hail-is/hail. ### Watched Branches. One of the core concepts of the CI service is the ""watched branch"". A watched branch is a branch in a github repository; which the CI service is configured to monitor for changes. The CI service will run tests against PRs which target the; watched branch, will merge PRs against watched branches when they are ready, and will deploy services when the watched ; branch is updated. > [!NOTE] ; > In `hail-is/hail`, there is one watched branch: `hail-is/hail:main`. ### Tracked State. For each branch, CI will track three types of state which can be marked as stale using appropriate flags:; - The state of github branches, commits, and PRs (""`github_changed`""); - The state of batches CI is",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:1778,Deployability,deploy,deploy,1778,"ration instead of being assumed to be true. ## Overview of the CI Service. The CI service has three functional purposes:. - Runs tests against pull requests which target designated 'watched branches'; - Merges PRs onto watched branches when they are ready; - Deploys services to the live infrastructure when the watched branch is updated. ## Structure and Operation. The CI service itself is deployed as a Kubernetes service in the Hail Batch cluster. See ; [architecture diagram](../Hail%20Batch%20Architectural%20Diagram.png). CI must be configured with an access token allowing it to operate on ; behalf of a github account . > [!NOTE] ; > This account is called `hail-ci-robot` in hail-is/hail. ### Watched Branches. One of the core concepts of the CI service is the ""watched branch"". A watched branch is a branch in a github repository; which the CI service is configured to monitor for changes. The CI service will run tests against PRs which target the; watched branch, will merge PRs against watched branches when they are ready, and will deploy services when the watched ; branch is updated. > [!NOTE] ; > In `hail-is/hail`, there is one watched branch: `hail-is/hail:main`. ### Tracked State. For each branch, CI will track three types of state which can be marked as stale using appropriate flags:; - The state of github branches, commits, and PRs (""`github_changed`""); - The state of batches CI is running to run tests or deploy to production (""`batch_changed`""); - Whether CI needs to act externally, to update Github or start new batches (""`state_changed`""). ### CI Update Loop. The core CI update loop iterates over its set of ""watched branches"". If any of the state flags for any of the watched; branches are marked as dirty, then the CI service will re-evaluate the state of the watched branch and potentially take; action to update either its own internal state or the state of github or the deployed system. A set of detailed flowcharts detailing the CI update loop is given in the",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:1823,Deployability,update,updated,1823,"ration instead of being assumed to be true. ## Overview of the CI Service. The CI service has three functional purposes:. - Runs tests against pull requests which target designated 'watched branches'; - Merges PRs onto watched branches when they are ready; - Deploys services to the live infrastructure when the watched branch is updated. ## Structure and Operation. The CI service itself is deployed as a Kubernetes service in the Hail Batch cluster. See ; [architecture diagram](../Hail%20Batch%20Architectural%20Diagram.png). CI must be configured with an access token allowing it to operate on ; behalf of a github account . > [!NOTE] ; > This account is called `hail-ci-robot` in hail-is/hail. ### Watched Branches. One of the core concepts of the CI service is the ""watched branch"". A watched branch is a branch in a github repository; which the CI service is configured to monitor for changes. The CI service will run tests against PRs which target the; watched branch, will merge PRs against watched branches when they are ready, and will deploy services when the watched ; branch is updated. > [!NOTE] ; > In `hail-is/hail`, there is one watched branch: `hail-is/hail:main`. ### Tracked State. For each branch, CI will track three types of state which can be marked as stale using appropriate flags:; - The state of github branches, commits, and PRs (""`github_changed`""); - The state of batches CI is running to run tests or deploy to production (""`batch_changed`""); - Whether CI needs to act externally, to update Github or start new batches (""`state_changed`""). ### CI Update Loop. The core CI update loop iterates over its set of ""watched branches"". If any of the state flags for any of the watched; branches are marked as dirty, then the CI service will re-evaluate the state of the watched branch and potentially take; action to update either its own internal state or the state of github or the deployed system. A set of detailed flowcharts detailing the CI update loop is given in the",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:2165,Deployability,deploy,deploy,2165,"f is deployed as a Kubernetes service in the Hail Batch cluster. See ; [architecture diagram](../Hail%20Batch%20Architectural%20Diagram.png). CI must be configured with an access token allowing it to operate on ; behalf of a github account . > [!NOTE] ; > This account is called `hail-ci-robot` in hail-is/hail. ### Watched Branches. One of the core concepts of the CI service is the ""watched branch"". A watched branch is a branch in a github repository; which the CI service is configured to monitor for changes. The CI service will run tests against PRs which target the; watched branch, will merge PRs against watched branches when they are ready, and will deploy services when the watched ; branch is updated. > [!NOTE] ; > In `hail-is/hail`, there is one watched branch: `hail-is/hail:main`. ### Tracked State. For each branch, CI will track three types of state which can be marked as stale using appropriate flags:; - The state of github branches, commits, and PRs (""`github_changed`""); - The state of batches CI is running to run tests or deploy to production (""`batch_changed`""); - Whether CI needs to act externally, to update Github or start new batches (""`state_changed`""). ### CI Update Loop. The core CI update loop iterates over its set of ""watched branches"". If any of the state flags for any of the watched; branches are marked as dirty, then the CI service will re-evaluate the state of the watched branch and potentially take; action to update either its own internal state or the state of github or the deployed system. A set of detailed flowcharts detailing the CI update loop is given in the [references](#references) section below, which; can be cross-referenced against the purposes and outcomes of the CI service as described in the following text sections. #### Detecting Stale State. ##### Regular Polling. At a configurable frequency, the CI service will mark **_all_** flags for **_all_** watched branches as dirty, which; will trigger a re-evaluation of the system state",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:2248,Deployability,update,update,2248,"f is deployed as a Kubernetes service in the Hail Batch cluster. See ; [architecture diagram](../Hail%20Batch%20Architectural%20Diagram.png). CI must be configured with an access token allowing it to operate on ; behalf of a github account . > [!NOTE] ; > This account is called `hail-ci-robot` in hail-is/hail. ### Watched Branches. One of the core concepts of the CI service is the ""watched branch"". A watched branch is a branch in a github repository; which the CI service is configured to monitor for changes. The CI service will run tests against PRs which target the; watched branch, will merge PRs against watched branches when they are ready, and will deploy services when the watched ; branch is updated. > [!NOTE] ; > In `hail-is/hail`, there is one watched branch: `hail-is/hail:main`. ### Tracked State. For each branch, CI will track three types of state which can be marked as stale using appropriate flags:; - The state of github branches, commits, and PRs (""`github_changed`""); - The state of batches CI is running to run tests or deploy to production (""`batch_changed`""); - Whether CI needs to act externally, to update Github or start new batches (""`state_changed`""). ### CI Update Loop. The core CI update loop iterates over its set of ""watched branches"". If any of the state flags for any of the watched; branches are marked as dirty, then the CI service will re-evaluate the state of the watched branch and potentially take; action to update either its own internal state or the state of github or the deployed system. A set of detailed flowcharts detailing the CI update loop is given in the [references](#references) section below, which; can be cross-referenced against the purposes and outcomes of the CI service as described in the following text sections. #### Detecting Stale State. ##### Regular Polling. At a configurable frequency, the CI service will mark **_all_** flags for **_all_** watched branches as dirty, which; will trigger a re-evaluation of the system state",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:2336,Deployability,update,update,2336," . > [!NOTE] ; > This account is called `hail-ci-robot` in hail-is/hail. ### Watched Branches. One of the core concepts of the CI service is the ""watched branch"". A watched branch is a branch in a github repository; which the CI service is configured to monitor for changes. The CI service will run tests against PRs which target the; watched branch, will merge PRs against watched branches when they are ready, and will deploy services when the watched ; branch is updated. > [!NOTE] ; > In `hail-is/hail`, there is one watched branch: `hail-is/hail:main`. ### Tracked State. For each branch, CI will track three types of state which can be marked as stale using appropriate flags:; - The state of github branches, commits, and PRs (""`github_changed`""); - The state of batches CI is running to run tests or deploy to production (""`batch_changed`""); - Whether CI needs to act externally, to update Github or start new batches (""`state_changed`""). ### CI Update Loop. The core CI update loop iterates over its set of ""watched branches"". If any of the state flags for any of the watched; branches are marked as dirty, then the CI service will re-evaluate the state of the watched branch and potentially take; action to update either its own internal state or the state of github or the deployed system. A set of detailed flowcharts detailing the CI update loop is given in the [references](#references) section below, which; can be cross-referenced against the purposes and outcomes of the CI service as described in the following text sections. #### Detecting Stale State. ##### Regular Polling. At a configurable frequency, the CI service will mark **_all_** flags for **_all_** watched branches as dirty, which; will trigger a re-evaluation of the system state. ##### Github Webhooks. To make CI more responsive it also has endpoints to receive webhook event triggers from the github repository. This endpoint; is part of the CI API, listening on `/github_callback`. Depending on the type of webhook ",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:2574,Deployability,update,update,2574,"tched branch is a branch in a github repository; which the CI service is configured to monitor for changes. The CI service will run tests against PRs which target the; watched branch, will merge PRs against watched branches when they are ready, and will deploy services when the watched ; branch is updated. > [!NOTE] ; > In `hail-is/hail`, there is one watched branch: `hail-is/hail:main`. ### Tracked State. For each branch, CI will track three types of state which can be marked as stale using appropriate flags:; - The state of github branches, commits, and PRs (""`github_changed`""); - The state of batches CI is running to run tests or deploy to production (""`batch_changed`""); - Whether CI needs to act externally, to update Github or start new batches (""`state_changed`""). ### CI Update Loop. The core CI update loop iterates over its set of ""watched branches"". If any of the state flags for any of the watched; branches are marked as dirty, then the CI service will re-evaluate the state of the watched branch and potentially take; action to update either its own internal state or the state of github or the deployed system. A set of detailed flowcharts detailing the CI update loop is given in the [references](#references) section below, which; can be cross-referenced against the purposes and outcomes of the CI service as described in the following text sections. #### Detecting Stale State. ##### Regular Polling. At a configurable frequency, the CI service will mark **_all_** flags for **_all_** watched branches as dirty, which; will trigger a re-evaluation of the system state. ##### Github Webhooks. To make CI more responsive it also has endpoints to receive webhook event triggers from the github repository. This endpoint; is part of the CI API, listening on `/github_callback`. Depending on the type of webhook received, and the branch or PR which it is targeting, the CI service will; potentially dirty one or more of its watched branches' state flags, meaning that CI will pri",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:2641,Deployability,deploy,deployed,2641,"tched branch is a branch in a github repository; which the CI service is configured to monitor for changes. The CI service will run tests against PRs which target the; watched branch, will merge PRs against watched branches when they are ready, and will deploy services when the watched ; branch is updated. > [!NOTE] ; > In `hail-is/hail`, there is one watched branch: `hail-is/hail:main`. ### Tracked State. For each branch, CI will track three types of state which can be marked as stale using appropriate flags:; - The state of github branches, commits, and PRs (""`github_changed`""); - The state of batches CI is running to run tests or deploy to production (""`batch_changed`""); - Whether CI needs to act externally, to update Github or start new batches (""`state_changed`""). ### CI Update Loop. The core CI update loop iterates over its set of ""watched branches"". If any of the state flags for any of the watched; branches are marked as dirty, then the CI service will re-evaluate the state of the watched branch and potentially take; action to update either its own internal state or the state of github or the deployed system. A set of detailed flowcharts detailing the CI update loop is given in the [references](#references) section below, which; can be cross-referenced against the purposes and outcomes of the CI service as described in the following text sections. #### Detecting Stale State. ##### Regular Polling. At a configurable frequency, the CI service will mark **_all_** flags for **_all_** watched branches as dirty, which; will trigger a re-evaluation of the system state. ##### Github Webhooks. To make CI more responsive it also has endpoints to receive webhook event triggers from the github repository. This endpoint; is part of the CI API, listening on `/github_callback`. Depending on the type of webhook received, and the branch or PR which it is targeting, the CI service will; potentially dirty one or more of its watched branches' state flags, meaning that CI will pri",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:2704,Deployability,update,update,2704,"deploy services when the watched ; branch is updated. > [!NOTE] ; > In `hail-is/hail`, there is one watched branch: `hail-is/hail:main`. ### Tracked State. For each branch, CI will track three types of state which can be marked as stale using appropriate flags:; - The state of github branches, commits, and PRs (""`github_changed`""); - The state of batches CI is running to run tests or deploy to production (""`batch_changed`""); - Whether CI needs to act externally, to update Github or start new batches (""`state_changed`""). ### CI Update Loop. The core CI update loop iterates over its set of ""watched branches"". If any of the state flags for any of the watched; branches are marked as dirty, then the CI service will re-evaluate the state of the watched branch and potentially take; action to update either its own internal state or the state of github or the deployed system. A set of detailed flowcharts detailing the CI update loop is given in the [references](#references) section below, which; can be cross-referenced against the purposes and outcomes of the CI service as described in the following text sections. #### Detecting Stale State. ##### Regular Polling. At a configurable frequency, the CI service will mark **_all_** flags for **_all_** watched branches as dirty, which; will trigger a re-evaluation of the system state. ##### Github Webhooks. To make CI more responsive it also has endpoints to receive webhook event triggers from the github repository. This endpoint; is part of the CI API, listening on `/github_callback`. Depending on the type of webhook received, and the branch or PR which it is targeting, the CI service will; potentially dirty one or more of its watched branches' state flags, meaning that CI will prioritize re-evaluating; the state of the branch in question. The webhooks themselves are configured manually within the github repository itself and not managed by terraform or deploy scripts. > [!NOTE] ; > - For `hail-is/hail`, the webhook target is: `ci",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:3701,Deployability,deploy,deploy,3701,"system. A set of detailed flowcharts detailing the CI update loop is given in the [references](#references) section below, which; can be cross-referenced against the purposes and outcomes of the CI service as described in the following text sections. #### Detecting Stale State. ##### Regular Polling. At a configurable frequency, the CI service will mark **_all_** flags for **_all_** watched branches as dirty, which; will trigger a re-evaluation of the system state. ##### Github Webhooks. To make CI more responsive it also has endpoints to receive webhook event triggers from the github repository. This endpoint; is part of the CI API, listening on `/github_callback`. Depending on the type of webhook received, and the branch or PR which it is targeting, the CI service will; potentially dirty one or more of its watched branches' state flags, meaning that CI will prioritize re-evaluating; the state of the branch in question. The webhooks themselves are configured manually within the github repository itself and not managed by terraform or deploy scripts. > [!NOTE] ; > - For `hail-is/hail`, the webhook target is: `ci.hail.is/github_callback`; > - For `hail-is/hail`, webhook callbacks are configured to happen on changes to the following:; > - Pull request reviews; > - Pull requests; > - Pushes. ## Running tests against pull requests. When a PR which targets a watched branch is created, updated or closed, the `github_changed` flag will be marked as dirty. . During its update loop, CI will potentially decide to run tests against all PRs targetting a watched branch:. - Authorization; - If the author is in the [`ci/ci/constants`](../../../ci/ci/constants.py) list of known developers, it can be tested.; - If the github SHA for the commit has been registered, it can be tested.; - Otherwise, it will be ignored.; - Check for validity; - The PR must be against a watched branch, or else it wouldn't be considered.; - If the current commit has already had tests run against it, no fur",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:4053,Deployability,update,updated,4053,"y, which; will trigger a re-evaluation of the system state. ##### Github Webhooks. To make CI more responsive it also has endpoints to receive webhook event triggers from the github repository. This endpoint; is part of the CI API, listening on `/github_callback`. Depending on the type of webhook received, and the branch or PR which it is targeting, the CI service will; potentially dirty one or more of its watched branches' state flags, meaning that CI will prioritize re-evaluating; the state of the branch in question. The webhooks themselves are configured manually within the github repository itself and not managed by terraform or deploy scripts. > [!NOTE] ; > - For `hail-is/hail`, the webhook target is: `ci.hail.is/github_callback`; > - For `hail-is/hail`, webhook callbacks are configured to happen on changes to the following:; > - Pull request reviews; > - Pull requests; > - Pushes. ## Running tests against pull requests. When a PR which targets a watched branch is created, updated or closed, the `github_changed` flag will be marked as dirty. . During its update loop, CI will potentially decide to run tests against all PRs targetting a watched branch:. - Authorization; - If the author is in the [`ci/ci/constants`](../../../ci/ci/constants.py) list of known developers, it can be tested.; - If the github SHA for the commit has been registered, it can be tested.; - Otherwise, it will be ignored.; - Check for validity; - The PR must be against a watched branch, or else it wouldn't be considered.; - If the current commit has already had tests run against it, no further action is taken `*`; - If the PR is marked with the ""do_not_test"" label, no further action is taken; - Run tests; - The CI service will run tests against the PR (see below) ; - Report results back to Github using the Checks interface. `*` Note: there is an important nuance to the statement ""If the current commit has already had tests run against it, no further action is taken"". - Every time CI loops thr",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:4136,Deployability,update,update,4136,"as endpoints to receive webhook event triggers from the github repository. This endpoint; is part of the CI API, listening on `/github_callback`. Depending on the type of webhook received, and the branch or PR which it is targeting, the CI service will; potentially dirty one or more of its watched branches' state flags, meaning that CI will prioritize re-evaluating; the state of the branch in question. The webhooks themselves are configured manually within the github repository itself and not managed by terraform or deploy scripts. > [!NOTE] ; > - For `hail-is/hail`, the webhook target is: `ci.hail.is/github_callback`; > - For `hail-is/hail`, webhook callbacks are configured to happen on changes to the following:; > - Pull request reviews; > - Pull requests; > - Pushes. ## Running tests against pull requests. When a PR which targets a watched branch is created, updated or closed, the `github_changed` flag will be marked as dirty. . During its update loop, CI will potentially decide to run tests against all PRs targetting a watched branch:. - Authorization; - If the author is in the [`ci/ci/constants`](../../../ci/ci/constants.py) list of known developers, it can be tested.; - If the github SHA for the commit has been registered, it can be tested.; - Otherwise, it will be ignored.; - Check for validity; - The PR must be against a watched branch, or else it wouldn't be considered.; - If the current commit has already had tests run against it, no further action is taken `*`; - If the PR is marked with the ""do_not_test"" label, no further action is taken; - Run tests; - The CI service will run tests against the PR (see below) ; - Report results back to Github using the Checks interface. `*` Note: there is an important nuance to the statement ""If the current commit has already had tests run against it, no further action is taken"". - Every time CI loops through its update cycle, it will determine a ""merge candidate"" from all of the PRs which are approved.; - If the batch t",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:5070,Deployability,update,update,5070,"will be marked as dirty. . During its update loop, CI will potentially decide to run tests against all PRs targetting a watched branch:. - Authorization; - If the author is in the [`ci/ci/constants`](../../../ci/ci/constants.py) list of known developers, it can be tested.; - If the github SHA for the commit has been registered, it can be tested.; - Otherwise, it will be ignored.; - Check for validity; - The PR must be against a watched branch, or else it wouldn't be considered.; - If the current commit has already had tests run against it, no further action is taken `*`; - If the PR is marked with the ""do_not_test"" label, no further action is taken; - Run tests; - The CI service will run tests against the PR (see below) ; - Report results back to Github using the Checks interface. `*` Note: there is an important nuance to the statement ""If the current commit has already had tests run against it, no further action is taken"". - Every time CI loops through its update cycle, it will determine a ""merge candidate"" from all of the PRs which are approved.; - If the batch tests for the merge candidate PR were run against a different SHA than the current SHA of the watched branch,; then CI trigger a new test batch. . ### Running Tests. The process of running tests goes like:. - CI will create a pending Check against the PR in question; - CI will generate a new batch and submit it against the production Hail Batch service using its own service credentials; - Tasks in the batch are defined in `build.yaml` in the top repo directory. CI generates jobs in the batch from the steps defined in there.; - The batch contains jobs which will:; - Clone the appropriate branch; - Squash and rebase onto `main`; - This is done in a way that means the resulting SHA will match the SHA which would be produced when the PR merges.; - Build new versions of all hail tools, packages and libraries; - Build a new set of docker images for hail query and the updated services; - Deploy the batch suite of ",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:6052,Deployability,update,updated,6052," of the PRs which are approved.; - If the batch tests for the merge candidate PR were run against a different SHA than the current SHA of the watched branch,; then CI trigger a new test batch. . ### Running Tests. The process of running tests goes like:. - CI will create a pending Check against the PR in question; - CI will generate a new batch and submit it against the production Hail Batch service using its own service credentials; - Tasks in the batch are defined in `build.yaml` in the top repo directory. CI generates jobs in the batch from the steps defined in there.; - The batch contains jobs which will:; - Clone the appropriate branch; - Squash and rebase onto `main`; - This is done in a way that means the resulting SHA will match the SHA which would be produced when the PR merges.; - Build new versions of all hail tools, packages and libraries; - Build a new set of docker images for hail query and the updated services; - Deploy the batch suite of k8s services into one of many CI-owned namespaces in the Hail Batch cluster; - These namespaces are named like `""pr-<PR_NUMBER>-<CI-NAMESPACE>-<RANDOM>""`; - Where `CI-NAMESPACE` is the namespace where CI is running (usually `default`). ; - Run a series of tests against the services; - Each test:; - Submits a specially crafted batch to the newly deployed batch namespace; - Checks that the results of running the batch are what we would expect; - The CI service polls the batch which it submits for overall success or failure.; - Once the batch has either succeeded or failed, CI uses that result to report status back to GitHub. Examples of CI test runs can be seen by searching through the production batch log, as long as you are a member; of the `ci` billing project. . > [!NOTE]; > CI test runs in hail.is can be seen [here](https://batch.hail.is/batches?q=user+%3D+ci%0D%0Atest+%3D+1).; > Note:; > - You will need to be a member of the `ci` billing project to view these logs.; > - Due to the numbers of batches involved, this",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:6445,Deployability,deploy,deployed,6445,"d submit it against the production Hail Batch service using its own service credentials; - Tasks in the batch are defined in `build.yaml` in the top repo directory. CI generates jobs in the batch from the steps defined in there.; - The batch contains jobs which will:; - Clone the appropriate branch; - Squash and rebase onto `main`; - This is done in a way that means the resulting SHA will match the SHA which would be produced when the PR merges.; - Build new versions of all hail tools, packages and libraries; - Build a new set of docker images for hail query and the updated services; - Deploy the batch suite of k8s services into one of many CI-owned namespaces in the Hail Batch cluster; - These namespaces are named like `""pr-<PR_NUMBER>-<CI-NAMESPACE>-<RANDOM>""`; - Where `CI-NAMESPACE` is the namespace where CI is running (usually `default`). ; - Run a series of tests against the services; - Each test:; - Submits a specially crafted batch to the newly deployed batch namespace; - Checks that the results of running the batch are what we would expect; - The CI service polls the batch which it submits for overall success or failure.; - Once the batch has either succeeded or failed, CI uses that result to report status back to GitHub. Examples of CI test runs can be seen by searching through the production batch log, as long as you are a member; of the `ci` billing project. . > [!NOTE]; > CI test runs in hail.is can be seen [here](https://batch.hail.is/batches?q=user+%3D+ci%0D%0Atest+%3D+1).; > Note:; > - You will need to be a member of the `ci` billing project to view these logs.; > - Due to the numbers of batches involved, this page may take a while to load . #### CI Testing Timeline. The image below shows the CI testing timeline:. ```mermaid; sequenceDiagram; participant Github; box Kubernetes (PROD: 'default' namespace); participant CI as PROD CI Service (default namespace); participant PB as PROD Batch (default namespace); end; box GCE (PROD VMs); participant PVM as",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:7677,Deployability,update,updated,7677,"or failed, CI uses that result to report status back to GitHub. Examples of CI test runs can be seen by searching through the production batch log, as long as you are a member; of the `ci` billing project. . > [!NOTE]; > CI test runs in hail.is can be seen [here](https://batch.hail.is/batches?q=user+%3D+ci%0D%0Atest+%3D+1).; > Note:; > - You will need to be a member of the `ci` billing project to view these logs.; > - Due to the numbers of batches involved, this page may take a while to load . #### CI Testing Timeline. The image below shows the CI testing timeline:. ```mermaid; sequenceDiagram; participant Github; box Kubernetes (PROD: 'default' namespace); participant CI as PROD CI Service (default namespace); participant PB as PROD Batch (default namespace); end; box GCE (PROD VMs); participant PVM as PROD VMs; end; box Kubernetes (CI: 'test-xyz' namespaces); participant CIB as CI Batch (test-xyz namespaces); end; box GCE (CI VMs); participant CIVM as CI VMs; end. Github->>CI: Notify PR created/updated. CI->>Github: Register github check (pending); CI->>PB: Submit CI test batch; activate PB; PB->>PVM: Submit build jobs; activate PVM; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit deploy jobs; activate PVM; PVM->>CIB: Deploy batch service; activate CIB; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit test jobs; activate PVM; PVM->>CIB: Submit test batches; CIB->>CIVM: Submit test batch jobs; activate CIVM; CIVM-->>CIB: Validate results; deactivate CIVM; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit cleanup jobs; activate PVM; PVM->>CIB: Clean up service; deactivate CIB; PVM-->>PB: Done; deactivate PVM; PB-->>CI: Detects completion; CI->>Github: Update github check; ```. ## Merging PRs. Mergability is determined by:. - All required checks must be successful; - There must have been a CI-generated test batch; - The most recent test batch must have run against the current SHA of the watched branch ; - The PR must be approved; - The PR must not have the ""DO_NOT",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:7869,Deployability,deploy,deploy,7869,"mber of the `ci` billing project to view these logs.; > - Due to the numbers of batches involved, this page may take a while to load . #### CI Testing Timeline. The image below shows the CI testing timeline:. ```mermaid; sequenceDiagram; participant Github; box Kubernetes (PROD: 'default' namespace); participant CI as PROD CI Service (default namespace); participant PB as PROD Batch (default namespace); end; box GCE (PROD VMs); participant PVM as PROD VMs; end; box Kubernetes (CI: 'test-xyz' namespaces); participant CIB as CI Batch (test-xyz namespaces); end; box GCE (CI VMs); participant CIVM as CI VMs; end. Github->>CI: Notify PR created/updated. CI->>Github: Register github check (pending); CI->>PB: Submit CI test batch; activate PB; PB->>PVM: Submit build jobs; activate PVM; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit deploy jobs; activate PVM; PVM->>CIB: Deploy batch service; activate CIB; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit test jobs; activate PVM; PVM->>CIB: Submit test batches; CIB->>CIVM: Submit test batch jobs; activate CIVM; CIVM-->>CIB: Validate results; deactivate CIVM; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit cleanup jobs; activate PVM; PVM->>CIB: Clean up service; deactivate CIB; PVM-->>PB: Done; deactivate PVM; PB-->>CI: Detects completion; CI->>Github: Update github check; ```. ## Merging PRs. Mergability is determined by:. - All required checks must be successful; - There must have been a CI-generated test batch; - The most recent test batch must have run against the current SHA of the watched branch ; - The PR must be approved; - The PR must not have the ""DO_NOT_MERGE"" label. The control flow from final approval to CI merging a PRs looks like:. - The PR's state will change in github (maybe a check changes to SUCCESS, or a review is approved); - The github webhook callback will cause the `github_changed` flag to be marked as dirty for the target `WatchedBranch`; - CI will detect that this PR is now considered mergeable ",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:9413,Deployability,update,update,9413,"There must have been a CI-generated test batch; - The most recent test batch must have run against the current SHA of the watched branch ; - The PR must be approved; - The PR must not have the ""DO_NOT_MERGE"" label. The control flow from final approval to CI merging a PRs looks like:. - The PR's state will change in github (maybe a check changes to SUCCESS, or a review is approved); - The github webhook callback will cause the `github_changed` flag to be marked as dirty for the target `WatchedBranch`; - CI will detect that this PR is now considered mergeable ; - CI will attempt to merge all PRs which are mergeable, in priority order; - Note: as soon as one PR merges, the remaining branches will no longer be mergeable because the target SHA will have changed. ## Deploying services to the live infrastructure. When a PR is merged into the `main` branch, a webhook will trigger. The CI service will set its `github_changed` flag. During its update loop, the CI service will determine that the SHA of the `WatchedBranch` has changed and trigger; a deployment. Like in the test case, the CI service uses targets in `build.yaml` to generate a set of jobs in a; batch run: . - Create a new batch job to:; - Build various components and service images; - Deploy to the `default` (ie prod) namespace; - Run various post-deployment tests; - A handful of final actions; - eg rolling out artifact registry cleanup policies, amongst many other things. Note: It's not actually quite as waterfall-y as this. In fact the jobs are all running in a hail; batch, and each package being built and service being deployed has its own path through the DAG. So it's quite possible; that services are deploy/test-ing in parallel, and that the deploy and test jobs for one service might all complete before ; the deploy and test for another have even started. This should all be fine. Because we only merge in the first place if the PR in question has already passed; tests against the current SHA of the watched bran",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:9519,Deployability,deploy,deployment,9519,"There must have been a CI-generated test batch; - The most recent test batch must have run against the current SHA of the watched branch ; - The PR must be approved; - The PR must not have the ""DO_NOT_MERGE"" label. The control flow from final approval to CI merging a PRs looks like:. - The PR's state will change in github (maybe a check changes to SUCCESS, or a review is approved); - The github webhook callback will cause the `github_changed` flag to be marked as dirty for the target `WatchedBranch`; - CI will detect that this PR is now considered mergeable ; - CI will attempt to merge all PRs which are mergeable, in priority order; - Note: as soon as one PR merges, the remaining branches will no longer be mergeable because the target SHA will have changed. ## Deploying services to the live infrastructure. When a PR is merged into the `main` branch, a webhook will trigger. The CI service will set its `github_changed` flag. During its update loop, the CI service will determine that the SHA of the `WatchedBranch` has changed and trigger; a deployment. Like in the test case, the CI service uses targets in `build.yaml` to generate a set of jobs in a; batch run: . - Create a new batch job to:; - Build various components and service images; - Deploy to the `default` (ie prod) namespace; - Run various post-deployment tests; - A handful of final actions; - eg rolling out artifact registry cleanup policies, amongst many other things. Note: It's not actually quite as waterfall-y as this. In fact the jobs are all running in a hail; batch, and each package being built and service being deployed has its own path through the DAG. So it's quite possible; that services are deploy/test-ing in parallel, and that the deploy and test jobs for one service might all complete before ; the deploy and test for another have even started. This should all be fine. Because we only merge in the first place if the PR in question has already passed; tests against the current SHA of the watched bran",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:9786,Deployability,deploy,deployment,9786,"e in github (maybe a check changes to SUCCESS, or a review is approved); - The github webhook callback will cause the `github_changed` flag to be marked as dirty for the target `WatchedBranch`; - CI will detect that this PR is now considered mergeable ; - CI will attempt to merge all PRs which are mergeable, in priority order; - Note: as soon as one PR merges, the remaining branches will no longer be mergeable because the target SHA will have changed. ## Deploying services to the live infrastructure. When a PR is merged into the `main` branch, a webhook will trigger. The CI service will set its `github_changed` flag. During its update loop, the CI service will determine that the SHA of the `WatchedBranch` has changed and trigger; a deployment. Like in the test case, the CI service uses targets in `build.yaml` to generate a set of jobs in a; batch run: . - Create a new batch job to:; - Build various components and service images; - Deploy to the `default` (ie prod) namespace; - Run various post-deployment tests; - A handful of final actions; - eg rolling out artifact registry cleanup policies, amongst many other things. Note: It's not actually quite as waterfall-y as this. In fact the jobs are all running in a hail; batch, and each package being built and service being deployed has its own path through the DAG. So it's quite possible; that services are deploy/test-ing in parallel, and that the deploy and test jobs for one service might all complete before ; the deploy and test for another have even started. This should all be fine. Because we only merge in the first place if the PR in question has already passed; tests against the current SHA of the watched branch. > [!NOTE]; > CI deploy runs in hail.is can be seen [here](https://batch.hail.is/batches?q=user+%3D+ci%0D%0Adeploy+%3D+1).; > Note:; > - You will need to be a member of the `ci` billing project to view these logs.; > - Due to the numbers of batches involved, this page may take a while to load . #### CI Deplo",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:9839,Deployability,rolling,rolling,9839,"e in github (maybe a check changes to SUCCESS, or a review is approved); - The github webhook callback will cause the `github_changed` flag to be marked as dirty for the target `WatchedBranch`; - CI will detect that this PR is now considered mergeable ; - CI will attempt to merge all PRs which are mergeable, in priority order; - Note: as soon as one PR merges, the remaining branches will no longer be mergeable because the target SHA will have changed. ## Deploying services to the live infrastructure. When a PR is merged into the `main` branch, a webhook will trigger. The CI service will set its `github_changed` flag. During its update loop, the CI service will determine that the SHA of the `WatchedBranch` has changed and trigger; a deployment. Like in the test case, the CI service uses targets in `build.yaml` to generate a set of jobs in a; batch run: . - Create a new batch job to:; - Build various components and service images; - Deploy to the `default` (ie prod) namespace; - Run various post-deployment tests; - A handful of final actions; - eg rolling out artifact registry cleanup policies, amongst many other things. Note: It's not actually quite as waterfall-y as this. In fact the jobs are all running in a hail; batch, and each package being built and service being deployed has its own path through the DAG. So it's quite possible; that services are deploy/test-ing in parallel, and that the deploy and test jobs for one service might all complete before ; the deploy and test for another have even started. This should all be fine. Because we only merge in the first place if the PR in question has already passed; tests against the current SHA of the watched branch. > [!NOTE]; > CI deploy runs in hail.is can be seen [here](https://batch.hail.is/batches?q=user+%3D+ci%0D%0Adeploy+%3D+1).; > Note:; > - You will need to be a member of the `ci` billing project to view these logs.; > - Due to the numbers of batches involved, this page may take a while to load . #### CI Deplo",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:10066,Deployability,deploy,deployed,10066,"ill attempt to merge all PRs which are mergeable, in priority order; - Note: as soon as one PR merges, the remaining branches will no longer be mergeable because the target SHA will have changed. ## Deploying services to the live infrastructure. When a PR is merged into the `main` branch, a webhook will trigger. The CI service will set its `github_changed` flag. During its update loop, the CI service will determine that the SHA of the `WatchedBranch` has changed and trigger; a deployment. Like in the test case, the CI service uses targets in `build.yaml` to generate a set of jobs in a; batch run: . - Create a new batch job to:; - Build various components and service images; - Deploy to the `default` (ie prod) namespace; - Run various post-deployment tests; - A handful of final actions; - eg rolling out artifact registry cleanup policies, amongst many other things. Note: It's not actually quite as waterfall-y as this. In fact the jobs are all running in a hail; batch, and each package being built and service being deployed has its own path through the DAG. So it's quite possible; that services are deploy/test-ing in parallel, and that the deploy and test jobs for one service might all complete before ; the deploy and test for another have even started. This should all be fine. Because we only merge in the first place if the PR in question has already passed; tests against the current SHA of the watched branch. > [!NOTE]; > CI deploy runs in hail.is can be seen [here](https://batch.hail.is/batches?q=user+%3D+ci%0D%0Adeploy+%3D+1).; > Note:; > - You will need to be a member of the `ci` billing project to view these logs.; > - Due to the numbers of batches involved, this page may take a while to load . #### CI Deploy Timeline. The image below shows the CI deployment timeline:. ```mermaid; sequenceDiagram; participant Github; box Kubernetes (PROD: 'default' namespace); participant CI as PROD CI Service (default namespace); participant PB as PROD Batch (default namespace);",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:10151,Deployability,deploy,deploy,10151,"t SHA will have changed. ## Deploying services to the live infrastructure. When a PR is merged into the `main` branch, a webhook will trigger. The CI service will set its `github_changed` flag. During its update loop, the CI service will determine that the SHA of the `WatchedBranch` has changed and trigger; a deployment. Like in the test case, the CI service uses targets in `build.yaml` to generate a set of jobs in a; batch run: . - Create a new batch job to:; - Build various components and service images; - Deploy to the `default` (ie prod) namespace; - Run various post-deployment tests; - A handful of final actions; - eg rolling out artifact registry cleanup policies, amongst many other things. Note: It's not actually quite as waterfall-y as this. In fact the jobs are all running in a hail; batch, and each package being built and service being deployed has its own path through the DAG. So it's quite possible; that services are deploy/test-ing in parallel, and that the deploy and test jobs for one service might all complete before ; the deploy and test for another have even started. This should all be fine. Because we only merge in the first place if the PR in question has already passed; tests against the current SHA of the watched branch. > [!NOTE]; > CI deploy runs in hail.is can be seen [here](https://batch.hail.is/batches?q=user+%3D+ci%0D%0Adeploy+%3D+1).; > Note:; > - You will need to be a member of the `ci` billing project to view these logs.; > - Due to the numbers of batches involved, this page may take a while to load . #### CI Deploy Timeline. The image below shows the CI deployment timeline:. ```mermaid; sequenceDiagram; participant Github; box Kubernetes (PROD: 'default' namespace); participant CI as PROD CI Service (default namespace); participant PB as PROD Batch (default namespace); end; box GCE (PROD VMs); participant PVM as PROD VMs; end. Github->>CI: Notify 'main' branch updated; CI->>PB: Submit CI deploy batch; activate PB; PB->>PVM: Submit buil",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:10193,Deployability,deploy,deploy,10193,"t SHA will have changed. ## Deploying services to the live infrastructure. When a PR is merged into the `main` branch, a webhook will trigger. The CI service will set its `github_changed` flag. During its update loop, the CI service will determine that the SHA of the `WatchedBranch` has changed and trigger; a deployment. Like in the test case, the CI service uses targets in `build.yaml` to generate a set of jobs in a; batch run: . - Create a new batch job to:; - Build various components and service images; - Deploy to the `default` (ie prod) namespace; - Run various post-deployment tests; - A handful of final actions; - eg rolling out artifact registry cleanup policies, amongst many other things. Note: It's not actually quite as waterfall-y as this. In fact the jobs are all running in a hail; batch, and each package being built and service being deployed has its own path through the DAG. So it's quite possible; that services are deploy/test-ing in parallel, and that the deploy and test jobs for one service might all complete before ; the deploy and test for another have even started. This should all be fine. Because we only merge in the first place if the PR in question has already passed; tests against the current SHA of the watched branch. > [!NOTE]; > CI deploy runs in hail.is can be seen [here](https://batch.hail.is/batches?q=user+%3D+ci%0D%0Adeploy+%3D+1).; > Note:; > - You will need to be a member of the `ci` billing project to view these logs.; > - Due to the numbers of batches involved, this page may take a while to load . #### CI Deploy Timeline. The image below shows the CI deployment timeline:. ```mermaid; sequenceDiagram; participant Github; box Kubernetes (PROD: 'default' namespace); participant CI as PROD CI Service (default namespace); participant PB as PROD Batch (default namespace); end; box GCE (PROD VMs); participant PVM as PROD VMs; end. Github->>CI: Notify 'main' branch updated; CI->>PB: Submit CI deploy batch; activate PB; PB->>PVM: Submit buil",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:10262,Deployability,deploy,deploy,10262,"t SHA will have changed. ## Deploying services to the live infrastructure. When a PR is merged into the `main` branch, a webhook will trigger. The CI service will set its `github_changed` flag. During its update loop, the CI service will determine that the SHA of the `WatchedBranch` has changed and trigger; a deployment. Like in the test case, the CI service uses targets in `build.yaml` to generate a set of jobs in a; batch run: . - Create a new batch job to:; - Build various components and service images; - Deploy to the `default` (ie prod) namespace; - Run various post-deployment tests; - A handful of final actions; - eg rolling out artifact registry cleanup policies, amongst many other things. Note: It's not actually quite as waterfall-y as this. In fact the jobs are all running in a hail; batch, and each package being built and service being deployed has its own path through the DAG. So it's quite possible; that services are deploy/test-ing in parallel, and that the deploy and test jobs for one service might all complete before ; the deploy and test for another have even started. This should all be fine. Because we only merge in the first place if the PR in question has already passed; tests against the current SHA of the watched branch. > [!NOTE]; > CI deploy runs in hail.is can be seen [here](https://batch.hail.is/batches?q=user+%3D+ci%0D%0Adeploy+%3D+1).; > Note:; > - You will need to be a member of the `ci` billing project to view these logs.; > - Due to the numbers of batches involved, this page may take a while to load . #### CI Deploy Timeline. The image below shows the CI deployment timeline:. ```mermaid; sequenceDiagram; participant Github; box Kubernetes (PROD: 'default' namespace); participant CI as PROD CI Service (default namespace); participant PB as PROD Batch (default namespace); end; box GCE (PROD VMs); participant PVM as PROD VMs; end. Github->>CI: Notify 'main' branch updated; CI->>PB: Submit CI deploy batch; activate PB; PB->>PVM: Submit buil",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:10486,Deployability,deploy,deploy,10486,"nch` has changed and trigger; a deployment. Like in the test case, the CI service uses targets in `build.yaml` to generate a set of jobs in a; batch run: . - Create a new batch job to:; - Build various components and service images; - Deploy to the `default` (ie prod) namespace; - Run various post-deployment tests; - A handful of final actions; - eg rolling out artifact registry cleanup policies, amongst many other things. Note: It's not actually quite as waterfall-y as this. In fact the jobs are all running in a hail; batch, and each package being built and service being deployed has its own path through the DAG. So it's quite possible; that services are deploy/test-ing in parallel, and that the deploy and test jobs for one service might all complete before ; the deploy and test for another have even started. This should all be fine. Because we only merge in the first place if the PR in question has already passed; tests against the current SHA of the watched branch. > [!NOTE]; > CI deploy runs in hail.is can be seen [here](https://batch.hail.is/batches?q=user+%3D+ci%0D%0Adeploy+%3D+1).; > Note:; > - You will need to be a member of the `ci` billing project to view these logs.; > - Due to the numbers of batches involved, this page may take a while to load . #### CI Deploy Timeline. The image below shows the CI deployment timeline:. ```mermaid; sequenceDiagram; participant Github; box Kubernetes (PROD: 'default' namespace); participant CI as PROD CI Service (default namespace); participant PB as PROD Batch (default namespace); end; box GCE (PROD VMs); participant PVM as PROD VMs; end. Github->>CI: Notify 'main' branch updated; CI->>PB: Submit CI deploy batch; activate PB; PB->>PVM: Submit build jobs; activate PVM; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit deploy jobs; activate PVM; PVM->>PB: Redeploy batch service; PVM->>CI: Redeploy CI service; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit test jobs; activate PVM; PVM->>PB: Submit test batches; PB->>PVM: ",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:10819,Deployability,deploy,deployment,10819,"l of final actions; - eg rolling out artifact registry cleanup policies, amongst many other things. Note: It's not actually quite as waterfall-y as this. In fact the jobs are all running in a hail; batch, and each package being built and service being deployed has its own path through the DAG. So it's quite possible; that services are deploy/test-ing in parallel, and that the deploy and test jobs for one service might all complete before ; the deploy and test for another have even started. This should all be fine. Because we only merge in the first place if the PR in question has already passed; tests against the current SHA of the watched branch. > [!NOTE]; > CI deploy runs in hail.is can be seen [here](https://batch.hail.is/batches?q=user+%3D+ci%0D%0Adeploy+%3D+1).; > Note:; > - You will need to be a member of the `ci` billing project to view these logs.; > - Due to the numbers of batches involved, this page may take a while to load . #### CI Deploy Timeline. The image below shows the CI deployment timeline:. ```mermaid; sequenceDiagram; participant Github; box Kubernetes (PROD: 'default' namespace); participant CI as PROD CI Service (default namespace); participant PB as PROD Batch (default namespace); end; box GCE (PROD VMs); participant PVM as PROD VMs; end. Github->>CI: Notify 'main' branch updated; CI->>PB: Submit CI deploy batch; activate PB; PB->>PVM: Submit build jobs; activate PVM; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit deploy jobs; activate PVM; PVM->>PB: Redeploy batch service; PVM->>CI: Redeploy CI service; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit test jobs; activate PVM; PVM->>PB: Submit test batches; PB->>PVM: Submit test batch jobs; activate PVM; PVM-->>PB: Validate results; deactivate PVM; PVM-->>PB: Done; deactivate PVM; PB-->>CI: Detects completion; ```. ## References. ### The update loop. The following reference diagrams show the actions taken during CI's main update loop. Note that these diagrams are code-up views of how CI w",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:11132,Deployability,update,updated,11132,"t place if the PR in question has already passed; tests against the current SHA of the watched branch. > [!NOTE]; > CI deploy runs in hail.is can be seen [here](https://batch.hail.is/batches?q=user+%3D+ci%0D%0Adeploy+%3D+1).; > Note:; > - You will need to be a member of the `ci` billing project to view these logs.; > - Due to the numbers of batches involved, this page may take a while to load . #### CI Deploy Timeline. The image below shows the CI deployment timeline:. ```mermaid; sequenceDiagram; participant Github; box Kubernetes (PROD: 'default' namespace); participant CI as PROD CI Service (default namespace); participant PB as PROD Batch (default namespace); end; box GCE (PROD VMs); participant PVM as PROD VMs; end. Github->>CI: Notify 'main' branch updated; CI->>PB: Submit CI deploy batch; activate PB; PB->>PVM: Submit build jobs; activate PVM; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit deploy jobs; activate PVM; PVM->>PB: Redeploy batch service; PVM->>CI: Redeploy CI service; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit test jobs; activate PVM; PVM->>PB: Submit test batches; PB->>PVM: Submit test batch jobs; activate PVM; PVM-->>PB: Validate results; deactivate PVM; PVM-->>PB: Done; deactivate PVM; PB-->>CI: Detects completion; ```. ## References. ### The update loop. The following reference diagrams show the actions taken during CI's main update loop. Note that these diagrams are code-up views of how CI works. They should be cross references against the; ""purpose down"" descriptions above, of how CI uses these flows to make the changes it wants to happen. ```mermaid; flowchart LR; UpdateLoop[Update Loop]; ; UpdateLoop --> UpdateGithub; subgraph UpdateGithub; GithubChanged{github_changed?} -->|yes| _update_github; end; ; UpdateGithub --> UpdateBatch; subgraph UpdateBatch; BatchChanged{batch_changed?} -->|yes| _update_batch; end. UpdateBatch --> UpdateState; subgraph UpdateState; StateChanged{state_changed?} -->|yes| _update_state; end. UpdateState -",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:11160,Deployability,deploy,deploy,11160,"t place if the PR in question has already passed; tests against the current SHA of the watched branch. > [!NOTE]; > CI deploy runs in hail.is can be seen [here](https://batch.hail.is/batches?q=user+%3D+ci%0D%0Adeploy+%3D+1).; > Note:; > - You will need to be a member of the `ci` billing project to view these logs.; > - Due to the numbers of batches involved, this page may take a while to load . #### CI Deploy Timeline. The image below shows the CI deployment timeline:. ```mermaid; sequenceDiagram; participant Github; box Kubernetes (PROD: 'default' namespace); participant CI as PROD CI Service (default namespace); participant PB as PROD Batch (default namespace); end; box GCE (PROD VMs); participant PVM as PROD VMs; end. Github->>CI: Notify 'main' branch updated; CI->>PB: Submit CI deploy batch; activate PB; PB->>PVM: Submit build jobs; activate PVM; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit deploy jobs; activate PVM; PVM->>PB: Redeploy batch service; PVM->>CI: Redeploy CI service; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit test jobs; activate PVM; PVM->>PB: Submit test batches; PB->>PVM: Submit test batch jobs; activate PVM; PVM-->>PB: Validate results; deactivate PVM; PVM-->>PB: Done; deactivate PVM; PB-->>CI: Detects completion; ```. ## References. ### The update loop. The following reference diagrams show the actions taken during CI's main update loop. Note that these diagrams are code-up views of how CI works. They should be cross references against the; ""purpose down"" descriptions above, of how CI uses these flows to make the changes it wants to happen. ```mermaid; flowchart LR; UpdateLoop[Update Loop]; ; UpdateLoop --> UpdateGithub; subgraph UpdateGithub; GithubChanged{github_changed?} -->|yes| _update_github; end; ; UpdateGithub --> UpdateBatch; subgraph UpdateBatch; BatchChanged{batch_changed?} -->|yes| _update_batch; end. UpdateBatch --> UpdateState; subgraph UpdateState; StateChanged{state_changed?} -->|yes| _update_state; end. UpdateState -",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:11280,Deployability,deploy,deploy,11280,"t place if the PR in question has already passed; tests against the current SHA of the watched branch. > [!NOTE]; > CI deploy runs in hail.is can be seen [here](https://batch.hail.is/batches?q=user+%3D+ci%0D%0Adeploy+%3D+1).; > Note:; > - You will need to be a member of the `ci` billing project to view these logs.; > - Due to the numbers of batches involved, this page may take a while to load . #### CI Deploy Timeline. The image below shows the CI deployment timeline:. ```mermaid; sequenceDiagram; participant Github; box Kubernetes (PROD: 'default' namespace); participant CI as PROD CI Service (default namespace); participant PB as PROD Batch (default namespace); end; box GCE (PROD VMs); participant PVM as PROD VMs; end. Github->>CI: Notify 'main' branch updated; CI->>PB: Submit CI deploy batch; activate PB; PB->>PVM: Submit build jobs; activate PVM; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit deploy jobs; activate PVM; PVM->>PB: Redeploy batch service; PVM->>CI: Redeploy CI service; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit test jobs; activate PVM; PVM->>PB: Submit test batches; PB->>PVM: Submit test batch jobs; activate PVM; PVM-->>PB: Validate results; deactivate PVM; PVM-->>PB: Done; deactivate PVM; PB-->>CI: Detects completion; ```. ## References. ### The update loop. The following reference diagrams show the actions taken during CI's main update loop. Note that these diagrams are code-up views of how CI works. They should be cross references against the; ""purpose down"" descriptions above, of how CI uses these flows to make the changes it wants to happen. ```mermaid; flowchart LR; UpdateLoop[Update Loop]; ; UpdateLoop --> UpdateGithub; subgraph UpdateGithub; GithubChanged{github_changed?} -->|yes| _update_github; end; ; UpdateGithub --> UpdateBatch; subgraph UpdateBatch; BatchChanged{batch_changed?} -->|yes| _update_batch; end. UpdateBatch --> UpdateState; subgraph UpdateState; StateChanged{state_changed?} -->|yes| _update_state; end. UpdateState -",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:11662,Deployability,update,update,11662,"to view these logs.; > - Due to the numbers of batches involved, this page may take a while to load . #### CI Deploy Timeline. The image below shows the CI deployment timeline:. ```mermaid; sequenceDiagram; participant Github; box Kubernetes (PROD: 'default' namespace); participant CI as PROD CI Service (default namespace); participant PB as PROD Batch (default namespace); end; box GCE (PROD VMs); participant PVM as PROD VMs; end. Github->>CI: Notify 'main' branch updated; CI->>PB: Submit CI deploy batch; activate PB; PB->>PVM: Submit build jobs; activate PVM; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit deploy jobs; activate PVM; PVM->>PB: Redeploy batch service; PVM->>CI: Redeploy CI service; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit test jobs; activate PVM; PVM->>PB: Submit test batches; PB->>PVM: Submit test batch jobs; activate PVM; PVM-->>PB: Validate results; deactivate PVM; PVM-->>PB: Done; deactivate PVM; PB-->>CI: Detects completion; ```. ## References. ### The update loop. The following reference diagrams show the actions taken during CI's main update loop. Note that these diagrams are code-up views of how CI works. They should be cross references against the; ""purpose down"" descriptions above, of how CI uses these flows to make the changes it wants to happen. ```mermaid; flowchart LR; UpdateLoop[Update Loop]; ; UpdateLoop --> UpdateGithub; subgraph UpdateGithub; GithubChanged{github_changed?} -->|yes| _update_github; end; ; UpdateGithub --> UpdateBatch; subgraph UpdateBatch; BatchChanged{batch_changed?} -->|yes| _update_batch; end. UpdateBatch --> UpdateState; subgraph UpdateState; StateChanged{state_changed?} -->|yes| _update_state; end. UpdateState --> Loop; ```. #### Update Github. ```mermaid; flowchart LR; UpdateLoop[Update Loop]; ; UpdateLoop -->|github_changed| UpdateGithub(Update Github); UpdateGithub --> UpdateSha(Update SHA for\nWatchedBranch); subgraph UpdatePRs; direction LR; UpdatePRList(Update list of PRs for\nWatchedBranch); Upd",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:11748,Deployability,update,update,11748," involved, this page may take a while to load . #### CI Deploy Timeline. The image below shows the CI deployment timeline:. ```mermaid; sequenceDiagram; participant Github; box Kubernetes (PROD: 'default' namespace); participant CI as PROD CI Service (default namespace); participant PB as PROD Batch (default namespace); end; box GCE (PROD VMs); participant PVM as PROD VMs; end. Github->>CI: Notify 'main' branch updated; CI->>PB: Submit CI deploy batch; activate PB; PB->>PVM: Submit build jobs; activate PVM; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit deploy jobs; activate PVM; PVM->>PB: Redeploy batch service; PVM->>CI: Redeploy CI service; PVM-->>PB: Done; deactivate PVM; PB->>PVM: Submit test jobs; activate PVM; PVM->>PB: Submit test batches; PB->>PVM: Submit test batch jobs; activate PVM; PVM-->>PB: Validate results; deactivate PVM; PVM-->>PB: Done; deactivate PVM; PB-->>CI: Detects completion; ```. ## References. ### The update loop. The following reference diagrams show the actions taken during CI's main update loop. Note that these diagrams are code-up views of how CI works. They should be cross references against the; ""purpose down"" descriptions above, of how CI uses these flows to make the changes it wants to happen. ```mermaid; flowchart LR; UpdateLoop[Update Loop]; ; UpdateLoop --> UpdateGithub; subgraph UpdateGithub; GithubChanged{github_changed?} -->|yes| _update_github; end; ; UpdateGithub --> UpdateBatch; subgraph UpdateBatch; BatchChanged{batch_changed?} -->|yes| _update_batch; end. UpdateBatch --> UpdateState; subgraph UpdateState; StateChanged{state_changed?} -->|yes| _update_state; end. UpdateState --> Loop; ```. #### Update Github. ```mermaid; flowchart LR; UpdateLoop[Update Loop]; ; UpdateLoop -->|github_changed| UpdateGithub(Update Github); UpdateGithub --> UpdateSha(Update SHA for\nWatchedBranch); subgraph UpdatePRs; direction LR; UpdatePRList(Update list of PRs for\nWatchedBranch); UpdatePRList --> AssignReviewers(Assign Reviewers); Assi",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:13180,Deployability,deploy,deployable,13180,tate; subgraph UpdateState; StateChanged{state_changed?} -->|yes| _update_state; end. UpdateState --> Loop; ```. #### Update Github. ```mermaid; flowchart LR; UpdateLoop[Update Loop]; ; UpdateLoop -->|github_changed| UpdateGithub(Update Github); UpdateGithub --> UpdateSha(Update SHA for\nWatchedBranch); subgraph UpdatePRs; direction LR; UpdatePRList(Update list of PRs for\nWatchedBranch); UpdatePRList --> AssignReviewers(Assign Reviewers); AssignReviewers --> UpdatePrGithub(Update local PR state from github); UpdatePrGithub --> ChecksChanged{New check\nor review\nstate?}; ChecksChanged -->|yes| SetStateChanged(Set state_changed). end. UpdateGithub --> UpdatePRs; ```. #### Update Batch. ```mermaid; flowchart LR; UpdateLoop[Update Loop]; ; UpdateLoop -->|batch_changed| UpdateBatch; UpdateBatch --> UpdateDeployable; UpdateBatch --> UpdatePRBatches ; subgraph UpdateDeployable; direction LR; Deployable{deployable?} --> UpdateDeployBatch(Update internal state of \ndeploy batch); end; subgraph UpdatePRBatches; direction LR; IterateOverPrs(Iterate over PRs) --> FindLatest(Find latest non-cancelled batch for SHA); FindLatest --> UpdateBatchState(Update local state of PR batch); UpdateBatchState --> BatchStateChanged{changed?}; BatchStateChanged -->|yes| SetStateChanged2(Set state_changed); end. ```. #### Update State. Updating the external state involves two steps: a 'heal' step and a 'try to merge' step. ```mermaid; flowchart LR; UpdateLoop[Update Loop]; ; UpdateLoop -->|state_changed| Heal; Heal --> TryToMerge(Try to merge); ```. ##### Heal. ```mermaid; flowchart LR; subgraph HealDeploy; direction LR; NewDeploy{new deployable SHA?} -->|yes| CreateDeploy(Create deploy\nbatch); end; HealDeploy --> DetermineMergeCandidate(Determine merge candidate); DetermineMergeCandidate --> HealPRs; subgraph HealPRs; direction LR; IterateOverPrs2(Iterate over PRs) --> PostGithubStatus(Post github status); PostGithubStatus --> StartTestBatch; subgraph StartTestBatch; direction LR; NoTestsYe,MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:13905,Deployability,deploy,deployable,13905,e_changed). end. UpdateGithub --> UpdatePRs; ```. #### Update Batch. ```mermaid; flowchart LR; UpdateLoop[Update Loop]; ; UpdateLoop -->|batch_changed| UpdateBatch; UpdateBatch --> UpdateDeployable; UpdateBatch --> UpdatePRBatches ; subgraph UpdateDeployable; direction LR; Deployable{deployable?} --> UpdateDeployBatch(Update internal state of \ndeploy batch); end; subgraph UpdatePRBatches; direction LR; IterateOverPrs(Iterate over PRs) --> FindLatest(Find latest non-cancelled batch for SHA); FindLatest --> UpdateBatchState(Update local state of PR batch); UpdateBatchState --> BatchStateChanged{changed?}; BatchStateChanged -->|yes| SetStateChanged2(Set state_changed); end. ```. #### Update State. Updating the external state involves two steps: a 'heal' step and a 'try to merge' step. ```mermaid; flowchart LR; UpdateLoop[Update Loop]; ; UpdateLoop -->|state_changed| Heal; Heal --> TryToMerge(Try to merge); ```. ##### Heal. ```mermaid; flowchart LR; subgraph HealDeploy; direction LR; NewDeploy{new deployable SHA?} -->|yes| CreateDeploy(Create deploy\nbatch); end; HealDeploy --> DetermineMergeCandidate(Determine merge candidate); DetermineMergeCandidate --> HealPRs; subgraph HealPRs; direction LR; IterateOverPrs2(Iterate over PRs) --> PostGithubStatus(Post github status); PostGithubStatus --> StartTestBatch; subgraph StartTestBatch; direction LR; NoTestsYet{no test batch yet?} -->|yes| CreateTestBatch(Create test batch); CurrentMergeCandidate{current merge\ncandidate} -->|yes| TestBatchStale(previous test batch\noutdated); TestBatchStale -->|yes| CreateTestBatch; end; end; HealPRs --> CancelOrphanBuilds ; ```. ##### Try to Merge. ```mermaid; flowchart LR; IterateOverMergeable(Iterate over\nmergeable PRs in\nmerge priority order); IterateOverMergeable --> IsMergeable{is_mergeable?\n\n-No DO_NOT_MERGE flag\n-Reviewed\n-Checks passed\n-Up to date test run}; ; IsMergeable -->|yes| Merge(Merge the PR). Merge -->|failure| IterateOverMergeable; Merge -->|success| Return; ```; ,MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:13951,Deployability,deploy,deploy,13951,e_changed). end. UpdateGithub --> UpdatePRs; ```. #### Update Batch. ```mermaid; flowchart LR; UpdateLoop[Update Loop]; ; UpdateLoop -->|batch_changed| UpdateBatch; UpdateBatch --> UpdateDeployable; UpdateBatch --> UpdatePRBatches ; subgraph UpdateDeployable; direction LR; Deployable{deployable?} --> UpdateDeployBatch(Update internal state of \ndeploy batch); end; subgraph UpdatePRBatches; direction LR; IterateOverPrs(Iterate over PRs) --> FindLatest(Find latest non-cancelled batch for SHA); FindLatest --> UpdateBatchState(Update local state of PR batch); UpdateBatchState --> BatchStateChanged{changed?}; BatchStateChanged -->|yes| SetStateChanged2(Set state_changed); end. ```. #### Update State. Updating the external state involves two steps: a 'heal' step and a 'try to merge' step. ```mermaid; flowchart LR; UpdateLoop[Update Loop]; ; UpdateLoop -->|state_changed| Heal; Heal --> TryToMerge(Try to merge); ```. ##### Heal. ```mermaid; flowchart LR; subgraph HealDeploy; direction LR; NewDeploy{new deployable SHA?} -->|yes| CreateDeploy(Create deploy\nbatch); end; HealDeploy --> DetermineMergeCandidate(Determine merge candidate); DetermineMergeCandidate --> HealPRs; subgraph HealPRs; direction LR; IterateOverPrs2(Iterate over PRs) --> PostGithubStatus(Post github status); PostGithubStatus --> StartTestBatch; subgraph StartTestBatch; direction LR; NoTestsYet{no test batch yet?} -->|yes| CreateTestBatch(Create test batch); CurrentMergeCandidate{current merge\ncandidate} -->|yes| TestBatchStale(previous test batch\noutdated); TestBatchStale -->|yes| CreateTestBatch; end; end; HealPRs --> CancelOrphanBuilds ; ```. ##### Try to Merge. ```mermaid; flowchart LR; IterateOverMergeable(Iterate over\nmergeable PRs in\nmerge priority order); IterateOverMergeable --> IsMergeable{is_mergeable?\n\n-No DO_NOT_MERGE flag\n-Reviewed\n-Checks passed\n-Up to date test run}; ; IsMergeable -->|yes| Merge(Merge the PR). Merge -->|failure| IterateOverMergeable; Merge -->|success| Return; ```; ,MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:1611,Energy Efficiency,monitor,monitor,1611,"annotated. ; Note that the Hail team's configuration is mostly stable, but should be treated with a grain of salt and validated ; against the actual configuration instead of being assumed to be true. ## Overview of the CI Service. The CI service has three functional purposes:. - Runs tests against pull requests which target designated 'watched branches'; - Merges PRs onto watched branches when they are ready; - Deploys services to the live infrastructure when the watched branch is updated. ## Structure and Operation. The CI service itself is deployed as a Kubernetes service in the Hail Batch cluster. See ; [architecture diagram](../Hail%20Batch%20Architectural%20Diagram.png). CI must be configured with an access token allowing it to operate on ; behalf of a github account . > [!NOTE] ; > This account is called `hail-ci-robot` in hail-is/hail. ### Watched Branches. One of the core concepts of the CI service is the ""watched branch"". A watched branch is a branch in a github repository; which the CI service is configured to monitor for changes. The CI service will run tests against PRs which target the; watched branch, will merge PRs against watched branches when they are ready, and will deploy services when the watched ; branch is updated. > [!NOTE] ; > In `hail-is/hail`, there is one watched branch: `hail-is/hail:main`. ### Tracked State. For each branch, CI will track three types of state which can be marked as stale using appropriate flags:; - The state of github branches, commits, and PRs (""`github_changed`""); - The state of batches CI is running to run tests or deploy to production (""`batch_changed`""); - Whether CI needs to act externally, to update Github or start new batches (""`state_changed`""). ### CI Update Loop. The core CI update loop iterates over its set of ""watched branches"". If any of the state flags for any of the watched; branches are marked as dirty, then the CI service will re-evaluate the state of the watched branch and potentially take; action to u",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md:4879,Integrability,interface,interface,4879,"or `hail-is/hail`, the webhook target is: `ci.hail.is/github_callback`; > - For `hail-is/hail`, webhook callbacks are configured to happen on changes to the following:; > - Pull request reviews; > - Pull requests; > - Pushes. ## Running tests against pull requests. When a PR which targets a watched branch is created, updated or closed, the `github_changed` flag will be marked as dirty. . During its update loop, CI will potentially decide to run tests against all PRs targetting a watched branch:. - Authorization; - If the author is in the [`ci/ci/constants`](../../../ci/ci/constants.py) list of known developers, it can be tested.; - If the github SHA for the commit has been registered, it can be tested.; - Otherwise, it will be ignored.; - Check for validity; - The PR must be against a watched branch, or else it wouldn't be considered.; - If the current commit has already had tests run against it, no further action is taken `*`; - If the PR is marked with the ""do_not_test"" label, no further action is taken; - Run tests; - The CI service will run tests against the PR (see below) ; - Report results back to Github using the Checks interface. `*` Note: there is an important nuance to the statement ""If the current commit has already had tests run against it, no further action is taken"". - Every time CI loops through its update cycle, it will determine a ""merge candidate"" from all of the PRs which are approved.; - If the batch tests for the merge candidate PR were run against a different SHA than the current SHA of the watched branch,; then CI trigger a new test batch. . ### Running Tests. The process of running tests goes like:. - CI will create a pending Check against the PR in question; - CI will generate a new batch and submit it against the production Hail Batch service using its own service credentials; - Tasks in the batch are defined in `build.yaml` in the top repo directory. CI generates jobs in the batch from the steps defined in there.; - The batch contains jobs ",MatchSource.DOCS,dev-docs/services/ci/README.md,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/dev-docs/services/ci/README.md
