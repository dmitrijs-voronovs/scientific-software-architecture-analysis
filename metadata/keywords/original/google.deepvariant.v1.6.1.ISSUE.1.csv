id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/172:109050,Modifiability,variab,variable,109050,ts; prev_var_name: Unchanged; I0415 07:34:38.030065 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030452 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030848 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.031589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031938 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:109218,Modifiability,variab,variable,109218,r_name: Unchanged; I0415 07:34:38.030452 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.030848 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.031589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031938 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting varia,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:109386,Modifiability,variab,variable,109386,v_var_name: Unchanged; I0415 07:34:38.030848 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.031589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031938 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:109561,Modifiability,variab,variable,109561,v_var_name: Unchanged; I0415 07:34:38.031235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.031589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031938 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:109736,Modifiability,variab,variable,109736,r_name: Unchanged; I0415 07:34:38.031589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.031938 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:109904,Modifiability,variab,variable,109904,v_var_name: Unchanged; I0415 07:34:38.031938 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:110079,Modifiability,variab,variable,110079,ame: Unchanged; I0415 07:34:38.032286 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:110254,Modifiability,variab,variable,110254, Unchanged; I0415 07:34:38.032630 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/M,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:110422,Modifiability,variab,variable,110422,rev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inceptio,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:110579,Modifiability,variab,variable,110579,Norm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting varia,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:110747,Modifiability,variab,variable,110747,eights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting va,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:110922,Modifiability,variab,variable,110922,m/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting va,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:111097,Modifiability,variab,variable,111097,m/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting va,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:111272,Modifiability,variab,variable,111272,ts; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting varia,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:111440,Modifiability,variab,variable,111440,ame: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:111615,Modifiability,variab,variable,111615,changed; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:111790,Modifiability,variab,variable,111790,changed; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Bra,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:111965,Modifiability,variab,variable,111965,changed; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/C,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:112140,Modifiability,variab,variable,112140, Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/B,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:112308,Modifiability,variab,variable,112308,var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:112458,Modifiability,variab,variable,112458,m/beta; prev_var_name: Unchanged; I0415 07:34:38.037974 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:112633,Modifiability,variab,variable,112633,Norm/beta; prev_var_name: Unchanged; I0415 07:34:38.038338 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:112801,Modifiability,variab,variable,112801,1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.038681 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/M,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:112969,Modifiability,variab,variable,112969,d_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.039011 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:113144,Modifiability,variab,variable,113144,/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.039341 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_4a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inceptio,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:113301,Modifiability,variab,variable,113301,eights; prev_var_name: Unchanged; I0415 07:34:38.039675 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:113476,Modifiability,variab,variable,113476,Norm/beta; prev_var_name: Unchanged; I0415 07:34:38.040005 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting varia,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:113644,Modifiability,variab,variable,113644,eights; prev_var_name: Unchanged; I0415 07:34:38.040335 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:113819,Modifiability,variab,variable,113819, prev_var_name: Unchanged; I0415 07:34:38.040720 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:113994,Modifiability,variab,variable,113994,ta; prev_var_name: Unchanged; I0415 07:34:38.041230 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:114162,Modifiability,variab,variable,114162,ame: Unchanged; I0415 07:34:38.041950 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:114337,Modifiability,variab,variable,114337,ame: Unchanged; I0415 07:34:38.042407 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Bran,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:114512,Modifiability,variab,variable,114512, Unchanged; I0415 07:34:38.042782 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:114680,Modifiability,variab,variable,114680,ame: Unchanged; I0415 07:34:38.043235 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Co,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:114855,Modifiability,variab,variable,114855,r_name: Unchanged; I0415 07:34:38.043617 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:115023,Modifiability,variab,variable,115023,r_name: Unchanged; I0415 07:34:38.044003 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:115191,Modifiability,variab,variable,115191,beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/B,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:115347,Modifiability,variab,variable,115347,x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inception,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:115515,Modifiability,variab,variable,115515,2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:115683,Modifiability,variab,variable,115683,2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:115851,Modifiability,variab,variable,115851,Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:116026,Modifiability,variab,variable,116026,_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting varia,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:116194,Modifiability,variab,variable,116194,es; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:116369,Modifiability,variab,variable,116369,v_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:116544,Modifiability,variab,variable,116544,r_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:116712,Modifiability,variab,variable,116712,ame: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:116887,Modifiability,variab,variable,116887,ame: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/C,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:117062,Modifiability,variab,variable,117062, Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/w,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:117230,Modifiability,variab,variable,117230,ame: Unchanged; I0415 07:34:38.052555 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weigh,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:117405,Modifiability,variab,variable,117405,r_name: Unchanged; I0415 07:34:38.052977 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights;,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:117573,Modifiability,variab,variable,117573,r_name: Unchanged; I0415 07:34:38.053325 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNor,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:117741,Modifiability,variab,variable,117741,prev_var_name: Unchanged; I0415 07:34:38.053670 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_n,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:117909,Modifiability,variab,variable,117909,chNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.054332 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:118066,Modifiability,variab,variable,118066,h_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.054721 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:118216,Modifiability,variab,variable,118216,_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.055120 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:118384,Modifiability,variab,variable,118384,3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055521 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:118552,Modifiability,variab,variable,118552,ixed_7b/Branch_1/Conv2d_0b_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.055919 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Bra,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:118727,Modifiability,variab,variable,118727,nV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.056318 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:118877,Modifiability,variab,variable,118877, InceptionV3/Conv2d_3b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.056755 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:119045,Modifiability,variab,variable,119045,3/Conv2d_1a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.057252 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:119213,Modifiability,variab,variable,119213,onv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.057799 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:119388,Modifiability,variab,variable,119388,a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.058212 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:119563,Modifiability,variab,variable,119563,atchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.058551 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_2a_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:119738,Modifiability,variab,variable,119738,r_name: Unchanged; I0415 07:34:38.058897 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights; prev_var_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inception,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:119906,Modifiability,variab,variable,119906,r_name: Unchanged; I0415 07:34:38.059231 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights; prev_var_name: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:120074,Modifiability,variab,variable,120074,ame: Unchanged; I0415 07:34:38.059657 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: Incept,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:120249,Modifiability,variab,variable,120249,r_name: Unchanged; I0415 07:34:38.060014 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:120417,Modifiability,variab,variable,120417,v_var_name: Unchanged; I0415 07:34:38.060353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:120592,Modifiability,variab,variable,120592,prev_var_name: Unchanged; I0415 07:34:38.060697 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: In,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:120760,Modifiability,variab,variable,120760,v_var_name: Unchanged; I0415 07:34:38.061029 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable:,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:120935,Modifiability,variab,variable,120935,r_name: Unchanged; I0415 07:34:38.061362 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting varia,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:121103,Modifiability,variab,variable,121103,v_var_name: Unchanged; I0415 07:34:38.061695 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting va,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:121278,Modifiability,variab,variable,121278,ame: Unchanged; I0415 07:34:38.062012 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting va,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:121453,Modifiability,variab,variable,121453,r_name: Unchanged; I0415 07:34:38.063043 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights; prev_var_name: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:121621,Modifiability,variab,variable,121621,ame: Unchanged; I0415 07:34:38.063481 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook.; I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized.; 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_featu,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:121796,Modifiability,variab,variable,121796,ame: Unchanged; I0415 07:34:38.063901 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook.; I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized.; 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_u,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:121971,Modifiability,variab,variable,121971,changed; I0415 07:34:38.064337 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook.; I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized.; 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz; 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:122146,Modifiability,variab,variable,122146,"changed; I0415 07:34:38.064796 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook.; I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized.; 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz; 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:; 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; I0415 07:34:52.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:122321,Modifiability,variab,variable,122321,"changed; I0415 07:34:38.065186 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook.; I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized.; 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz; 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:; 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op.; I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op.; I0415 07:35:1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:1146,Testability,log,log,1146,"el_train, every process was working fine. When I try to run model_train and model_eval in my computer, model_train seemed to work but model_eval returned ValueError: Must specify steps > 0, given: 0. The error came from estimator.py file in tensorflow. In Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0; I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt; W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:1177,Testability,log,log,1177,"king fine. When I try to run model_train and model_eval in my computer, model_train seemed to work but model_eval returned ValueError: Must specify steps > 0, given: 0. The error came from estimator.py file in tensorflow. In Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0; I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt; W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:1478,Testability,log,log,1478,"ller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0; I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt; W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-???",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:1508,Testability,log,log,1508,"at . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0; I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt; W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:1564,Testability,log,log,1564,"model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0; I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt; W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode=eval with model DeepVariantMod",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:26279,Testability,Log,Logits,26279,atchNorm/moving_variance|InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights/RMSProp|InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/moving_variance|InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean|InceptionV3/Logits/Conv2d_1c_1x1/biases|InceptionV3/Conv2d_1a_3x3/weights/RMSProp_1|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights/RMSProp_1|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean|InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights/RMSProp_1|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights/RMSProp|InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Bra,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:36693,Testability,Log,Logits,36693,Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights/RMSProp_1|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/weights|InceptionV3/Logits/Conv2d_1c_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights/RMSProp|InceptionV3/Conv2d_2a_3x3/weights/RMSProp_1|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/weights|InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/weights|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights/RMSProp|InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights|InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_variance/Expo,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:42445,Testability,Log,Logits,42445,tionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights|InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights/RMSProp|InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_variance|InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Logits/Conv2d_1c_1x1/biases/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights/RMSProp_1|InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights/RMSProp|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:43576,Testability,Log,Logits,43576,d/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights/RMSProp|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Logits/Conv2d_1c_1x1/weights|InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Conv2d_2b_3x3/BatchNorm/moving_mean|InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta|InceptionV3/Mixed_6e/Branch_1/Co,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:52227,Testability,Log,Logits,52227,p|InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights/RMSProp|InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights|InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Logits/Conv2d_1c_1x1/biases/RMSProp|InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights|InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1|InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights/RMSProp_1|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weigh,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:59566,Testability,Log,Logits,59566,nce/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean|InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta|InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights/RMSProp|InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta|InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights|InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta/RMSProp_1|InceptionV3/Logits/Conv2d_1c_1x1/biases/RMSProp_1|InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights/RMSProp|InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean|InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights|InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/moving_variance|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance|InceptionV3/Mixed_6c/Branc,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:64412,Testability,Log,Logits,64412,d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Conv2d_2a_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights/RMSProp|InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights/RMSProp_1|InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights/RMSProp|InceptionV3/Logits/Conv2d_1c_1x1/weights/RMSProp|InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta|InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights|InceptionV3/Conv2d_2b_3x3/weights/RMSProp|InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights/RMSProp_1|InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:88743,Testability,Log,Logits,88743,hNorm/beta|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights|InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights|InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights/RMSProp_1|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights|InceptionV3/Conv2d_4a_3x3/weights/RMSProp_1|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_variance|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Logits/Conv2d_1c_1x1/weights/RMSProp_1|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance|InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights|InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Conv2d_3b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Conv2d_4a_3x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:110444,Testability,Log,Logits,110444,rev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inceptio,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:115213,Testability,Log,Logits,115213,beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/B,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:7399,Usability,clear,clear,7399,"ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False; I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1; I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt; I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting....; I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true; graph_options {; rewrite_options {; meta_optimizer_iterations: ONE; }; }; , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}; W0415 07:34:19.583559 140368878327552 deprecation.py:323] ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/175:162,Availability,error,error,162,"Hello deepvariant community,; I am running deepvariant analysis on whole genome sequencing data. The first part of the analysis, the ""make_examples"" runs without error. However, in the ""call_variants"" step, I get the following error: . ![Screen Shot 2019-04-16 at 17 10 54](https://user-images.githubusercontent.com/9975286/56221509-aacef300-606a-11e9-9302-e3c7c2a9eed2.png). My code is as follows:. N_SHARDS=""8"" \; seq 0 $((N_SHARDS-1)) | parallel -k --line-buffer \; python ../bin/make_examples.zip \; --mode calling \; --ref /Volumes/workspace/input/reference/GRCh38/Homo_sapiens_hg38.fa \; --reads /Volumes/workspace/input_march_mac7/AS-196110-LR-30685_HISATmapping/Aligned.out.bam \; --examples /Volumes/workspace/output_march_mac7/AS-196110-LR-30685.tfrecord@${N_SHARDS}.gz \; --gvcf /Volumes/workspace/output_march_mac7/AS-196110-LR-30685.gvcf.tfrecord@${N_SHARDS}.gz \; --task {}. python ../bin/call_variants.zip \; --outfile /Volumes/workspace/output_march_mac7/AS-196110-LR-30685.vcf.gz \; --examples /Volumes/workspace/output_march_mac7/AS-196110-LR-30685.tfrecord@8.gz \; --checkpoint /Volumes/workspace/input/models/WGS/model.ckpt \; --batch_size 32. However, when I run the same scripts on a BAM file generated using reads extracted from chromosome 1, all the steps run successfully. Please let me know where I could be going wrong with the commands.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/175
https://github.com/google/deepvariant/issues/175:227,Availability,error,error,227,"Hello deepvariant community,; I am running deepvariant analysis on whole genome sequencing data. The first part of the analysis, the ""make_examples"" runs without error. However, in the ""call_variants"" step, I get the following error: . ![Screen Shot 2019-04-16 at 17 10 54](https://user-images.githubusercontent.com/9975286/56221509-aacef300-606a-11e9-9302-e3c7c2a9eed2.png). My code is as follows:. N_SHARDS=""8"" \; seq 0 $((N_SHARDS-1)) | parallel -k --line-buffer \; python ../bin/make_examples.zip \; --mode calling \; --ref /Volumes/workspace/input/reference/GRCh38/Homo_sapiens_hg38.fa \; --reads /Volumes/workspace/input_march_mac7/AS-196110-LR-30685_HISATmapping/Aligned.out.bam \; --examples /Volumes/workspace/output_march_mac7/AS-196110-LR-30685.tfrecord@${N_SHARDS}.gz \; --gvcf /Volumes/workspace/output_march_mac7/AS-196110-LR-30685.gvcf.tfrecord@${N_SHARDS}.gz \; --task {}. python ../bin/call_variants.zip \; --outfile /Volumes/workspace/output_march_mac7/AS-196110-LR-30685.vcf.gz \; --examples /Volumes/workspace/output_march_mac7/AS-196110-LR-30685.tfrecord@8.gz \; --checkpoint /Volumes/workspace/input/models/WGS/model.ckpt \; --batch_size 32. However, when I run the same scripts on a BAM file generated using reads extracted from chromosome 1, all the steps run successfully. Please let me know where I could be going wrong with the commands.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/175
https://github.com/google/deepvariant/issues/175:1086,Availability,checkpoint,checkpoint,1086,"Hello deepvariant community,; I am running deepvariant analysis on whole genome sequencing data. The first part of the analysis, the ""make_examples"" runs without error. However, in the ""call_variants"" step, I get the following error: . ![Screen Shot 2019-04-16 at 17 10 54](https://user-images.githubusercontent.com/9975286/56221509-aacef300-606a-11e9-9302-e3c7c2a9eed2.png). My code is as follows:. N_SHARDS=""8"" \; seq 0 $((N_SHARDS-1)) | parallel -k --line-buffer \; python ../bin/make_examples.zip \; --mode calling \; --ref /Volumes/workspace/input/reference/GRCh38/Homo_sapiens_hg38.fa \; --reads /Volumes/workspace/input_march_mac7/AS-196110-LR-30685_HISATmapping/Aligned.out.bam \; --examples /Volumes/workspace/output_march_mac7/AS-196110-LR-30685.tfrecord@${N_SHARDS}.gz \; --gvcf /Volumes/workspace/output_march_mac7/AS-196110-LR-30685.gvcf.tfrecord@${N_SHARDS}.gz \; --task {}. python ../bin/call_variants.zip \; --outfile /Volumes/workspace/output_march_mac7/AS-196110-LR-30685.vcf.gz \; --examples /Volumes/workspace/output_march_mac7/AS-196110-LR-30685.tfrecord@8.gz \; --checkpoint /Volumes/workspace/input/models/WGS/model.ckpt \; --batch_size 32. However, when I run the same scripts on a BAM file generated using reads extracted from chromosome 1, all the steps run successfully. Please let me know where I could be going wrong with the commands.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/175
https://github.com/google/deepvariant/issues/176:48,Availability,down,downsampling,48,"Hi,. I just wonder if Is there a way to disable downsampling? I work with high depth data from targeted tumor panel. The depth could be as high as 20,000x. With deepvariant 8.0, we miss some variants with low maf such as 0.05, 0.02... Thanks a lot for rhe help!. Best,. Sean",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/176
https://github.com/google/deepvariant/issues/177:0,Deployability,Install,Installation,0,Installation of 0.8.0 using bioconda:; ```; conda create -n deepvariant deepvariant; ```. fails on CentOS 6.6 with: . ```; CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/.travis.yml'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/README.rst'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/documentation/Makefile'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/documentation/conf.pyc'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/documentation/index.rst'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/six/setup.cfg'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177
https://github.com/google/deepvariant/issues/177:4445,Modifiability,config,config,4445,t cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/socksipy-branch/socks.pyc'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/rpm/mapping/command_mapping.yaml'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/rpm/mapping/component_mapping.yaml'; specified in the package manifest cannot be found.; ```; (there are many similar CondaVerificationErrors before this); Conda info:; ```; active environment : longshot; active env location : /home/pedge/anaconda3/envs/longshot; shell level : 2; user config file : /home/pedge/.condarc; populated config files : /home/pedge/.condarc; conda version : 4.6.9; conda-build version : 3.17.6; python version : 3.7.1.final.0; base environment : /home/pedge/anaconda3 (writable); channel URLs : https://conda.anaconda.org/conda-forge/linux-64; https://conda.anaconda.org/conda-forge/noarch; https://conda.anaconda.org/bioconda/linux-64; https://conda.anaconda.org/bioconda/noarch; https://repo.anaconda.com/pkgs/main/linux-64; https://repo.anaconda.com/pkgs/main/noarch; https://repo.anaconda.com/pkgs/free/linux-64; https://repo.anaconda.com/pkgs/free/noarch; https://repo.anaconda.com/pkgs/r/linux-64; https://repo.anaconda.com/pkgs/r/noarch; https://conda.anaconda.org/OpenMDAO/linux-64; https://conda.anaconda.org/OpenMDAO/noarch; package cache : /home/pedge/anaconda3/pkgs; /home/pedge/.conda/pkgs; envs directories : /home/pedge/anaconda3/envs; /home/pedge/.conda/env,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177
https://github.com/google/deepvariant/issues/177:4491,Modifiability,config,config,4491,.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/platform/gsutil/third_party/socksipy-branch/socks.pyc'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/rpm/mapping/command_mapping.yaml'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/rpm/mapping/component_mapping.yaml'; specified in the package manifest cannot be found.; ```; (there are many similar CondaVerificationErrors before this); Conda info:; ```; active environment : longshot; active env location : /home/pedge/anaconda3/envs/longshot; shell level : 2; user config file : /home/pedge/.condarc; populated config files : /home/pedge/.condarc; conda version : 4.6.9; conda-build version : 3.17.6; python version : 3.7.1.final.0; base environment : /home/pedge/anaconda3 (writable); channel URLs : https://conda.anaconda.org/conda-forge/linux-64; https://conda.anaconda.org/conda-forge/noarch; https://conda.anaconda.org/bioconda/linux-64; https://conda.anaconda.org/bioconda/noarch; https://repo.anaconda.com/pkgs/main/linux-64; https://repo.anaconda.com/pkgs/main/noarch; https://repo.anaconda.com/pkgs/free/linux-64; https://repo.anaconda.com/pkgs/free/noarch; https://repo.anaconda.com/pkgs/r/linux-64; https://repo.anaconda.com/pkgs/r/noarch; https://conda.anaconda.org/OpenMDAO/linux-64; https://conda.anaconda.org/OpenMDAO/noarch; package cache : /home/pedge/anaconda3/pkgs; /home/pedge/.conda/pkgs; envs directories : /home/pedge/anaconda3/envs; /home/pedge/.conda/envs; platform : linux-64; user-agent : conda/4.6.9 requests/2.21.0 CPython/3.7.1 Linux/2.6.32-696.18.7.el6.x86_64 centos/6.6 glibc/2.12; UI,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177
https://github.com/google/deepvariant/issues/177:5229,Performance,cache,cache,5229,dk-243.0.0-0/platform/gsutil/third_party/socksipy-branch/socks.pyc'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/rpm/mapping/command_mapping.yaml'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/rpm/mapping/component_mapping.yaml'; specified in the package manifest cannot be found.; ```; (there are many similar CondaVerificationErrors before this); Conda info:; ```; active environment : longshot; active env location : /home/pedge/anaconda3/envs/longshot; shell level : 2; user config file : /home/pedge/.condarc; populated config files : /home/pedge/.condarc; conda version : 4.6.9; conda-build version : 3.17.6; python version : 3.7.1.final.0; base environment : /home/pedge/anaconda3 (writable); channel URLs : https://conda.anaconda.org/conda-forge/linux-64; https://conda.anaconda.org/conda-forge/noarch; https://conda.anaconda.org/bioconda/linux-64; https://conda.anaconda.org/bioconda/noarch; https://repo.anaconda.com/pkgs/main/linux-64; https://repo.anaconda.com/pkgs/main/noarch; https://repo.anaconda.com/pkgs/free/linux-64; https://repo.anaconda.com/pkgs/free/noarch; https://repo.anaconda.com/pkgs/r/linux-64; https://repo.anaconda.com/pkgs/r/noarch; https://conda.anaconda.org/OpenMDAO/linux-64; https://conda.anaconda.org/OpenMDAO/noarch; package cache : /home/pedge/anaconda3/pkgs; /home/pedge/.conda/pkgs; envs directories : /home/pedge/anaconda3/envs; /home/pedge/.conda/envs; platform : linux-64; user-agent : conda/4.6.9 requests/2.21.0 CPython/3.7.1 Linux/2.6.32-696.18.7.el6.x86_64 centos/6.6 glibc/2.12; UID:GID : 511626:8162; netrc file : None; offline mode : False. ```,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177
https://github.com/google/deepvariant/issues/178:367,Deployability,release,release,367,"I'm opening another issue to follow up on:; https://github.com/google/deepvariant/issues/132#issuecomment-482956117; (The original thread is getting a bit too long, and this issue seems more specific). Relevant system information in another comment:; https://github.com/google/deepvariant/issues/132#issuecomment-483551683; ""The super computer has an OS CentOS Linux release 7.6.1810 (LSB Version: core-4.1-amd64:core-4.1-noarch) and singularity version 2.5.2; I created the image on Amazon instance with Ubuntu 16.04. I tried using singularity version 2.5.2 & 2.6.0 but both did not help"". Adding @drtamermansour here as well.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/178
https://github.com/google/deepvariant/issues/179:148,Availability,fault,fault,148,"Hey guys,. I got deepvariant installed with conda fine, but my run failed by needing glibc, when I installed glibc all processes get a segmentation fault, if I remove glibc it works until failing needing the dependency. Any advice? We can't install docker images on our HPC, I haven't tried converting docker to singularity, as I have no experience with that but I'm comfortable compiling from source, but couldn't find the right files/instructions. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/179
https://github.com/google/deepvariant/issues/179:29,Deployability,install,installed,29,"Hey guys,. I got deepvariant installed with conda fine, but my run failed by needing glibc, when I installed glibc all processes get a segmentation fault, if I remove glibc it works until failing needing the dependency. Any advice? We can't install docker images on our HPC, I haven't tried converting docker to singularity, as I have no experience with that but I'm comfortable compiling from source, but couldn't find the right files/instructions. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/179
https://github.com/google/deepvariant/issues/179:99,Deployability,install,installed,99,"Hey guys,. I got deepvariant installed with conda fine, but my run failed by needing glibc, when I installed glibc all processes get a segmentation fault, if I remove glibc it works until failing needing the dependency. Any advice? We can't install docker images on our HPC, I haven't tried converting docker to singularity, as I have no experience with that but I'm comfortable compiling from source, but couldn't find the right files/instructions. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/179
https://github.com/google/deepvariant/issues/179:241,Deployability,install,install,241,"Hey guys,. I got deepvariant installed with conda fine, but my run failed by needing glibc, when I installed glibc all processes get a segmentation fault, if I remove glibc it works until failing needing the dependency. Any advice? We can't install docker images on our HPC, I haven't tried converting docker to singularity, as I have no experience with that but I'm comfortable compiling from source, but couldn't find the right files/instructions. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/179
https://github.com/google/deepvariant/issues/179:208,Integrability,depend,dependency,208,"Hey guys,. I got deepvariant installed with conda fine, but my run failed by needing glibc, when I installed glibc all processes get a segmentation fault, if I remove glibc it works until failing needing the dependency. Any advice? We can't install docker images on our HPC, I haven't tried converting docker to singularity, as I have no experience with that but I'm comfortable compiling from source, but couldn't find the right files/instructions. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/179
https://github.com/google/deepvariant/issues/180:35,Performance,perform,performs,35,"I was wondering if how DeepVariant performs on short tandem repeats and small inversions has been characterized? . I imagine small inversions aren't a problem, but short tandem repeats might be. Also, how does it spell del in variants (where the sequence has been deleted and a new sequence has been inserted)?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180
https://github.com/google/deepvariant/issues/181:420,Availability,error,error,420,"Hello, . I working through the DeepVariant set up workflow under the docker umbrella, but the variable ""ref"" indicating the path of the reference .fasta is not responding properly. . sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}:/output"" \; > gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \ ; **<cmd-b>FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.</cmd-b>**; Pass --helpshort or --helpfull to see help on flags.; shaba033$ <cmd-b> --ref=/input/ucsc.hg19.chr20.unittest.fasta </cmd-b>\; > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=4; -bash: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/181
https://github.com/google/deepvariant/issues/181:94,Modifiability,variab,variable,94,"Hello, . I working through the DeepVariant set up workflow under the docker umbrella, but the variable ""ref"" indicating the path of the reference .fasta is not responding properly. . sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}:/output"" \; > gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \ ; **<cmd-b>FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.</cmd-b>**; Pass --helpshort or --helpfull to see help on flags.; shaba033$ <cmd-b> --ref=/input/ucsc.hg19.chr20.unittest.fasta </cmd-b>\; > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=4; -bash: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/181
https://github.com/google/deepvariant/issues/183:332,Safety,detect,detection,332,"Dear DeepVariant team,; congrats for this excellent software. Are there any plans to train/offer DeepVariant (or a fork) for haploid (bacterial) genomes? . As the diploid results are fantastic, I think this might be a very helpful and valuable contribution to the microbial bioinformatics community as well!. In particular, for SNP detection in bacterial pathogens this might be very interesting!. Best regards!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/183
https://github.com/google/deepvariant/issues/184:13,Availability,error,error,13,"Hi, I got an error when running ; `( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21 chr22'"" \; ) >""${LOG_DIR}/validation_set.with_label.make_examples.log"" 2>&1`. the log message in validation_set.with_label.make_examples.log is as below:. `[E::hts_open_format] Failed to open file /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184
https://github.com/google/deepvariant/issues/184:611,Integrability,message,message,611,"Hi, I got an error when running ; `( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21 chr22'"" \; ) >""${LOG_DIR}/validation_set.with_label.make_examples.log"" 2>&1`. the log message in validation_set.with_label.make_examples.log is as below:. `[E::hts_open_format] Failed to open file /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184
https://github.com/google/deepvariant/issues/184:3501,Modifiability,variab,variables,3501,"ve_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam; parallel: This job failed:; sudo docker run -v /home/root:/home/root gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/make_examples --mode training --ref /home/chenyangwang600/training-case-study/input/data/ucsc_hg19.fa --reads /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam --examples /home/chenyangwang600/training-case-study/output/validation_set.with_label.tfrecord@8.gz --truth_variants /home/chenyangwang600/training-case-study/input/data/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz --confident_regions /home/chenyangwang600/training-case-study/input/data/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed --task 1 --regions 'chr21 chr22'. real 0m4.444s; user 0m0.318s; sys 0m0.216s`. I thought I followed the instructions in the guide(Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data) except that I used a 8vCPUs with ; `gcloud beta compute instances create ""cpu-eight"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-8"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""`. and set variables; N_SHARDS=""8"". I tried to use another VM but also failed. How can I solve this issue?. Thanks,; Yang",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184
https://github.com/google/deepvariant/issues/184:108,Testability,log,log,108,"Hi, I got an error when running ; `( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21 chr22'"" \; ) >""${LOG_DIR}/validation_set.with_label.make_examples.log"" 2>&1`. the log message in validation_set.with_label.make_examples.log is as below:. `[E::hts_open_format] Failed to open file /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184
https://github.com/google/deepvariant/issues/184:591,Testability,log,log,591,"Hi, I got an error when running ; `( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21 chr22'"" \; ) >""${LOG_DIR}/validation_set.with_label.make_examples.log"" 2>&1`. the log message in validation_set.with_label.make_examples.log is as below:. `[E::hts_open_format] Failed to open file /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184
https://github.com/google/deepvariant/issues/184:607,Testability,log,log,607,"Hi, I got an error when running ; `( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21 chr22'"" \; ) >""${LOG_DIR}/validation_set.with_label.make_examples.log"" 2>&1`. the log message in validation_set.with_label.make_examples.log is as below:. `[E::hts_open_format] Failed to open file /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184
https://github.com/google/deepvariant/issues/184:662,Testability,log,log,662,"Hi, I got an error when running ; `( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21 chr22'"" \; ) >""${LOG_DIR}/validation_set.with_label.make_examples.log"" 2>&1`. the log message in validation_set.with_label.make_examples.log is as below:. `[E::hts_open_format] Failed to open file /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184
https://github.com/google/deepvariant/issues/184:3057,Usability,guid,guide,3057,"ve_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam; parallel: This job failed:; sudo docker run -v /home/root:/home/root gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/make_examples --mode training --ref /home/chenyangwang600/training-case-study/input/data/ucsc_hg19.fa --reads /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam --examples /home/chenyangwang600/training-case-study/output/validation_set.with_label.tfrecord@8.gz --truth_variants /home/chenyangwang600/training-case-study/input/data/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz --confident_regions /home/chenyangwang600/training-case-study/input/data/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed --task 1 --regions 'chr21 chr22'. real 0m4.444s; user 0m0.318s; sys 0m0.216s`. I thought I followed the instructions in the guide(Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data) except that I used a 8vCPUs with ; `gcloud beta compute instances create ""cpu-eight"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-8"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""`. and set variables; N_SHARDS=""8"". I tried to use another VM but also failed. How can I solve this issue?. Thanks,; Yang",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184
https://github.com/google/deepvariant/issues/185:164,Energy Efficiency,power,power,164,"I'm trying to fine tune the original DeepVariant model with some extra data.; However, during the fine tuning process, the model suddenly losses all its predictive power in the first 10000 or 20000 steps. The call-variants output of these models are like all sites have homo-alt variants with a same qual value, 6.8 for example.; The sudden change in the model happens in the first step of fine tuning, as the saved model.ckpt-0 in the begining already gives the above output.; I find this result quite confusing, as the loss should increase dramatically. Is that a normal output of the fine tuning process?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185
https://github.com/google/deepvariant/issues/185:19,Performance,tune,tune,19,"I'm trying to fine tune the original DeepVariant model with some extra data.; However, during the fine tuning process, the model suddenly losses all its predictive power in the first 10000 or 20000 steps. The call-variants output of these models are like all sites have homo-alt variants with a same qual value, 6.8 for example.; The sudden change in the model happens in the first step of fine tuning, as the saved model.ckpt-0 in the begining already gives the above output.; I find this result quite confusing, as the loss should increase dramatically. Is that a normal output of the fine tuning process?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185
https://github.com/google/deepvariant/issues/185:153,Safety,predict,predictive,153,"I'm trying to fine tune the original DeepVariant model with some extra data.; However, during the fine tuning process, the model suddenly losses all its predictive power in the first 10000 or 20000 steps. The call-variants output of these models are like all sites have homo-alt variants with a same qual value, 6.8 for example.; The sudden change in the model happens in the first step of fine tuning, as the saved model.ckpt-0 in the begining already gives the above output.; I find this result quite confusing, as the loss should increase dramatically. Is that a normal output of the fine tuning process?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185
https://github.com/google/deepvariant/issues/189:178,Availability,ERROR,ERROR,178,"Hello, I am attempting to compile deepvariant from source. ; running ; `./build-prereq.sh; `; returns; ```; Installing numpy with -no-binary=:all:. This will take a bit longer.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ERROR: silico 1.0.1 has requirement pysam==0.8.4, but you'll have pysam 0.15.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/189
https://github.com/google/deepvariant/issues/189:296,Availability,ERROR,ERROR,296,"Hello, I am attempting to compile deepvariant from source. ; running ; `./build-prereq.sh; `; returns; ```; Installing numpy with -no-binary=:all:. This will take a bit longer.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ERROR: silico 1.0.1 has requirement pysam==0.8.4, but you'll have pysam 0.15.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/189
https://github.com/google/deepvariant/issues/189:415,Availability,ERROR,ERROR,415,"Hello, I am attempting to compile deepvariant from source. ; running ; `./build-prereq.sh; `; returns; ```; Installing numpy with -no-binary=:all:. This will take a bit longer.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ERROR: silico 1.0.1 has requirement pysam==0.8.4, but you'll have pysam 0.15.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/189
https://github.com/google/deepvariant/issues/189:518,Availability,ERROR,ERROR,518,"Hello, I am attempting to compile deepvariant from source. ; running ; `./build-prereq.sh; `; returns; ```; Installing numpy with -no-binary=:all:. This will take a bit longer.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ERROR: silico 1.0.1 has requirement pysam==0.8.4, but you'll have pysam 0.15.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/189
https://github.com/google/deepvariant/issues/189:636,Availability,ERROR,ERROR,636,"Hello, I am attempting to compile deepvariant from source. ; running ; `./build-prereq.sh; `; returns; ```; Installing numpy with -no-binary=:all:. This will take a bit longer.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ERROR: silico 1.0.1 has requirement pysam==0.8.4, but you'll have pysam 0.15.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/189
https://github.com/google/deepvariant/issues/189:896,Availability,ERROR,ERROR,896,"Hello, I am attempting to compile deepvariant from source. ; running ; `./build-prereq.sh; `; returns; ```; Installing numpy with -no-binary=:all:. This will take a bit longer.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ERROR: silico 1.0.1 has requirement pysam==0.8.4, but you'll have pysam 0.15.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/189
https://github.com/google/deepvariant/issues/189:1023,Availability,ERROR,ERROR,1023,"pile deepvariant from source. ; running ; `./build-prereq.sh; `; returns; ```; Installing numpy with -no-binary=:all:. This will take a bit longer.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ERROR: silico 1.0.1 has requirement pysam==0.8.4, but you'll have pysam 0.15.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/189
https://github.com/google/deepvariant/issues/189:1204,Availability,ERROR,ERROR,1204,"quirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ERROR: silico 1.0.1 has requirement pysam==0.8.4, but you'll have pysam 0.15.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/third_party/nucleus/io/python/_objs/hts_verbose_cclib/hts_verbose.pic.d '-frandom-seed=bazel-out/k8-opt/bin/third_party/nucleus/io/python/_objs/hts_verbose_cclib/hts_verbose.pic.o' -fPIC -",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/189
https://github.com/google/deepvariant/issues/189:1428,Availability,error,error,1428,"quirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ERROR: silico 1.0.1 has requirement pysam==0.8.4, but you'll have pysam 0.15.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/third_party/nucleus/io/python/_objs/hts_verbose_cclib/hts_verbose.pic.d '-frandom-seed=bazel-out/k8-opt/bin/third_party/nucleus/io/python/_objs/hts_verbose_cclib/hts_verbose.pic.o' -fPIC -",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/189
https://github.com/google/deepvariant/issues/189:4324,Availability,error,error,4324,"isystem bazel-out/k8-opt/bin/external/local_config_python/python_include -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/k8-opt/bin/external/protobuf_archive/src -Wno-maybe-uninitialized -Wno-unused-function '-march=corei7' -Wno-sign-compare -Wno-write-strings '-std=c++11' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c bazel-out/k8-opt/genfiles/third_party/nucleus/io/python/hts_verbose.cc -o bazel-out/k8-opt/bin/third_party/nucleus/io/python/_objs/hts_verbose_cclib/hts_verbose.pic.o); Execution platform: @bazel_tools//platforms:host_platform; bazel-out/k8-opt/genfiles/third_party/nucleus/io/python/hts_verbose.cc: In function 'PyObject* third__party_nucleus_io_python_hts__verbose_clifwrap::Init()':; bazel-out/k8-opt/genfiles/third_party/nucleus/io/python/hts_verbose.cc:134:22: error: 'Py_InitModule3' was not declared in this scope; PyObject* module = Py_InitModule3(""third_party.nucleus.io.python.hts_verbose"", Methods, ""CLIF-generated module for third_party/nucleus/io/hts_verbose.h"");; ^~~~~~~~~~~~~~; bazel-out/k8-opt/genfiles/third_party/nucleus/io/python/hts_verbose.cc:134:22: note: suggested alternative: 'Py_Initialize'; PyObject* module = Py_InitModule3(""third_party.nucleus.io.python.hts_verbose"", Methods, ""CLIF-generated module for third_party/nucleus/io/hts_verbose.h"");; ^~~~~~~~~~~~~~; Py_Initialize; (13:01:20) INFO: Elapsed time: 180.151s, Critical Path: 34.76s; (13:01:20) INFO: 1380 processes: 1380 local.; (13:01:20) FAILED: Build did NOT complete successfully; //deepvariant:allelecounter_test NO STATUS; //deepvariant:call_variants_test NO STATUS; //deepvariant:data_providers_test NO STATUS; //deepvariant:dv_vcf_constants_test NO STATUS; //deepvariant:exclude_contigs_test NO STATUS; //deepvariant:haplotypes_test NO STATUS; //deepvariant:make_examples_test NO STATUS; //deepvariant:model_eval_test",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/189
https://github.com/google/deepvariant/issues/189:108,Deployability,Install,Installing,108,"Hello, I am attempting to compile deepvariant from source. ; running ; `./build-prereq.sh; `; returns; ```; Installing numpy with -no-binary=:all:. This will take a bit longer.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ERROR: silico 1.0.1 has requirement pysam==0.8.4, but you'll have pysam 0.15.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/189
https://github.com/google/deepvariant/issues/189:804,Deployability,Install,Install,804,"Hello, I am attempting to compile deepvariant from source. ; running ; `./build-prereq.sh; `; returns; ```; Installing numpy with -no-binary=:all:. This will take a bit longer.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ERROR: silico 1.0.1 has requirement pysam==0.8.4, but you'll have pysam 0.15.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/189
https://github.com/google/deepvariant/issues/189:846,Deployability,Install,Installing,846,"Hello, I am attempting to compile deepvariant from source. ; running ; `./build-prereq.sh; `; returns; ```; Installing numpy with -no-binary=:all:. This will take a bit longer.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ERROR: silico 1.0.1 has requirement pysam==0.8.4, but you'll have pysam 0.15.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/189
https://github.com/google/deepvariant/issues/189:1470,Performance,cache,cache,1470,"14.5,>=1.13.3, but you'll have numpy 1.16.0 which is incompatible.; ERROR: tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 40.8.0 which is incompatible.; ========== [Di Jun 18 12:55:53 CEST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ERROR: keras 2.2.2 has requirement keras_applications==1.0.4, but you'll have keras-applications 1.0.8 which is incompatible.; ERROR: keras 2.2.2 has requirement keras_preprocessing==1.0.2, but you'll have keras-preprocessing 1.1.0 which is incompatible. ```; And then ; `./build_and_test.sh`; returns; ```; ERROR: /media/urbe/MyBDrive/12-06-2019_masurca_instaGRAAL_final/deepvariant/third_party/nucleus/io/python/BUILD:309:1: C++ compilation of rule '//third_party/nucleus/io/python:hts_verbose_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /home/urbe/.cache/bazel/_bazel_urbe/83a209cfb2bd2efbd35b40f0662be001/execroot/com_google_deepvariant && \; exec env - \; PATH=/bin:/usr/bin \; PWD=/proc/self/cwd \; PYTHONPATH=/home/urbe/Tools/MARVEL/bin/lib.python:/usr/local/lib.python: \; PYTHON_BIN_PATH=/home/urbe/anaconda3/bin/python \; PYTHON_LIB_PATH=/home/urbe/Tools/MARVEL/bin/lib.python \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; TF_NEED_ROCM=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/third_party/nucleus/io/python/_objs/hts_verbose_cclib/hts_verbose.pic.d '-frandom-seed=bazel-out/k8-opt/bin/third_party/nucleus/io/python/_objs/hts_verbose_cclib/hts_verbose.pic.o' -fPIC -iquote . -iquote bazel-out/k8-opt/genfiles -iquote bazel-out/k8-opt/bin -iquote external/htslib -iquote bazel-out/k8-opt/genfiles/external/htslib -iquote bazel-out/k8-opt/bin/external/htslib -iquote external/clif -iquote bazel-out/k8-opt/ge",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/189
https://github.com/google/deepvariant/issues/190:1570,Availability,error,error,1570," of DeepVariant from docker I get. ```; I0618 16:51:26.002005 140449503209216 make_examples.py:1116] Writing examples to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00000-of-00048.gz; I0618 16:51:26.059495 140449503209216 make_examples.py:1120] Writing gvcf records to /tmp/deepvariant_tmp_output/gvcf.tfrecord-00000-of-00048.gz; 2019-06-18 16:51:26.064354: I third_party/nucleus/io/sam_reader.cc:600] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-06-18 16:51:26.183271: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0618 16:51:26.184211 140449503209216 genomics_reader.py:218] Reading /input/out.bam with NativeSamReader; I0618 16:51:26.728389 140449503209216 make_examples.py:1149] Task 0: 0 candidates (0 examples) [0.67s elapsed]; I0618 16:51:53.064081 140449503209216 make_examples.py:1149] Task 0: 119 candidates (119 examples) [26.34s elapsed]; I0618 16:52:15.696561 140449503209216 make_examples.py:1149] Task 0: 230 candidates (230 examples) [22.63s elapsed]; I0618 16:52:28.003529 140449503209216 make_examples.py:1149] Task 0: 346 candidates (346 examples) [12.31s elapsed]; I0618 16:52:33.347760 140449503209216 make_examples.py:1149] Task 0: 438 candidates (438 examples) [5.34s elapsed]; I0618 16:52:39.804543 140449503209216 make_examples.py:1149] Task 0: 501 candidates (501 examples) [6.46s elapsed]; I0618 16:53:01.098323 140449503209216 make_examples.py:1149] Task 0: 606 candidates (606 examples) [21.29s elapsed]. ```; It seems to be running anyway, is it a benign error? ; Here is the command I used. ```; sudo docker run \ ; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/Avaga.Masurca.Graal.l5_n100_c1_N5_unpolished.fasta.sorted.min1000.renamed \; --reads=/input/out.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=48; ```; Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/190
https://github.com/google/deepvariant/issues/191:339,Availability,error,error,339,"Hi,. If I run the following command:; ```bash; dv_make_examples.py \; --cores 16 \; --sample ISO_349.bam \; --ref PlasmoDB-41_Pfalciparum3D7_Genome.fasta.gz \; --reads ISO_349.bam \; --regions PlasmoDB-41_Pfalciparum3D7_Genome_ISO_349.per-base.bed.gz \; --logdir logs \; --examples ISO_349_shardedExamples; ```. I get the following output/error:; ```bash; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s 2019-06-13 12:05:15.630247: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I0613 12:05:15.631112 47747793808000 genomics_reader.py:213] Reading ISO_349.bam with NativeSamReader; Traceback (most recent call last):; File ""/state/part",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191
https://github.com/google/deepvariant/issues/191:4164,Deployability,pipeline,pipeline,4164,"ke_examples.py"", line 1074, in make_examples_runner; resource_monitor = resources.ResourceMonitor().start(); File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 59, in __init__; self.metrics_pb = self._initial_metrics_protobuf(); File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 72, in _initial_metrics_protobuf; cpu_frequency_mhz=_get_cpu_frequency(),; File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 158, in _get_cpu_frequency; freq = psutil.cpu_freq(); File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/__init__.py"", line 1853, in cpu_freq; ret = _psplatform.cpu_freq(); File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); NotImplementedError: can't find current frequency file; parallel: This job failed:; /opt/conda/envs/nf-core-deepvariant-1.0/bin/python /opt/conda/envs/nf-core-deepvariant-1.0/share/deepvariant-0.7.0-0/binaries/DeepVariant/0.7.0/DeepVariant-0.7.0+cl-208818123/make_examples.zip --mode calling --ref PlasmoDB-41_Pfalciparum3D7_Genome.fasta.gz --reads ISO_349.bam --regions PlasmoDB-41_Pfalciparum3D7_Genome_ISO_349.per-base.bed.gz --examples ISO_349_shardedExamples/ISO_349.bam.tfrecord@16.gz --task 10; ```. This is using the Nextflow wrapper script [nf-core/deepvariant](https://github.com/nf-core/deepvariant). The pipeline works with other input data. So it seems unlikely that it is a problem with installation or parameters. DeepVariant was installed using Conda & is using v0.7.0. . Any ideas what the problem is? . Could it be a problem to do with the input data? Perhaps the organism being used? What is the frequency file which it is referring to?. Any help would be much appreciated. Many thanks in advance,; Phil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191
https://github.com/google/deepvariant/issues/191:4249,Deployability,install,installation,4249,"ke_examples.py"", line 1074, in make_examples_runner; resource_monitor = resources.ResourceMonitor().start(); File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 59, in __init__; self.metrics_pb = self._initial_metrics_protobuf(); File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 72, in _initial_metrics_protobuf; cpu_frequency_mhz=_get_cpu_frequency(),; File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 158, in _get_cpu_frequency; freq = psutil.cpu_freq(); File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/__init__.py"", line 1853, in cpu_freq; ret = _psplatform.cpu_freq(); File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); NotImplementedError: can't find current frequency file; parallel: This job failed:; /opt/conda/envs/nf-core-deepvariant-1.0/bin/python /opt/conda/envs/nf-core-deepvariant-1.0/share/deepvariant-0.7.0-0/binaries/DeepVariant/0.7.0/DeepVariant-0.7.0+cl-208818123/make_examples.zip --mode calling --ref PlasmoDB-41_Pfalciparum3D7_Genome.fasta.gz --reads ISO_349.bam --regions PlasmoDB-41_Pfalciparum3D7_Genome_ISO_349.per-base.bed.gz --examples ISO_349_shardedExamples/ISO_349.bam.tfrecord@16.gz --task 10; ```. This is using the Nextflow wrapper script [nf-core/deepvariant](https://github.com/nf-core/deepvariant). The pipeline works with other input data. So it seems unlikely that it is a problem with installation or parameters. DeepVariant was installed using Conda & is using v0.7.0. . Any ideas what the problem is? . Could it be a problem to do with the input data? Perhaps the organism being used? What is the frequency file which it is referring to?. Any help would be much appreciated. Many thanks in advance,; Phil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191
https://github.com/google/deepvariant/issues/191:4293,Deployability,install,installed,4293,"ke_examples.py"", line 1074, in make_examples_runner; resource_monitor = resources.ResourceMonitor().start(); File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 59, in __init__; self.metrics_pb = self._initial_metrics_protobuf(); File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 72, in _initial_metrics_protobuf; cpu_frequency_mhz=_get_cpu_frequency(),; File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 158, in _get_cpu_frequency; freq = psutil.cpu_freq(); File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/__init__.py"", line 1853, in cpu_freq; ret = _psplatform.cpu_freq(); File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); NotImplementedError: can't find current frequency file; parallel: This job failed:; /opt/conda/envs/nf-core-deepvariant-1.0/bin/python /opt/conda/envs/nf-core-deepvariant-1.0/share/deepvariant-0.7.0-0/binaries/DeepVariant/0.7.0/DeepVariant-0.7.0+cl-208818123/make_examples.zip --mode calling --ref PlasmoDB-41_Pfalciparum3D7_Genome.fasta.gz --reads ISO_349.bam --regions PlasmoDB-41_Pfalciparum3D7_Genome_ISO_349.per-base.bed.gz --examples ISO_349_shardedExamples/ISO_349.bam.tfrecord@16.gz --task 10; ```. This is using the Nextflow wrapper script [nf-core/deepvariant](https://github.com/nf-core/deepvariant). The pipeline works with other input data. So it seems unlikely that it is a problem with installation or parameters. DeepVariant was installed using Conda & is using v0.7.0. . Any ideas what the problem is? . Could it be a problem to do with the input data? Perhaps the organism being used? What is the frequency file which it is referring to?. Any help would be much appreciated. Many thanks in advance,; Phil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191
https://github.com/google/deepvariant/issues/191:4082,Integrability,wrap,wrapper,4082,"ke_examples.py"", line 1074, in make_examples_runner; resource_monitor = resources.ResourceMonitor().start(); File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 59, in __init__; self.metrics_pb = self._initial_metrics_protobuf(); File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 72, in _initial_metrics_protobuf; cpu_frequency_mhz=_get_cpu_frequency(),; File ""/state/partition1/job-1690025/Bazel.runfiles_ISl2VA/runfiles/com_google_deepvariant/deepvariant/resources.py"", line 158, in _get_cpu_frequency; freq = psutil.cpu_freq(); File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/__init__.py"", line 1853, in cpu_freq; ret = _psplatform.cpu_freq(); File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); NotImplementedError: can't find current frequency file; parallel: This job failed:; /opt/conda/envs/nf-core-deepvariant-1.0/bin/python /opt/conda/envs/nf-core-deepvariant-1.0/share/deepvariant-0.7.0-0/binaries/DeepVariant/0.7.0/DeepVariant-0.7.0+cl-208818123/make_examples.zip --mode calling --ref PlasmoDB-41_Pfalciparum3D7_Genome.fasta.gz --reads ISO_349.bam --regions PlasmoDB-41_Pfalciparum3D7_Genome_ISO_349.per-base.bed.gz --examples ISO_349_shardedExamples/ISO_349.bam.tfrecord@16.gz --task 10; ```. This is using the Nextflow wrapper script [nf-core/deepvariant](https://github.com/nf-core/deepvariant). The pipeline works with other input data. So it seems unlikely that it is a problem with installation or parameters. DeepVariant was installed using Conda & is using v0.7.0. . Any ideas what the problem is? . Could it be a problem to do with the input data? Perhaps the organism being used? What is the frequency file which it is referring to?. Any help would be much appreciated. Many thanks in advance,; Phil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191
https://github.com/google/deepvariant/issues/191:256,Testability,log,logdir,256,"Hi,. If I run the following command:; ```bash; dv_make_examples.py \; --cores 16 \; --sample ISO_349.bam \; --ref PlasmoDB-41_Pfalciparum3D7_Genome.fasta.gz \; --reads ISO_349.bam \; --regions PlasmoDB-41_Pfalciparum3D7_Genome_ISO_349.per-base.bed.gz \; --logdir logs \; --examples ISO_349_shardedExamples; ```. I get the following output/error:; ```bash; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s 2019-06-13 12:05:15.630247: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I0613 12:05:15.631112 47747793808000 genomics_reader.py:213] Reading ISO_349.bam with NativeSamReader; Traceback (most recent call last):; File ""/state/part",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191
https://github.com/google/deepvariant/issues/191:263,Testability,log,logs,263,"Hi,. If I run the following command:; ```bash; dv_make_examples.py \; --cores 16 \; --sample ISO_349.bam \; --ref PlasmoDB-41_Pfalciparum3D7_Genome.fasta.gz \; --reads ISO_349.bam \; --regions PlasmoDB-41_Pfalciparum3D7_Genome_ISO_349.per-base.bed.gz \; --logdir logs \; --examples ISO_349_shardedExamples; ```. I get the following output/error:; ```bash; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s 2019-06-13 12:05:15.630247: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I0613 12:05:15.631112 47747793808000 genomics_reader.py:213] Reading ISO_349.bam with NativeSamReader; Traceback (most recent call last):; File ""/state/part",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191
https://github.com/google/deepvariant/issues/193:365,Deployability,pipeline,pipeline,365,"I'm trying to output gVCF's via DeepVariant as described by the tutorial here: https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-gvcf-support.md . Is there a way that I could just modify the gcp_deepvariant_runner.py script (linked below) rather than having to manually run the commands step by step? I have many BAM files to process and running the pipeline manually is intractable. https://github.com/googlegenomics/gcp-deepvariant-runner/blob/master/gcp_deepvariant_runner.py. I'm guessing I would need to fork the gcp-deepvariant-runner repo, edit the python file, then push the new repo to some sort of container registry? Any guidance here would be much appreciated.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/193
https://github.com/google/deepvariant/issues/193:647,Usability,guid,guidance,647,"I'm trying to output gVCF's via DeepVariant as described by the tutorial here: https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-gvcf-support.md . Is there a way that I could just modify the gcp_deepvariant_runner.py script (linked below) rather than having to manually run the commands step by step? I have many BAM files to process and running the pipeline manually is intractable. https://github.com/googlegenomics/gcp-deepvariant-runner/blob/master/gcp_deepvariant_runner.py. I'm guessing I would need to fork the gcp-deepvariant-runner repo, edit the python file, then push the new repo to some sort of container registry? Any guidance here would be much appreciated.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/193
https://github.com/google/deepvariant/issues/194:194,Performance,perform,performance,194,"I'm trying to train a deepvariant model with a very simple topology.; After a few thousands of training steps, the logged training loss starts to vibrate around a rather high value. However the performance of saved models still keeps improving on my validation data set.; Why does this happen?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/194
https://github.com/google/deepvariant/issues/194:250,Security,validat,validation,250,"I'm trying to train a deepvariant model with a very simple topology.; After a few thousands of training steps, the logged training loss starts to vibrate around a rather high value. However the performance of saved models still keeps improving on my validation data set.; Why does this happen?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/194
https://github.com/google/deepvariant/issues/194:115,Testability,log,logged,115,"I'm trying to train a deepvariant model with a very simple topology.; After a few thousands of training steps, the logged training loss starts to vibrate around a rather high value. However the performance of saved models still keeps improving on my validation data set.; Why does this happen?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/194
https://github.com/google/deepvariant/issues/194:52,Usability,simpl,simple,52,"I'm trying to train a deepvariant model with a very simple topology.; After a few thousands of training steps, the logged training loss starts to vibrate around a rather high value. However the performance of saved models still keeps improving on my validation data set.; Why does this happen?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/194
https://github.com/google/deepvariant/issues/196:15,Deployability,pipeline,pipeline,15,When I run the pipeline on females I get lots of PASSED variants on chrY. Why is that?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/196
https://github.com/google/deepvariant/issues/199:161,Availability,down,downloaded,161,"Hi! I compiled from source on Ubuntu 14.4. . Tests pass, including `./bazel-bin/deepvariant/make_examples_test`. However, running as simple `make_examples` with downloaded example data fails with confusing (for me) error in realign script. Same script worked fine when I ran pre-compiled binary from the Docker container . ```; input=""./quickstart-testdata"". ./bazel-bin/deepvariant/make_examples \; --ref=$input/ucsc.hg19.chr20.unittest.fasta \; --reads=$input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --examples examples.tfrecord@1.gz \; --mode calling \; --logging_every_n_candidates 10 \; --realign_reads; ```. ```; ./make_examples_demo.sh ; 2019-07-16 17:49:02.877175: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:02.877284 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.117142 139897470359360 make_examples.py:1110] Preparing inputs; 2019-07-16 17:49:03.117644: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.117749 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.118745 139897470359360 make_examples.py:1034] Common contigs are [u'chr20']; I0716 17:49:03.120177 139897470359360 make_examples.py:1116] Writing examples to examples.tfrecord-00000-of-00001.gz; 2019-07-16 17:49:03.121118: I third_party/nucleus/io/sam_reader.cc:600] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-07-16 17:49:03.124279: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.124422 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199
https://github.com/google/deepvariant/issues/199:215,Availability,error,error,215,"Hi! I compiled from source on Ubuntu 14.4. . Tests pass, including `./bazel-bin/deepvariant/make_examples_test`. However, running as simple `make_examples` with downloaded example data fails with confusing (for me) error in realign script. Same script worked fine when I ran pre-compiled binary from the Docker container . ```; input=""./quickstart-testdata"". ./bazel-bin/deepvariant/make_examples \; --ref=$input/ucsc.hg19.chr20.unittest.fasta \; --reads=$input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --examples examples.tfrecord@1.gz \; --mode calling \; --logging_every_n_candidates 10 \; --realign_reads; ```. ```; ./make_examples_demo.sh ; 2019-07-16 17:49:02.877175: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:02.877284 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.117142 139897470359360 make_examples.py:1110] Preparing inputs; 2019-07-16 17:49:03.117644: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.117749 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.118745 139897470359360 make_examples.py:1034] Common contigs are [u'chr20']; I0716 17:49:03.120177 139897470359360 make_examples.py:1116] Writing examples to examples.tfrecord-00000-of-00001.gz; 2019-07-16 17:49:03.121118: I third_party/nucleus/io/sam_reader.cc:600] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-07-16 17:49:03.124279: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.124422 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199
https://github.com/google/deepvariant/issues/199:3013,Modifiability,config,config,3013,"epvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/home/nyakovenko/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1225, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1127, in make_examples_runner; candidates, examples, gvcfs = region_processor.process(region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 849, in process; self.in_memory_sam_reader.replace_reads(self.region_reads(region)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 889, in region_reads; _, reads = self.realigner.realign_reads(reads, region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 606, in realign_reads; self.config.ws_config, self.ref_reader, reads, region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 232, in select_windows; candidates = _candidates_from_reads(config, ref_reader, reads, region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 84, in _candidates_from_reads; region, expanded_region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 152, in _allele_count_linear_selector; allele_counter, model_conf)); TypeError: allele_count_linear_candidates_from_allele_counter() argument counter is not valid for ::learning::genomics::deepvariant::AlleleCounter (deepvariant.python.allelecounter.AlleleCounter instance given): expecting deepvariant.python.allelecounter.AlleleCounter instance, got deepvariant.python.allelecounter.AlleleCounter instance; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199
https://github.com/google/deepvariant/issues/199:3237,Modifiability,config,config,3237,"epvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/home/nyakovenko/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1225, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1127, in make_examples_runner; candidates, examples, gvcfs = region_processor.process(region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 849, in process; self.in_memory_sam_reader.replace_reads(self.region_reads(region)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 889, in region_reads; _, reads = self.realigner.realign_reads(reads, region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 606, in realign_reads; self.config.ws_config, self.ref_reader, reads, region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 232, in select_windows; candidates = _candidates_from_reads(config, ref_reader, reads, region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 84, in _candidates_from_reads; region, expanded_region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 152, in _allele_count_linear_selector; allele_counter, model_conf)); TypeError: allele_count_linear_candidates_from_allele_counter() argument counter is not valid for ::learning::genomics::deepvariant::AlleleCounter (deepvariant.python.allelecounter.AlleleCounter instance given): expecting deepvariant.python.allelecounter.AlleleCounter instance, got deepvariant.python.allelecounter.AlleleCounter instance; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199
https://github.com/google/deepvariant/issues/199:45,Testability,Test,Tests,45,"Hi! I compiled from source on Ubuntu 14.4. . Tests pass, including `./bazel-bin/deepvariant/make_examples_test`. However, running as simple `make_examples` with downloaded example data fails with confusing (for me) error in realign script. Same script worked fine when I ran pre-compiled binary from the Docker container . ```; input=""./quickstart-testdata"". ./bazel-bin/deepvariant/make_examples \; --ref=$input/ucsc.hg19.chr20.unittest.fasta \; --reads=$input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --examples examples.tfrecord@1.gz \; --mode calling \; --logging_every_n_candidates 10 \; --realign_reads; ```. ```; ./make_examples_demo.sh ; 2019-07-16 17:49:02.877175: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:02.877284 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.117142 139897470359360 make_examples.py:1110] Preparing inputs; 2019-07-16 17:49:03.117644: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.117749 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.118745 139897470359360 make_examples.py:1034] Common contigs are [u'chr20']; I0716 17:49:03.120177 139897470359360 make_examples.py:1116] Writing examples to examples.tfrecord-00000-of-00001.gz; 2019-07-16 17:49:03.121118: I third_party/nucleus/io/sam_reader.cc:600] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-07-16 17:49:03.124279: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.124422 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199
https://github.com/google/deepvariant/issues/199:352,Testability,test,testdata,352,"Hi! I compiled from source on Ubuntu 14.4. . Tests pass, including `./bazel-bin/deepvariant/make_examples_test`. However, running as simple `make_examples` with downloaded example data fails with confusing (for me) error in realign script. Same script worked fine when I ran pre-compiled binary from the Docker container . ```; input=""./quickstart-testdata"". ./bazel-bin/deepvariant/make_examples \; --ref=$input/ucsc.hg19.chr20.unittest.fasta \; --reads=$input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --examples examples.tfrecord@1.gz \; --mode calling \; --logging_every_n_candidates 10 \; --realign_reads; ```. ```; ./make_examples_demo.sh ; 2019-07-16 17:49:02.877175: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:02.877284 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.117142 139897470359360 make_examples.py:1110] Preparing inputs; 2019-07-16 17:49:03.117644: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.117749 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.118745 139897470359360 make_examples.py:1034] Common contigs are [u'chr20']; I0716 17:49:03.120177 139897470359360 make_examples.py:1116] Writing examples to examples.tfrecord-00000-of-00001.gz; 2019-07-16 17:49:03.121118: I third_party/nucleus/io/sam_reader.cc:600] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-07-16 17:49:03.124279: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.124422 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199
https://github.com/google/deepvariant/issues/199:883,Testability,test,testdata,883,"Hi! I compiled from source on Ubuntu 14.4. . Tests pass, including `./bazel-bin/deepvariant/make_examples_test`. However, running as simple `make_examples` with downloaded example data fails with confusing (for me) error in realign script. Same script worked fine when I ran pre-compiled binary from the Docker container . ```; input=""./quickstart-testdata"". ./bazel-bin/deepvariant/make_examples \; --ref=$input/ucsc.hg19.chr20.unittest.fasta \; --reads=$input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --examples examples.tfrecord@1.gz \; --mode calling \; --logging_every_n_candidates 10 \; --realign_reads; ```. ```; ./make_examples_demo.sh ; 2019-07-16 17:49:02.877175: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:02.877284 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.117142 139897470359360 make_examples.py:1110] Preparing inputs; 2019-07-16 17:49:03.117644: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.117749 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.118745 139897470359360 make_examples.py:1034] Common contigs are [u'chr20']; I0716 17:49:03.120177 139897470359360 make_examples.py:1116] Writing examples to examples.tfrecord-00000-of-00001.gz; 2019-07-16 17:49:03.121118: I third_party/nucleus/io/sam_reader.cc:600] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-07-16 17:49:03.124279: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.124422 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199
https://github.com/google/deepvariant/issues/199:1221,Testability,test,testdata,1221,"ealign script. Same script worked fine when I ran pre-compiled binary from the Docker container . ```; input=""./quickstart-testdata"". ./bazel-bin/deepvariant/make_examples \; --ref=$input/ucsc.hg19.chr20.unittest.fasta \; --reads=$input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --examples examples.tfrecord@1.gz \; --mode calling \; --logging_every_n_candidates 10 \; --realign_reads; ```. ```; ./make_examples_demo.sh ; 2019-07-16 17:49:02.877175: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:02.877284 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.117142 139897470359360 make_examples.py:1110] Preparing inputs; 2019-07-16 17:49:03.117644: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.117749 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.118745 139897470359360 make_examples.py:1034] Common contigs are [u'chr20']; I0716 17:49:03.120177 139897470359360 make_examples.py:1116] Writing examples to examples.tfrecord-00000-of-00001.gz; 2019-07-16 17:49:03.121118: I third_party/nucleus/io/sam_reader.cc:600] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-07-16 17:49:03.124279: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.124422 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/home/nyakovenko/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199
https://github.com/google/deepvariant/issues/199:1803,Testability,test,testdata,1803,"7:49:02.877284 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.117142 139897470359360 make_examples.py:1110] Preparing inputs; 2019-07-16 17:49:03.117644: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.117749 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.118745 139897470359360 make_examples.py:1034] Common contigs are [u'chr20']; I0716 17:49:03.120177 139897470359360 make_examples.py:1116] Writing examples to examples.tfrecord-00000-of-00001.gz; 2019-07-16 17:49:03.121118: I third_party/nucleus/io/sam_reader.cc:600] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-07-16 17:49:03.124279: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.124422 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/home/nyakovenko/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1225, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1127, in make_examples_runner; candidates, examples, gvcfs = region_processor.process(region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 849, in process; self.in_memory_sam_reader.replace_reads(self.region_reads(region)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 889, in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199
https://github.com/google/deepvariant/issues/199:133,Usability,simpl,simple,133,"Hi! I compiled from source on Ubuntu 14.4. . Tests pass, including `./bazel-bin/deepvariant/make_examples_test`. However, running as simple `make_examples` with downloaded example data fails with confusing (for me) error in realign script. Same script worked fine when I ran pre-compiled binary from the Docker container . ```; input=""./quickstart-testdata"". ./bazel-bin/deepvariant/make_examples \; --ref=$input/ucsc.hg19.chr20.unittest.fasta \; --reads=$input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --examples examples.tfrecord@1.gz \; --mode calling \; --logging_every_n_candidates 10 \; --realign_reads; ```. ```; ./make_examples_demo.sh ; 2019-07-16 17:49:02.877175: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:02.877284 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.117142 139897470359360 make_examples.py:1110] Preparing inputs; 2019-07-16 17:49:03.117644: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.117749 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.118745 139897470359360 make_examples.py:1034] Common contigs are [u'chr20']; I0716 17:49:03.120177 139897470359360 make_examples.py:1116] Writing examples to examples.tfrecord-00000-of-00001.gz; 2019-07-16 17:49:03.121118: I third_party/nucleus/io/sam_reader.cc:600] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-07-16 17:49:03.124279: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.124422 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199
https://github.com/google/deepvariant/issues/199:3725,Usability,learn,learning,3725,"epvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/home/nyakovenko/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1225, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1127, in make_examples_runner; candidates, examples, gvcfs = region_processor.process(region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 849, in process; self.in_memory_sam_reader.replace_reads(self.region_reads(region)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 889, in region_reads; _, reads = self.realigner.realign_reads(reads, region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 606, in realign_reads; self.config.ws_config, self.ref_reader, reads, region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 232, in select_windows; candidates = _candidates_from_reads(config, ref_reader, reads, region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 84, in _candidates_from_reads; region, expanded_region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 152, in _allele_count_linear_selector; allele_counter, model_conf)); TypeError: allele_count_linear_candidates_from_allele_counter() argument counter is not valid for ::learning::genomics::deepvariant::AlleleCounter (deepvariant.python.allelecounter.AlleleCounter instance given): expecting deepvariant.python.allelecounter.AlleleCounter instance, got deepvariant.python.allelecounter.AlleleCounter instance; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199
https://github.com/google/deepvariant/issues/201:61,Availability,down,downloaded,61,"I ran an eval on chr20 of HG002 for PacBio CCS Sequel 2 data downloaded from https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/PacBio_SequelII_CCS_11kb/. The results are different from that given in the following blog:; https://google.github.io/deepvariant/posts/2019-01-14-highly-accurate-snp-and-indel-calling-on-pacbio-ccs-with-deepvariant/. Specifically, the indel results are improved by ~4% (F1 is 0.982) and SNV results are largely unchanged. I wonder what is the reason for the difference in the results. Are the two datasets (the one I mentioned, and the one in the blog) the same? If they are the same, has the model been improved since the publication of the blog?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/201
https://github.com/google/deepvariant/issues/202:2184,Usability,simpl,simple,2184,"5208 . C T 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:44:42:22,20:0.47619:44,0,56; Chrom_6 5425277 . G A 32 PASS . GT:GQ:DP:AD:VAF:PL 0/1:32:45:28,17:0.377778:31,0,57; Chrom_6 **5425401** . G T 30.1 PASS . GT:GQ:DP:AD:VAF:PL 0/1:30:60:46,14:0.233333:30,0,50; Chrom_6 **5425402** . G T 18.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:59:46,13:0.220339:18,0,47; Chrom_6 **5425403** . T C 13.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:14:59:46,13:0.220339:13,0,45; Chrom_6 5425421 . T C 6.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:7:63:50,13:0.206349:5,0,40; Chrom_6 5425460 . G A 8.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:8:61:48,12:0.196721:7,0,40; Chrom_6 5425539 . A C 14.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:15:45:36,9:0.2:14,0,45; Chrom_6 5425555 . G A 20.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:21:44:34,10:0.227273:20,0,50; ```; As you see, the event which is a change of 3 consecutive nucleotides, is called in one sample as a single T to TTC event and in the other sample as 3 SNPs. ; This is annoying if one wants to compare the 2 samples, as simple comparison scripts will not understand those events are in fact identical. . We ran GATK in parallel and GATK calls the event in both sample as T to TTC. . The only difference between the 2 samples is the sequencing coverage which is higher in Sample1. ; I also looked into the gVCF and the same phenomenom happens.; Sample 1:; ```; Chrom_6	5425343	.	G	<*>	0	.	END=5425398	GT:GQ:MIN_DP:PL	0/0:50:104:0,300,2999; Chrom_6	5425399	.	TGG	T,<*>	33.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:33:108:47,58,0:0.537037,0:33,0,61,990,990,990; Chrom_6	5425402	.	G	<*>	0	.	END=5425402	GT:GQ:MIN_DP:PL	0/0:50:106:0,180,2759; Chrom_6	**5425403**	.	T	TTC,<*>	36.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:36:105:47,57,0:0.542857,0:36,0,61,990,990,990; Chrom_6	5425404	.	A	<*>	0	.	END=5425420	GT:GQ:MIN_DP:PL	0/0:50:106:0,300,2999; Chrom_6	5425421	.	T	C,<*>	39.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:39:105:46,59,0:0.561905,0:39,0,62,990,990,990; Chrom_6	5425422	.	C	<*>	0	.	END=5425459	GT:GQ:MIN_DP:PL	0/0:50:105:0,180,2759; Chrom_6	5425460	.	G	A,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/202
https://github.com/google/deepvariant/issues/203:1161,Availability,checkpoint,checkpoint-,1161,"We are trying to customize the WES model to call variants on RNA-Seq reads. . Regardless of how low we set the learning rate or the batch size or saving the intervals, the value of either the values of (TNs/All) and (FNs/All) is set to 0. or the values of (TPs/All) and (FPs/All) is set to zero. . I am not sure I clarified the case enough so here is a sample of the first case:; {'loss': 1.9104841, 'global_step': 0, 'TNs/All': 0.0, 'FNs/All': 0.0, 'F1/Class2': 0.0, 'Recall/Class1': 1.0, 'Recall/All': 1.0, 'Precision/All': 0.49536133, 'Precision/Class2': 0.0, 'Precision/Class1': 0.18457031, 'Accuracy/All': 0.18457031, 'F1/All': 0.6625306, 'FPs/All': 2067.0, 'Recall/Class2': 0.0, 'TPs/All': 2029.0, 'F1/Class1': 0.31162408}. Here is an example of the second case ; {'loss': 1.0544469, 'global_step': 3252, 'TNs/All': 2067.0, 'FNs/All': 2029.0, 'F1/Class2': 0.0, 'Recall/Class1': 0.0, 'Recall/All': 0.0, 'Precision/All': 0.0, 'Precision/Class2': 0.0, 'Precision/Class1': 0.0, 'Accuracy/All': 0.5046387, 'F1/All': 0.0, 'FPs/All': 0.0, 'Recall/Class2': 0.0, 'TPs/All': 0.0, 'F1/Class1': 0.0}. One experiment the first case was the case for all the model from checkpoint-0 until checkpoint-1779 then the second case suddenly appeared till the end of the evaluation. We run shorter experiments and as I mentioned before, no matter how we change the parameters for the learning rate, batch size, or saving the interval, one of the two cases appears since checkpoint-0. Can you guess any reason for such a behavior?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/203
https://github.com/google/deepvariant/issues/203:1180,Availability,checkpoint,checkpoint-,1180,"We are trying to customize the WES model to call variants on RNA-Seq reads. . Regardless of how low we set the learning rate or the batch size or saving the intervals, the value of either the values of (TNs/All) and (FNs/All) is set to 0. or the values of (TPs/All) and (FPs/All) is set to zero. . I am not sure I clarified the case enough so here is a sample of the first case:; {'loss': 1.9104841, 'global_step': 0, 'TNs/All': 0.0, 'FNs/All': 0.0, 'F1/Class2': 0.0, 'Recall/Class1': 1.0, 'Recall/All': 1.0, 'Precision/All': 0.49536133, 'Precision/Class2': 0.0, 'Precision/Class1': 0.18457031, 'Accuracy/All': 0.18457031, 'F1/All': 0.6625306, 'FPs/All': 2067.0, 'Recall/Class2': 0.0, 'TPs/All': 2029.0, 'F1/Class1': 0.31162408}. Here is an example of the second case ; {'loss': 1.0544469, 'global_step': 3252, 'TNs/All': 2067.0, 'FNs/All': 2029.0, 'F1/Class2': 0.0, 'Recall/Class1': 0.0, 'Recall/All': 0.0, 'Precision/All': 0.0, 'Precision/Class2': 0.0, 'Precision/Class1': 0.0, 'Accuracy/All': 0.5046387, 'F1/All': 0.0, 'FPs/All': 0.0, 'Recall/Class2': 0.0, 'TPs/All': 0.0, 'F1/Class1': 0.0}. One experiment the first case was the case for all the model from checkpoint-0 until checkpoint-1779 then the second case suddenly appeared till the end of the evaluation. We run shorter experiments and as I mentioned before, no matter how we change the parameters for the learning rate, batch size, or saving the interval, one of the two cases appears since checkpoint-0. Can you guess any reason for such a behavior?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/203
https://github.com/google/deepvariant/issues/203:1454,Availability,checkpoint,checkpoint-,1454,"We are trying to customize the WES model to call variants on RNA-Seq reads. . Regardless of how low we set the learning rate or the batch size or saving the intervals, the value of either the values of (TNs/All) and (FNs/All) is set to 0. or the values of (TPs/All) and (FPs/All) is set to zero. . I am not sure I clarified the case enough so here is a sample of the first case:; {'loss': 1.9104841, 'global_step': 0, 'TNs/All': 0.0, 'FNs/All': 0.0, 'F1/Class2': 0.0, 'Recall/Class1': 1.0, 'Recall/All': 1.0, 'Precision/All': 0.49536133, 'Precision/Class2': 0.0, 'Precision/Class1': 0.18457031, 'Accuracy/All': 0.18457031, 'F1/All': 0.6625306, 'FPs/All': 2067.0, 'Recall/Class2': 0.0, 'TPs/All': 2029.0, 'F1/Class1': 0.31162408}. Here is an example of the second case ; {'loss': 1.0544469, 'global_step': 3252, 'TNs/All': 2067.0, 'FNs/All': 2029.0, 'F1/Class2': 0.0, 'Recall/Class1': 0.0, 'Recall/All': 0.0, 'Precision/All': 0.0, 'Precision/Class2': 0.0, 'Precision/Class1': 0.0, 'Accuracy/All': 0.5046387, 'F1/All': 0.0, 'FPs/All': 0.0, 'Recall/Class2': 0.0, 'TPs/All': 0.0, 'F1/Class1': 0.0}. One experiment the first case was the case for all the model from checkpoint-0 until checkpoint-1779 then the second case suddenly appeared till the end of the evaluation. We run shorter experiments and as I mentioned before, no matter how we change the parameters for the learning rate, batch size, or saving the interval, one of the two cases appears since checkpoint-0. Can you guess any reason for such a behavior?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/203
https://github.com/google/deepvariant/issues/203:111,Usability,learn,learning,111,"We are trying to customize the WES model to call variants on RNA-Seq reads. . Regardless of how low we set the learning rate or the batch size or saving the intervals, the value of either the values of (TNs/All) and (FNs/All) is set to 0. or the values of (TPs/All) and (FPs/All) is set to zero. . I am not sure I clarified the case enough so here is a sample of the first case:; {'loss': 1.9104841, 'global_step': 0, 'TNs/All': 0.0, 'FNs/All': 0.0, 'F1/Class2': 0.0, 'Recall/Class1': 1.0, 'Recall/All': 1.0, 'Precision/All': 0.49536133, 'Precision/Class2': 0.0, 'Precision/Class1': 0.18457031, 'Accuracy/All': 0.18457031, 'F1/All': 0.6625306, 'FPs/All': 2067.0, 'Recall/Class2': 0.0, 'TPs/All': 2029.0, 'F1/Class1': 0.31162408}. Here is an example of the second case ; {'loss': 1.0544469, 'global_step': 3252, 'TNs/All': 2067.0, 'FNs/All': 2029.0, 'F1/Class2': 0.0, 'Recall/Class1': 0.0, 'Recall/All': 0.0, 'Precision/All': 0.0, 'Precision/Class2': 0.0, 'Precision/Class1': 0.0, 'Accuracy/All': 0.5046387, 'F1/All': 0.0, 'FPs/All': 0.0, 'Recall/Class2': 0.0, 'TPs/All': 0.0, 'F1/Class1': 0.0}. One experiment the first case was the case for all the model from checkpoint-0 until checkpoint-1779 then the second case suddenly appeared till the end of the evaluation. We run shorter experiments and as I mentioned before, no matter how we change the parameters for the learning rate, batch size, or saving the interval, one of the two cases appears since checkpoint-0. Can you guess any reason for such a behavior?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/203
https://github.com/google/deepvariant/issues/203:1368,Usability,learn,learning,1368,"We are trying to customize the WES model to call variants on RNA-Seq reads. . Regardless of how low we set the learning rate or the batch size or saving the intervals, the value of either the values of (TNs/All) and (FNs/All) is set to 0. or the values of (TPs/All) and (FPs/All) is set to zero. . I am not sure I clarified the case enough so here is a sample of the first case:; {'loss': 1.9104841, 'global_step': 0, 'TNs/All': 0.0, 'FNs/All': 0.0, 'F1/Class2': 0.0, 'Recall/Class1': 1.0, 'Recall/All': 1.0, 'Precision/All': 0.49536133, 'Precision/Class2': 0.0, 'Precision/Class1': 0.18457031, 'Accuracy/All': 0.18457031, 'F1/All': 0.6625306, 'FPs/All': 2067.0, 'Recall/Class2': 0.0, 'TPs/All': 2029.0, 'F1/Class1': 0.31162408}. Here is an example of the second case ; {'loss': 1.0544469, 'global_step': 3252, 'TNs/All': 2067.0, 'FNs/All': 2029.0, 'F1/Class2': 0.0, 'Recall/Class1': 0.0, 'Recall/All': 0.0, 'Precision/All': 0.0, 'Precision/Class2': 0.0, 'Precision/Class1': 0.0, 'Accuracy/All': 0.5046387, 'F1/All': 0.0, 'FPs/All': 0.0, 'Recall/Class2': 0.0, 'TPs/All': 0.0, 'F1/Class1': 0.0}. One experiment the first case was the case for all the model from checkpoint-0 until checkpoint-1779 then the second case suddenly appeared till the end of the evaluation. We run shorter experiments and as I mentioned before, no matter how we change the parameters for the learning rate, batch size, or saving the interval, one of the two cases appears since checkpoint-0. Can you guess any reason for such a behavior?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/203
https://github.com/google/deepvariant/issues/204:99,Deployability,release,release,99,"Hello,. I apologise if my question is naive I am a beginner with neural networks. ; Do you plan to release models trained with non-humans? Like the mosquito analysis published on your blog? And would it made sense to have a kind of ""universal model""? . thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/204
https://github.com/google/deepvariant/issues/206:205,Safety,detect,detected,205,"Hi there!. How is the QUAL score calculated when using DeepVariant? E.g., with GATK, it's the phred-scaled posterior probability of a variant being homozygous ref. I've been comparing known true positives detected by both HaplotypeCaller and DeepVariant, and found that DV's QUAL score is approximately correlated with HC's QualByDepth, so I'm wondering whether the QUAL score is already normalized by depth or is calculated in some other way. Thanks!; Bari",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/206
https://github.com/google/deepvariant/issues/207:38,Availability,error,error,38,"Hi. I'm running a pipeline but get an error on ""make_examples"" stage. Could you help me to debug possible cause, please?. Here is a log:; ```; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:51.456603: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; WARNING: Logging before flag parsing goes to stderr.; I0814 12:36:53.581777 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:36:54.201131 140158089049856 make_examples.py:1024] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:58.286794: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I0814 12:36:59.914532 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:45:36.568115 140158089049856 make_examples.py:946] Common contigs are [u'LKUA01000001.1', u'LKUA01000002.1', ...<ANOTHER 300k NAMES>..., u'LKUA01311038.1', u'LKUA01311039.1']; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/cannabis-3k-vcf/staging/SRS1107973_LKUA01/staging/examples/0/examples_output.tfrecord@512.gz --reads /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam --ref /mnt/google/.google/input/cannabis-3k/reference/LKUA01/LKUA01.fa --task 8; ```. For me it looks that the error message is `parallel: This job failed:` and failure doesn't relate to the warnings at the beginning of the file?. Regards,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207
https://github.com/google/deepvariant/issues/207:1837,Availability,error,error,1837,"Hi. I'm running a pipeline but get an error on ""make_examples"" stage. Could you help me to debug possible cause, please?. Here is a log:; ```; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:51.456603: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; WARNING: Logging before flag parsing goes to stderr.; I0814 12:36:53.581777 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:36:54.201131 140158089049856 make_examples.py:1024] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:58.286794: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I0814 12:36:59.914532 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:45:36.568115 140158089049856 make_examples.py:946] Common contigs are [u'LKUA01000001.1', u'LKUA01000002.1', ...<ANOTHER 300k NAMES>..., u'LKUA01311038.1', u'LKUA01311039.1']; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/cannabis-3k-vcf/staging/SRS1107973_LKUA01/staging/examples/0/examples_output.tfrecord@512.gz --reads /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam --ref /mnt/google/.google/input/cannabis-3k/reference/LKUA01/LKUA01.fa --task 8; ```. For me it looks that the error message is `parallel: This job failed:` and failure doesn't relate to the warnings at the beginning of the file?. Regards,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207
https://github.com/google/deepvariant/issues/207:1887,Availability,failure,failure,1887,"Hi. I'm running a pipeline but get an error on ""make_examples"" stage. Could you help me to debug possible cause, please?. Here is a log:; ```; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:51.456603: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; WARNING: Logging before flag parsing goes to stderr.; I0814 12:36:53.581777 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:36:54.201131 140158089049856 make_examples.py:1024] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:58.286794: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I0814 12:36:59.914532 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:45:36.568115 140158089049856 make_examples.py:946] Common contigs are [u'LKUA01000001.1', u'LKUA01000002.1', ...<ANOTHER 300k NAMES>..., u'LKUA01311038.1', u'LKUA01311039.1']; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/cannabis-3k-vcf/staging/SRS1107973_LKUA01/staging/examples/0/examples_output.tfrecord@512.gz --reads /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam --ref /mnt/google/.google/input/cannabis-3k/reference/LKUA01/LKUA01.fa --task 8; ```. For me it looks that the error message is `parallel: This job failed:` and failure doesn't relate to the warnings at the beginning of the file?. Regards,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207
https://github.com/google/deepvariant/issues/207:18,Deployability,pipeline,pipeline,18,"Hi. I'm running a pipeline but get an error on ""make_examples"" stage. Could you help me to debug possible cause, please?. Here is a log:; ```; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:51.456603: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; WARNING: Logging before flag parsing goes to stderr.; I0814 12:36:53.581777 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:36:54.201131 140158089049856 make_examples.py:1024] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:58.286794: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I0814 12:36:59.914532 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:45:36.568115 140158089049856 make_examples.py:946] Common contigs are [u'LKUA01000001.1', u'LKUA01000002.1', ...<ANOTHER 300k NAMES>..., u'LKUA01311038.1', u'LKUA01311039.1']; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/cannabis-3k-vcf/staging/SRS1107973_LKUA01/staging/examples/0/examples_output.tfrecord@512.gz --reads /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam --ref /mnt/google/.google/input/cannabis-3k/reference/LKUA01/LKUA01.fa --task 8; ```. For me it looks that the error message is `parallel: This job failed:` and failure doesn't relate to the warnings at the beginning of the file?. Regards,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207
https://github.com/google/deepvariant/issues/207:1843,Integrability,message,message,1843,"Hi. I'm running a pipeline but get an error on ""make_examples"" stage. Could you help me to debug possible cause, please?. Here is a log:; ```; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:51.456603: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; WARNING: Logging before flag parsing goes to stderr.; I0814 12:36:53.581777 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:36:54.201131 140158089049856 make_examples.py:1024] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:58.286794: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I0814 12:36:59.914532 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:45:36.568115 140158089049856 make_examples.py:946] Common contigs are [u'LKUA01000001.1', u'LKUA01000002.1', ...<ANOTHER 300k NAMES>..., u'LKUA01311038.1', u'LKUA01311039.1']; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/cannabis-3k-vcf/staging/SRS1107973_LKUA01/staging/examples/0/examples_output.tfrecord@512.gz --reads /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam --ref /mnt/google/.google/input/cannabis-3k/reference/LKUA01/LKUA01.fa --task 8; ```. For me it looks that the error message is `parallel: This job failed:` and failure doesn't relate to the warnings at the beginning of the file?. Regards,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207
https://github.com/google/deepvariant/issues/207:132,Testability,log,log,132,"Hi. I'm running a pipeline but get an error on ""make_examples"" stage. Could you help me to debug possible cause, please?. Here is a log:; ```; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:51.456603: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; WARNING: Logging before flag parsing goes to stderr.; I0814 12:36:53.581777 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:36:54.201131 140158089049856 make_examples.py:1024] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:58.286794: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I0814 12:36:59.914532 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:45:36.568115 140158089049856 make_examples.py:946] Common contigs are [u'LKUA01000001.1', u'LKUA01000002.1', ...<ANOTHER 300k NAMES>..., u'LKUA01311038.1', u'LKUA01311039.1']; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/cannabis-3k-vcf/staging/SRS1107973_LKUA01/staging/examples/0/examples_output.tfrecord@512.gz --reads /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam --ref /mnt/google/.google/input/cannabis-3k/reference/LKUA01/LKUA01.fa --task 8; ```. For me it looks that the error message is `parallel: This job failed:` and failure doesn't relate to the warnings at the beginning of the file?. Regards,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207
https://github.com/google/deepvariant/issues/207:434,Testability,Log,Logging,434,"Hi. I'm running a pipeline but get an error on ""make_examples"" stage. Could you help me to debug possible cause, please?. Here is a log:; ```; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:51.456603: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; WARNING: Logging before flag parsing goes to stderr.; I0814 12:36:53.581777 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:36:54.201131 140158089049856 make_examples.py:1024] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:58.286794: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I0814 12:36:59.914532 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:45:36.568115 140158089049856 make_examples.py:946] Common contigs are [u'LKUA01000001.1', u'LKUA01000002.1', ...<ANOTHER 300k NAMES>..., u'LKUA01311038.1', u'LKUA01311039.1']; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/cannabis-3k-vcf/staging/SRS1107973_LKUA01/staging/examples/0/examples_output.tfrecord@512.gz --reads /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam --ref /mnt/google/.google/input/cannabis-3k/reference/LKUA01/LKUA01.fa --task 8; ```. For me it looks that the error message is `parallel: This job failed:` and failure doesn't relate to the warnings at the beginning of the file?. Regards,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207
https://github.com/google/deepvariant/issues/208:53,Deployability,install,installing,53,"I am currently running DeepVariant on CCS data after installing via Docker on an AWS instance. The make_examples step of the pipeline is taking much longer than expected. I have performed this in the past with 30X Illumina Data, and it has taken a few hours. This has been running for about a week on 23X coverage PacBio HiFi with a 16 core machine (CPU optimized), and I was wondering if that was expected. Below, I have the command issued:. `sudo time seq 0 $((N_SHARDS-1)) | sudo parallel --eta --halt 2 --joblog ""${LOGDIR}""/log --res ""${LOGDIR}"" sudo docker run -v ${HOME}:${HOME} -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/make_examples --mode calling --ref=/input/ucsc.hg38.no_alts.fasta --reads=/input/hg00733_ccs_to_hg38.bam --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" --task {}`. All variables listed have been set as expected. When I ssh in to the node I can see that it is running python in parallel and writing to the proper output files, but it is just taking forever to process anything. Any help would be greatly appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/208
https://github.com/google/deepvariant/issues/208:125,Deployability,pipeline,pipeline,125,"I am currently running DeepVariant on CCS data after installing via Docker on an AWS instance. The make_examples step of the pipeline is taking much longer than expected. I have performed this in the past with 30X Illumina Data, and it has taken a few hours. This has been running for about a week on 23X coverage PacBio HiFi with a 16 core machine (CPU optimized), and I was wondering if that was expected. Below, I have the command issued:. `sudo time seq 0 $((N_SHARDS-1)) | sudo parallel --eta --halt 2 --joblog ""${LOGDIR}""/log --res ""${LOGDIR}"" sudo docker run -v ${HOME}:${HOME} -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/make_examples --mode calling --ref=/input/ucsc.hg38.no_alts.fasta --reads=/input/hg00733_ccs_to_hg38.bam --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" --task {}`. All variables listed have been set as expected. When I ssh in to the node I can see that it is running python in parallel and writing to the proper output files, but it is just taking forever to process anything. Any help would be greatly appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/208
https://github.com/google/deepvariant/issues/208:896,Modifiability,variab,variables,896,"I am currently running DeepVariant on CCS data after installing via Docker on an AWS instance. The make_examples step of the pipeline is taking much longer than expected. I have performed this in the past with 30X Illumina Data, and it has taken a few hours. This has been running for about a week on 23X coverage PacBio HiFi with a 16 core machine (CPU optimized), and I was wondering if that was expected. Below, I have the command issued:. `sudo time seq 0 $((N_SHARDS-1)) | sudo parallel --eta --halt 2 --joblog ""${LOGDIR}""/log --res ""${LOGDIR}"" sudo docker run -v ${HOME}:${HOME} -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/make_examples --mode calling --ref=/input/ucsc.hg38.no_alts.fasta --reads=/input/hg00733_ccs_to_hg38.bam --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" --task {}`. All variables listed have been set as expected. When I ssh in to the node I can see that it is running python in parallel and writing to the proper output files, but it is just taking forever to process anything. Any help would be greatly appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/208
https://github.com/google/deepvariant/issues/208:178,Performance,perform,performed,178,"I am currently running DeepVariant on CCS data after installing via Docker on an AWS instance. The make_examples step of the pipeline is taking much longer than expected. I have performed this in the past with 30X Illumina Data, and it has taken a few hours. This has been running for about a week on 23X coverage PacBio HiFi with a 16 core machine (CPU optimized), and I was wondering if that was expected. Below, I have the command issued:. `sudo time seq 0 $((N_SHARDS-1)) | sudo parallel --eta --halt 2 --joblog ""${LOGDIR}""/log --res ""${LOGDIR}"" sudo docker run -v ${HOME}:${HOME} -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/make_examples --mode calling --ref=/input/ucsc.hg38.no_alts.fasta --reads=/input/hg00733_ccs_to_hg38.bam --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" --task {}`. All variables listed have been set as expected. When I ssh in to the node I can see that it is running python in parallel and writing to the proper output files, but it is just taking forever to process anything. Any help would be greatly appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/208
https://github.com/google/deepvariant/issues/208:354,Performance,optimiz,optimized,354,"I am currently running DeepVariant on CCS data after installing via Docker on an AWS instance. The make_examples step of the pipeline is taking much longer than expected. I have performed this in the past with 30X Illumina Data, and it has taken a few hours. This has been running for about a week on 23X coverage PacBio HiFi with a 16 core machine (CPU optimized), and I was wondering if that was expected. Below, I have the command issued:. `sudo time seq 0 $((N_SHARDS-1)) | sudo parallel --eta --halt 2 --joblog ""${LOGDIR}""/log --res ""${LOGDIR}"" sudo docker run -v ${HOME}:${HOME} -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/make_examples --mode calling --ref=/input/ucsc.hg38.no_alts.fasta --reads=/input/hg00733_ccs_to_hg38.bam --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" --task {}`. All variables listed have been set as expected. When I ssh in to the node I can see that it is running python in parallel and writing to the proper output files, but it is just taking forever to process anything. Any help would be greatly appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/208
https://github.com/google/deepvariant/issues/208:519,Testability,LOG,LOGDIR,519,"I am currently running DeepVariant on CCS data after installing via Docker on an AWS instance. The make_examples step of the pipeline is taking much longer than expected. I have performed this in the past with 30X Illumina Data, and it has taken a few hours. This has been running for about a week on 23X coverage PacBio HiFi with a 16 core machine (CPU optimized), and I was wondering if that was expected. Below, I have the command issued:. `sudo time seq 0 $((N_SHARDS-1)) | sudo parallel --eta --halt 2 --joblog ""${LOGDIR}""/log --res ""${LOGDIR}"" sudo docker run -v ${HOME}:${HOME} -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/make_examples --mode calling --ref=/input/ucsc.hg38.no_alts.fasta --reads=/input/hg00733_ccs_to_hg38.bam --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" --task {}`. All variables listed have been set as expected. When I ssh in to the node I can see that it is running python in parallel and writing to the proper output files, but it is just taking forever to process anything. Any help would be greatly appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/208
https://github.com/google/deepvariant/issues/208:528,Testability,log,log,528,"I am currently running DeepVariant on CCS data after installing via Docker on an AWS instance. The make_examples step of the pipeline is taking much longer than expected. I have performed this in the past with 30X Illumina Data, and it has taken a few hours. This has been running for about a week on 23X coverage PacBio HiFi with a 16 core machine (CPU optimized), and I was wondering if that was expected. Below, I have the command issued:. `sudo time seq 0 $((N_SHARDS-1)) | sudo parallel --eta --halt 2 --joblog ""${LOGDIR}""/log --res ""${LOGDIR}"" sudo docker run -v ${HOME}:${HOME} -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/make_examples --mode calling --ref=/input/ucsc.hg38.no_alts.fasta --reads=/input/hg00733_ccs_to_hg38.bam --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" --task {}`. All variables listed have been set as expected. When I ssh in to the node I can see that it is running python in parallel and writing to the proper output files, but it is just taking forever to process anything. Any help would be greatly appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/208
https://github.com/google/deepvariant/issues/208:541,Testability,LOG,LOGDIR,541,"I am currently running DeepVariant on CCS data after installing via Docker on an AWS instance. The make_examples step of the pipeline is taking much longer than expected. I have performed this in the past with 30X Illumina Data, and it has taken a few hours. This has been running for about a week on 23X coverage PacBio HiFi with a 16 core machine (CPU optimized), and I was wondering if that was expected. Below, I have the command issued:. `sudo time seq 0 $((N_SHARDS-1)) | sudo parallel --eta --halt 2 --joblog ""${LOGDIR}""/log --res ""${LOGDIR}"" sudo docker run -v ${HOME}:${HOME} -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/make_examples --mode calling --ref=/input/ucsc.hg38.no_alts.fasta --reads=/input/hg00733_ccs_to_hg38.bam --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" --task {}`. All variables listed have been set as expected. When I ssh in to the node I can see that it is running python in parallel and writing to the proper output files, but it is just taking forever to process anything. Any help would be greatly appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/208
https://github.com/google/deepvariant/issues/210:1178,Deployability,pipeline,pipeline,1178,"# Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_gVCF_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://files_jays/bam/bqsr.realign.markdup.sorted.merged.bam \; --bai gs://files_jays/bam/bqsr.realign.markdup.sorted.merged.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --preemptible \; --max_preemptible_tries 5 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". # logs on one of the VMs; /bin/bash: gcsfuse: command not found; parallel: This job failed:; mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs files_jays /input-gcsfused-0; /bin/bash: gcsfuse: command not found; parallel: This job failed:; mkdir -p ./input-gcsfused-1 && gcsfuse --implicit-dirs files_jays /input-gcsfused-1; /bin/bash: gcsfuse: command not found",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/210
https://github.com/google/deepvariant/issues/210:1211,Deployability,pipeline,pipelines,1211,"# Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_gVCF_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://files_jays/bam/bqsr.realign.markdup.sorted.merged.bam \; --bai gs://files_jays/bam/bqsr.realign.markdup.sorted.merged.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --preemptible \; --max_preemptible_tries 5 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". # logs on one of the VMs; /bin/bash: gcsfuse: command not found; parallel: This job failed:; mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs files_jays /input-gcsfused-0; /bin/bash: gcsfuse: command not found; parallel: This job failed:; mkdir -p ./input-gcsfused-1 && gcsfuse --implicit-dirs files_jays /input-gcsfused-1; /bin/bash: gcsfuse: command not found",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/210
https://github.com/google/deepvariant/issues/210:1487,Deployability,pipeline,pipelines,1487,"# Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_gVCF_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://files_jays/bam/bqsr.realign.markdup.sorted.merged.bam \; --bai gs://files_jays/bam/bqsr.realign.markdup.sorted.merged.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --preemptible \; --max_preemptible_tries 5 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". # logs on one of the VMs; /bin/bash: gcsfuse: command not found; parallel: This job failed:; mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs files_jays /input-gcsfused-0; /bin/bash: gcsfuse: command not found; parallel: This job failed:; mkdir -p ./input-gcsfused-1 && gcsfuse --implicit-dirs files_jays /input-gcsfused-1; /bin/bash: gcsfuse: command not found",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/210
https://github.com/google/deepvariant/issues/210:1336,Testability,log,logging,1336,"# Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_gVCF_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://files_jays/bam/bqsr.realign.markdup.sorted.merged.bam \; --bai gs://files_jays/bam/bqsr.realign.markdup.sorted.merged.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --preemptible \; --max_preemptible_tries 5 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". # logs on one of the VMs; /bin/bash: gcsfuse: command not found; parallel: This job failed:; mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs files_jays /input-gcsfused-0; /bin/bash: gcsfuse: command not found; parallel: This job failed:; mkdir -p ./input-gcsfused-1 && gcsfuse --implicit-dirs files_jays /input-gcsfused-1; /bin/bash: gcsfuse: command not found",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/210
https://github.com/google/deepvariant/issues/210:1420,Testability,log,log,1420,"# Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_gVCF_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://files_jays/bam/bqsr.realign.markdup.sorted.merged.bam \; --bai gs://files_jays/bam/bqsr.realign.markdup.sorted.merged.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --preemptible \; --max_preemptible_tries 5 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". # logs on one of the VMs; /bin/bash: gcsfuse: command not found; parallel: This job failed:; mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs files_jays /input-gcsfused-0; /bin/bash: gcsfuse: command not found; parallel: This job failed:; mkdir -p ./input-gcsfused-1 && gcsfuse --implicit-dirs files_jays /input-gcsfused-1; /bin/bash: gcsfuse: command not found",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/210
https://github.com/google/deepvariant/issues/210:1554,Testability,log,logs,1554,"# Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_gVCF_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://files_jays/bam/bqsr.realign.markdup.sorted.merged.bam \; --bai gs://files_jays/bam/bqsr.realign.markdup.sorted.merged.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --preemptible \; --max_preemptible_tries 5 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". # logs on one of the VMs; /bin/bash: gcsfuse: command not found; parallel: This job failed:; mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs files_jays /input-gcsfused-0; /bin/bash: gcsfuse: command not found; parallel: This job failed:; mkdir -p ./input-gcsfused-1 && gcsfuse --implicit-dirs files_jays /input-gcsfused-1; /bin/bash: gcsfuse: command not found",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/210
https://github.com/google/deepvariant/issues/211:16,Deployability,install,installed,16,Hi ; I recently installed docker version of deep variant for cancer exome analysis. I like to know if I don't use paired cancer samples for variant calling with deep variant will it be right approach,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/211
https://github.com/google/deepvariant/issues/214:178,Availability,recover,recover,178,"Hi, . I am trying to run the following:. #!/bin/bash; #set -euo pipefail <- it fails if I use this!; # Set common settings.; PROJECT_ID=ms-deepvariant; OUTPUT_BUCKET=gs://ms_bam/recover; STAGING_FOLDER_NAME=recover_tmp; OUTPUT_FILE_NAME=recover.gvcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_I",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/214
https://github.com/google/deepvariant/issues/214:237,Availability,recover,recover,237,"Hi, . I am trying to run the following:. #!/bin/bash; #set -euo pipefail <- it fails if I use this!; # Set common settings.; PROJECT_ID=ms-deepvariant; OUTPUT_BUCKET=gs://ms_bam/recover; STAGING_FOLDER_NAME=recover_tmp; OUTPUT_FILE_NAME=recover.gvcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_I",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/214
https://github.com/google/deepvariant/issues/214:1725,Availability,error,error,1725,"_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --reads \""/input-gcsfused-{}/${BAM}\"" --ref \""${INPUT_REF}\"" --task {} --gvcf \""${GVCF}\""/gvcf_output.tfrecord@\""${SHARDS}\"".gz\"""": exit status 127: bash: gcsfuse: command not found. Is it possible to identify the problem/typo?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/214
https://github.com/google/deepvariant/issues/214:1323,Deployability,pipeline,pipeline,1323,"D=ms-deepvariant; OUTPUT_BUCKET=gs://ms_bam/recover; STAGING_FOLDER_NAME=recover_tmp; OUTPUT_FILE_NAME=recover.gvcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out wa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/214
https://github.com/google/deepvariant/issues/214:1356,Deployability,pipeline,pipelines,1356,"wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/214
https://github.com/google/deepvariant/issues/214:1636,Deployability,pipeline,pipelines,1636,"_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --reads \""/input-gcsfused-{}/${BAM}\"" --ref \""${INPUT_REF}\"" --task {} --gvcf \""${GVCF}\""/gvcf_output.tfrecord@\""${SHARDS}\"".gz\"""": exit status 127: bash: gcsfuse: command not found. Is it possible to identify the problem/typo?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/214
https://github.com/google/deepvariant/issues/214:178,Safety,recover,recover,178,"Hi, . I am trying to run the following:. #!/bin/bash; #set -euo pipefail <- it fails if I use this!; # Set common settings.; PROJECT_ID=ms-deepvariant; OUTPUT_BUCKET=gs://ms_bam/recover; STAGING_FOLDER_NAME=recover_tmp; OUTPUT_FILE_NAME=recover.gvcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_I",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/214
https://github.com/google/deepvariant/issues/214:237,Safety,recover,recover,237,"Hi, . I am trying to run the following:. #!/bin/bash; #set -euo pipefail <- it fails if I use this!; # Set common settings.; PROJECT_ID=ms-deepvariant; OUTPUT_BUCKET=gs://ms_bam/recover; STAGING_FOLDER_NAME=recover_tmp; OUTPUT_FILE_NAME=recover.gvcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_I",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/214
https://github.com/google/deepvariant/issues/214:1762,Safety,timeout,timeout,1762,"_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --reads \""/input-gcsfused-{}/${BAM}\"" --ref \""${INPUT_REF}\"" --task {} --gvcf \""${GVCF}\""/gvcf_output.tfrecord@\""${SHARDS}\"".gz\"""": exit status 127: bash: gcsfuse: command not found. Is it possible to identify the problem/typo?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/214
https://github.com/google/deepvariant/issues/214:2102,Safety,timeout,timeout,2102,"_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --reads \""/input-gcsfused-{}/${BAM}\"" --ref \""${INPUT_REF}\"" --task {} --gvcf \""${GVCF}\""/gvcf_output.tfrecord@\""${SHARDS}\"".gz\"""": exit status 127: bash: gcsfuse: command not found. Is it possible to identify the problem/typo?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/214
https://github.com/google/deepvariant/issues/214:1481,Testability,log,logging,1481,"epvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --reads \""/input-gcsfused-{}/${BAM}\"" --ref \""${INPUT_REF}\"" --task {} --gvcf \""${GVCF}\""/gvcf_output.tf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/214
https://github.com/google/deepvariant/issues/214:1565,Testability,log,log,1565,"urope-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --reads \""/input-gcsfused-{}/${BAM}\"" --ref \""${INPUT_REF}\"" --task {} --gvcf \""${GVCF}\""/gvcf_output.tfrecord@\""${SHARDS}\"".gz\"""": exit status 127: bash: gcsfuse: command not found. Is it",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/214
https://github.com/google/deepvariant/issues/214:2074,Testability,test,test,2074,"_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --reads \""/input-gcsfused-{}/${BAM}\"" --ref \""${INPUT_REF}\"" --task {} --gvcf \""${GVCF}\""/gvcf_output.tfrecord@\""${SHARDS}\"".gz\"""": exit status 127: bash: gcsfuse: command not found. Is it possible to identify the problem/typo?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/214
https://github.com/google/deepvariant/pull/216:102,Deployability,update,update,102,"We are not taking pull requests at this time.; (Note: this PR is on the gh-pages branch, meant for an update for the goo.gl/deepvariant blog. It's submitted by @pichuan , a DeepVariant team member.)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/216
https://github.com/google/deepvariant/issues/217:225,Availability,down,downloading,225,"Dear Developer,. I am going to use DeepVariant for variant calling. I have followed the instruction provided [here](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), to install the tool. After downloading the all needed sample data I run the command:; ```; sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}:/output"" \; > gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/input/ucsc.hg19.chr20.unittest.fasta \; > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=1 ; ```; I faced the error:; ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}. 2019-09-11 14:44:44.030589: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m2.456s; user	0m1.443s; sys	0m1.926s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Comman",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/217
https://github.com/google/deepvariant/issues/217:760,Availability,error,error,760,"Dear Developer,. I am going to use DeepVariant for variant calling. I have followed the instruction provided [here](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), to install the tool. After downloading the all needed sample data I run the command:; ```; sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}:/output"" \; > gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/input/ucsc.hg19.chr20.unittest.fasta \; > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=1 ; ```; I faced the error:; ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}. 2019-09-11 14:44:44.030589: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m2.456s; user	0m1.443s; sys	0m1.926s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Comman",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/217
https://github.com/google/deepvariant/issues/217:1325,Availability,avail,available,1325,"ocker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}:/output"" \; > gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/input/ucsc.hg19.chr20.unittest.fasta \; > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=1 ; ```; I faced the error:; ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}. 2019-09-11 14:44:44.030589: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m2.456s; user	0m1.443s; sys	0m1.926s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/217
https://github.com/google/deepvariant/issues/217:201,Deployability,install,install,201,"Dear Developer,. I am going to use DeepVariant for variant calling. I have followed the instruction provided [here](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), to install the tool. After downloading the all needed sample data I run the command:; ```; sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}:/output"" \; > gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/input/ucsc.hg19.chr20.unittest.fasta \; > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=1 ; ```; I faced the error:; ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}. 2019-09-11 14:44:44.030589: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m2.456s; user	0m1.443s; sys	0m1.926s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Comman",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/217
https://github.com/google/deepvariant/issues/217:2561,Deployability,install,install,2561,"ds=1 ; ```; I faced the error:; ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}. 2019-09-11 14:44:44.030589: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m2.456s; user	0m1.443s; sys	0m1.926s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. I googled for solution also asked you before and found may be compiling from source code would works for me. I FAILED TO DO THIS.; Is still any chance to use install and work with DeepVariant?. Please consider that a number of potential users -like me- are biologist with limited knowledge of informatics. ; Thanks in advance.; Hamid",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/217
https://github.com/google/deepvariant/pull/218:64,Deployability,update,update,64,(Note: We are not taking pull requests at this time. This is an update to the blog from within the DeepVariant team.). This adds a system for showing author images at the top of blog posts.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/218
https://github.com/google/deepvariant/issues/219:3094,Testability,test,test,3094,"le>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/_0_/Exp/seq1/seq1.fa"" --reads ""/home/platon/_0_/Exp/seq1/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Second attempt. This time with paths consisting only of latin characters.; `sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/platon/test/seq1.fa --reads=/home/platon/test/seq1.bam --output_vcf=/home/platon/test/seq1.vcf`. ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}. [E::hts_open_format] Failed to open file /home/platon/test/seq1.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvari",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/219
https://github.com/google/deepvariant/issues/219:3128,Testability,test,test,3128,"ages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/_0_/Exp/seq1/seq1.fa"" --reads ""/home/platon/_0_/Exp/seq1/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Second attempt. This time with paths consisting only of latin characters.; `sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/platon/test/seq1.fa --reads=/home/platon/test/seq1.bam --output_vcf=/home/platon/test/seq1.vcf`. ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}. [E::hts_open_format] Failed to open file /home/platon/test/seq1.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_opt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/219
https://github.com/google/deepvariant/issues/219:3168,Testability,test,test,3168,"_run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/_0_/Exp/seq1/seq1.fa"" --reads ""/home/platon/_0_/Exp/seq1/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Second attempt. This time with paths consisting only of latin characters.; `sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/platon/test/seq1.fa --reads=/home/platon/test/seq1.bam --output_vcf=/home/platon/test/seq1.vcf`. ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}. [E::hts_open_format] Failed to open file /home/platon/test/seq1.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/219
https://github.com/google/deepvariant/issues/219:3333,Testability,test,test,3333," sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/_0_/Exp/seq1/seq1.fa"" --reads ""/home/platon/_0_/Exp/seq1/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Second attempt. This time with paths consisting only of latin characters.; `sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/platon/test/seq1.fa --reads=/home/platon/test/seq1.bam --output_vcf=/home/platon/test/seq1.vcf`. ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}. [E::hts_open_format] Failed to open file /home/platon/test/seq1.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/219
https://github.com/google/deepvariant/issues/219:3369,Testability,test,test,3369,"ss.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/_0_/Exp/seq1/seq1.fa"" --reads ""/home/platon/_0_/Exp/seq1/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Second attempt. This time with paths consisting only of latin characters.; `sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/platon/test/seq1.fa --reads=/home/platon/test/seq1.bam --output_vcf=/home/platon/test/seq1.vcf`. ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}. [E::hts_open_format] Failed to open file /home/platon/test/seq1.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **k",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/219
https://github.com/google/deepvariant/issues/219:3518,Testability,test,test,3518,"lledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/_0_/Exp/seq1/seq1.fa"" --reads ""/home/platon/_0_/Exp/seq1/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Second attempt. This time with paths consisting only of latin characters.; `sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/platon/test/seq1.fa --reads=/home/platon/test/seq1.bam --output_vcf=/home/platon/test/seq1.vcf`. ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}. [E::hts_open_format] Failed to open file /home/platon/test/seq1.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/219
https://github.com/google/deepvariant/issues/219:4780,Testability,test,test,4780,"sr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open /home/platon/test/seq1.bam. real	0m1.571s; user	0m1.481s; sys	0m0.786s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/219
https://github.com/google/deepvariant/issues/219:5558,Testability,test,test,5558,"azel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open /home/platon/test/seq1.bam. real	0m1.571s; user	0m1.481s; sys	0m0.786s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Source data:; [test_seq_data.tar.gz](https://github.com/google/deepvariant/files/3611565/test_seq_data.tar.gz)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/219
https://github.com/google/deepvariant/issues/219:5594,Testability,test,test,5594,"azel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open /home/platon/test/seq1.bam. real	0m1.571s; user	0m1.481s; sys	0m0.786s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Source data:; [test_seq_data.tar.gz](https://github.com/google/deepvariant/files/3611565/test_seq_data.tar.gz)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/219
https://github.com/google/deepvariant/issues/221:158,Availability,Error,Error,158,"Your dockerhub repo is currently having issues when trying to pull it, regardless of how:. ```; $> docker pull google/deepvariant; Using default tag: latest; Error response from daemon: manifest for google/deepvariant:latest not found: manifest unknown: manifest unknown; ```. ```; $> singularity pull deepvariant.img docker://google/deepvariant:latest; FATAL: While making image from oci registry: failed to get checksum for docker://google/deepvariant:latest: Error reading manifest latest in docker.io/google/deepvariant: manifest unknown: manifest unknown; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/221
https://github.com/google/deepvariant/issues/221:462,Availability,Error,Error,462,"Your dockerhub repo is currently having issues when trying to pull it, regardless of how:. ```; $> docker pull google/deepvariant; Using default tag: latest; Error response from daemon: manifest for google/deepvariant:latest not found: manifest unknown: manifest unknown; ```. ```; $> singularity pull deepvariant.img docker://google/deepvariant:latest; FATAL: While making image from oci registry: failed to get checksum for docker://google/deepvariant:latest: Error reading manifest latest in docker.io/google/deepvariant: manifest unknown: manifest unknown; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/221
https://github.com/google/deepvariant/issues/221:413,Security,checksum,checksum,413,"Your dockerhub repo is currently having issues when trying to pull it, regardless of how:. ```; $> docker pull google/deepvariant; Using default tag: latest; Error response from daemon: manifest for google/deepvariant:latest not found: manifest unknown: manifest unknown; ```. ```; $> singularity pull deepvariant.img docker://google/deepvariant:latest; FATAL: While making image from oci registry: failed to get checksum for docker://google/deepvariant:latest: Error reading manifest latest in docker.io/google/deepvariant: manifest unknown: manifest unknown; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/221
https://github.com/google/deepvariant/issues/222:7,Availability,down,downloaded,7,"I just downloaded the docker image and ran the following test on Centos OS 7. Everything worked fine. . INPUT_DIR=""/test/DeepVariant/quickstart-testdata""; OUTPUT_DIR=""/test/DeepVariant/quickstart-output""; BIN_VERSION=""0.8.0"". docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. Then I modified the shell script to run my sample ; > I'm using the custom ref - included fa, fai, .gz. gzi files in the input dir. >RHA; CTGGG ..... > I aligned my reads to the ref and extracted only mapped paired-end reads. @HD VN:1.6 SO:coordinate; @SQ SN:RHA LN:911; @PG ID:bwa PN:bwa VN:0.7.17-r1194-dirty CL:bwa mem -M -t 10 /..... > now when I run the docker tool, I get the following error message. 2019-09-24 15:23:14.405094: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring:; I0924 15:23:14.405213 139913087186688 genomics_reader.py:218] Reading /input/test.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 316, in default_options; sample_name = extract_sample_name_from_sam_reader(sam_reader); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222
https://github.com/google/deepvariant/issues/222:1044,Availability,error,error,1044,"ocker image and ran the following test on Centos OS 7. Everything worked fine. . INPUT_DIR=""/test/DeepVariant/quickstart-testdata""; OUTPUT_DIR=""/test/DeepVariant/quickstart-output""; BIN_VERSION=""0.8.0"". docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. Then I modified the shell script to run my sample ; > I'm using the custom ref - included fa, fai, .gz. gzi files in the input dir. >RHA; CTGGG ..... > I aligned my reads to the ref and extracted only mapped paired-end reads. @HD VN:1.6 SO:coordinate; @SQ SN:RHA LN:911; @PG ID:bwa PN:bwa VN:0.7.17-r1194-dirty CL:bwa mem -M -t 10 /..... > now when I run the docker tool, I get the following error message. 2019-09-24 15:23:14.405094: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring:; I0924 15:23:14.405213 139913087186688 genomics_reader.py:218] Reading /input/test.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 316, in default_options; sample_name = extract_sample_name_from_sam_reader(sam_reader); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222
https://github.com/google/deepvariant/issues/222:1050,Integrability,message,message,1050,"ocker image and ran the following test on Centos OS 7. Everything worked fine. . INPUT_DIR=""/test/DeepVariant/quickstart-testdata""; OUTPUT_DIR=""/test/DeepVariant/quickstart-output""; BIN_VERSION=""0.8.0"". docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. Then I modified the shell script to run my sample ; > I'm using the custom ref - included fa, fai, .gz. gzi files in the input dir. >RHA; CTGGG ..... > I aligned my reads to the ref and extracted only mapped paired-end reads. @HD VN:1.6 SO:coordinate; @SQ SN:RHA LN:911; @PG ID:bwa PN:bwa VN:0.7.17-r1194-dirty CL:bwa mem -M -t 10 /..... > now when I run the docker tool, I get the following error message. 2019-09-24 15:23:14.405094: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring:; I0924 15:23:14.405213 139913087186688 genomics_reader.py:218] Reading /input/test.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 316, in default_options; sample_name = extract_sample_name_from_sam_reader(sam_reader); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222
https://github.com/google/deepvariant/issues/222:57,Testability,test,test,57,"I just downloaded the docker image and ran the following test on Centos OS 7. Everything worked fine. . INPUT_DIR=""/test/DeepVariant/quickstart-testdata""; OUTPUT_DIR=""/test/DeepVariant/quickstart-output""; BIN_VERSION=""0.8.0"". docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. Then I modified the shell script to run my sample ; > I'm using the custom ref - included fa, fai, .gz. gzi files in the input dir. >RHA; CTGGG ..... > I aligned my reads to the ref and extracted only mapped paired-end reads. @HD VN:1.6 SO:coordinate; @SQ SN:RHA LN:911; @PG ID:bwa PN:bwa VN:0.7.17-r1194-dirty CL:bwa mem -M -t 10 /..... > now when I run the docker tool, I get the following error message. 2019-09-24 15:23:14.405094: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring:; I0924 15:23:14.405213 139913087186688 genomics_reader.py:218] Reading /input/test.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 316, in default_options; sample_name = extract_sample_name_from_sam_reader(sam_reader); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222
https://github.com/google/deepvariant/issues/222:116,Testability,test,test,116,"I just downloaded the docker image and ran the following test on Centos OS 7. Everything worked fine. . INPUT_DIR=""/test/DeepVariant/quickstart-testdata""; OUTPUT_DIR=""/test/DeepVariant/quickstart-output""; BIN_VERSION=""0.8.0"". docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. Then I modified the shell script to run my sample ; > I'm using the custom ref - included fa, fai, .gz. gzi files in the input dir. >RHA; CTGGG ..... > I aligned my reads to the ref and extracted only mapped paired-end reads. @HD VN:1.6 SO:coordinate; @SQ SN:RHA LN:911; @PG ID:bwa PN:bwa VN:0.7.17-r1194-dirty CL:bwa mem -M -t 10 /..... > now when I run the docker tool, I get the following error message. 2019-09-24 15:23:14.405094: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring:; I0924 15:23:14.405213 139913087186688 genomics_reader.py:218] Reading /input/test.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 316, in default_options; sample_name = extract_sample_name_from_sam_reader(sam_reader); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222
https://github.com/google/deepvariant/issues/222:144,Testability,test,testdata,144,"I just downloaded the docker image and ran the following test on Centos OS 7. Everything worked fine. . INPUT_DIR=""/test/DeepVariant/quickstart-testdata""; OUTPUT_DIR=""/test/DeepVariant/quickstart-output""; BIN_VERSION=""0.8.0"". docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. Then I modified the shell script to run my sample ; > I'm using the custom ref - included fa, fai, .gz. gzi files in the input dir. >RHA; CTGGG ..... > I aligned my reads to the ref and extracted only mapped paired-end reads. @HD VN:1.6 SO:coordinate; @SQ SN:RHA LN:911; @PG ID:bwa PN:bwa VN:0.7.17-r1194-dirty CL:bwa mem -M -t 10 /..... > now when I run the docker tool, I get the following error message. 2019-09-24 15:23:14.405094: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring:; I0924 15:23:14.405213 139913087186688 genomics_reader.py:218] Reading /input/test.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 316, in default_options; sample_name = extract_sample_name_from_sam_reader(sam_reader); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222
https://github.com/google/deepvariant/issues/222:168,Testability,test,test,168,"I just downloaded the docker image and ran the following test on Centos OS 7. Everything worked fine. . INPUT_DIR=""/test/DeepVariant/quickstart-testdata""; OUTPUT_DIR=""/test/DeepVariant/quickstart-output""; BIN_VERSION=""0.8.0"". docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. Then I modified the shell script to run my sample ; > I'm using the custom ref - included fa, fai, .gz. gzi files in the input dir. >RHA; CTGGG ..... > I aligned my reads to the ref and extracted only mapped paired-end reads. @HD VN:1.6 SO:coordinate; @SQ SN:RHA LN:911; @PG ID:bwa PN:bwa VN:0.7.17-r1194-dirty CL:bwa mem -M -t 10 /..... > now when I run the docker tool, I get the following error message. 2019-09-24 15:23:14.405094: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring:; I0924 15:23:14.405213 139913087186688 genomics_reader.py:218] Reading /input/test.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 316, in default_options; sample_name = extract_sample_name_from_sam_reader(sam_reader); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222
https://github.com/google/deepvariant/issues/222:1249,Testability,test,test,1249,"-v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. Then I modified the shell script to run my sample ; > I'm using the custom ref - included fa, fai, .gz. gzi files in the input dir. >RHA; CTGGG ..... > I aligned my reads to the ref and extracted only mapped paired-end reads. @HD VN:1.6 SO:coordinate; @SQ SN:RHA LN:911; @PG ID:bwa PN:bwa VN:0.7.17-r1194-dirty CL:bwa mem -M -t 10 /..... > now when I run the docker tool, I get the following error message. 2019-09-24 15:23:14.405094: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring:; I0924 15:23:14.405213 139913087186688 genomics_reader.py:218] Reading /input/test.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 316, in default_options; sample_name = extract_sample_name_from_sam_reader(sam_reader); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 458, in extract_sample_name_from_sam_reader; 'No non-empty sample name found in the input reads. Please provide the '; ValueError: No non-empty sample name found in the input reads. Please provide the n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222
https://github.com/google/deepvariant/issues/223:80,Availability,down,downloaded,80,"Dears,; I'm working on Ubuntu 16.04.5 LTS, and Docker API version 1.39,; I have downloaded the data according to the script:. INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see h",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/223
https://github.com/google/deepvariant/issues/223:1855,Availability,error,error,1855,"ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different computers, and the error was the same.; There is a previous issue in this forum (https://github.com/google/deepvariant/issues/181) where the user did not set BIN_VERSION variable correctly, and **IT IS NOT MY CASE**!!!!. I tested if the volumes were mounted correctly, according to the script:; OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; sudo docker run \; -i \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:0.8.0 \; find /input. And the result was:; /input/NA12878_S1.chr20.10_10p1mb.bam; /input/NA12878_S1.chr20.10_10p1mb.bam.bai; /input/test_nist.b37_chr20_100kbp_at_10mb.bed; /input/test_nist.b37_chr20_100kbp_at_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/223
https://github.com/google/deepvariant/issues/223:1891,Availability,error,error,1891,"ATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different computers, and the error was the same.; There is a previous issue in this forum (https://github.com/google/deepvariant/issues/181) where the user did not set BIN_VERSION variable correctly, and **IT IS NOT MY CASE**!!!!. I tested if the volumes were mounted correctly, according to the script:; OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; sudo docker run \; -i \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:0.8.0 \; find /input. And the result was:; /input/NA12878_S1.chr20.10_10p1mb.bam; /input/NA12878_S1.chr20.10_10p1mb.bam.bai; /input/test_nist.b37_chr20_100kbp_at_10mb.bed; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.g",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/223
https://github.com/google/deepvariant/issues/223:2168,Availability,error,error,2168,"-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different computers, and the error was the same.; There is a previous issue in this forum (https://github.com/google/deepvariant/issues/181) where the user did not set BIN_VERSION variable correctly, and **IT IS NOT MY CASE**!!!!. I tested if the volumes were mounted correctly, according to the script:; OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; sudo docker run \; -i \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:0.8.0 \; find /input. And the result was:; /input/NA12878_S1.chr20.10_10p1mb.bam; /input/NA12878_S1.chr20.10_10p1mb.bam.bai; /input/test_nist.b37_chr20_100kbp_at_10mb.bed; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; /input/ucsc.hg19.chr20.unittest.fasta; /input/ucsc.hg19.chr20.unittest.fasta.fai; /input/ucsc.hg19.chr20.unittest.fasta.gz; /input/ucsc.hg19.chr20.unittest.fasta.gz.fai; /input/ucsc.hg19.chr20.unittest.fasta.gz.gzi. It means that a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/223
https://github.com/google/deepvariant/issues/223:1055,Deployability,install,installed,1055," have downloaded the data according to the script:. INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/223
https://github.com/google/deepvariant/issues/223:1136,Deployability,update,update,1136,"ckstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/223
https://github.com/google/deepvariant/issues/223:1160,Deployability,install,install,1160,"ckstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/223
https://github.com/google/deepvariant/issues/223:2319,Modifiability,variab,variable,2319,"VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different computers, and the error was the same.; There is a previous issue in this forum (https://github.com/google/deepvariant/issues/181) where the user did not set BIN_VERSION variable correctly, and **IT IS NOT MY CASE**!!!!. I tested if the volumes were mounted correctly, according to the script:; OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; sudo docker run \; -i \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:0.8.0 \; find /input. And the result was:; /input/NA12878_S1.chr20.10_10p1mb.bam; /input/NA12878_S1.chr20.10_10p1mb.bam.bai; /input/test_nist.b37_chr20_100kbp_at_10mb.bed; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; /input/ucsc.hg19.chr20.unittest.fasta; /input/ucsc.hg19.chr20.unittest.fasta.fai; /input/ucsc.hg19.chr20.unittest.fasta.gz; /input/ucsc.hg19.chr20.unittest.fasta.gz.fai; /input/ucsc.hg19.chr20.unittest.fasta.gz.gzi. It means that all files are in the mounted Docker volume /input . Thanks so much for any help,; Rogrio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/223
https://github.com/google/deepvariant/issues/223:155,Testability,test,testdata,155,"Dears,; I'm working on Ubuntu 16.04.5 LTS, and Docker API version 1.39,; I have downloaded the data according to the script:. INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see h",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/223
https://github.com/google/deepvariant/issues/223:235,Testability,test,testdata,235,"Dears,; I'm working on Ubuntu 16.04.5 LTS, and Docker API version 1.39,; I have downloaded the data according to the script:. INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see h",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/223
https://github.com/google/deepvariant/issues/223:1274,Testability,test,test,1274,"}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different computers, and the error was the same.; There is a previous issue in this forum (https://github.com/google/deepvarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/223
https://github.com/google/deepvariant/issues/223:1350,Testability,test,testdata,1350,"mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different computers, and the error was the same.; There is a previous issue in this forum (https://github.com/google/deepvariant/issues/181) where the user did not set BIN_VERSION variable correc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/223
https://github.com/google/deepvariant/issues/223:2372,Testability,test,tested,2372,"VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different computers, and the error was the same.; There is a previous issue in this forum (https://github.com/google/deepvariant/issues/181) where the user did not set BIN_VERSION variable correctly, and **IT IS NOT MY CASE**!!!!. I tested if the volumes were mounted correctly, according to the script:; OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; sudo docker run \; -i \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:0.8.0 \; find /input. And the result was:; /input/NA12878_S1.chr20.10_10p1mb.bam; /input/NA12878_S1.chr20.10_10p1mb.bam.bai; /input/test_nist.b37_chr20_100kbp_at_10mb.bed; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; /input/ucsc.hg19.chr20.unittest.fasta; /input/ucsc.hg19.chr20.unittest.fasta.fai; /input/ucsc.hg19.chr20.unittest.fasta.gz; /input/ucsc.hg19.chr20.unittest.fasta.gz.fai; /input/ucsc.hg19.chr20.unittest.fasta.gz.gzi. It means that all files are in the mounted Docker volume /input . Thanks so much for any help,; Rogrio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/223
https://github.com/google/deepvariant/issues/223:2512,Testability,test,testdata,2512,"VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different computers, and the error was the same.; There is a previous issue in this forum (https://github.com/google/deepvariant/issues/181) where the user did not set BIN_VERSION variable correctly, and **IT IS NOT MY CASE**!!!!. I tested if the volumes were mounted correctly, according to the script:; OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; sudo docker run \; -i \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:0.8.0 \; find /input. And the result was:; /input/NA12878_S1.chr20.10_10p1mb.bam; /input/NA12878_S1.chr20.10_10p1mb.bam.bai; /input/test_nist.b37_chr20_100kbp_at_10mb.bed; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; /input/ucsc.hg19.chr20.unittest.fasta; /input/ucsc.hg19.chr20.unittest.fasta.fai; /input/ucsc.hg19.chr20.unittest.fasta.gz; /input/ucsc.hg19.chr20.unittest.fasta.gz.fai; /input/ucsc.hg19.chr20.unittest.fasta.gz.gzi. It means that all files are in the mounted Docker volume /input . Thanks so much for any help,; Rogrio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/223
https://github.com/google/deepvariant/issues/224:42,Security,validat,validation,42,"Hi,. I had a question regarding how train/validation split was determined for training the models. From the DeepVariant paper, I noticed that chromosome 1-19 was used for training. Within chr1-19, is there a reason to prefer one of the following training-validation splits:; 1. Designate chr-a, 1 <= a <= 19, as validation set, and remaining as training set.; 2. Use some random subset of all labeled images as a validation set, and the disjoint set as training set (the random split may be grouped by sites, or not). Specifically, I am interested in knowing whether there was a reason for preferring one split over the other, or whether it is an arbitrary choice, that is considered equivalent. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/224
https://github.com/google/deepvariant/issues/224:255,Security,validat,validation,255,"Hi,. I had a question regarding how train/validation split was determined for training the models. From the DeepVariant paper, I noticed that chromosome 1-19 was used for training. Within chr1-19, is there a reason to prefer one of the following training-validation splits:; 1. Designate chr-a, 1 <= a <= 19, as validation set, and remaining as training set.; 2. Use some random subset of all labeled images as a validation set, and the disjoint set as training set (the random split may be grouped by sites, or not). Specifically, I am interested in knowing whether there was a reason for preferring one split over the other, or whether it is an arbitrary choice, that is considered equivalent. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/224
https://github.com/google/deepvariant/issues/224:312,Security,validat,validation,312,"Hi,. I had a question regarding how train/validation split was determined for training the models. From the DeepVariant paper, I noticed that chromosome 1-19 was used for training. Within chr1-19, is there a reason to prefer one of the following training-validation splits:; 1. Designate chr-a, 1 <= a <= 19, as validation set, and remaining as training set.; 2. Use some random subset of all labeled images as a validation set, and the disjoint set as training set (the random split may be grouped by sites, or not). Specifically, I am interested in knowing whether there was a reason for preferring one split over the other, or whether it is an arbitrary choice, that is considered equivalent. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/224
https://github.com/google/deepvariant/issues/224:413,Security,validat,validation,413,"Hi,. I had a question regarding how train/validation split was determined for training the models. From the DeepVariant paper, I noticed that chromosome 1-19 was used for training. Within chr1-19, is there a reason to prefer one of the following training-validation splits:; 1. Designate chr-a, 1 <= a <= 19, as validation set, and remaining as training set.; 2. Use some random subset of all labeled images as a validation set, and the disjoint set as training set (the random split may be grouped by sites, or not). Specifically, I am interested in knowing whether there was a reason for preferring one split over the other, or whether it is an arbitrary choice, that is considered equivalent. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/224
https://github.com/google/deepvariant/issues/225:121,Availability,error,error,121,"Attempting to run variant analysis on bam file aligned to GRCh38.p13 using GRCh38.p13 as the reference but receiving the error below from make_examples. Running in Google Cloud with the following script.sh:. #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=mbh-deepvariant-1; OUTPUT_BUCKET=gs://mbh-deepvariant-ouput-vcf; STAGING_FOLDER_NAME=staging_folder1; OUTPUT_FILE_NAME=HR090610illuminagr38DeepVariant.vcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/225
https://github.com/google/deepvariant/issues/225:1447,Deployability,pipeline,pipeline,1447,"deepvariant-1; OUTPUT_BUCKET=gs://mbh-deepvariant-ouput-vcf; STAGING_FOLDER_NAME=staging_folder1; OUTPUT_FILE_NAME=HR090610illuminagr38DeepVariant.vcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam file? The .fai file was created from the .fa file using samtools index command. . ValueError: Reference contigs span 3270284521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Con",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/225
https://github.com/google/deepvariant/issues/225:1480,Deployability,pipeline,pipelines,1480,"s/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam file? The .fai file was created from the .fa file using samtools index command. . ValueError: Reference contigs span 3270284521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""CM000663.2"" is 248956422 bp and IS MISSING, ""KI270706.1"" is 175055 bp and IS MISSING, ""KI270707.1"" is 32032 bp and IS MISSING, ""KI270708.1"" is 127682 bp and IS MISSING, ""KI270709.1"" is 66860 bp and IS MISSING, """,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/225
https://github.com/google/deepvariant/issues/225:1605,Testability,log,logging,1605,"-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam file? The .fai file was created from the .fa file using samtools index command. . ValueError: Reference contigs span 3270284521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""CM000663.2"" is 248956422 bp and IS MISSING, ""KI270706.1"" is 175055 bp and IS MISSING, ""KI270707.1"" is 32032 bp and IS MISSING, ""KI270708.1"" is 127682 bp and IS MISSING, ""KI270709.1"" is 66860 bp and IS MISSING, ""KI270710.1"" is 40176 bp and IS MISSING... Any feedback would be appreciated. Thanks, -Matt. [staging_folder1_logs_make_exa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/225
https://github.com/google/deepvariant/issues/225:1689,Testability,log,log,1689,"deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam file? The .fai file was created from the .fa file using samtools index command. . ValueError: Reference contigs span 3270284521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""CM000663.2"" is 248956422 bp and IS MISSING, ""KI270706.1"" is 175055 bp and IS MISSING, ""KI270707.1"" is 32032 bp and IS MISSING, ""KI270708.1"" is 127682 bp and IS MISSING, ""KI270709.1"" is 66860 bp and IS MISSING, ""KI270710.1"" is 40176 bp and IS MISSING... Any feedback would be appreciated. Thanks, -Matt. [staging_folder1_logs_make_examples_7.txt](https://github.com/google/deepvariant/files/3700231/staging_folder1_l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/225
https://github.com/google/deepvariant/issues/225:1819,Testability,log,log,1819,"roject ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam file? The .fai file was created from the .fa file using samtools index command. . ValueError: Reference contigs span 3270284521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""CM000663.2"" is 248956422 bp and IS MISSING, ""KI270706.1"" is 175055 bp and IS MISSING, ""KI270707.1"" is 32032 bp and IS MISSING, ""KI270708.1"" is 127682 bp and IS MISSING, ""KI270709.1"" is 66860 bp and IS MISSING, ""KI270710.1"" is 40176 bp and IS MISSING... Any feedback would be appreciated. Thanks, -Matt. [staging_folder1_logs_make_examples_7.txt](https://github.com/google/deepvariant/files/3700231/staging_folder1_logs_make_examples_7.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/225
https://github.com/google/deepvariant/issues/225:2556,Usability,feedback,feedback,2556,"roject ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam file? The .fai file was created from the .fa file using samtools index command. . ValueError: Reference contigs span 3270284521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""CM000663.2"" is 248956422 bp and IS MISSING, ""KI270706.1"" is 175055 bp and IS MISSING, ""KI270707.1"" is 32032 bp and IS MISSING, ""KI270708.1"" is 127682 bp and IS MISSING, ""KI270709.1"" is 66860 bp and IS MISSING, ""KI270710.1"" is 40176 bp and IS MISSING... Any feedback would be appreciated. Thanks, -Matt. [staging_folder1_logs_make_examples_7.txt](https://github.com/google/deepvariant/files/3700231/staging_folder1_logs_make_examples_7.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/225
https://github.com/google/deepvariant/issues/226:53,Availability,error,errors,53,"I ran deepvariant in docker on centos 7 but had some errors:; docker run -v \; > /db_students/genetic_map/dv_workarea/test \; > dajunluo/deepvariant:latest \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; > --reads=/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=test_output.vcf.gz \; > --output_gvcf=test_output.g.vcf.gz \; > --num_shards=2. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}. 2019-10-15 11:18:39.819615: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; 2019-10-15 11:18:39.819621: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m23.020s; user	0m1.610s; sys	0m3.206s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 1 | parallel -k --line-buffer /opt/de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/226
https://github.com/google/deepvariant/issues/226:1087,Availability,avail,available,1087,"rs:; docker run -v \; > /db_students/genetic_map/dv_workarea/test \; > dajunluo/deepvariant:latest \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; > --reads=/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=test_output.vcf.gz \; > --output_gvcf=test_output.g.vcf.gz \; > --num_shards=2. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}. 2019-10-15 11:18:39.819615: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; 2019-10-15 11:18:39.819621: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m23.020s; user	0m1.610s; sys	0m3.206s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/qu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/226
https://github.com/google/deepvariant/issues/226:1273,Availability,avail,available,1273,"art-testdata/ucsc.hg19.chr20.unittest.fasta \; > --reads=/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=test_output.vcf.gz \; > --output_gvcf=test_output.g.vcf.gz \; > --num_shards=2. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}. 2019-10-15 11:18:39.819615: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; 2019-10-15 11:18:39.819621: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m23.020s; user	0m1.610s; sys	0m3.206s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" -",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/226
https://github.com/google/deepvariant/issues/226:118,Testability,test,test,118,"I ran deepvariant in docker on centos 7 but had some errors:; docker run -v \; > /db_students/genetic_map/dv_workarea/test \; > dajunluo/deepvariant:latest \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; > --reads=/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=test_output.vcf.gz \; > --output_gvcf=test_output.g.vcf.gz \; > --num_shards=2. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}. 2019-10-15 11:18:39.819615: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; 2019-10-15 11:18:39.819621: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m23.020s; user	0m1.610s; sys	0m3.206s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 1 | parallel -k --line-buffer /opt/de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/226
https://github.com/google/deepvariant/issues/226:247,Testability,test,testdata,247,"I ran deepvariant in docker on centos 7 but had some errors:; docker run -v \; > /db_students/genetic_map/dv_workarea/test \; > dajunluo/deepvariant:latest \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; > --reads=/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=test_output.vcf.gz \; > --output_gvcf=test_output.g.vcf.gz \; > --num_shards=2. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}. 2019-10-15 11:18:39.819615: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; 2019-10-15 11:18:39.819621: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m23.020s; user	0m1.610s; sys	0m3.206s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 1 | parallel -k --line-buffer /opt/de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/226
https://github.com/google/deepvariant/issues/226:316,Testability,test,testdata,316,"I ran deepvariant in docker on centos 7 but had some errors:; docker run -v \; > /db_students/genetic_map/dv_workarea/test \; > dajunluo/deepvariant:latest \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; > --reads=/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=test_output.vcf.gz \; > --output_gvcf=test_output.g.vcf.gz \; > --num_shards=2. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}. 2019-10-15 11:18:39.819615: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; 2019-10-15 11:18:39.819621: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m23.020s; user	0m1.610s; sys	0m3.206s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 1 | parallel -k --line-buffer /opt/de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/226
https://github.com/google/deepvariant/issues/226:646,Testability,test,testdata,646,"I ran deepvariant in docker on centos 7 but had some errors:; docker run -v \; > /db_students/genetic_map/dv_workarea/test \; > dajunluo/deepvariant:latest \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; > --reads=/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=test_output.vcf.gz \; > --output_gvcf=test_output.g.vcf.gz \; > --num_shards=2. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}. 2019-10-15 11:18:39.819615: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; 2019-10-15 11:18:39.819621: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m23.020s; user	0m1.610s; sys	0m3.206s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 1 | parallel -k --line-buffer /opt/de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/226
https://github.com/google/deepvariant/issues/226:712,Testability,test,testdata,712,"I ran deepvariant in docker on centos 7 but had some errors:; docker run -v \; > /db_students/genetic_map/dv_workarea/test \; > dajunluo/deepvariant:latest \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; > --reads=/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=test_output.vcf.gz \; > --output_gvcf=test_output.g.vcf.gz \; > --num_shards=2. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}. 2019-10-15 11:18:39.819615: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; 2019-10-15 11:18:39.819621: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m23.020s; user	0m1.610s; sys	0m3.206s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 1 | parallel -k --line-buffer /opt/de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/226
https://github.com/google/deepvariant/issues/226:2067,Testability,test,testdata,2067,"vcf=test_output.vcf.gz \; > --output_gvcf=test_output.g.vcf.gz \; > --num_shards=2. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}. 2019-10-15 11:18:39.819615: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; 2019-10-15 11:18:39.819621: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m23.020s; user	0m1.610s; sys	0m3.206s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}' returned non-zero exit status 2. Do anyone know how to handle it?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/226
https://github.com/google/deepvariant/issues/226:2133,Testability,test,testdata,2133,"vcf=test_output.vcf.gz \; > --output_gvcf=test_output.g.vcf.gz \; > --num_shards=2. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}. 2019-10-15 11:18:39.819615: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; 2019-10-15 11:18:39.819621: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m23.020s; user	0m1.610s; sys	0m3.206s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}' returned non-zero exit status 2. Do anyone know how to handle it?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/226
https://github.com/google/deepvariant/issues/228:160,Availability,error,errors,160,"Hi, ; when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:. CondaError: Downloaded bytes did not match Content-Length; url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; Content-Length: 229846992; downloaded bytes: 217650750. Best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228
https://github.com/google/deepvariant/issues/228:208,Availability,Down,Downloaded,208,"Hi, ; when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:. CondaError: Downloaded bytes did not match Content-Length; url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; Content-Length: 229846992; downloaded bytes: 217650750. Best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228
https://github.com/google/deepvariant/issues/228:456,Availability,down,downloaded,456,"Hi, ; when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:. CondaError: Downloaded bytes did not match Content-Length; url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; Content-Length: 229846992; downloaded bytes: 217650750. Best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228
https://github.com/google/deepvariant/issues/228:13,Deployability,install,install,13,"Hi, ; when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:. CondaError: Downloaded bytes did not match Content-Length; url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; Content-Length: 229846992; downloaded bytes: 217650750. Best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228
https://github.com/google/deepvariant/issues/228:69,Deployability,install,install,69,"Hi, ; when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:. CondaError: Downloaded bytes did not match Content-Length; url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; Content-Length: 229846992; downloaded bytes: 217650750. Best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228
https://github.com/google/deepvariant/issues/228:125,Deployability,install,installed,125,"Hi, ; when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:. CondaError: Downloaded bytes did not match Content-Length; url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; Content-Length: 229846992; downloaded bytes: 217650750. Best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228
https://github.com/google/deepvariant/issues/228:181,Deployability,install,installation,181,"Hi, ; when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:. CondaError: Downloaded bytes did not match Content-Length; url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; Content-Length: 229846992; downloaded bytes: 217650750. Best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228
https://github.com/google/deepvariant/issues/229:726,Availability,error,error,726,"I would like to view and save the pileup images generated. . I was trying to get the features (image/encoded, and image/shape) from the example genereated, and decode then save it using the following code. ````; for candidate in candidates:; for example in self.create_pileup_examples(candidate):; features_temp = tf.parse_single_example(example_temp, features={'image/encoded': tf.FixedLenFeature([], tf.string),""image/shape"": tf.FixedLenFeature([], tf.int64)}); ; image_temp = features_temp['image/encoded'] ; shape_temp = features_temp['image/shape'] . image_temp1 = tf.reshape(tf.decode_raw(image_temp, tf.uint8), shape_temp). fh = open(filename_temp, ""wb""); fh.write(image_temp1); fh.close(); ````. and got the following error. ````; TypeError: Failed to convert object of type <class 'tensorflow.core.example.example_pb2.Example'> to Tensor.; ````; Is there a neat way to save all pileup images generated when we run the ""make example"" code?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/229
https://github.com/google/deepvariant/issues/230:539,Availability,down,downsampled,539,"I looked at https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md, where it says ""and, more `dowsample_fraction` during training"". Looking through make_examples it seems `downsample_fraction` is a fixed number for any given run of make_examples. My assumption currently is that make_examples is run multiple times with different `downsample_fraction` settings to obtain a wider mix of coverages in the training data, before running training (please correct me if wrong). I also had questions of how the downsampled mix was selected. E.g., were coverages in [20, 60]x equally distributed, or were the lower coverages used sparingly (since, for example, 20x is not a very interesting case)? Also, training a single model on multiple coverages and testing (variant calling) on a single coverage could be problematic (but in DeepVariant it is not), because the data distribution is different between the training and test sets. If there is a blog or article with some of these details, that would also be great!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230
https://github.com/google/deepvariant/issues/230:781,Testability,test,testing,781,"I looked at https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md, where it says ""and, more `dowsample_fraction` during training"". Looking through make_examples it seems `downsample_fraction` is a fixed number for any given run of make_examples. My assumption currently is that make_examples is run multiple times with different `downsample_fraction` settings to obtain a wider mix of coverages in the training data, before running training (please correct me if wrong). I also had questions of how the downsampled mix was selected. E.g., were coverages in [20, 60]x equally distributed, or were the lower coverages used sparingly (since, for example, 20x is not a very interesting case)? Also, training a single model on multiple coverages and testing (variant calling) on a single coverage could be problematic (but in DeepVariant it is not), because the data distribution is different between the training and test sets. If there is a blog or article with some of these details, that would also be great!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230
https://github.com/google/deepvariant/issues/230:949,Testability,test,test,949,"I looked at https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md, where it says ""and, more `dowsample_fraction` during training"". Looking through make_examples it seems `downsample_fraction` is a fixed number for any given run of make_examples. My assumption currently is that make_examples is run multiple times with different `downsample_fraction` settings to obtain a wider mix of coverages in the training data, before running training (please correct me if wrong). I also had questions of how the downsampled mix was selected. E.g., were coverages in [20, 60]x equally distributed, or were the lower coverages used sparingly (since, for example, 20x is not a very interesting case)? Also, training a single model on multiple coverages and testing (variant calling) on a single coverage could be problematic (but in DeepVariant it is not), because the data distribution is different between the training and test sets. If there is a blog or article with some of these details, that would also be great!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230
https://github.com/google/deepvariant/issues/231:373,Availability,error,error,373,"./build-prereq.sh ; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:527,Availability,avail,available,527,"./build-prereq.sh ; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:1461,Availability,error,error,1461,"x/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Un",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:1615,Availability,avail,available,1615,"s://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Unexpected error reading .blazerc file '/home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc'; ~/bazel ~/Downloads/deepvariant-r0.8; % Total % Received % Xfe",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:2049,Availability,Down,Downloads,2049,"28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Unexpected error reading .blazerc file '/home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc'; ~/bazel ~/Downloads/deepvariant-r0.8; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:2206,Availability,Down,Downloads,2206,"28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Unexpected error reading .blazerc file '/home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc'; ~/bazel ~/Downloads/deepvariant-r0.8; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:2323,Availability,Down,Downloads,2323,"28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Unexpected error reading .blazerc file '/home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc'; ~/bazel ~/Downloads/deepvariant-r0.8; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:2426,Availability,error,error,2426,"28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Unexpected error reading .blazerc file '/home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc'; ~/bazel ~/Downloads/deepvariant-r0.8; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:2476,Availability,Down,Downloads,2476,"28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Unexpected error reading .blazerc file '/home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc'; ~/bazel ~/Downloads/deepvariant-r0.8; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:2538,Availability,Down,Downloads,2538,"28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Unexpected error reading .blazerc file '/home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc'; ~/bazel ~/Downloads/deepvariant-r0.8; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:2723,Availability,error,error,2723,"28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Unexpected error reading .blazerc file '/home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc'; ~/bazel ~/Downloads/deepvariant-r0.8; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:103,Deployability,Install,Install,103,"./build-prereq.sh ; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:297,Deployability,Update,Update,297,"./build-prereq.sh ; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:721,Deployability,Install,Install,721,"./build-prereq.sh ; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:810,Deployability,Install,Install,810,"./build-prereq.sh ; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:1032,Deployability,Install,Install,1032," settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56]",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:1116,Deployability,Install,Install,1116," settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56]",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:1158,Deployability,Install,Installing,1158," settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56]",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:1257,Deployability,Install,Install,1257," settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56]",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:1423,Deployability,Update,Update,1423,"x/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Un",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:1809,Deployability,Install,Install,1809,"28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Unexpected error reading .blazerc file '/home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc'; ~/bazel ~/Downloads/deepvariant-r0.8; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:1898,Deployability,Install,Install,1898,"28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Unexpected error reading .blazerc file '/home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc'; ~/bazel ~/Downloads/deepvariant-r0.8; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:2742,Integrability,rout,routines,2742,"28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc files: /etc/bazel.bazelrc,/home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc,/home/bioinformatics/.bazelrc,/dev/null; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/.bazelrc; [bazel INFO src/main/cpp/rc_file.cc:56] Parsing the RcFile /home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc; [bazel FATAL src/main/cpp/blaze.cc:1311] Unexpected error reading .blazerc file '/home/bioinformatics/Downloads/deepvariant-r0.8/../tensorflow/.bazelrc'; ~/bazel ~/Downloads/deepvariant-r0.8; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0curl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:36,Modifiability,config,config,36,"./build-prereq.sh ; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:159,Modifiability,config,config,159,"./build-prereq.sh ; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:31,Performance,Load,Load,31,"./build-prereq.sh ; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:154,Performance,Load,Load,154,"./build-prereq.sh ; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/231:335,Security,password,password,335,"./build-prereq.sh ; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231
https://github.com/google/deepvariant/issues/232:218,Availability,error,errors,218,"Germline deep variant 0.8.0.; I have verified that the input mapping, the bed file, and the ref genome are all based on the same reference. The same script was used for several other samples and they completed without errors. ```; I1030 18:44:39.325231 140247813654272 make_examples.py:1164] Writing MakeExamplesRunInfo to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00063-of-00064.gz.run_info.pbtxt; I1030 18:44:39.367234 140247813654272 make_examples.py:1167] Found 484 candidate variants; I1030 18:44:39.367513 140247813654272 make_examples.py:1168] Created 496 examples. real	5m38.670s; user	85m59.188s; sys	1m53.684s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 63 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""ref.fa"" --reads ""input.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@64.gz"" --regions ""regions.bed"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@64.gz"" --task {}' returned non-zero exit status 16. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232
https://github.com/google/deepvariant/issues/233:12,Testability,test,testing,12,"Hi, ; I was testing the variants calling by deepvariant (DV) using PacBio CCS long reads https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/PacBio_SequelII_CCS_11kb/HG002.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.10x.trio.bam.; But I found some issues in the region 22:16,977,867-16,978,040 (hs37d5) show in the following IGV screenshot.; ![fig1](https://user-images.githubusercontent.com/27715065/68264666-81b49500-0084-11ea-8887-2283259b3df3.png). In the region, DV can call SNPs at position 16977891(A->G) and 17977975(A->G). A strange question, all the variants come out simultaneously in some reads at pos 16977870(A->T), 16977879(7bp insertion), 16977911(G->A), 16977924(T->C), 16977984(A->G) and 16978027(2bp deletion), the read counts support reference and variant allele is 15:12. What's more, I also test GATK4, both DV and GATK4 don't give these variants.; So I doubt that why DV doesn't give these positions as variants, if so, are there some tips for filtering these positions? Looking forward to your answer. Best!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/233
https://github.com/google/deepvariant/issues/233:856,Testability,test,test,856,"Hi, ; I was testing the variants calling by deepvariant (DV) using PacBio CCS long reads https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/PacBio_SequelII_CCS_11kb/HG002.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.10x.trio.bam.; But I found some issues in the region 22:16,977,867-16,978,040 (hs37d5) show in the following IGV screenshot.; ![fig1](https://user-images.githubusercontent.com/27715065/68264666-81b49500-0084-11ea-8887-2283259b3df3.png). In the region, DV can call SNPs at position 16977891(A->G) and 17977975(A->G). A strange question, all the variants come out simultaneously in some reads at pos 16977870(A->T), 16977879(7bp insertion), 16977911(G->A), 16977924(T->C), 16977984(A->G) and 16978027(2bp deletion), the read counts support reference and variant allele is 15:12. What's more, I also test GATK4, both DV and GATK4 don't give these variants.; So I doubt that why DV doesn't give these positions as variants, if so, are there some tips for filtering these positions? Looking forward to your answer. Best!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/233
https://github.com/google/deepvariant/pull/234:194,Deployability,Update,Update,194,"ed information. Note that previously there was a typo in WGS case study -- instead of 209m 53s, it was 309m 53s.; (2) Remove BIN_VERSION in the *_binaries.sh script because it was unused.; (3) Update a few instructions in training case study. PiperOrigin-RevId: 220115965. We are not taking pull requests at this time.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/234
https://github.com/google/deepvariant/issues/235:43,Availability,error,error,43,"Hi Deep Variant team,. I receive the below error when attempting to follow the [VCF stats report documentation](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); however, attempting to run vcf stats report interactively inside of the docker yielded an error message that was a bit more informative, telling me that the .py file does not exist. Error message from following the documentation ; > docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused ""exec: \""/opt/deepvariant/bin/vcf_stats_report\"": permission denied"": unknown. Error message from inside the docker; > python: can't open file '/opt/deepvariant/bin/vcf_stats_report.py': [Errno 2] No such file or directory. It seems that [lines 79 to 81 of the Dockerfile](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L79-L82) create the file called, /opt/deepvariant/bin/vcf_stats_report, but the underlying python script does not seem to be copied into the Docker. It looks like other files in the /opt/deepvariant/bin directory are copied over in [lines 42 to 50](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L42-L50), maybe a similar line needs to be added for vcf_stats_report?. Thank you for the 0.9.0 release and for being so active on Github. Im looking forward to using this tool for my research.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/235
https://github.com/google/deepvariant/issues/235:289,Availability,error,error,289,"Hi Deep Variant team,. I receive the below error when attempting to follow the [VCF stats report documentation](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); however, attempting to run vcf stats report interactively inside of the docker yielded an error message that was a bit more informative, telling me that the .py file does not exist. Error message from following the documentation ; > docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused ""exec: \""/opt/deepvariant/bin/vcf_stats_report\"": permission denied"": unknown. Error message from inside the docker; > python: can't open file '/opt/deepvariant/bin/vcf_stats_report.py': [Errno 2] No such file or directory. It seems that [lines 79 to 81 of the Dockerfile](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L79-L82) create the file called, /opt/deepvariant/bin/vcf_stats_report, but the underlying python script does not seem to be copied into the Docker. It looks like other files in the /opt/deepvariant/bin directory are copied over in [lines 42 to 50](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L42-L50), maybe a similar line needs to be added for vcf_stats_report?. Thank you for the 0.9.0 release and for being so active on Github. Im looking forward to using this tool for my research.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/235
https://github.com/google/deepvariant/issues/235:381,Availability,Error,Error,381,"Hi Deep Variant team,. I receive the below error when attempting to follow the [VCF stats report documentation](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); however, attempting to run vcf stats report interactively inside of the docker yielded an error message that was a bit more informative, telling me that the .py file does not exist. Error message from following the documentation ; > docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused ""exec: \""/opt/deepvariant/bin/vcf_stats_report\"": permission denied"": unknown. Error message from inside the docker; > python: can't open file '/opt/deepvariant/bin/vcf_stats_report.py': [Errno 2] No such file or directory. It seems that [lines 79 to 81 of the Dockerfile](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L79-L82) create the file called, /opt/deepvariant/bin/vcf_stats_report, but the underlying python script does not seem to be copied into the Docker. It looks like other files in the /opt/deepvariant/bin directory are copied over in [lines 42 to 50](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L42-L50), maybe a similar line needs to be added for vcf_stats_report?. Thank you for the 0.9.0 release and for being so active on Github. Im looking forward to using this tool for my research.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/235
https://github.com/google/deepvariant/issues/235:440,Availability,Error,Error,440,"Hi Deep Variant team,. I receive the below error when attempting to follow the [VCF stats report documentation](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); however, attempting to run vcf stats report interactively inside of the docker yielded an error message that was a bit more informative, telling me that the .py file does not exist. Error message from following the documentation ; > docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused ""exec: \""/opt/deepvariant/bin/vcf_stats_report\"": permission denied"": unknown. Error message from inside the docker; > python: can't open file '/opt/deepvariant/bin/vcf_stats_report.py': [Errno 2] No such file or directory. It seems that [lines 79 to 81 of the Dockerfile](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L79-L82) create the file called, /opt/deepvariant/bin/vcf_stats_report, but the underlying python script does not seem to be copied into the Docker. It looks like other files in the /opt/deepvariant/bin directory are copied over in [lines 42 to 50](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L42-L50), maybe a similar line needs to be added for vcf_stats_report?. Thank you for the 0.9.0 release and for being so active on Github. Im looking forward to using this tool for my research.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/235
https://github.com/google/deepvariant/issues/235:632,Availability,Error,Error,632,"Hi Deep Variant team,. I receive the below error when attempting to follow the [VCF stats report documentation](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); however, attempting to run vcf stats report interactively inside of the docker yielded an error message that was a bit more informative, telling me that the .py file does not exist. Error message from following the documentation ; > docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused ""exec: \""/opt/deepvariant/bin/vcf_stats_report\"": permission denied"": unknown. Error message from inside the docker; > python: can't open file '/opt/deepvariant/bin/vcf_stats_report.py': [Errno 2] No such file or directory. It seems that [lines 79 to 81 of the Dockerfile](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L79-L82) create the file called, /opt/deepvariant/bin/vcf_stats_report, but the underlying python script does not seem to be copied into the Docker. It looks like other files in the /opt/deepvariant/bin directory are copied over in [lines 42 to 50](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L42-L50), maybe a similar line needs to be added for vcf_stats_report?. Thank you for the 0.9.0 release and for being so active on Github. Im looking forward to using this tool for my research.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/235
https://github.com/google/deepvariant/issues/235:1289,Deployability,release,release,1289,"Hi Deep Variant team,. I receive the below error when attempting to follow the [VCF stats report documentation](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); however, attempting to run vcf stats report interactively inside of the docker yielded an error message that was a bit more informative, telling me that the .py file does not exist. Error message from following the documentation ; > docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused ""exec: \""/opt/deepvariant/bin/vcf_stats_report\"": permission denied"": unknown. Error message from inside the docker; > python: can't open file '/opt/deepvariant/bin/vcf_stats_report.py': [Errno 2] No such file or directory. It seems that [lines 79 to 81 of the Dockerfile](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L79-L82) create the file called, /opt/deepvariant/bin/vcf_stats_report, but the underlying python script does not seem to be copied into the Docker. It looks like other files in the /opt/deepvariant/bin directory are copied over in [lines 42 to 50](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L42-L50), maybe a similar line needs to be added for vcf_stats_report?. Thank you for the 0.9.0 release and for being so active on Github. Im looking forward to using this tool for my research.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/235
https://github.com/google/deepvariant/issues/235:295,Integrability,message,message,295,"Hi Deep Variant team,. I receive the below error when attempting to follow the [VCF stats report documentation](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); however, attempting to run vcf stats report interactively inside of the docker yielded an error message that was a bit more informative, telling me that the .py file does not exist. Error message from following the documentation ; > docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused ""exec: \""/opt/deepvariant/bin/vcf_stats_report\"": permission denied"": unknown. Error message from inside the docker; > python: can't open file '/opt/deepvariant/bin/vcf_stats_report.py': [Errno 2] No such file or directory. It seems that [lines 79 to 81 of the Dockerfile](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L79-L82) create the file called, /opt/deepvariant/bin/vcf_stats_report, but the underlying python script does not seem to be copied into the Docker. It looks like other files in the /opt/deepvariant/bin directory are copied over in [lines 42 to 50](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L42-L50), maybe a similar line needs to be added for vcf_stats_report?. Thank you for the 0.9.0 release and for being so active on Github. Im looking forward to using this tool for my research.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/235
https://github.com/google/deepvariant/issues/235:387,Integrability,message,message,387,"Hi Deep Variant team,. I receive the below error when attempting to follow the [VCF stats report documentation](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); however, attempting to run vcf stats report interactively inside of the docker yielded an error message that was a bit more informative, telling me that the .py file does not exist. Error message from following the documentation ; > docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused ""exec: \""/opt/deepvariant/bin/vcf_stats_report\"": permission denied"": unknown. Error message from inside the docker; > python: can't open file '/opt/deepvariant/bin/vcf_stats_report.py': [Errno 2] No such file or directory. It seems that [lines 79 to 81 of the Dockerfile](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L79-L82) create the file called, /opt/deepvariant/bin/vcf_stats_report, but the underlying python script does not seem to be copied into the Docker. It looks like other files in the /opt/deepvariant/bin directory are copied over in [lines 42 to 50](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L42-L50), maybe a similar line needs to be added for vcf_stats_report?. Thank you for the 0.9.0 release and for being so active on Github. Im looking forward to using this tool for my research.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/235
https://github.com/google/deepvariant/issues/235:638,Integrability,message,message,638,"Hi Deep Variant team,. I receive the below error when attempting to follow the [VCF stats report documentation](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); however, attempting to run vcf stats report interactively inside of the docker yielded an error message that was a bit more informative, telling me that the .py file does not exist. Error message from following the documentation ; > docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused ""exec: \""/opt/deepvariant/bin/vcf_stats_report\"": permission denied"": unknown. Error message from inside the docker; > python: can't open file '/opt/deepvariant/bin/vcf_stats_report.py': [Errno 2] No such file or directory. It seems that [lines 79 to 81 of the Dockerfile](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L79-L82) create the file called, /opt/deepvariant/bin/vcf_stats_report, but the underlying python script does not seem to be copied into the Docker. It looks like other files in the /opt/deepvariant/bin directory are copied over in [lines 42 to 50](https://github.com/google/deepvariant/blob/r0.9/Dockerfile#L42-L50), maybe a similar line needs to be added for vcf_stats_report?. Thank you for the 0.9.0 release and for being so active on Github. Im looking forward to using this tool for my research.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/235
https://github.com/google/deepvariant/issues/236:288,Availability,error,error,288,Hello DeepVariant team. We're trying to use your great tool and I'm creating our own Docker for this.; Yesterday I finally fixed all issues with v0.8.0 version - all tests from build_and_test.sh passed and saw v0.9.0 release happened several days before - I tried to switch to in and get error at build_and_test.sh stage:; ```; deepvariant/variant_calling.cc:36:20: fatal error: optional: No such file or directory; #include <optional>; ^; compilation terminated.; ```; Looks like you added new include lines in `deepvariant/variant_calling.cc` between releases - [this](https://github.com/google/deepvariant/blob/r0.9/deepvariant/variant_calling.cc#L36) line causing my error.; Can you help me please? Do I need to install some external dependencies or what?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236
https://github.com/google/deepvariant/issues/236:372,Availability,error,error,372,Hello DeepVariant team. We're trying to use your great tool and I'm creating our own Docker for this.; Yesterday I finally fixed all issues with v0.8.0 version - all tests from build_and_test.sh passed and saw v0.9.0 release happened several days before - I tried to switch to in and get error at build_and_test.sh stage:; ```; deepvariant/variant_calling.cc:36:20: fatal error: optional: No such file or directory; #include <optional>; ^; compilation terminated.; ```; Looks like you added new include lines in `deepvariant/variant_calling.cc` between releases - [this](https://github.com/google/deepvariant/blob/r0.9/deepvariant/variant_calling.cc#L36) line causing my error.; Can you help me please? Do I need to install some external dependencies or what?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236
https://github.com/google/deepvariant/issues/236:671,Availability,error,error,671,Hello DeepVariant team. We're trying to use your great tool and I'm creating our own Docker for this.; Yesterday I finally fixed all issues with v0.8.0 version - all tests from build_and_test.sh passed and saw v0.9.0 release happened several days before - I tried to switch to in and get error at build_and_test.sh stage:; ```; deepvariant/variant_calling.cc:36:20: fatal error: optional: No such file or directory; #include <optional>; ^; compilation terminated.; ```; Looks like you added new include lines in `deepvariant/variant_calling.cc` between releases - [this](https://github.com/google/deepvariant/blob/r0.9/deepvariant/variant_calling.cc#L36) line causing my error.; Can you help me please? Do I need to install some external dependencies or what?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236
https://github.com/google/deepvariant/issues/236:217,Deployability,release,release,217,Hello DeepVariant team. We're trying to use your great tool and I'm creating our own Docker for this.; Yesterday I finally fixed all issues with v0.8.0 version - all tests from build_and_test.sh passed and saw v0.9.0 release happened several days before - I tried to switch to in and get error at build_and_test.sh stage:; ```; deepvariant/variant_calling.cc:36:20: fatal error: optional: No such file or directory; #include <optional>; ^; compilation terminated.; ```; Looks like you added new include lines in `deepvariant/variant_calling.cc` between releases - [this](https://github.com/google/deepvariant/blob/r0.9/deepvariant/variant_calling.cc#L36) line causing my error.; Can you help me please? Do I need to install some external dependencies or what?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236
https://github.com/google/deepvariant/issues/236:553,Deployability,release,releases,553,Hello DeepVariant team. We're trying to use your great tool and I'm creating our own Docker for this.; Yesterday I finally fixed all issues with v0.8.0 version - all tests from build_and_test.sh passed and saw v0.9.0 release happened several days before - I tried to switch to in and get error at build_and_test.sh stage:; ```; deepvariant/variant_calling.cc:36:20: fatal error: optional: No such file or directory; #include <optional>; ^; compilation terminated.; ```; Looks like you added new include lines in `deepvariant/variant_calling.cc` between releases - [this](https://github.com/google/deepvariant/blob/r0.9/deepvariant/variant_calling.cc#L36) line causing my error.; Can you help me please? Do I need to install some external dependencies or what?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236
https://github.com/google/deepvariant/issues/236:716,Deployability,install,install,716,Hello DeepVariant team. We're trying to use your great tool and I'm creating our own Docker for this.; Yesterday I finally fixed all issues with v0.8.0 version - all tests from build_and_test.sh passed and saw v0.9.0 release happened several days before - I tried to switch to in and get error at build_and_test.sh stage:; ```; deepvariant/variant_calling.cc:36:20: fatal error: optional: No such file or directory; #include <optional>; ^; compilation terminated.; ```; Looks like you added new include lines in `deepvariant/variant_calling.cc` between releases - [this](https://github.com/google/deepvariant/blob/r0.9/deepvariant/variant_calling.cc#L36) line causing my error.; Can you help me please? Do I need to install some external dependencies or what?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236
https://github.com/google/deepvariant/issues/236:738,Integrability,depend,dependencies,738,Hello DeepVariant team. We're trying to use your great tool and I'm creating our own Docker for this.; Yesterday I finally fixed all issues with v0.8.0 version - all tests from build_and_test.sh passed and saw v0.9.0 release happened several days before - I tried to switch to in and get error at build_and_test.sh stage:; ```; deepvariant/variant_calling.cc:36:20: fatal error: optional: No such file or directory; #include <optional>; ^; compilation terminated.; ```; Looks like you added new include lines in `deepvariant/variant_calling.cc` between releases - [this](https://github.com/google/deepvariant/blob/r0.9/deepvariant/variant_calling.cc#L36) line causing my error.; Can you help me please? Do I need to install some external dependencies or what?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236
https://github.com/google/deepvariant/issues/236:166,Testability,test,tests,166,Hello DeepVariant team. We're trying to use your great tool and I'm creating our own Docker for this.; Yesterday I finally fixed all issues with v0.8.0 version - all tests from build_and_test.sh passed and saw v0.9.0 release happened several days before - I tried to switch to in and get error at build_and_test.sh stage:; ```; deepvariant/variant_calling.cc:36:20: fatal error: optional: No such file or directory; #include <optional>; ^; compilation terminated.; ```; Looks like you added new include lines in `deepvariant/variant_calling.cc` between releases - [this](https://github.com/google/deepvariant/blob/r0.9/deepvariant/variant_calling.cc#L36) line causing my error.; Can you help me please? Do I need to install some external dependencies or what?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236
https://github.com/google/deepvariant/issues/237:160,Availability,avail,available,160,"I have searched at the [Evaluation regions for HG002  Issue #15  google/deepvariant](https://github.com/google/deepvariant/issues/15), but the website is not available. Here is the non-intersected BED file:. https://dl.dnanex.us/F/D/y61fXf4qz5G8F4QvXz14bkYQ5Q9Yy5q5JPf1K3kg/agilent_sureselect_human_all_exon_v5_b37_targets.bed. Would you please offer the reachable site of the file ""agilent_sureselect_human_all_exon_v5_b37_targets.bed""?. Thanks a lot.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/237
https://github.com/google/deepvariant/issues/238:164,Availability,down,downstream,164,"Dear DeepVariant Team,. I have a short question. Are there any plans for calling MNPs in near future? This would be really helpfull for variant annotations further downstream. Sebastian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/238
https://github.com/google/deepvariant/issues/239:28848,Deployability,INTEGRAT,INTEGRATION,28848,"mbly=b37>; ##contig=<ID=chr8,length=146364022,assembly=b37>; ##contig=<ID=chr9,length=141213431,assembly=b37>; ##contig=<ID=chr10,length=135534747,assembly=b37>; ##contig=<ID=chr11,length=135006516,assembly=b37>; ##contig=<ID=chr12,length=133851895,assembly=b37>; ##contig=<ID=chr13,length=115169878,assembly=b37>; ##contig=<ID=chr14,length=107349540,assembly=b37>; ##contig=<ID=chr15,length=102531392,assembly=b37>; ##contig=<ID=chr16,length=90354753,assembly=b37>; ##contig=<ID=chr17,length=81195210,assembly=b37>; ##contig=<ID=chr18,length=78077248,assembly=b37>; ##contig=<ID=chr19,length=59128983,assembly=b37>; ##contig=<ID=chr20,length=63025520,assembly=b37>; ##contig=<ID=chr21,length=48129895,assembly=b37>; ##contig=<ID=chr22,length=51304566,assembly=b37>; ##contig=<ID=chrX,length=155270560,assembly=b37>; ##contig=<ID=chrY,length=59373566,assembly=b37>; ##contig=<ID=chrM,length=16569,assembly=b37>; ##fileDate=20160329; ##reference=human_g1k_v37.fasta; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	INTEGRATION; chr20	10000117	.	C	T	50	PASS	platforms=3;platformnames=Illumina,CG,Solid;datasets=3;datasetnames=HiSeqPE300x,CGnormal,SolidSE75bp;callsets=4;callsetnames=HiSeqPE300xfreebayes,HiSeqPE300xGATK,CGnormal,SolidSE75GATKHC;datasetsmissingcall=IonExome,SolidPE50x50bp;lowcov=CS_IonExomeTVC_lowcov,CS_SolidPE50x50GATKHC_lowcov,CS_SolidSE75GATKHC_lowcov;filt=CS_HiSeqPE300xGATK_filt	GT:PS:DP:GQ	0/1:.:706:878; chr20	10000211	.	C	T	50	PASS	platforms=2;platformnames=Illumina,CG;datasets=2;datasetnames=HiSeqPE300x,CGnormal;callsets=3;callsetnames=HiSeqPE300xfreebayes,HiSeqPE300xGATK,CGnormal;datasetsmissingcall=IonExome,SolidPE50x50bp,SolidSE75bp;lowcov=CS_IonExomeTVC_lowcov,CS_SolidPE50x50GATKHC_lowcov,CS_SolidSE75GATKHC_lowcov;filt=CS_SolidPE50x50GATKHC_filt	GT:PS:DP:GQ	0/1:.:695:984; chr20	10000439	.	T	G	50	PASS	platforms=3;platformnames=Illumina,CG,Solid;datasets=4;datasetnames=HiSeqPE300x,CGnormal,SolidPE50x50bp,SolidSE75bp;callsets=5;callsetnames=HiSeqPE300xfreebayes,HiSe",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/239
https://github.com/google/deepvariant/issues/239:28848,Integrability,INTEGRAT,INTEGRATION,28848,"mbly=b37>; ##contig=<ID=chr8,length=146364022,assembly=b37>; ##contig=<ID=chr9,length=141213431,assembly=b37>; ##contig=<ID=chr10,length=135534747,assembly=b37>; ##contig=<ID=chr11,length=135006516,assembly=b37>; ##contig=<ID=chr12,length=133851895,assembly=b37>; ##contig=<ID=chr13,length=115169878,assembly=b37>; ##contig=<ID=chr14,length=107349540,assembly=b37>; ##contig=<ID=chr15,length=102531392,assembly=b37>; ##contig=<ID=chr16,length=90354753,assembly=b37>; ##contig=<ID=chr17,length=81195210,assembly=b37>; ##contig=<ID=chr18,length=78077248,assembly=b37>; ##contig=<ID=chr19,length=59128983,assembly=b37>; ##contig=<ID=chr20,length=63025520,assembly=b37>; ##contig=<ID=chr21,length=48129895,assembly=b37>; ##contig=<ID=chr22,length=51304566,assembly=b37>; ##contig=<ID=chrX,length=155270560,assembly=b37>; ##contig=<ID=chrY,length=59373566,assembly=b37>; ##contig=<ID=chrM,length=16569,assembly=b37>; ##fileDate=20160329; ##reference=human_g1k_v37.fasta; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	INTEGRATION; chr20	10000117	.	C	T	50	PASS	platforms=3;platformnames=Illumina,CG,Solid;datasets=3;datasetnames=HiSeqPE300x,CGnormal,SolidSE75bp;callsets=4;callsetnames=HiSeqPE300xfreebayes,HiSeqPE300xGATK,CGnormal,SolidSE75GATKHC;datasetsmissingcall=IonExome,SolidPE50x50bp;lowcov=CS_IonExomeTVC_lowcov,CS_SolidPE50x50GATKHC_lowcov,CS_SolidSE75GATKHC_lowcov;filt=CS_HiSeqPE300xGATK_filt	GT:PS:DP:GQ	0/1:.:706:878; chr20	10000211	.	C	T	50	PASS	platforms=2;platformnames=Illumina,CG;datasets=2;datasetnames=HiSeqPE300x,CGnormal;callsets=3;callsetnames=HiSeqPE300xfreebayes,HiSeqPE300xGATK,CGnormal;datasetsmissingcall=IonExome,SolidPE50x50bp,SolidSE75bp;lowcov=CS_IonExomeTVC_lowcov,CS_SolidPE50x50GATKHC_lowcov,CS_SolidSE75GATKHC_lowcov;filt=CS_SolidPE50x50GATKHC_filt	GT:PS:DP:GQ	0/1:.:695:984; chr20	10000439	.	T	G	50	PASS	platforms=3;platformnames=Illumina,CG,Solid;datasets=4;datasetnames=HiSeqPE300x,CGnormal,SolidPE50x50bp,SolidSE75bp;callsets=5;callsetnames=HiSeqPE300xfreebayes,HiSe",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/239
https://github.com/google/deepvariant/issues/239:101,Testability,test,test,101,"Hello DeepVariant team. We trying to implement your tool, it works, but we have different results on test data:. <details>; <summary>Our test VCF (DeepVariant v0.8.0)</summary>. ```; ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##FILTER=<ID=RefCall,Description=""Genotyping model thinks this site is reference."">; ##FILTER=<ID=LowQual,Description=""Confidence in this variant being real is below calling threshold."">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End position (for use with symbolic alleles)"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">; ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">; ##contig=<ID=chr20,length=63025520>; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	NA12878; chr20	9999996	.	A	ACT	44.4	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:41:44:0,44:1:44,44,0; chr20	10000117	.	C	T	36.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:34:55:25,30:0.545455:36,0,37; chr20	10000211	.	C	T	34.1	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:34:59:30,29:0.491525:34,0,51; chr20	10000439	.	T	G	40	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:39:72:0,72:1:40,45,0; chr20	10000598	.	T	A	62.2	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:54:46:0,46:1:62,55,0; chr20	10000694	.	G	A	35.1	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:35:48:26,22:0.458333:35,0,47; chr20	10000758	.	T	A	69.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:56:56:0,56:1:69,55,0; chr20	10001019	.	T	G	2.1	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:4:44:31,13:0.295455:0,2,34; chr20	10001298	.	T	A	54.7	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:49:43:0,43:1:54,50,0; chr20	10001436	.	A	AAGGCT	38.7	PASS	.	GT:G",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/239
https://github.com/google/deepvariant/issues/239:137,Testability,test,test,137,"Hello DeepVariant team. We trying to implement your tool, it works, but we have different results on test data:. <details>; <summary>Our test VCF (DeepVariant v0.8.0)</summary>. ```; ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##FILTER=<ID=RefCall,Description=""Genotyping model thinks this site is reference."">; ##FILTER=<ID=LowQual,Description=""Confidence in this variant being real is below calling threshold."">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End position (for use with symbolic alleles)"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">; ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">; ##contig=<ID=chr20,length=63025520>; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	NA12878; chr20	9999996	.	A	ACT	44.4	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:41:44:0,44:1:44,44,0; chr20	10000117	.	C	T	36.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:34:55:25,30:0.545455:36,0,37; chr20	10000211	.	C	T	34.1	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:34:59:30,29:0.491525:34,0,51; chr20	10000439	.	T	G	40	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:39:72:0,72:1:40,45,0; chr20	10000598	.	T	A	62.2	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:54:46:0,46:1:62,55,0; chr20	10000694	.	G	A	35.1	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:35:48:26,22:0.458333:35,0,47; chr20	10000758	.	T	A	69.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:56:56:0,56:1:69,55,0; chr20	10001019	.	T	G	2.1	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:4:44:31,13:0.295455:0,2,34; chr20	10001298	.	T	A	54.7	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:49:43:0,43:1:54,50,0; chr20	10001436	.	A	AAGGCT	38.7	PASS	.	GT:G",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/239
https://github.com/google/deepvariant/issues/241:159,Deployability,release,released,159,"Hi,. Are there any plans to extend DeepVariant to somatic variant calling? The current model seems to be inherently diploid. What is the training time for the released versions of DeepVariant? The Supplementary information from the Nature paper mentions something about ""80 hours"" but does not specify which kind of hardware was used?. Do you have any numbers on how much the neural network improves the accuracy as compared to the raw (over-sensitive) variant calls output after the haplotype-aware realignment step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/241
https://github.com/google/deepvariant/issues/241:28,Modifiability,extend,extend,28,"Hi,. Are there any plans to extend DeepVariant to somatic variant calling? The current model seems to be inherently diploid. What is the training time for the released versions of DeepVariant? The Supplementary information from the Nature paper mentions something about ""80 hours"" but does not specify which kind of hardware was used?. Do you have any numbers on how much the neural network improves the accuracy as compared to the raw (over-sensitive) variant calls output after the haplotype-aware realignment step?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/241
https://github.com/google/deepvariant/issues/242:836,Security,expose,exposed,836,"Running several DeepVariant jobs (v0.9.0) on the same server, e.g., by splitting a dataset via the `--regions` option, results in corrupted files (DataLossError) because of non-unique file names for the temp files generated under `/tmp` on the server (confirmed, see below). > I just checked the code, and you're right that the temp file names will be the same:; > https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py#L264-L266; > For now, please pass in different `intermediate_results_dir` for each run.; > For example:; > `--intermediate_results_dir=""/tmp/deepvariant_tmp_output/chr1""` for chr1, and so on. > I'll think about how we want to improve this in the future. I can think of a few options for future improvements, such as :. > 1. Use a random name for the internal /tmp files. Given that these are not exposed to the users anyway.; > 2. Use a unique name derived from the output VCF file, instead of calling all temp files the same name. > For now, using the `--intermediate_results_dir` should hopefully resolve your issue. Let me know if it works. If you have a suggestion on what's the best future improvement for better user experience, please let me know. _Originally posted by @pichuan in https://github.com/google/deepvariant/issues/175#issuecomment-560625427_. @pichuan; I think as an immediate step, updating the docs and making this explicit (also the option of using the `intermediate_results_dir`) would be reasonable. Concerning a proper solution, creating a randomly named temp dir for all temp files of the job (which then could have non-random names) seems like a very straightforward way; ideally, this should also take care of the clean-up, e.g.,as it is provided by Python's `tempfile` module. I assume basic functionality like this exists in all relevant programming languages. +Peter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/242
https://github.com/google/deepvariant/issues/242:1158,Usability,user experience,user experience,1158,"Running several DeepVariant jobs (v0.9.0) on the same server, e.g., by splitting a dataset via the `--regions` option, results in corrupted files (DataLossError) because of non-unique file names for the temp files generated under `/tmp` on the server (confirmed, see below). > I just checked the code, and you're right that the temp file names will be the same:; > https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py#L264-L266; > For now, please pass in different `intermediate_results_dir` for each run.; > For example:; > `--intermediate_results_dir=""/tmp/deepvariant_tmp_output/chr1""` for chr1, and so on. > I'll think about how we want to improve this in the future. I can think of a few options for future improvements, such as :. > 1. Use a random name for the internal /tmp files. Given that these are not exposed to the users anyway.; > 2. Use a unique name derived from the output VCF file, instead of calling all temp files the same name. > For now, using the `--intermediate_results_dir` should hopefully resolve your issue. Let me know if it works. If you have a suggestion on what's the best future improvement for better user experience, please let me know. _Originally posted by @pichuan in https://github.com/google/deepvariant/issues/175#issuecomment-560625427_. @pichuan; I think as an immediate step, updating the docs and making this explicit (also the option of using the `intermediate_results_dir`) would be reasonable. Concerning a proper solution, creating a randomly named temp dir for all temp files of the job (which then could have non-random names) seems like a very straightforward way; ideally, this should also take care of the clean-up, e.g.,as it is provided by Python's `tempfile` module. I assume basic functionality like this exists in all relevant programming languages. +Peter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/242
https://github.com/google/deepvariant/issues/243:2193,Availability,error,error,2193,"kages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>; import numpy as np; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import core; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 47, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the multiarray numpy extension module failed. Most; likely you are trying to import a failed build of numpy.; Here is how to proceed:; - If you're working with a numpy git repository, try `git clean -xdf`; (removes all files not under version control) and rebuild numpy.; - If you are simply trying to use the numpy version that you have installed:; your installation is broken - please reinstall numpy.; - If you have already reinstalled and that did not fix the problem, then:; 1. Check that you are using the Python you expect (you're using /usr/bin/python),; and that you have no directories in your PATH or PYTHONPATH that can; interfere with the Python and numpy versions you're trying to use.; 2. If (1) looks fine, you can open a new issue at; https://github.com/numpy/numpy/issues. Please include details on:; - how you installed Python; - how you installed numpy; - your operating system; - whether or not you have multiple versions of Python installed; - if you built from source, your compiler versions and ideally a build log. Note: this error has many possible causes, so please don't comment on; an existing issue about this - open a new one instead. Original error was: libopenblas.so.0: cannot open shared object file: No such file or directory; ```. I need to run deepvariant as a non-root user via singulairty on the HPC platform. The non-GPU version works just fine.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243
https://github.com/google/deepvariant/issues/243:2317,Availability,error,error,2317,"kages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>; import numpy as np; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import core; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 47, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the multiarray numpy extension module failed. Most; likely you are trying to import a failed build of numpy.; Here is how to proceed:; - If you're working with a numpy git repository, try `git clean -xdf`; (removes all files not under version control) and rebuild numpy.; - If you are simply trying to use the numpy version that you have installed:; your installation is broken - please reinstall numpy.; - If you have already reinstalled and that did not fix the problem, then:; 1. Check that you are using the Python you expect (you're using /usr/bin/python),; and that you have no directories in your PATH or PYTHONPATH that can; interfere with the Python and numpy versions you're trying to use.; 2. If (1) looks fine, you can open a new issue at; https://github.com/numpy/numpy/issues. Please include details on:; - how you installed Python; - how you installed numpy; - your operating system; - whether or not you have multiple versions of Python installed; - if you built from source, your compiler versions and ideally a build log. Note: this error has many possible causes, so please don't comment on; an existing issue about this - open a new one instead. Original error was: libopenblas.so.0: cannot open shared object file: No such file or directory; ```. I need to run deepvariant as a non-root user via singulairty on the HPC platform. The non-GPU version works just fine.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243
https://github.com/google/deepvariant/issues/243:1480,Deployability,install,installed,1480," ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>; import numpy as np; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import core; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 47, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the multiarray numpy extension module failed. Most; likely you are trying to import a failed build of numpy.; Here is how to proceed:; - If you're working with a numpy git repository, try `git clean -xdf`; (removes all files not under version control) and rebuild numpy.; - If you are simply trying to use the numpy version that you have installed:; your installation is broken - please reinstall numpy.; - If you have already reinstalled and that did not fix the problem, then:; 1. Check that you are using the Python you expect (you're using /usr/bin/python),; and that you have no directories in your PATH or PYTHONPATH that can; interfere with the Python and numpy versions you're trying to use.; 2. If (1) looks fine, you can open a new issue at; https://github.com/numpy/numpy/issues. Please include details on:; - how you installed Python; - how you installed numpy; - your operating system; - whether or not you have multiple versions of Python installed; - if you built from source, your compiler versions and ideally a build log. Note: this error has many possible causes, so please don't comment on; an existing issue about this - open a new one instead. Original error was: libopenblas.so.0: cannot open shared object file: No such file or directory; ```. I need to run deepvariant as a non-root user via singulairty on the H",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243
https://github.com/google/deepvariant/issues/243:1497,Deployability,install,installation,1497," ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>; import numpy as np; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import core; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 47, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the multiarray numpy extension module failed. Most; likely you are trying to import a failed build of numpy.; Here is how to proceed:; - If you're working with a numpy git repository, try `git clean -xdf`; (removes all files not under version control) and rebuild numpy.; - If you are simply trying to use the numpy version that you have installed:; your installation is broken - please reinstall numpy.; - If you have already reinstalled and that did not fix the problem, then:; 1. Check that you are using the Python you expect (you're using /usr/bin/python),; and that you have no directories in your PATH or PYTHONPATH that can; interfere with the Python and numpy versions you're trying to use.; 2. If (1) looks fine, you can open a new issue at; https://github.com/numpy/numpy/issues. Please include details on:; - how you installed Python; - how you installed numpy; - your operating system; - whether or not you have multiple versions of Python installed; - if you built from source, your compiler versions and ideally a build log. Note: this error has many possible causes, so please don't comment on; an existing issue about this - open a new one instead. Original error was: libopenblas.so.0: cannot open shared object file: No such file or directory; ```. I need to run deepvariant as a non-root user via singulairty on the H",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243
https://github.com/google/deepvariant/issues/243:1971,Deployability,install,installed,1971,"kages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>; import numpy as np; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import core; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 47, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the multiarray numpy extension module failed. Most; likely you are trying to import a failed build of numpy.; Here is how to proceed:; - If you're working with a numpy git repository, try `git clean -xdf`; (removes all files not under version control) and rebuild numpy.; - If you are simply trying to use the numpy version that you have installed:; your installation is broken - please reinstall numpy.; - If you have already reinstalled and that did not fix the problem, then:; 1. Check that you are using the Python you expect (you're using /usr/bin/python),; and that you have no directories in your PATH or PYTHONPATH that can; interfere with the Python and numpy versions you're trying to use.; 2. If (1) looks fine, you can open a new issue at; https://github.com/numpy/numpy/issues. Please include details on:; - how you installed Python; - how you installed numpy; - your operating system; - whether or not you have multiple versions of Python installed; - if you built from source, your compiler versions and ideally a build log. Note: this error has many possible causes, so please don't comment on; an existing issue about this - open a new one instead. Original error was: libopenblas.so.0: cannot open shared object file: No such file or directory; ```. I need to run deepvariant as a non-root user via singulairty on the HPC platform. The non-GPU version works just fine.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243
https://github.com/google/deepvariant/issues/243:1999,Deployability,install,installed,1999,"kages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>; import numpy as np; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import core; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 47, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the multiarray numpy extension module failed. Most; likely you are trying to import a failed build of numpy.; Here is how to proceed:; - If you're working with a numpy git repository, try `git clean -xdf`; (removes all files not under version control) and rebuild numpy.; - If you are simply trying to use the numpy version that you have installed:; your installation is broken - please reinstall numpy.; - If you have already reinstalled and that did not fix the problem, then:; 1. Check that you are using the Python you expect (you're using /usr/bin/python),; and that you have no directories in your PATH or PYTHONPATH that can; interfere with the Python and numpy versions you're trying to use.; 2. If (1) looks fine, you can open a new issue at; https://github.com/numpy/numpy/issues. Please include details on:; - how you installed Python; - how you installed numpy; - your operating system; - whether or not you have multiple versions of Python installed; - if you built from source, your compiler versions and ideally a build log. Note: this error has many possible causes, so please don't comment on; an existing issue about this - open a new one instead. Original error was: libopenblas.so.0: cannot open shared object file: No such file or directory; ```. I need to run deepvariant as a non-root user via singulairty on the HPC platform. The non-GPU version works just fine.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243
https://github.com/google/deepvariant/issues/243:2095,Deployability,install,installed,2095,"kages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>; import numpy as np; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import core; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 47, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the multiarray numpy extension module failed. Most; likely you are trying to import a failed build of numpy.; Here is how to proceed:; - If you're working with a numpy git repository, try `git clean -xdf`; (removes all files not under version control) and rebuild numpy.; - If you are simply trying to use the numpy version that you have installed:; your installation is broken - please reinstall numpy.; - If you have already reinstalled and that did not fix the problem, then:; 1. Check that you are using the Python you expect (you're using /usr/bin/python),; and that you have no directories in your PATH or PYTHONPATH that can; interfere with the Python and numpy versions you're trying to use.; 2. If (1) looks fine, you can open a new issue at; https://github.com/numpy/numpy/issues. Please include details on:; - how you installed Python; - how you installed numpy; - your operating system; - whether or not you have multiple versions of Python installed; - if you built from source, your compiler versions and ideally a build log. Note: this error has many possible causes, so please don't comment on; an existing issue about this - open a new one instead. Original error was: libopenblas.so.0: cannot open shared object file: No such file or directory; ```. I need to run deepvariant as a non-root user via singulairty on the HPC platform. The non-GPU version works just fine.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243
https://github.com/google/deepvariant/issues/243:2177,Testability,log,log,2177,"kages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>; import numpy as np; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import core; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 47, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the multiarray numpy extension module failed. Most; likely you are trying to import a failed build of numpy.; Here is how to proceed:; - If you're working with a numpy git repository, try `git clean -xdf`; (removes all files not under version control) and rebuild numpy.; - If you are simply trying to use the numpy version that you have installed:; your installation is broken - please reinstall numpy.; - If you have already reinstalled and that did not fix the problem, then:; 1. Check that you are using the Python you expect (you're using /usr/bin/python),; and that you have no directories in your PATH or PYTHONPATH that can; interfere with the Python and numpy versions you're trying to use.; 2. If (1) looks fine, you can open a new issue at; https://github.com/numpy/numpy/issues. Please include details on:; - how you installed Python; - how you installed numpy; - your operating system; - whether or not you have multiple versions of Python installed; - if you built from source, your compiler versions and ideally a build log. Note: this error has many possible causes, so please don't comment on; an existing issue about this - open a new one instead. Original error was: libopenblas.so.0: cannot open shared object file: No such file or directory; ```. I need to run deepvariant as a non-root user via singulairty on the HPC platform. The non-GPU version works just fine.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243
https://github.com/google/deepvariant/issues/243:1427,Usability,simpl,simply,1427," ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>; import numpy as np; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import core; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 47, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the multiarray numpy extension module failed. Most; likely you are trying to import a failed build of numpy.; Here is how to proceed:; - If you're working with a numpy git repository, try `git clean -xdf`; (removes all files not under version control) and rebuild numpy.; - If you are simply trying to use the numpy version that you have installed:; your installation is broken - please reinstall numpy.; - If you have already reinstalled and that did not fix the problem, then:; 1. Check that you are using the Python you expect (you're using /usr/bin/python),; and that you have no directories in your PATH or PYTHONPATH that can; interfere with the Python and numpy versions you're trying to use.; 2. If (1) looks fine, you can open a new issue at; https://github.com/numpy/numpy/issues. Please include details on:; - how you installed Python; - how you installed numpy; - your operating system; - whether or not you have multiple versions of Python installed; - if you built from source, your compiler versions and ideally a build log. Note: this error has many possible causes, so please don't comment on; an existing issue about this - open a new one instead. Original error was: libopenblas.so.0: cannot open shared object file: No such file or directory; ```. I need to run deepvariant as a non-root user via singulairty on the H",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243
https://github.com/google/deepvariant/pull/244:54,Deployability,update,update,54,"Note: this PR is on the gh-pages branch, meant for an update for the goo.gl/deepvariant blog. It's submitted by a DeepVariant team member.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/244
https://github.com/google/deepvariant/pull/245:54,Deployability,update,update,54,"Note: this PR is on the gh-pages branch, meant for an update for the goo.gl/deepvariant blog. It's submitted by a DeepVariant team member.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/245
https://github.com/google/deepvariant/pull/246:54,Deployability,update,update,54,"Note: this PR is on the gh-pages branch, meant for an update for the goo.gl/deepvariant blog. It's submitted by a DeepVariant team member.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/246
https://github.com/google/deepvariant/issues/247:794,Availability,down,down,794,"Hi, My question is regarding the following comment: https://github.com/google/deepvariant/blob/43e54207bbf37dc511d3ea092cc84cd0476e5efe/deepvariant/haplotypes.py#L335. Does ""biallelic deletion ... and inside it a biallelic SNP"" refer to something like this?; ```; REF: ACATTACGATC; READ1 AC----CGA; READ2 ACAGTACG; ```; where we have the combined variant; ref = CATTA -> C/CAGTA. In this case, following the comment, it seems the recommended variant representation is (assuming reference starts at position 1 (1-based index for VCF)); ```; POS REF ALT GT field; 2 CATTA C 0/1; 4 T G 0/1; ```. However, I am wondering whether this is a valid VCF representation, since at position 4, we do not have the ref allele, but 0/1 indicates that ref allele is present at position 4. Apologize if this is down to my misunderstanding of the VCF format.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/247
https://github.com/google/deepvariant/issues/248:118,Availability,avail,available,118,"Any idea why I can not run the docker?. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. In the quick start guide https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md . They show using sudo to run the docker. I do not have sudo permission on this machine. The machine is set up to use the group permission. I do not think this is the issue. . Any suggestions would be greatly appreciated. Andy. ```; (base) -bash-4.2$ groups; giuser kimlab docker; (base) -bash-4.2$ ; ```. ```; docker run -v /public/home/dkim142/quickstart-testdata:/input \; -v /public/home/dkim142/quickstart-output:/output google/deepvariant:0.9.0 \; /opt/deepvariant/bin/run_deepvariant --model_type=WGS \; --ref=/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads=/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions chr20:10,000,000-10,010,000 \; --output_vcf=/public/home/dkim142/quickstart-output/output.vcf.gz \; --output_gvcf=/public/home/dkim142/quickstart-output/output.g.vcf.gz \; --num_shards=1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling \; --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2019-12-08 02:35:44.105906: F tensorflow/core/platform/cpu_feature_guard.cc:37] ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m1.146s; user	0m1.709s; sys	0m4.191s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/248
https://github.com/google/deepvariant/issues/248:1748,Availability,avail,available,1748,"ant/bin/run_deepvariant --model_type=WGS \; --ref=/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads=/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions chr20:10,000,000-10,010,000 \; --output_vcf=/public/home/dkim142/quickstart-output/output.vcf.gz \; --output_gvcf=/public/home/dkim142/quickstart-output/output.g.vcf.gz \; --num_shards=1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling \; --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2019-12-08 02:35:44.105906: F tensorflow/core/platform/cpu_feature_guard.cc:37] ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m1.146s; user	0m1.709s; sys	0m4.191s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_outpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/248
https://github.com/google/deepvariant/issues/248:607,Testability,test,testdata,607,"Any idea why I can not run the docker?. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. In the quick start guide https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md . They show using sudo to run the docker. I do not have sudo permission on this machine. The machine is set up to use the group permission. I do not think this is the issue. . Any suggestions would be greatly appreciated. Andy. ```; (base) -bash-4.2$ groups; giuser kimlab docker; (base) -bash-4.2$ ; ```. ```; docker run -v /public/home/dkim142/quickstart-testdata:/input \; -v /public/home/dkim142/quickstart-output:/output google/deepvariant:0.9.0 \; /opt/deepvariant/bin/run_deepvariant --model_type=WGS \; --ref=/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads=/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions chr20:10,000,000-10,010,000 \; --output_vcf=/public/home/dkim142/quickstart-output/output.vcf.gz \; --output_gvcf=/public/home/dkim142/quickstart-output/output.g.vcf.gz \; --num_shards=1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling \; --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2019-12-08 02:35:44.105906: F tensorflow/core/platform/cpu_feature_guard.cc:37] ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m1.146s; user	0m1.709s; sys	0m4.191s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/248
https://github.com/google/deepvariant/issues/248:799,Testability,test,testdata,799,"Any idea why I can not run the docker?. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. In the quick start guide https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md . They show using sudo to run the docker. I do not have sudo permission on this machine. The machine is set up to use the group permission. I do not think this is the issue. . Any suggestions would be greatly appreciated. Andy. ```; (base) -bash-4.2$ groups; giuser kimlab docker; (base) -bash-4.2$ ; ```. ```; docker run -v /public/home/dkim142/quickstart-testdata:/input \; -v /public/home/dkim142/quickstart-output:/output google/deepvariant:0.9.0 \; /opt/deepvariant/bin/run_deepvariant --model_type=WGS \; --ref=/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads=/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions chr20:10,000,000-10,010,000 \; --output_vcf=/public/home/dkim142/quickstart-output/output.vcf.gz \; --output_gvcf=/public/home/dkim142/quickstart-output/output.g.vcf.gz \; --num_shards=1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling \; --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2019-12-08 02:35:44.105906: F tensorflow/core/platform/cpu_feature_guard.cc:37] ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m1.146s; user	0m1.709s; sys	0m4.191s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/248
https://github.com/google/deepvariant/issues/248:882,Testability,test,testdata,882,"Any idea why I can not run the docker?. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. In the quick start guide https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md . They show using sudo to run the docker. I do not have sudo permission on this machine. The machine is set up to use the group permission. I do not think this is the issue. . Any suggestions would be greatly appreciated. Andy. ```; (base) -bash-4.2$ groups; giuser kimlab docker; (base) -bash-4.2$ ; ```. ```; docker run -v /public/home/dkim142/quickstart-testdata:/input \; -v /public/home/dkim142/quickstart-output:/output google/deepvariant:0.9.0 \; /opt/deepvariant/bin/run_deepvariant --model_type=WGS \; --ref=/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads=/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions chr20:10,000,000-10,010,000 \; --output_vcf=/public/home/dkim142/quickstart-output/output.vcf.gz \; --output_gvcf=/public/home/dkim142/quickstart-output/output.g.vcf.gz \; --num_shards=1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling \; --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2019-12-08 02:35:44.105906: F tensorflow/core/platform/cpu_feature_guard.cc:37] ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m1.146s; user	0m1.709s; sys	0m4.191s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/248
https://github.com/google/deepvariant/issues/248:1289,Testability,test,testdata,1289,"/r0.9/docs/deepvariant-quick-start.md . They show using sudo to run the docker. I do not have sudo permission on this machine. The machine is set up to use the group permission. I do not think this is the issue. . Any suggestions would be greatly appreciated. Andy. ```; (base) -bash-4.2$ groups; giuser kimlab docker; (base) -bash-4.2$ ; ```. ```; docker run -v /public/home/dkim142/quickstart-testdata:/input \; -v /public/home/dkim142/quickstart-output:/output google/deepvariant:0.9.0 \; /opt/deepvariant/bin/run_deepvariant --model_type=WGS \; --ref=/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads=/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions chr20:10,000,000-10,010,000 \; --output_vcf=/public/home/dkim142/quickstart-output/output.vcf.gz \; --output_gvcf=/public/home/dkim142/quickstart-output/output.g.vcf.gz \; --num_shards=1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling \; --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2019-12-08 02:35:44.105906: F tensorflow/core/platform/cpu_feature_guard.cc:37] ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m1.146s; user	0m1.709s; sys	0m4.191s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/248
https://github.com/google/deepvariant/issues/248:1371,Testability,test,testdata,1371," up to use the group permission. I do not think this is the issue. . Any suggestions would be greatly appreciated. Andy. ```; (base) -bash-4.2$ groups; giuser kimlab docker; (base) -bash-4.2$ ; ```. ```; docker run -v /public/home/dkim142/quickstart-testdata:/input \; -v /public/home/dkim142/quickstart-output:/output google/deepvariant:0.9.0 \; /opt/deepvariant/bin/run_deepvariant --model_type=WGS \; --ref=/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads=/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions chr20:10,000,000-10,010,000 \; --output_vcf=/public/home/dkim142/quickstart-output/output.vcf.gz \; --output_gvcf=/public/home/dkim142/quickstart-output/output.g.vcf.gz \; --num_shards=1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling \; --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2019-12-08 02:35:44.105906: F tensorflow/core/platform/cpu_feature_guard.cc:37] ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m1.146s; user	0m1.709s; sys	0m4.191s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise Call",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/248
https://github.com/google/deepvariant/issues/248:2557,Testability,test,testdata,2557,"0p1mb.bam \; --regions chr20:10,000,000-10,010,000 \; --output_vcf=/public/home/dkim142/quickstart-output/output.vcf.gz \; --output_gvcf=/public/home/dkim142/quickstart-output/output.g.vcf.gz \; --num_shards=1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling \; --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2019-12-08 02:35:44.105906: F tensorflow/core/platform/cpu_feature_guard.cc:37] ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m1.146s; user	0m1.709s; sys	0m4.191s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1; (base) -bash-4.2$ ; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/248
https://github.com/google/deepvariant/issues/248:2639,Testability,test,testdata,2639,"0p1mb.bam \; --regions chr20:10,000,000-10,010,000 \; --output_vcf=/public/home/dkim142/quickstart-output/output.vcf.gz \; --output_gvcf=/public/home/dkim142/quickstart-output/output.g.vcf.gz \; --num_shards=1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling \; --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2019-12-08 02:35:44.105906: F tensorflow/core/platform/cpu_feature_guard.cc:37] ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m1.146s; user	0m1.709s; sys	0m4.191s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1; (base) -bash-4.2$ ; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/248
https://github.com/google/deepvariant/issues/248:164,Usability,guid,guide,164,"Any idea why I can not run the docker?. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. In the quick start guide https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md . They show using sudo to run the docker. I do not have sudo permission on this machine. The machine is set up to use the group permission. I do not think this is the issue. . Any suggestions would be greatly appreciated. Andy. ```; (base) -bash-4.2$ groups; giuser kimlab docker; (base) -bash-4.2$ ; ```. ```; docker run -v /public/home/dkim142/quickstart-testdata:/input \; -v /public/home/dkim142/quickstart-output:/output google/deepvariant:0.9.0 \; /opt/deepvariant/bin/run_deepvariant --model_type=WGS \; --ref=/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads=/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions chr20:10,000,000-10,010,000 \; --output_vcf=/public/home/dkim142/quickstart-output/output.vcf.gz \; --output_gvcf=/public/home/dkim142/quickstart-output/output.g.vcf.gz \; --num_shards=1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling \; --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2019-12-08 02:35:44.105906: F tensorflow/core/platform/cpu_feature_guard.cc:37] ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m1.146s; user	0m1.709s; sys	0m4.191s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/248
https://github.com/google/deepvariant/issues/249:274,Testability,test,testdata,274,"Dears, . I just followed ""DeepVariant quick start"" guide, which was successful. ; Then, I've replaced the reference and reads with my own files and ran following commands but failed to get vcf files. . > OUTPUT_DIR=""${PWD}/quickstart-output""; > INPUT_DIR=""${PWD}/quickstart-testdata""; > mkdir -p ""${OUTPUT_DIR}""; > BIN_VERSION=""0.9.0"". > sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}:/output"" \; > gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WES \; > --ref=/input/genome.fa \; > --reads=/input/HC3-BC_RG_bwa.bam \; > --regions ""20:10,000,000-10,100,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=12. Here is a log: ; ```; ***** Running the command:*****; time seq 0 11 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/genome.fa"" --reads ""/input/HC3-BC_RG_bwa.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@12.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@12.gz"" --regions ""20:10,000,000-10,100,000"" --task {}. I1208 07:03:00.340749 140610455504640 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1208 07:03:00.598805 140610455504640 genomics_reader.py:223] Reading /input/HC3-BC_RG_bwa.bam with NativeSamReader; I1208 07:03:00.619033 140610455504640 make_examples.py:1324] Preparing inputs; I1208 07:03:00.814711 140610455504640 genomics_reader.py:223] Reading /input/HC3-BC_RG_bwa.bam with NativeSamReader; I1208 07:03:00.841161 140610455504640 make_examples.py:1248] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; I1208 07:03:00.851603 140610455504640 make_examples.py:1330] Writing examples to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00000-of",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249
https://github.com/google/deepvariant/issues/249:765,Testability,log,log,765,"Dears, . I just followed ""DeepVariant quick start"" guide, which was successful. ; Then, I've replaced the reference and reads with my own files and ran following commands but failed to get vcf files. . > OUTPUT_DIR=""${PWD}/quickstart-output""; > INPUT_DIR=""${PWD}/quickstart-testdata""; > mkdir -p ""${OUTPUT_DIR}""; > BIN_VERSION=""0.9.0"". > sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}:/output"" \; > gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WES \; > --ref=/input/genome.fa \; > --reads=/input/HC3-BC_RG_bwa.bam \; > --regions ""20:10,000,000-10,100,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=12. Here is a log: ; ```; ***** Running the command:*****; time seq 0 11 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/genome.fa"" --reads ""/input/HC3-BC_RG_bwa.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@12.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@12.gz"" --regions ""20:10,000,000-10,100,000"" --task {}. I1208 07:03:00.340749 140610455504640 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1208 07:03:00.598805 140610455504640 genomics_reader.py:223] Reading /input/HC3-BC_RG_bwa.bam with NativeSamReader; I1208 07:03:00.619033 140610455504640 make_examples.py:1324] Preparing inputs; I1208 07:03:00.814711 140610455504640 genomics_reader.py:223] Reading /input/HC3-BC_RG_bwa.bam with NativeSamReader; I1208 07:03:00.841161 140610455504640 make_examples.py:1248] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; I1208 07:03:00.851603 140610455504640 make_examples.py:1330] Writing examples to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00000-of",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249
https://github.com/google/deepvariant/issues/249:51,Usability,guid,guide,51,"Dears, . I just followed ""DeepVariant quick start"" guide, which was successful. ; Then, I've replaced the reference and reads with my own files and ran following commands but failed to get vcf files. . > OUTPUT_DIR=""${PWD}/quickstart-output""; > INPUT_DIR=""${PWD}/quickstart-testdata""; > mkdir -p ""${OUTPUT_DIR}""; > BIN_VERSION=""0.9.0"". > sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}:/output"" \; > gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WES \; > --ref=/input/genome.fa \; > --reads=/input/HC3-BC_RG_bwa.bam \; > --regions ""20:10,000,000-10,100,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=12. Here is a log: ; ```; ***** Running the command:*****; time seq 0 11 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/genome.fa"" --reads ""/input/HC3-BC_RG_bwa.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@12.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@12.gz"" --regions ""20:10,000,000-10,100,000"" --task {}. I1208 07:03:00.340749 140610455504640 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1208 07:03:00.598805 140610455504640 genomics_reader.py:223] Reading /input/HC3-BC_RG_bwa.bam with NativeSamReader; I1208 07:03:00.619033 140610455504640 make_examples.py:1324] Preparing inputs; I1208 07:03:00.814711 140610455504640 genomics_reader.py:223] Reading /input/HC3-BC_RG_bwa.bam with NativeSamReader; I1208 07:03:00.841161 140610455504640 make_examples.py:1248] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; I1208 07:03:00.851603 140610455504640 make_examples.py:1330] Writing examples to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00000-of",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249
https://github.com/google/deepvariant/issues/250:1038,Availability,error,error,1038,"9.7, build 2d0083d; Bowtie 2; Samtools 1.9; DeepVariant 0.9.0. Original source files.; - _SRR062634.filt.fastq_ from ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/HG00096/sequence_read/; - _Homo_sapiens.GRCh38.dna.primary_assembly.fa_ from ftp://ftp.ensembl.org/pub/release-98/fasta/homo_sapiens/dna/. Actions.; 1. Bowtie 2: indexing _Homo_sapiens.GRCh38.dna.primary_assembly.fa_; 2. Bowtie 2: aligning _SRR062634.filt.fastq_ on _Homo_sapiens.GRCh38.dna.primary_assembly.fa_; 3. Samtools: converting _SRR062634.sam_ to _SRR062634.bam_; 4. Samtools: indexing _SRR062634.filt.fastq_; 5. DeepVariant: trying to call SNPs. DeepVariant command syntax.; `sudo docker run -v ""/home/platon/_0_/Exp/seq1/bowtie2/"":""/input"" -v ""/home/platon/_0_/Exp/seq1/bowtie2/"":""/output"" google/deepvariant:""0.9.0"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/SRR062634.filt.fastq --reads=/input/SRR062634.bam --output_vcf=/output/SRR062634.vcf.gz --num_shards=4`. Part of error log.; ```; ***** Running the command:*****; time seq 0 3 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/SRR062634.filt.fastq"" --reads ""/input/SRR062634.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --task {}. I1208 19:49:03.680470 140573386819328 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1208 19:49:03.681448 140573386819328 genomics_reader.py:223] Reading /input/SRR062634.bam with NativeSamReader; W1208 19:49:03.681570 140573386819328 make_examples.py:558] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I1208 19:49:03.742767 140573386819328 make_examples.py:1324] Preparing inputs; I1208 19:49:05.745795 140573386819328 genomics_reader.py:223] Reading /input/SRR062634.bam with NativeSamReader; Traceba",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/250
https://github.com/google/deepvariant/issues/250:311,Deployability,release,release-,311,"Elementary OS 5.1; Docker version 18.09.7, build 2d0083d; Bowtie 2; Samtools 1.9; DeepVariant 0.9.0. Original source files.; - _SRR062634.filt.fastq_ from ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/HG00096/sequence_read/; - _Homo_sapiens.GRCh38.dna.primary_assembly.fa_ from ftp://ftp.ensembl.org/pub/release-98/fasta/homo_sapiens/dna/. Actions.; 1. Bowtie 2: indexing _Homo_sapiens.GRCh38.dna.primary_assembly.fa_; 2. Bowtie 2: aligning _SRR062634.filt.fastq_ on _Homo_sapiens.GRCh38.dna.primary_assembly.fa_; 3. Samtools: converting _SRR062634.sam_ to _SRR062634.bam_; 4. Samtools: indexing _SRR062634.filt.fastq_; 5. DeepVariant: trying to call SNPs. DeepVariant command syntax.; `sudo docker run -v ""/home/platon/_0_/Exp/seq1/bowtie2/"":""/input"" -v ""/home/platon/_0_/Exp/seq1/bowtie2/"":""/output"" google/deepvariant:""0.9.0"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/SRR062634.filt.fastq --reads=/input/SRR062634.bam --output_vcf=/output/SRR062634.vcf.gz --num_shards=4`. Part of error log.; ```; ***** Running the command:*****; time seq 0 3 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/SRR062634.filt.fastq"" --reads ""/input/SRR062634.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --task {}. I1208 19:49:03.680470 140573386819328 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1208 19:49:03.681448 140573386819328 genomics_reader.py:223] Reading /input/SRR062634.bam with NativeSamReader; W1208 19:49:03.681570 140573386819328 make_examples.py:558] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I1208 19:49:03.742767 140573386819328 make_examples.py:1324] Preparing inputs; I1208 19:49:05.745795 140573386819328 genomics_reader.py:223] Reading /input/SRR06",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/250
https://github.com/google/deepvariant/issues/250:1044,Testability,log,log,1044,"9.7, build 2d0083d; Bowtie 2; Samtools 1.9; DeepVariant 0.9.0. Original source files.; - _SRR062634.filt.fastq_ from ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/HG00096/sequence_read/; - _Homo_sapiens.GRCh38.dna.primary_assembly.fa_ from ftp://ftp.ensembl.org/pub/release-98/fasta/homo_sapiens/dna/. Actions.; 1. Bowtie 2: indexing _Homo_sapiens.GRCh38.dna.primary_assembly.fa_; 2. Bowtie 2: aligning _SRR062634.filt.fastq_ on _Homo_sapiens.GRCh38.dna.primary_assembly.fa_; 3. Samtools: converting _SRR062634.sam_ to _SRR062634.bam_; 4. Samtools: indexing _SRR062634.filt.fastq_; 5. DeepVariant: trying to call SNPs. DeepVariant command syntax.; `sudo docker run -v ""/home/platon/_0_/Exp/seq1/bowtie2/"":""/input"" -v ""/home/platon/_0_/Exp/seq1/bowtie2/"":""/output"" google/deepvariant:""0.9.0"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/SRR062634.filt.fastq --reads=/input/SRR062634.bam --output_vcf=/output/SRR062634.vcf.gz --num_shards=4`. Part of error log.; ```; ***** Running the command:*****; time seq 0 3 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/SRR062634.filt.fastq"" --reads ""/input/SRR062634.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --task {}. I1208 19:49:03.680470 140573386819328 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1208 19:49:03.681448 140573386819328 genomics_reader.py:223] Reading /input/SRR062634.bam with NativeSamReader; W1208 19:49:03.681570 140573386819328 make_examples.py:558] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I1208 19:49:03.742767 140573386819328 make_examples.py:1324] Preparing inputs; I1208 19:49:05.745795 140573386819328 genomics_reader.py:223] Reading /input/SRR062634.bam with NativeSamReader; Traceba",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/250
https://github.com/google/deepvariant/issues/251:2252,Availability,error,error,2252,"6 \; --train_dir=${training_dir} \; --model_name=""inception_v3"" \; --learning_rate=0.008 \; --start_from_checkpoint=/opt/models/wgs/model.ckpt \; --number_of_steps=50000 \; --save_interval_secs 300; ```. Here is the run info for just one sample's examples set (only a single chromosome, for testing purposes, from the .run_info.pbtxt file):. ```; labeling_metrics {; n_truth_variant_sites: 3469; n_truth_variant_alleles: 3474; n_candidate_variant_sites: 9778; n_candidate_variant_alleles: 9943; n_non_confident_candidate_variant_sites: 2219; n_true_positive_sites: 3468; n_true_positive_alleles: 3845; n_false_negative_sites: 1; n_false_negative_alleles: 1; n_false_positive_sites: 6309; n_false_positive_alleles: 6469; n_inexact_position_matches: 1; n_exact_position_matches: 3469; n_exact_position_and_allele_matches: 3443; n_exact_position_and_allele_and_genotype_matches: 3443; }; ```. Training runs just fine, with loss starting at ~1.2 and dropping to 0.04. Batch size is relatively small (memory error on the GPU with any larger). Is it simply my patience or is something else going on? I can provide tensorboard stats as well, but taking any model and performing make_examples(calling) -> postprocess results in only refcalls. Thanks, and let me know what other info I can provide. Edit: Here is some of the output from model_eval; ```; Saving dict for global step 0: Accuracy/All = 0.17285156, Accuracy/Indels = 0.078431375, Accuracy/SNPs = 0.19634147, F1/All = 0.39246467, F1/Het = 0.0, F1/HomRef = 0.39246467, F1/HomVar = 0.2947544, FNs/All = 0.0, FNs/Indels = 0.0, FNs/SNPs = 0.0, FPs/All = 774.0, FPs/Indels = 164.0, FPs/SNPs = 610.0, Precision/All = 0.24414062, Precision/Het = 0.0, Precision/HomRef = 0.24414062, Precision/HomVar = 0.17285156, Precision/Indels = 0.19607843, Precision/SNPs = 0.25609756, Recall/All = 1.0, Recall/Het = 0.0, Recall/HomRef = 1.0, Recall/HomVar = 1.0, Recall/Indels = 1.0, Recall/SNPs = 1.0, TNs/All = 0.0, TNs/Indels = 0.0, TNs/SNPs = 0.0, TPs/All = 250.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:2409,Performance,perform,performing,2409,"el.ckpt \; --number_of_steps=50000 \; --save_interval_secs 300; ```. Here is the run info for just one sample's examples set (only a single chromosome, for testing purposes, from the .run_info.pbtxt file):. ```; labeling_metrics {; n_truth_variant_sites: 3469; n_truth_variant_alleles: 3474; n_candidate_variant_sites: 9778; n_candidate_variant_alleles: 9943; n_non_confident_candidate_variant_sites: 2219; n_true_positive_sites: 3468; n_true_positive_alleles: 3845; n_false_negative_sites: 1; n_false_negative_alleles: 1; n_false_positive_sites: 6309; n_false_positive_alleles: 6469; n_inexact_position_matches: 1; n_exact_position_matches: 3469; n_exact_position_and_allele_matches: 3443; n_exact_position_and_allele_and_genotype_matches: 3443; }; ```. Training runs just fine, with loss starting at ~1.2 and dropping to 0.04. Batch size is relatively small (memory error on the GPU with any larger). Is it simply my patience or is something else going on? I can provide tensorboard stats as well, but taking any model and performing make_examples(calling) -> postprocess results in only refcalls. Thanks, and let me know what other info I can provide. Edit: Here is some of the output from model_eval; ```; Saving dict for global step 0: Accuracy/All = 0.17285156, Accuracy/Indels = 0.078431375, Accuracy/SNPs = 0.19634147, F1/All = 0.39246467, F1/Het = 0.0, F1/HomRef = 0.39246467, F1/HomVar = 0.2947544, FNs/All = 0.0, FNs/Indels = 0.0, FNs/SNPs = 0.0, FPs/All = 774.0, FPs/Indels = 164.0, FPs/SNPs = 610.0, Precision/All = 0.24414062, Precision/Het = 0.0, Precision/HomRef = 0.24414062, Precision/HomVar = 0.17285156, Precision/Indels = 0.19607843, Precision/SNPs = 0.25609756, Recall/All = 1.0, Recall/Het = 0.0, Recall/HomRef = 1.0, Recall/HomVar = 1.0, Recall/Indels = 1.0, Recall/SNPs = 1.0, TNs/All = 0.0, TNs/Indels = 0.0, TNs/SNPs = 0.0, TPs/All = 250.0, TPs/Indels = 40.0, TPs/SNPs = 210.0, global_step = 0, loss = 1.3173751; I1209 06:57:08.677582 46912496317632 estimator.py:2039] Savin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:180,Security,validat,validation,180,"Hello, I'm currently attempting to train DV on a non-model organism. I believe I have the training examples being made properly. For ~170 samples, I take 30 out randomly to use as validation and shuffle both sets by splitting into 4096 files, then repeating a few times (shuffling the iterator as well). Training proceeds quickly on the Tesla GPU, but all stats end up being homozygous reference. I can't tell what is going wrong. Here's my make_examples; ```; /opt/deepvariant/bin/make_examples \; --mode=training \; --use_ref_for_cram=true \; --ref=${reference} \; --examples ${accession}.ds0.with_labels.examples \; --sample_name ${accession} \; --reads ${cram} \; --truth_variants=${accession}.vcf.gz \; --confident_regions=${accession}.bed \; --regions CM0XXXXXX.1 \; --write_run_info. ```. Shuffling code is similar to this, but repeated multiple time:; ```; raw_dataset = tf.data.TFRecordDataset(inputs); for raw_record in itershuffle(raw_dataset, 2000):; example = tf.train.Example(); example.ParseFromString(raw_record.numpy()); writer = random.choice(out_fhs); writer.write(example.SerializeToString()); ```. And training code is like this:; ```; /opt/deepvariant/bin/model_train \; --dataset_config_pbtxt=${config_path} \; --batch_size=256 \; --train_dir=${training_dir} \; --model_name=""inception_v3"" \; --learning_rate=0.008 \; --start_from_checkpoint=/opt/models/wgs/model.ckpt \; --number_of_steps=50000 \; --save_interval_secs 300; ```. Here is the run info for just one sample's examples set (only a single chromosome, for testing purposes, from the .run_info.pbtxt file):. ```; labeling_metrics {; n_truth_variant_sites: 3469; n_truth_variant_alleles: 3474; n_candidate_variant_sites: 9778; n_candidate_variant_alleles: 9943; n_non_confident_candidate_variant_sites: 2219; n_true_positive_sites: 3468; n_true_positive_alleles: 3845; n_false_negative_sites: 1; n_false_negative_alleles: 1; n_false_positive_sites: 6309; n_false_positive_alleles: 6469; n_inexact_position_matches: 1; n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:580,Security,access,accession,580,"Hello, I'm currently attempting to train DV on a non-model organism. I believe I have the training examples being made properly. For ~170 samples, I take 30 out randomly to use as validation and shuffle both sets by splitting into 4096 files, then repeating a few times (shuffling the iterator as well). Training proceeds quickly on the Tesla GPU, but all stats end up being homozygous reference. I can't tell what is going wrong. Here's my make_examples; ```; /opt/deepvariant/bin/make_examples \; --mode=training \; --use_ref_for_cram=true \; --ref=${reference} \; --examples ${accession}.ds0.with_labels.examples \; --sample_name ${accession} \; --reads ${cram} \; --truth_variants=${accession}.vcf.gz \; --confident_regions=${accession}.bed \; --regions CM0XXXXXX.1 \; --write_run_info. ```. Shuffling code is similar to this, but repeated multiple time:; ```; raw_dataset = tf.data.TFRecordDataset(inputs); for raw_record in itershuffle(raw_dataset, 2000):; example = tf.train.Example(); example.ParseFromString(raw_record.numpy()); writer = random.choice(out_fhs); writer.write(example.SerializeToString()); ```. And training code is like this:; ```; /opt/deepvariant/bin/model_train \; --dataset_config_pbtxt=${config_path} \; --batch_size=256 \; --train_dir=${training_dir} \; --model_name=""inception_v3"" \; --learning_rate=0.008 \; --start_from_checkpoint=/opt/models/wgs/model.ckpt \; --number_of_steps=50000 \; --save_interval_secs 300; ```. Here is the run info for just one sample's examples set (only a single chromosome, for testing purposes, from the .run_info.pbtxt file):. ```; labeling_metrics {; n_truth_variant_sites: 3469; n_truth_variant_alleles: 3474; n_candidate_variant_sites: 9778; n_candidate_variant_alleles: 9943; n_non_confident_candidate_variant_sites: 2219; n_true_positive_sites: 3468; n_true_positive_alleles: 3845; n_false_negative_sites: 1; n_false_negative_alleles: 1; n_false_positive_sites: 6309; n_false_positive_alleles: 6469; n_inexact_position_matches: 1; n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:635,Security,access,accession,635,"Hello, I'm currently attempting to train DV on a non-model organism. I believe I have the training examples being made properly. For ~170 samples, I take 30 out randomly to use as validation and shuffle both sets by splitting into 4096 files, then repeating a few times (shuffling the iterator as well). Training proceeds quickly on the Tesla GPU, but all stats end up being homozygous reference. I can't tell what is going wrong. Here's my make_examples; ```; /opt/deepvariant/bin/make_examples \; --mode=training \; --use_ref_for_cram=true \; --ref=${reference} \; --examples ${accession}.ds0.with_labels.examples \; --sample_name ${accession} \; --reads ${cram} \; --truth_variants=${accession}.vcf.gz \; --confident_regions=${accession}.bed \; --regions CM0XXXXXX.1 \; --write_run_info. ```. Shuffling code is similar to this, but repeated multiple time:; ```; raw_dataset = tf.data.TFRecordDataset(inputs); for raw_record in itershuffle(raw_dataset, 2000):; example = tf.train.Example(); example.ParseFromString(raw_record.numpy()); writer = random.choice(out_fhs); writer.write(example.SerializeToString()); ```. And training code is like this:; ```; /opt/deepvariant/bin/model_train \; --dataset_config_pbtxt=${config_path} \; --batch_size=256 \; --train_dir=${training_dir} \; --model_name=""inception_v3"" \; --learning_rate=0.008 \; --start_from_checkpoint=/opt/models/wgs/model.ckpt \; --number_of_steps=50000 \; --save_interval_secs 300; ```. Here is the run info for just one sample's examples set (only a single chromosome, for testing purposes, from the .run_info.pbtxt file):. ```; labeling_metrics {; n_truth_variant_sites: 3469; n_truth_variant_alleles: 3474; n_candidate_variant_sites: 9778; n_candidate_variant_alleles: 9943; n_non_confident_candidate_variant_sites: 2219; n_true_positive_sites: 3468; n_true_positive_alleles: 3845; n_false_negative_sites: 1; n_false_negative_alleles: 1; n_false_positive_sites: 6309; n_false_positive_alleles: 6469; n_inexact_position_matches: 1; n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:687,Security,access,accession,687,"Hello, I'm currently attempting to train DV on a non-model organism. I believe I have the training examples being made properly. For ~170 samples, I take 30 out randomly to use as validation and shuffle both sets by splitting into 4096 files, then repeating a few times (shuffling the iterator as well). Training proceeds quickly on the Tesla GPU, but all stats end up being homozygous reference. I can't tell what is going wrong. Here's my make_examples; ```; /opt/deepvariant/bin/make_examples \; --mode=training \; --use_ref_for_cram=true \; --ref=${reference} \; --examples ${accession}.ds0.with_labels.examples \; --sample_name ${accession} \; --reads ${cram} \; --truth_variants=${accession}.vcf.gz \; --confident_regions=${accession}.bed \; --regions CM0XXXXXX.1 \; --write_run_info. ```. Shuffling code is similar to this, but repeated multiple time:; ```; raw_dataset = tf.data.TFRecordDataset(inputs); for raw_record in itershuffle(raw_dataset, 2000):; example = tf.train.Example(); example.ParseFromString(raw_record.numpy()); writer = random.choice(out_fhs); writer.write(example.SerializeToString()); ```. And training code is like this:; ```; /opt/deepvariant/bin/model_train \; --dataset_config_pbtxt=${config_path} \; --batch_size=256 \; --train_dir=${training_dir} \; --model_name=""inception_v3"" \; --learning_rate=0.008 \; --start_from_checkpoint=/opt/models/wgs/model.ckpt \; --number_of_steps=50000 \; --save_interval_secs 300; ```. Here is the run info for just one sample's examples set (only a single chromosome, for testing purposes, from the .run_info.pbtxt file):. ```; labeling_metrics {; n_truth_variant_sites: 3469; n_truth_variant_alleles: 3474; n_candidate_variant_sites: 9778; n_candidate_variant_alleles: 9943; n_non_confident_candidate_variant_sites: 2219; n_true_positive_sites: 3468; n_true_positive_alleles: 3845; n_false_negative_sites: 1; n_false_negative_alleles: 1; n_false_positive_sites: 6309; n_false_positive_alleles: 6469; n_inexact_position_matches: 1; n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:730,Security,access,accession,730,"Hello, I'm currently attempting to train DV on a non-model organism. I believe I have the training examples being made properly. For ~170 samples, I take 30 out randomly to use as validation and shuffle both sets by splitting into 4096 files, then repeating a few times (shuffling the iterator as well). Training proceeds quickly on the Tesla GPU, but all stats end up being homozygous reference. I can't tell what is going wrong. Here's my make_examples; ```; /opt/deepvariant/bin/make_examples \; --mode=training \; --use_ref_for_cram=true \; --ref=${reference} \; --examples ${accession}.ds0.with_labels.examples \; --sample_name ${accession} \; --reads ${cram} \; --truth_variants=${accession}.vcf.gz \; --confident_regions=${accession}.bed \; --regions CM0XXXXXX.1 \; --write_run_info. ```. Shuffling code is similar to this, but repeated multiple time:; ```; raw_dataset = tf.data.TFRecordDataset(inputs); for raw_record in itershuffle(raw_dataset, 2000):; example = tf.train.Example(); example.ParseFromString(raw_record.numpy()); writer = random.choice(out_fhs); writer.write(example.SerializeToString()); ```. And training code is like this:; ```; /opt/deepvariant/bin/model_train \; --dataset_config_pbtxt=${config_path} \; --batch_size=256 \; --train_dir=${training_dir} \; --model_name=""inception_v3"" \; --learning_rate=0.008 \; --start_from_checkpoint=/opt/models/wgs/model.ckpt \; --number_of_steps=50000 \; --save_interval_secs 300; ```. Here is the run info for just one sample's examples set (only a single chromosome, for testing purposes, from the .run_info.pbtxt file):. ```; labeling_metrics {; n_truth_variant_sites: 3469; n_truth_variant_alleles: 3474; n_candidate_variant_sites: 9778; n_candidate_variant_alleles: 9943; n_non_confident_candidate_variant_sites: 2219; n_true_positive_sites: 3468; n_true_positive_alleles: 3845; n_false_negative_sites: 1; n_false_negative_alleles: 1; n_false_positive_sites: 6309; n_false_positive_alleles: 6469; n_inexact_position_matches: 1; n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:1540,Testability,test,testing,1540,"ning \; --use_ref_for_cram=true \; --ref=${reference} \; --examples ${accession}.ds0.with_labels.examples \; --sample_name ${accession} \; --reads ${cram} \; --truth_variants=${accession}.vcf.gz \; --confident_regions=${accession}.bed \; --regions CM0XXXXXX.1 \; --write_run_info. ```. Shuffling code is similar to this, but repeated multiple time:; ```; raw_dataset = tf.data.TFRecordDataset(inputs); for raw_record in itershuffle(raw_dataset, 2000):; example = tf.train.Example(); example.ParseFromString(raw_record.numpy()); writer = random.choice(out_fhs); writer.write(example.SerializeToString()); ```. And training code is like this:; ```; /opt/deepvariant/bin/model_train \; --dataset_config_pbtxt=${config_path} \; --batch_size=256 \; --train_dir=${training_dir} \; --model_name=""inception_v3"" \; --learning_rate=0.008 \; --start_from_checkpoint=/opt/models/wgs/model.ckpt \; --number_of_steps=50000 \; --save_interval_secs 300; ```. Here is the run info for just one sample's examples set (only a single chromosome, for testing purposes, from the .run_info.pbtxt file):. ```; labeling_metrics {; n_truth_variant_sites: 3469; n_truth_variant_alleles: 3474; n_candidate_variant_sites: 9778; n_candidate_variant_alleles: 9943; n_non_confident_candidate_variant_sites: 2219; n_true_positive_sites: 3468; n_true_positive_alleles: 3845; n_false_negative_sites: 1; n_false_negative_alleles: 1; n_false_positive_sites: 6309; n_false_positive_alleles: 6469; n_inexact_position_matches: 1; n_exact_position_matches: 3469; n_exact_position_and_allele_matches: 3443; n_exact_position_and_allele_and_genotype_matches: 3443; }; ```. Training runs just fine, with loss starting at ~1.2 and dropping to 0.04. Batch size is relatively small (memory error on the GPU with any larger). Is it simply my patience or is something else going on? I can provide tensorboard stats as well, but taking any model and performing make_examples(calling) -> postprocess results in only refcalls. Thanks, and let me know wh",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/251:2293,Usability,simpl,simply,2293,"el.ckpt \; --number_of_steps=50000 \; --save_interval_secs 300; ```. Here is the run info for just one sample's examples set (only a single chromosome, for testing purposes, from the .run_info.pbtxt file):. ```; labeling_metrics {; n_truth_variant_sites: 3469; n_truth_variant_alleles: 3474; n_candidate_variant_sites: 9778; n_candidate_variant_alleles: 9943; n_non_confident_candidate_variant_sites: 2219; n_true_positive_sites: 3468; n_true_positive_alleles: 3845; n_false_negative_sites: 1; n_false_negative_alleles: 1; n_false_positive_sites: 6309; n_false_positive_alleles: 6469; n_inexact_position_matches: 1; n_exact_position_matches: 3469; n_exact_position_and_allele_matches: 3443; n_exact_position_and_allele_and_genotype_matches: 3443; }; ```. Training runs just fine, with loss starting at ~1.2 and dropping to 0.04. Batch size is relatively small (memory error on the GPU with any larger). Is it simply my patience or is something else going on? I can provide tensorboard stats as well, but taking any model and performing make_examples(calling) -> postprocess results in only refcalls. Thanks, and let me know what other info I can provide. Edit: Here is some of the output from model_eval; ```; Saving dict for global step 0: Accuracy/All = 0.17285156, Accuracy/Indels = 0.078431375, Accuracy/SNPs = 0.19634147, F1/All = 0.39246467, F1/Het = 0.0, F1/HomRef = 0.39246467, F1/HomVar = 0.2947544, FNs/All = 0.0, FNs/Indels = 0.0, FNs/SNPs = 0.0, FPs/All = 774.0, FPs/Indels = 164.0, FPs/SNPs = 610.0, Precision/All = 0.24414062, Precision/Het = 0.0, Precision/HomRef = 0.24414062, Precision/HomVar = 0.17285156, Precision/Indels = 0.19607843, Precision/SNPs = 0.25609756, Recall/All = 1.0, Recall/Het = 0.0, Recall/HomRef = 1.0, Recall/HomVar = 1.0, Recall/Indels = 1.0, Recall/SNPs = 1.0, TNs/All = 0.0, TNs/Indels = 0.0, TNs/SNPs = 0.0, TPs/All = 250.0, TPs/Indels = 40.0, TPs/SNPs = 210.0, global_step = 0, loss = 1.3173751; I1209 06:57:08.677582 46912496317632 estimator.py:2039] Savin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251
https://github.com/google/deepvariant/issues/252:4203,Availability,down,download,4203,"ll last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer, since conda installed successfully all the dependencies, I've then tried to download the precompiled binaries and use them, but couldn't find a guide on how to install them.; Is there a page where to find guidelines on how to install the precompiled deepvariant?; If not, is there a way to fix the anaconda environment issue?. Thank you in advance,. Andrea",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:46,Deployability,install,install,46,"Hello,; I'm writing you because I'm trying to install deepvariant, but I'm encountering several difficulties in doing so.; I've tried at first to install through anaconda (```conda install -c bioconda deepvariant```), but I alway get the same problem:; ```. Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 497, in run_script; subprocess_call(command_args, env=env, path=dirname(path)); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/gateways/subprocess.py"", line 56, in subprocess_call; output=_format_output(command_str, path, rc, stdout, stderr)); subprocess.CalledProcessError: Command '['/bin/bash', '-x', '/PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh']' returned non-zero exit status 1.; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 327, in _execute_actions; run_script(target_prefix, Dist(pkg_data), 'post-unlink' if is_unlink else 'post-link'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 513, in run_script; raise LinkError(message); conda.exceptions.LinkError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 281, in execute; pkg_data, actions); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/cond",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:146,Deployability,install,install,146,"Hello,; I'm writing you because I'm trying to install deepvariant, but I'm encountering several difficulties in doing so.; I've tried at first to install through anaconda (```conda install -c bioconda deepvariant```), but I alway get the same problem:; ```. Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 497, in run_script; subprocess_call(command_args, env=env, path=dirname(path)); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/gateways/subprocess.py"", line 56, in subprocess_call; output=_format_output(command_str, path, rc, stdout, stderr)); subprocess.CalledProcessError: Command '['/bin/bash', '-x', '/PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh']' returned non-zero exit status 1.; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 327, in _execute_actions; run_script(target_prefix, Dist(pkg_data), 'post-unlink' if is_unlink else 'post-link'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 513, in run_script; raise LinkError(message); conda.exceptions.LinkError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 281, in execute; pkg_data, actions); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/cond",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:181,Deployability,install,install,181,"Hello,; I'm writing you because I'm trying to install deepvariant, but I'm encountering several difficulties in doing so.; I've tried at first to install through anaconda (```conda install -c bioconda deepvariant```), but I alway get the same problem:; ```. Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 497, in run_script; subprocess_call(command_args, env=env, path=dirname(path)); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/gateways/subprocess.py"", line 56, in subprocess_call; output=_format_output(command_str, path, rc, stdout, stderr)); subprocess.CalledProcessError: Command '['/bin/bash', '-x', '/PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh']' returned non-zero exit status 1.; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 327, in _execute_actions; run_script(target_prefix, Dist(pkg_data), 'post-unlink' if is_unlink else 'post-link'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 513, in run_script; raise LinkError(message); conda.exceptions.LinkError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 281, in execute; pkg_data, actions); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/cond",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:2953,Deployability,install,install,2953,"6/site-packages/conda/core/link.py"", line 344, in _execute_actions; reverse_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:2976,Deployability,install,install,2976,"6/site-packages/conda/core/link.py"", line 344, in _execute_actions; reverse_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:3077,Deployability,install,install,3077,"a.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-po",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:3103,Deployability,install,install,3103,"ioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:4139,Deployability,install,installed,4139,"ll last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer, since conda installed successfully all the dependencies, I've then tried to download the precompiled binaries and use them, but couldn't find a guide on how to install them.; Is there a page where to find guidelines on how to install the precompiled deepvariant?; If not, is there a way to fix the anaconda environment issue?. Thank you in advance,. Andrea",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:4287,Deployability,install,install,4287,"ll last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer, since conda installed successfully all the dependencies, I've then tried to download the precompiled binaries and use them, but couldn't find a guide on how to install them.; Is there a page where to find guidelines on how to install the precompiled deepvariant?; If not, is there a way to fix the anaconda environment issue?. Thank you in advance,. Andrea",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:4353,Deployability,install,install,4353,"ll last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer, since conda installed successfully all the dependencies, I've then tried to download the precompiled binaries and use them, but couldn't find a guide on how to install them.; Is there a page where to find guidelines on how to install the precompiled deepvariant?; If not, is there a way to fix the anaconda environment issue?. Thank you in advance,. Andrea",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1331,Integrability,message,message,1331,"tions/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 497, in run_script; subprocess_call(command_args, env=env, path=dirname(path)); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/gateways/subprocess.py"", line 56, in subprocess_call; output=_format_output(command_str, path, rc, stdout, stderr)); subprocess.CalledProcessError: Command '['/bin/bash', '-x', '/PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh']' returned non-zero exit status 1.; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 327, in _execute_actions; run_script(target_prefix, Dist(pkg_data), 'post-unlink' if is_unlink else 'post-link'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 513, in run_script; raise LinkError(message); conda.exceptions.LinkError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 281, in execute; pkg_data, actions); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 344, in _execute_actions; reverse_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:1641,Integrability,message,messages,1641,"'/bin/bash', '-x', '/PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh']' returned non-zero exit status 1.; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 327, in _execute_actions; run_script(target_prefix, Dist(pkg_data), 'post-unlink' if is_unlink else 'post-link'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 513, in run_script; raise LinkError(message); conda.exceptions.LinkError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 281, in execute; pkg_data, actions); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 344, in _execute_actions; reverse_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:2359,Integrability,message,messages,2359,"49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 281, in execute; pkg_data, actions); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 344, in _execute_actions; reverse_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ; ; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:4091,Integrability,message,messages,4091,"ll last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer, since conda installed successfully all the dependencies, I've then tried to download the precompiled binaries and use them, but couldn't find a guide on how to install them.; Is there a page where to find guidelines on how to install the precompiled deepvariant?; If not, is there a way to fix the anaconda environment issue?. Thank you in advance,. Andrea",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:4170,Integrability,depend,dependencies,4170,"ll last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer, since conda installed successfully all the dependencies, I've then tried to download the precompiled binaries and use them, but couldn't find a guide on how to install them.; Is there a page where to find guidelines on how to install the precompiled deepvariant?; If not, is there a way to fix the anaconda environment issue?. Thank you in advance,. Andrea",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:4271,Usability,guid,guide,4271,"ll last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer, since conda installed successfully all the dependencies, I've then tried to download the precompiled binaries and use them, but couldn't find a guide on how to install them.; Is there a page where to find guidelines on how to install the precompiled deepvariant?; If not, is there a way to fix the anaconda environment issue?. Thank you in advance,. Andrea",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/252:4332,Usability,guid,guidelines,4332,"ll last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer, since conda installed successfully all the dependencies, I've then tried to download the precompiled binaries and use them, but couldn't find a guide on how to install them.; Is there a page where to find guidelines on how to install the precompiled deepvariant?; If not, is there a way to fix the anaconda environment issue?. Thank you in advance,. Andrea",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252
https://github.com/google/deepvariant/issues/253:803,Availability,error,error,803,"Is there a way to redirect the DeepVariant output to another program? For example, to any annotation tool. I tried this command:; ```; sudo -S docker run -v ""/home/platon/_0_/Exp/seq1/bowtie2/"":""/ref"" -v ""/home/platon/_0_/Exp//_/SRR062634.filt/"":""/trg"" \; > google/deepvariant /opt/deepvariant/bin/run_deepvariant \; > --num_shards=4 --model_type=WGS \; > --ref=/ref/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz \; > --reads=/trg/SRR062634.filt_srtd.bam |; > sudo docker run -a stdin -v $HOME/vep_data:/opt/vep/.vep -v ""$HOME/_0_/Exp//_/SRR062634.filt/"":""/SRR062634_filt"" \; > ensemblorg/ensembl-vep ./vep \; > --tab --quiet --no_stats --offline --cache --dir_cache /opt/vep/.vep/ \; > -o /SRR062634_filt/SRR062634.filt_ann.tsv; ```. Then an error message appears:; `FATAL Flags parsing error: flag --output_vcf=None: Flag --output_vcf must have a value other than None.`",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:848,Availability,error,error,848,"Is there a way to redirect the DeepVariant output to another program? For example, to any annotation tool. I tried this command:; ```; sudo -S docker run -v ""/home/platon/_0_/Exp/seq1/bowtie2/"":""/ref"" -v ""/home/platon/_0_/Exp//_/SRR062634.filt/"":""/trg"" \; > google/deepvariant /opt/deepvariant/bin/run_deepvariant \; > --num_shards=4 --model_type=WGS \; > --ref=/ref/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz \; > --reads=/trg/SRR062634.filt_srtd.bam |; > sudo docker run -a stdin -v $HOME/vep_data:/opt/vep/.vep -v ""$HOME/_0_/Exp//_/SRR062634.filt/"":""/SRR062634_filt"" \; > ensemblorg/ensembl-vep ./vep \; > --tab --quiet --no_stats --offline --cache --dir_cache /opt/vep/.vep/ \; > -o /SRR062634_filt/SRR062634.filt_ann.tsv; ```. Then an error message appears:; `FATAL Flags parsing error: flag --output_vcf=None: Flag --output_vcf must have a value other than None.`",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:809,Integrability,message,message,809,"Is there a way to redirect the DeepVariant output to another program? For example, to any annotation tool. I tried this command:; ```; sudo -S docker run -v ""/home/platon/_0_/Exp/seq1/bowtie2/"":""/ref"" -v ""/home/platon/_0_/Exp//_/SRR062634.filt/"":""/trg"" \; > google/deepvariant /opt/deepvariant/bin/run_deepvariant \; > --num_shards=4 --model_type=WGS \; > --ref=/ref/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz \; > --reads=/trg/SRR062634.filt_srtd.bam |; > sudo docker run -a stdin -v $HOME/vep_data:/opt/vep/.vep -v ""$HOME/_0_/Exp//_/SRR062634.filt/"":""/SRR062634_filt"" \; > ensemblorg/ensembl-vep ./vep \; > --tab --quiet --no_stats --offline --cache --dir_cache /opt/vep/.vep/ \; > -o /SRR062634_filt/SRR062634.filt_ann.tsv; ```. Then an error message appears:; `FATAL Flags parsing error: flag --output_vcf=None: Flag --output_vcf must have a value other than None.`",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/253:709,Performance,cache,cache,709,"Is there a way to redirect the DeepVariant output to another program? For example, to any annotation tool. I tried this command:; ```; sudo -S docker run -v ""/home/platon/_0_/Exp/seq1/bowtie2/"":""/ref"" -v ""/home/platon/_0_/Exp//_/SRR062634.filt/"":""/trg"" \; > google/deepvariant /opt/deepvariant/bin/run_deepvariant \; > --num_shards=4 --model_type=WGS \; > --ref=/ref/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz \; > --reads=/trg/SRR062634.filt_srtd.bam |; > sudo docker run -a stdin -v $HOME/vep_data:/opt/vep/.vep -v ""$HOME/_0_/Exp//_/SRR062634.filt/"":""/SRR062634_filt"" \; > ensemblorg/ensembl-vep ./vep \; > --tab --quiet --no_stats --offline --cache --dir_cache /opt/vep/.vep/ \; > -o /SRR062634_filt/SRR062634.filt_ann.tsv; ```. Then an error message appears:; `FATAL Flags parsing error: flag --output_vcf=None: Flag --output_vcf must have a value other than None.`",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253
https://github.com/google/deepvariant/issues/254:20,Usability,learn,learning,20,"Hi . I am a student learning about bioinformatics. I am able to us the docker image to run the quick start data, however, I am not able to run on my data. . I used samtools 1.9 to filter out low quality reads as follows. ```; quality=11; samtools view -bSq ${quality} ""${originalBAMFile}"" > ""${filteredBAMFile}""; samtools index ""${filteredBAMFile}""; ```. Note the output from the docker run script bellow is a little misleading. I copied all the data from an aws s3 buck to the local disk before the run. Also, the data is RNA not DNA. I was asked to ""see if it would just work"". I think maybe we have to change Uracil to look like Thymine. . Any suggestions would be greatly appreciated. Happy Holidays. Andy; ```; ubuntu@ip-172-31-1-186:~$ cat nohup.out ; + BIN_VERSION=0.9.0; + s3Root=/data; + INPUT_DIR=/data/aligned; + OUTPUT_DIR=/data/output; + quality=q11; + ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; + reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam; + date +%Y-%m-%d-%H.%M.%S-%Z%n; + dateStamp=2019-12-19-00.20.49-UTC; + output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz; + output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz; + nproc; + num_shards=4; + sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ***** Running the command:*****; time seq 0 3 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam"" --examples ""/tmp/deepvariant_tmp_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254
https://github.com/google/deepvariant/issues/255:20,Availability,error,error,20,"I get the following error when running the example provided in the quick start document:. merge_overlaps() got an unexpected keyword argument 'strict'. Any advice as to how I can resolve the issue is greatly appreciated. ```; This is the context of the error.; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/data/hs37d5.fa.gz"" --reads ""/input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" `--gvcf` ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""20"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I1220 08:40:22.262234 46912496321664 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1220 08:40:22.268675 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.272100 46912496321664 make_examples.py:1324] Preparing inputs; I1220 08:40:22.280786 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.292714 46912496321664 make_examples.py:1248] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_UJ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:253,Availability,error,error,253,"I get the following error when running the example provided in the quick start document:. merge_overlaps() got an unexpected keyword argument 'strict'. Any advice as to how I can resolve the issue is greatly appreciated. ```; This is the context of the error.; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/data/hs37d5.fa.gz"" --reads ""/input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" `--gvcf` ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""20"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I1220 08:40:22.262234 46912496321664 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1220 08:40:22.268675 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.272100 46912496321664 make_examples.py:1324] Preparing inputs; I1220 08:40:22.280786 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.292714 46912496321664 make_examples.py:1248] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_UJ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:811,Deployability,install,installed,811,"I get the following error when running the example provided in the quick start document:. merge_overlaps() got an unexpected keyword argument 'strict'. Any advice as to how I can resolve the issue is greatly appreciated. ```; This is the context of the error.; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/data/hs37d5.fa.gz"" --reads ""/input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" `--gvcf` ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""20"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I1220 08:40:22.262234 46912496321664 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1220 08:40:22.268675 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.272100 46912496321664 make_examples.py:1324] Preparing inputs; I1220 08:40:22.280786 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.292714 46912496321664 make_examples.py:1248] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_UJ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/255:1072,Deployability,install,installed,1072,"k start document:. merge_overlaps() got an unexpected keyword argument 'strict'. Any advice as to how I can resolve the issue is greatly appreciated. ```; This is the context of the error.; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/data/hs37d5.fa.gz"" --reads ""/input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" `--gvcf` ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""20"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I1220 08:40:22.262234 46912496321664 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1220 08:40:22.268675 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.272100 46912496321664 make_examples.py:1324] Preparing inputs; I1220 08:40:22.280786 46912496321664 genomics_reader.py:223] Reading /input/data/HG002_NIST_150bp_chr20_downsampled_30x.bam with NativeSamReader; I1220 08:40:22.292714 46912496321664 make_examples.py:1248] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_UJ59Z1/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255
https://github.com/google/deepvariant/issues/256:59,Deployability,release,release,59,"The very_sensitive_caller seems to be a new feature in the release 0.9.0, what's the difference between the very sensitive caller and the usual one?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256
https://github.com/google/deepvariant/issues/259:87,Deployability,update,update,87,Hello!. Should deepvariant work with a newer version of `bazel`?; It would be great to update it from `0.21.0` to fresh `2.0.0`:; https://github.com/google/deepvariant/blob/master/settings.sh#L42,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/259
https://github.com/google/deepvariant/issues/260:911,Integrability,message,message,911,"Hi . I am a Newbie/Student with some ""best practices"" questions. I am using the docker version 0.9.0 and following the script in the quick start example. I have two data sets of Illumina reads one was from a control group, the other from a treatment group. I think the number of reads is comparable. One batch has been generating examples for over 3.5 days. The other batch is running on 32 vCPU machine. It is still generating images after 1.5 days. I checked the OS level stats, the machines are not swapping. all cpu's are at 100%. Still alot of unused memory and disk. 1) Any idea how I might estimate the expected run time? . 2) Any idea of how I do a better job sizing compute resources?. 3) How do I know if docker is making progress or not?; I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. 4) I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. thanks. Andy. p.s. I am running in AWS . not sure if that makes a difference or not. p.p.s. Is there a better place to ask questions like this?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/260:907,Testability,log,log,907,"Hi . I am a Newbie/Student with some ""best practices"" questions. I am using the docker version 0.9.0 and following the script in the quick start example. I have two data sets of Illumina reads one was from a control group, the other from a treatment group. I think the number of reads is comparable. One batch has been generating examples for over 3.5 days. The other batch is running on 32 vCPU machine. It is still generating images after 1.5 days. I checked the OS level stats, the machines are not swapping. all cpu's are at 100%. Still alot of unused memory and disk. 1) Any idea how I might estimate the expected run time? . 2) Any idea of how I do a better job sizing compute resources?. 3) How do I know if docker is making progress or not?; I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. 4) I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. thanks. Andy. p.s. I am running in AWS . not sure if that makes a difference or not. p.p.s. Is there a better place to ask questions like this?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260
https://github.com/google/deepvariant/issues/262:91,Availability,error,error,91,"I don't have much experience using containers so it's difficult for me to troubleshoot the error I'm getting back when I run the test DeepVariant command:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/input"": invalid volume specification: ':/input'.; See '/usr/bin/docker-current run --help'`. As it might be relevant, the OS i'm using is CentOS Linux 7. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:182,Availability,Error,Error,182,"I don't have much experience using containers so it's difficult for me to troubleshoot the error I'm getting back when I run the test DeepVariant command:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/input"": invalid volume specification: ':/input'.; See '/usr/bin/docker-current run --help'`. As it might be relevant, the OS i'm using is CentOS Linux 7. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/262:129,Testability,test,test,129,"I don't have much experience using containers so it's difficult for me to troubleshoot the error I'm getting back when I run the test DeepVariant command:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/input"": invalid volume specification: ':/input'.; See '/usr/bin/docker-current run --help'`. As it might be relevant, the OS i'm using is CentOS Linux 7. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262
https://github.com/google/deepvariant/issues/263:142,Availability,error,error,142,"Hi all,. The DeepVariant case study scripts for running via binaries on CPU install the `intel-tensorflow` package. We have noticed the below error when installing this package and are looking into how this can be fixed. ```; $ pip install intel-tensorflow; ERROR: intel-tensorflow has an invalid wheel, multiple .dist-info directories found: intel_tensorflow-2.0.0.dist-info, tensorflow-2.0.0.dist-info; ```; If you run into this issue, we recommend one of the following options in the meantime:; * Use the Docker scripts instead of the binaries scripts.; * Set the [`DV_USE_GCP_OPTIMIZED_TF_WHL`](https://github.com/google/deepvariant/blob/r0.9/settings.sh#L90) variable to 0 prior to setting up DeepVariant and running the case study scripts for binaries. `intel-tensorflow` is only installed when this variable is set.; * Use the GPU scripts instead of the CPU scripts. Best,; The DeepVariant Team",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263
https://github.com/google/deepvariant/issues/263:258,Availability,ERROR,ERROR,258,"Hi all,. The DeepVariant case study scripts for running via binaries on CPU install the `intel-tensorflow` package. We have noticed the below error when installing this package and are looking into how this can be fixed. ```; $ pip install intel-tensorflow; ERROR: intel-tensorflow has an invalid wheel, multiple .dist-info directories found: intel_tensorflow-2.0.0.dist-info, tensorflow-2.0.0.dist-info; ```; If you run into this issue, we recommend one of the following options in the meantime:; * Use the Docker scripts instead of the binaries scripts.; * Set the [`DV_USE_GCP_OPTIMIZED_TF_WHL`](https://github.com/google/deepvariant/blob/r0.9/settings.sh#L90) variable to 0 prior to setting up DeepVariant and running the case study scripts for binaries. `intel-tensorflow` is only installed when this variable is set.; * Use the GPU scripts instead of the CPU scripts. Best,; The DeepVariant Team",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263
https://github.com/google/deepvariant/issues/263:76,Deployability,install,install,76,"Hi all,. The DeepVariant case study scripts for running via binaries on CPU install the `intel-tensorflow` package. We have noticed the below error when installing this package and are looking into how this can be fixed. ```; $ pip install intel-tensorflow; ERROR: intel-tensorflow has an invalid wheel, multiple .dist-info directories found: intel_tensorflow-2.0.0.dist-info, tensorflow-2.0.0.dist-info; ```; If you run into this issue, we recommend one of the following options in the meantime:; * Use the Docker scripts instead of the binaries scripts.; * Set the [`DV_USE_GCP_OPTIMIZED_TF_WHL`](https://github.com/google/deepvariant/blob/r0.9/settings.sh#L90) variable to 0 prior to setting up DeepVariant and running the case study scripts for binaries. `intel-tensorflow` is only installed when this variable is set.; * Use the GPU scripts instead of the CPU scripts. Best,; The DeepVariant Team",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263
https://github.com/google/deepvariant/issues/263:153,Deployability,install,installing,153,"Hi all,. The DeepVariant case study scripts for running via binaries on CPU install the `intel-tensorflow` package. We have noticed the below error when installing this package and are looking into how this can be fixed. ```; $ pip install intel-tensorflow; ERROR: intel-tensorflow has an invalid wheel, multiple .dist-info directories found: intel_tensorflow-2.0.0.dist-info, tensorflow-2.0.0.dist-info; ```; If you run into this issue, we recommend one of the following options in the meantime:; * Use the Docker scripts instead of the binaries scripts.; * Set the [`DV_USE_GCP_OPTIMIZED_TF_WHL`](https://github.com/google/deepvariant/blob/r0.9/settings.sh#L90) variable to 0 prior to setting up DeepVariant and running the case study scripts for binaries. `intel-tensorflow` is only installed when this variable is set.; * Use the GPU scripts instead of the CPU scripts. Best,; The DeepVariant Team",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263
https://github.com/google/deepvariant/issues/263:232,Deployability,install,install,232,"Hi all,. The DeepVariant case study scripts for running via binaries on CPU install the `intel-tensorflow` package. We have noticed the below error when installing this package and are looking into how this can be fixed. ```; $ pip install intel-tensorflow; ERROR: intel-tensorflow has an invalid wheel, multiple .dist-info directories found: intel_tensorflow-2.0.0.dist-info, tensorflow-2.0.0.dist-info; ```; If you run into this issue, we recommend one of the following options in the meantime:; * Use the Docker scripts instead of the binaries scripts.; * Set the [`DV_USE_GCP_OPTIMIZED_TF_WHL`](https://github.com/google/deepvariant/blob/r0.9/settings.sh#L90) variable to 0 prior to setting up DeepVariant and running the case study scripts for binaries. `intel-tensorflow` is only installed when this variable is set.; * Use the GPU scripts instead of the CPU scripts. Best,; The DeepVariant Team",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263
https://github.com/google/deepvariant/issues/263:786,Deployability,install,installed,786,"Hi all,. The DeepVariant case study scripts for running via binaries on CPU install the `intel-tensorflow` package. We have noticed the below error when installing this package and are looking into how this can be fixed. ```; $ pip install intel-tensorflow; ERROR: intel-tensorflow has an invalid wheel, multiple .dist-info directories found: intel_tensorflow-2.0.0.dist-info, tensorflow-2.0.0.dist-info; ```; If you run into this issue, we recommend one of the following options in the meantime:; * Use the Docker scripts instead of the binaries scripts.; * Set the [`DV_USE_GCP_OPTIMIZED_TF_WHL`](https://github.com/google/deepvariant/blob/r0.9/settings.sh#L90) variable to 0 prior to setting up DeepVariant and running the case study scripts for binaries. `intel-tensorflow` is only installed when this variable is set.; * Use the GPU scripts instead of the CPU scripts. Best,; The DeepVariant Team",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263
https://github.com/google/deepvariant/issues/263:664,Modifiability,variab,variable,664,"Hi all,. The DeepVariant case study scripts for running via binaries on CPU install the `intel-tensorflow` package. We have noticed the below error when installing this package and are looking into how this can be fixed. ```; $ pip install intel-tensorflow; ERROR: intel-tensorflow has an invalid wheel, multiple .dist-info directories found: intel_tensorflow-2.0.0.dist-info, tensorflow-2.0.0.dist-info; ```; If you run into this issue, we recommend one of the following options in the meantime:; * Use the Docker scripts instead of the binaries scripts.; * Set the [`DV_USE_GCP_OPTIMIZED_TF_WHL`](https://github.com/google/deepvariant/blob/r0.9/settings.sh#L90) variable to 0 prior to setting up DeepVariant and running the case study scripts for binaries. `intel-tensorflow` is only installed when this variable is set.; * Use the GPU scripts instead of the CPU scripts. Best,; The DeepVariant Team",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263
https://github.com/google/deepvariant/issues/263:806,Modifiability,variab,variable,806,"Hi all,. The DeepVariant case study scripts for running via binaries on CPU install the `intel-tensorflow` package. We have noticed the below error when installing this package and are looking into how this can be fixed. ```; $ pip install intel-tensorflow; ERROR: intel-tensorflow has an invalid wheel, multiple .dist-info directories found: intel_tensorflow-2.0.0.dist-info, tensorflow-2.0.0.dist-info; ```; If you run into this issue, we recommend one of the following options in the meantime:; * Use the Docker scripts instead of the binaries scripts.; * Set the [`DV_USE_GCP_OPTIMIZED_TF_WHL`](https://github.com/google/deepvariant/blob/r0.9/settings.sh#L90) variable to 0 prior to setting up DeepVariant and running the case study scripts for binaries. `intel-tensorflow` is only installed when this variable is set.; * Use the GPU scripts instead of the CPU scripts. Best,; The DeepVariant Team",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263
https://github.com/google/deepvariant/issues/265:163,Availability,error,errors,163,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! (I realized posting in a new issue made more sense than adding to a closed issue). ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```. _Originally posted by @ksw9 in https://github.com/google/deepvariant/issues/243#issuecomment-579406829_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1108,Energy Efficiency,Power,Power,1108,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! (I realized posting in a new issue made more sense than adding to a closed issue). ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```. _Originally posted by @ksw9 in https://github.com/google/deepvariant/issues/243#issuecomment-579406829_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:64,Testability,test,tested,64,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! (I realized posting in a new issue made more sense than adding to a closed issue). ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```. _Originally posted by @ksw9 in https://github.com/google/deepvariant/issues/243#issuecomment-579406829_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:106,Testability,test,test,106,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! (I realized posting in a new issue made more sense than adding to a closed issue). ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```. _Originally posted by @ksw9 in https://github.com/google/deepvariant/issues/243#issuecomment-579406829_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:564,Testability,test,test,564,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! (I realized posting in a new issue made more sense than adding to a closed issue). ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```. _Originally posted by @ksw9 in https://github.com/google/deepvariant/issues/243#issuecomment-579406829_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:586,Testability,test,testdata,586,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! (I realized posting in a new issue made more sense than adding to a closed issue). ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```. _Originally posted by @ksw9 in https://github.com/google/deepvariant/issues/243#issuecomment-579406829_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:658,Testability,test,test,658,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! (I realized posting in a new issue made more sense than adding to a closed issue). ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```. _Originally posted by @ksw9 in https://github.com/google/deepvariant/issues/243#issuecomment-579406829_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:680,Testability,test,testdata,680,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! (I realized posting in a new issue made more sense than adding to a closed issue). ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```. _Originally posted by @ksw9 in https://github.com/google/deepvariant/issues/243#issuecomment-579406829_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/265:1122,Testability,log,login,1122,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! (I realized posting in a new issue made more sense than adding to a closed issue). ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```. _Originally posted by @ksw9 in https://github.com/google/deepvariant/issues/243#issuecomment-579406829_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265
https://github.com/google/deepvariant/issues/268:1089,Energy Efficiency,schedul,schedule,1089,"Hello guys,. following an email discussion ... I am unable to use a different model. The help seems to suggest one can set --model_type=WGS and then use --customized_model=""PATH_to_model.cpk"". ```; --model_type: <WGS|WES|PACBIO>: Required. Type of model to use for variant ; calling. Each model_type has an associated default model, which can be ; overridden by the --customized_model flag.; ```. But then the run produces. `""I0206 11:58:24.997612 140003716306688 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt""; `; which I interpret as deepvariant falling back on its default model. ; What I notice is that even with the customized-model flag, it doesn't run if I don't set up --model-type. It seems like when both flags are set (and there is no way to do otherwise) it gives priority to its default model, which is the opposite of the intended behaviour right? . Or is there something I am missing? . Thanks a lot for any help or suggestion (and sorry for the many messages, I just really want to try that model because it seems very promising and have it fit in my schedule)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/268:988,Integrability,message,messages,988,"Hello guys,. following an email discussion ... I am unable to use a different model. The help seems to suggest one can set --model_type=WGS and then use --customized_model=""PATH_to_model.cpk"". ```; --model_type: <WGS|WES|PACBIO>: Required. Type of model to use for variant ; calling. Each model_type has an associated default model, which can be ; overridden by the --customized_model flag.; ```. But then the run produces. `""I0206 11:58:24.997612 140003716306688 saver.py:1270] Restoring parameters from /opt/models/wgs/model.ckpt""; `; which I interpret as deepvariant falling back on its default model. ; What I notice is that even with the customized-model flag, it doesn't run if I don't set up --model-type. It seems like when both flags are set (and there is no way to do otherwise) it gives priority to its default model, which is the opposite of the intended behaviour right? . Or is there something I am missing? . Thanks a lot for any help or suggestion (and sorry for the many messages, I just really want to try that model because it seems very promising and have it fit in my schedule)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268
https://github.com/google/deepvariant/issues/269:234,Availability,avail,available,234,"Hi . I am using the quick start docker image. I think all the examples have been created at this point. When it first started creating examples top showed that all 64 of my cpu's where at 100% utilization, and there was still lots of available memory. I have not seen any new log files in over a day. I have check top several times over the last 2 days. It only shows 2 python processes and each of them is at 800% utilization. In my experience training models takes a long time, however making predictions is quick. Should I kill my job and try and start over again? I ran into a problem like this before on a much smaller machine. After 11 days I killed the jobs. I do not know much about docker. I looked in /var/lib/docker/containers. I did not find anything that looked a like a log file. any debugging tips would be appreciated. Andy. config ; ```; google/deepvariant:0.9.0; --model_type=WES; --regions=/input/agilent_sureselect_human_all_exon_v5_b37_targets.bed; --num_shards=64; ```; Looks like make_example completed; ```; I0208 03:49:00.939260 140440947410688 make_examples.py:1330] Writing examples to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00063-of-00064.gz; I0208 03:49:00.940793 140440947410688 make_examples.py:1334] Writing gvcf records to /tmp/deepvariant_tmp_output/gvcf.tfrecord-00063-of-00064.gz; I0208 03:49:01.427521 140440947410688 make_examples.py:905] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2020-02-08 03:49:01.428281: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OPT_BLOCK_SIZE to 134217728; I0208 03:49:01.743115 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam with NativeSamReader; I0208 03:49:01.755232 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam wit",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:2749,Availability,checkpoint,checkpoint,2749,"ta/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam with NativeSamReader; I0208 03:49:01.755232 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam with NativeSamReader; I0208 03:49:02.136116 140440947410688 make_examples.py:1363] Task 63: 0 candidates (0 examples) [1.19s elapsed]; I0208 06:50:01.437930 140440947410688 make_examples.py:1363] Task 63: 101 candidates (101 examples) [10859.30s elapsed]; I0208 07:30:38.055526 140440947410688 make_examples.py:1380] Found 176 candidate variants; I0208 07:30:38.056374 140440947410688 make_examples.py:1381] Created 178 examples. real	346m1.860s; user	7558m17.436s; sys	11m12.192s; ```. looks like it starts making predictions; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". ```. the tail of my noup.out has not changed in over a day; ```; packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.; Instructions for updating:; Use standard file APIs to check for files with this prefix.; I0208 09:29:54.405941 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:55.469674 139859027293952 session_manager.py:491] Running local_init_op.; I0208 09:29:55.510524 139859027293952 session_manager.py:493] Done running local_init_op.; I0208 09:29:55.864006 139859027293952 modeling.py:410] Reloading EMA...; I0208 09:29:55.864634 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]; ```; top. Looks like there is a lot of under utilized com",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:4118,Availability,avail,avail,4118,"his prefix.; I0208 09:29:54.405941 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:55.469674 139859027293952 session_manager.py:491] Running local_init_op.; I0208 09:29:55.510524 139859027293952 session_manager.py:493] Done running local_init_op.; I0208 09:29:55.864006 139859027293952 modeling.py:410] Reloading EMA...; I0208 09:29:55.864634 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]; ```; top. Looks like there is a lot of under utilized compute resources; ```; top - 00:16:22 up 1 day, 23:24, 1 user, load average: 16.03, 16.02, 16.00; Tasks: 621 total, 1 running, 620 sleeping, 0 stopped, 0 zombie; %Cpu(s): 25.0 us, 0.0 sy, 0.0 ni, 75.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st; KiB Mem : 52262400+total, 48358937+free, 8878616 used, 30156016 buff/cache; KiB Swap: 0 total, 0 free, 0 used. 51216832+avail Mem . PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND ; 23437 root 20 0 27.319g 3.709g 136440 S 800.0 0.7 18646:29 python ; 23944 root 20 0 27.259g 3.687g 137396 S 799.7 0.7 18613:17 python ; 1 root 20 0 119604 5788 4112 S 0.0 0.0 0:03.37 systemd ; 2 root 20 0 0 0 0 S 0.0 0.0 0:00.01 kthreadd ; 3 root 20 0 0 0 0 S 0.0 0.0 0:00.06 ksoftirqd/0 ; 4 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0 ; ```; search for recently modified files; ```; ubuntu@ip-172-31-21-181:/deepTmp$ sudo find . -type f -printf '%T@ %p\n' | sort -n | tail -1 | cut -f2- -d"" ""; ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; ubuntu@ip-172-31-21-181:/deepTmp$ ls -l ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; -rw-r--r-- 1 root 0 Feb 8 09:29 ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; ubuntu@ip-172-31-21-181:/deepTmp$ date; Mon Feb 10 00:22:46 UTC 2020; ubuntu@ip-172-31-21-181:/deepTmp$ ; ```. I also noticed there are several deprecation warnings in the log file",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:841,Modifiability,config,config,841,"Hi . I am using the quick start docker image. I think all the examples have been created at this point. When it first started creating examples top showed that all 64 of my cpu's where at 100% utilization, and there was still lots of available memory. I have not seen any new log files in over a day. I have check top several times over the last 2 days. It only shows 2 python processes and each of them is at 800% utilization. In my experience training models takes a long time, however making predictions is quick. Should I kill my job and try and start over again? I ran into a problem like this before on a much smaller machine. After 11 days I killed the jobs. I do not know much about docker. I looked in /var/lib/docker/containers. I did not find anything that looked a like a log file. any debugging tips would be appreciated. Andy. config ; ```; google/deepvariant:0.9.0; --model_type=WES; --regions=/input/agilent_sureselect_human_all_exon_v5_b37_targets.bed; --num_shards=64; ```; Looks like make_example completed; ```; I0208 03:49:00.939260 140440947410688 make_examples.py:1330] Writing examples to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00063-of-00064.gz; I0208 03:49:00.940793 140440947410688 make_examples.py:1334] Writing gvcf records to /tmp/deepvariant_tmp_output/gvcf.tfrecord-00063-of-00064.gz; I0208 03:49:01.427521 140440947410688 make_examples.py:905] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2020-02-08 03:49:01.428281: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OPT_BLOCK_SIZE to 134217728; I0208 03:49:01.743115 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam with NativeSamReader; I0208 03:49:01.755232 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam wit",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:3824,Performance,load,load,3824,"odel.ckpt"". ```. the tail of my noup.out has not changed in over a day; ```; packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.; Instructions for updating:; Use standard file APIs to check for files with this prefix.; I0208 09:29:54.405941 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:55.469674 139859027293952 session_manager.py:491] Running local_init_op.; I0208 09:29:55.510524 139859027293952 session_manager.py:493] Done running local_init_op.; I0208 09:29:55.864006 139859027293952 modeling.py:410] Reloading EMA...; I0208 09:29:55.864634 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]; ```; top. Looks like there is a lot of under utilized compute resources; ```; top - 00:16:22 up 1 day, 23:24, 1 user, load average: 16.03, 16.02, 16.00; Tasks: 621 total, 1 running, 620 sleeping, 0 stopped, 0 zombie; %Cpu(s): 25.0 us, 0.0 sy, 0.0 ni, 75.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st; KiB Mem : 52262400+total, 48358937+free, 8878616 used, 30156016 buff/cache; KiB Swap: 0 total, 0 free, 0 used. 51216832+avail Mem . PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND ; 23437 root 20 0 27.319g 3.709g 136440 S 800.0 0.7 18646:29 python ; 23944 root 20 0 27.259g 3.687g 137396 S 799.7 0.7 18613:17 python ; 1 root 20 0 119604 5788 4112 S 0.0 0.0 0:03.37 systemd ; 2 root 20 0 0 0 0 S 0.0 0.0 0:00.01 kthreadd ; 3 root 20 0 0 0 0 S 0.0 0.0 0:00.06 ksoftirqd/0 ; 4 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0 ; ```; search for recently modified files; ```; ubuntu@ip-172-31-21-181:/deepTmp$ sudo find . -type f -printf '%T@ %p\n' | sort -n | tail -1 | cut -f2- -d"" ""; ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; ubuntu@ip-172-31-21-181:/deepTmp$ ls",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:4067,Performance,cache,cache,4067,"or updating:; Use standard file APIs to check for files with this prefix.; I0208 09:29:54.405941 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:55.469674 139859027293952 session_manager.py:491] Running local_init_op.; I0208 09:29:55.510524 139859027293952 session_manager.py:493] Done running local_init_op.; I0208 09:29:55.864006 139859027293952 modeling.py:410] Reloading EMA...; I0208 09:29:55.864634 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]; ```; top. Looks like there is a lot of under utilized compute resources; ```; top - 00:16:22 up 1 day, 23:24, 1 user, load average: 16.03, 16.02, 16.00; Tasks: 621 total, 1 running, 620 sleeping, 0 stopped, 0 zombie; %Cpu(s): 25.0 us, 0.0 sy, 0.0 ni, 75.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st; KiB Mem : 52262400+total, 48358937+free, 8878616 used, 30156016 buff/cache; KiB Swap: 0 total, 0 free, 0 used. 51216832+avail Mem . PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND ; 23437 root 20 0 27.319g 3.709g 136440 S 800.0 0.7 18646:29 python ; 23944 root 20 0 27.259g 3.687g 137396 S 799.7 0.7 18613:17 python ; 1 root 20 0 119604 5788 4112 S 0.0 0.0 0:03.37 systemd ; 2 root 20 0 0 0 0 S 0.0 0.0 0:00.01 kthreadd ; 3 root 20 0 0 0 0 S 0.0 0.0 0:00.06 ksoftirqd/0 ; 4 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0 ; ```; search for recently modified files; ```; ubuntu@ip-172-31-21-181:/deepTmp$ sudo find . -type f -printf '%T@ %p\n' | sort -n | tail -1 | cut -f2- -d"" ""; ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; ubuntu@ip-172-31-21-181:/deepTmp$ ls -l ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; -rw-r--r-- 1 root 0 Feb 8 09:29 ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; ubuntu@ip-172-31-21-181:/deepTmp$ date; Mon Feb 10 00:22:46 UTC 2020; ubuntu@ip-172-31-21-181:/deepTmp$ ; ```. I also ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:495,Safety,predict,predictions,495,"Hi . I am using the quick start docker image. I think all the examples have been created at this point. When it first started creating examples top showed that all 64 of my cpu's where at 100% utilization, and there was still lots of available memory. I have not seen any new log files in over a day. I have check top several times over the last 2 days. It only shows 2 python processes and each of them is at 800% utilization. In my experience training models takes a long time, however making predictions is quick. Should I kill my job and try and start over again? I ran into a problem like this before on a much smaller machine. After 11 days I killed the jobs. I do not know much about docker. I looked in /var/lib/docker/containers. I did not find anything that looked a like a log file. any debugging tips would be appreciated. Andy. config ; ```; google/deepvariant:0.9.0; --model_type=WES; --regions=/input/agilent_sureselect_human_all_exon_v5_b37_targets.bed; --num_shards=64; ```; Looks like make_example completed; ```; I0208 03:49:00.939260 140440947410688 make_examples.py:1330] Writing examples to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00063-of-00064.gz; I0208 03:49:00.940793 140440947410688 make_examples.py:1334] Writing gvcf records to /tmp/deepvariant_tmp_output/gvcf.tfrecord-00063-of-00064.gz; I0208 03:49:01.427521 140440947410688 make_examples.py:905] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2020-02-08 03:49:01.428281: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OPT_BLOCK_SIZE to 134217728; I0208 03:49:01.743115 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam with NativeSamReader; I0208 03:49:01.755232 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam wit",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:2513,Safety,predict,predictions,2513,".428281: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OPT_BLOCK_SIZE to 134217728; I0208 03:49:01.743115 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam with NativeSamReader; I0208 03:49:01.755232 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam with NativeSamReader; I0208 03:49:02.136116 140440947410688 make_examples.py:1363] Task 63: 0 candidates (0 examples) [1.19s elapsed]; I0208 06:50:01.437930 140440947410688 make_examples.py:1363] Task 63: 101 candidates (101 examples) [10859.30s elapsed]; I0208 07:30:38.055526 140440947410688 make_examples.py:1380] Found 176 candidate variants; I0208 07:30:38.056374 140440947410688 make_examples.py:1381] Created 178 examples. real	346m1.860s; user	7558m17.436s; sys	11m12.192s; ```. looks like it starts making predictions; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". ```. the tail of my noup.out has not changed in over a day; ```; packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.; Instructions for updating:; Use standard file APIs to check for files with this prefix.; I0208 09:29:54.405941 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:55.469674 139859027293952 session_manager.py:491] Running local_init_op.; I0208 09:29:55.510524 139859027293952 session_manager.py:493] Done running local_init_op.; I0208 09:29:55.864006 139859027293952 modeling.py:410] Reloading EMA...; I0208 09:29:55.864634 139859027293952 saver.py:1270] Restoring parameters from /opt/models/w",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:276,Testability,log,log,276,"Hi . I am using the quick start docker image. I think all the examples have been created at this point. When it first started creating examples top showed that all 64 of my cpu's where at 100% utilization, and there was still lots of available memory. I have not seen any new log files in over a day. I have check top several times over the last 2 days. It only shows 2 python processes and each of them is at 800% utilization. In my experience training models takes a long time, however making predictions is quick. Should I kill my job and try and start over again? I ran into a problem like this before on a much smaller machine. After 11 days I killed the jobs. I do not know much about docker. I looked in /var/lib/docker/containers. I did not find anything that looked a like a log file. any debugging tips would be appreciated. Andy. config ; ```; google/deepvariant:0.9.0; --model_type=WES; --regions=/input/agilent_sureselect_human_all_exon_v5_b37_targets.bed; --num_shards=64; ```; Looks like make_example completed; ```; I0208 03:49:00.939260 140440947410688 make_examples.py:1330] Writing examples to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00063-of-00064.gz; I0208 03:49:00.940793 140440947410688 make_examples.py:1334] Writing gvcf records to /tmp/deepvariant_tmp_output/gvcf.tfrecord-00063-of-00064.gz; I0208 03:49:01.427521 140440947410688 make_examples.py:905] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2020-02-08 03:49:01.428281: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OPT_BLOCK_SIZE to 134217728; I0208 03:49:01.743115 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam with NativeSamReader; I0208 03:49:01.755232 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam wit",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:784,Testability,log,log,784,"Hi . I am using the quick start docker image. I think all the examples have been created at this point. When it first started creating examples top showed that all 64 of my cpu's where at 100% utilization, and there was still lots of available memory. I have not seen any new log files in over a day. I have check top several times over the last 2 days. It only shows 2 python processes and each of them is at 800% utilization. In my experience training models takes a long time, however making predictions is quick. Should I kill my job and try and start over again? I ran into a problem like this before on a much smaller machine. After 11 days I killed the jobs. I do not know much about docker. I looked in /var/lib/docker/containers. I did not find anything that looked a like a log file. any debugging tips would be appreciated. Andy. config ; ```; google/deepvariant:0.9.0; --model_type=WES; --regions=/input/agilent_sureselect_human_all_exon_v5_b37_targets.bed; --num_shards=64; ```; Looks like make_example completed; ```; I0208 03:49:00.939260 140440947410688 make_examples.py:1330] Writing examples to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00063-of-00064.gz; I0208 03:49:00.940793 140440947410688 make_examples.py:1334] Writing gvcf records to /tmp/deepvariant_tmp_output/gvcf.tfrecord-00063-of-00064.gz; I0208 03:49:01.427521 140440947410688 make_examples.py:905] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2020-02-08 03:49:01.428281: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OPT_BLOCK_SIZE to 134217728; I0208 03:49:01.743115 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam with NativeSamReader; I0208 03:49:01.755232 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam wit",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/269:5104,Testability,log,log,5104,"his prefix.; I0208 09:29:54.405941 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:55.469674 139859027293952 session_manager.py:491] Running local_init_op.; I0208 09:29:55.510524 139859027293952 session_manager.py:493] Done running local_init_op.; I0208 09:29:55.864006 139859027293952 modeling.py:410] Reloading EMA...; I0208 09:29:55.864634 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]; ```; top. Looks like there is a lot of under utilized compute resources; ```; top - 00:16:22 up 1 day, 23:24, 1 user, load average: 16.03, 16.02, 16.00; Tasks: 621 total, 1 running, 620 sleeping, 0 stopped, 0 zombie; %Cpu(s): 25.0 us, 0.0 sy, 0.0 ni, 75.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st; KiB Mem : 52262400+total, 48358937+free, 8878616 used, 30156016 buff/cache; KiB Swap: 0 total, 0 free, 0 used. 51216832+avail Mem . PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND ; 23437 root 20 0 27.319g 3.709g 136440 S 800.0 0.7 18646:29 python ; 23944 root 20 0 27.259g 3.687g 137396 S 799.7 0.7 18613:17 python ; 1 root 20 0 119604 5788 4112 S 0.0 0.0 0:03.37 systemd ; 2 root 20 0 0 0 0 S 0.0 0.0 0:00.01 kthreadd ; 3 root 20 0 0 0 0 S 0.0 0.0 0:00.06 ksoftirqd/0 ; 4 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0 ; ```; search for recently modified files; ```; ubuntu@ip-172-31-21-181:/deepTmp$ sudo find . -type f -printf '%T@ %p\n' | sort -n | tail -1 | cut -f2- -d"" ""; ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; ubuntu@ip-172-31-21-181:/deepTmp$ ls -l ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; -rw-r--r-- 1 root 0 Feb 8 09:29 ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; ubuntu@ip-172-31-21-181:/deepTmp$ date; Mon Feb 10 00:22:46 UTC 2020; ubuntu@ip-172-31-21-181:/deepTmp$ ; ```. I also noticed there are several deprecation warnings in the log file",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/269
https://github.com/google/deepvariant/issues/270:73,Availability,error,error,73,"Hello,; I've been trying to run deepvariant, but I keep getting the same error:. d not read base quality scores; 2020-02-10 20:52:23.436669: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/0_2762: Not found: Could not read base quality scores; 2020-02-10 20:52:23.436770: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/22518_26356: Not found: Could not read base quality scores; 2020-02-10 20:52:23.436874: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/2839_6783: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437269: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/14677_18575: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437378: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/26432_30218: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437453: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/34281_36602: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437552: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/6879_10765: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437662: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/30289_34208: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437766: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/18650_22440: Not found: Could not read base quality scores; 2020-02-10 20:52:23.438311: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_18",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:2235,Testability,test,test,2235,"m54191_180528_182730/29557366/26432_30218: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437453: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/34281_36602: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437552: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/6879_10765: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437662: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/30289_34208: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437766: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/18650_22440: Not found: Could not read base quality scores; 2020-02-10 20:52:23.438311: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29163785/119_9105: Not found: Could not read base quality scores; 2020-02-10 20:52:23.440373: F deepvariant/allelecounter.cc:103] Check failed: offset + len <= read.aligned_quality_size() (4265 vs. 0). The packages work with the test Illumina files. After testing several options, I notice what I got as raw data is either the bam file (without quality scores, only ""!"") or a fasta files. However, it seems that using those reads during the mapping wuth BWA (or with pbmm2) produces bam files without mapping scores which seems to be the problem. . This is how I mapped reads:. bwa mem -x pacbio -t 8 -R $ReadGroup $1 $2 | java -jar /sw/apps/bioinfo/picard/2.20.4/rackham/picard.jar SortSam \; INPUT=/dev/stdin \; OUTPUT=""$sample.bwa.picardSort.bam"" \; SORT_ORDER=coordinate. I feel this is a very basic question, but it is actually possible to run DeepVariant maping PB reads but in fasta format? Am I missing somthing to get the mapping scores in the bam file?. Thanks a lot,. Cheers,. /Sergio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/270:2262,Testability,test,testing,2262,"m54191_180528_182730/29557366/26432_30218: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437453: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/34281_36602: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437552: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/6879_10765: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437662: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/30289_34208: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437766: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/18650_22440: Not found: Could not read base quality scores; 2020-02-10 20:52:23.438311: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29163785/119_9105: Not found: Could not read base quality scores; 2020-02-10 20:52:23.440373: F deepvariant/allelecounter.cc:103] Check failed: offset + len <= read.aligned_quality_size() (4265 vs. 0). The packages work with the test Illumina files. After testing several options, I notice what I got as raw data is either the bam file (without quality scores, only ""!"") or a fasta files. However, it seems that using those reads during the mapping wuth BWA (or with pbmm2) produces bam files without mapping scores which seems to be the problem. . This is how I mapped reads:. bwa mem -x pacbio -t 8 -R $ReadGroup $1 $2 | java -jar /sw/apps/bioinfo/picard/2.20.4/rackham/picard.jar SortSam \; INPUT=/dev/stdin \; OUTPUT=""$sample.bwa.picardSort.bam"" \; SORT_ORDER=coordinate. I feel this is a very basic question, but it is actually possible to run DeepVariant maping PB reads but in fasta format? Am I missing somthing to get the mapping scores in the bam file?. Thanks a lot,. Cheers,. /Sergio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270
https://github.com/google/deepvariant/issues/271:195,Usability,clear,clearly,195,"Hello,; It seems that 1 shard = 1 thread, however at some point in its run it seems that DeepVariant takes more threads N_SHARD is set to 20, no one else is using the computer at this moment and clearly more than 20 threads are taken. ![higdpgpplenjbaao](https://user-images.githubusercontent.com/23341393/74519762-a12a4c00-4f16-11ea-986c-b785044a618b.png). Is it expected?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271
https://github.com/google/deepvariant/issues/272:370,Modifiability,inherit,inherits,370,"hi, I am using DV 0.9 to find *de novo* variants in trios and so I am enriching for weird stuff. ; I am sure your team is aware of some/all of these, but I'll document at least 1 case here for the record. Nearly all obvious false positive de novo calls follow the same pattern. Mostly there is a haplotype from mom , and a haplotype from dad that are different. The kid inherits both the variable haplotypes, but the combination of DV and glnexus gentoype such that the variant appears as de novo. But, here is an example where there is just an incorrect call from DV that leads to 3 neighboring spurious de novo calls in the kid (top row):; ![bad-dn](https://user-images.githubusercontent.com/1739/74561469-d68a6600-4f25-11ea-8c71-105596810ace.png). note that dad in the 3rd row has a single read with a 1-base deletion (dash) followed by an insertion (purple tick). I can't show all of the reads in this image, but I have scrolled through and verified that is the only read. . here is the content of the dad's VCF for that region (the mom's is actually very similar):; ```; chr8	75144980	.	CT	C	44.7	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:44:34:16,18:0.529412:44,0,53; chr8	75144983	.	T	TG	49.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:49:35:18,17:0.485714:49,0,64; ```; note that that is the single-base del and the insertion that occurs in only 1 read. Since the mom's is the same, maybe there's something akin to realignment going on, but; by contrast, here is the kid's (seemingly more sensible) VCF for that region:; ```; chr8	75144981	.	T	A	71.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:66:27:11,16:0.592593:71,0,67; chr8	75144982	.	A	T	63.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:61:27:11,16:0.592593:63,0,63; chr8	75144983	.	T	G	67.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:66:27:11,16:0.592593:67,0,71; ```; here is the content of the gvcf for dad:; ```; chr8	75144980	.	CT	C,<*>	44.7	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:44:34:16,18,0:0.529412,0:44,0,53,990,990,990; chr8	75144982	.	A	<*>	0	.	END=75144982	GT:GQ:MIN_DP:PL	0/0:48:16:0,48,479; chr8",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:388,Modifiability,variab,variable,388,"hi, I am using DV 0.9 to find *de novo* variants in trios and so I am enriching for weird stuff. ; I am sure your team is aware of some/all of these, but I'll document at least 1 case here for the record. Nearly all obvious false positive de novo calls follow the same pattern. Mostly there is a haplotype from mom , and a haplotype from dad that are different. The kid inherits both the variable haplotypes, but the combination of DV and glnexus gentoype such that the variant appears as de novo. But, here is an example where there is just an incorrect call from DV that leads to 3 neighboring spurious de novo calls in the kid (top row):; ![bad-dn](https://user-images.githubusercontent.com/1739/74561469-d68a6600-4f25-11ea-8c71-105596810ace.png). note that dad in the 3rd row has a single read with a 1-base deletion (dash) followed by an insertion (purple tick). I can't show all of the reads in this image, but I have scrolled through and verified that is the only read. . here is the content of the dad's VCF for that region (the mom's is actually very similar):; ```; chr8	75144980	.	CT	C	44.7	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:44:34:16,18:0.529412:44,0,53; chr8	75144983	.	T	TG	49.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:49:35:18,17:0.485714:49,0,64; ```; note that that is the single-base del and the insertion that occurs in only 1 read. Since the mom's is the same, maybe there's something akin to realignment going on, but; by contrast, here is the kid's (seemingly more sensible) VCF for that region:; ```; chr8	75144981	.	T	A	71.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:66:27:11,16:0.592593:71,0,67; chr8	75144982	.	A	T	63.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:61:27:11,16:0.592593:63,0,63; chr8	75144983	.	T	G	67.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:66:27:11,16:0.592593:67,0,71; ```; here is the content of the gvcf for dad:; ```; chr8	75144980	.	CT	C,<*>	44.7	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:44:34:16,18,0:0.529412,0:44,0,53,990,990,990; chr8	75144982	.	A	<*>	0	.	END=75144982	GT:GQ:MIN_DP:PL	0/0:48:16:0,48,479; chr8",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/272:2866,Usability,clear,clearly,2866,"rolled through and verified that is the only read. . here is the content of the dad's VCF for that region (the mom's is actually very similar):; ```; chr8	75144980	.	CT	C	44.7	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:44:34:16,18:0.529412:44,0,53; chr8	75144983	.	T	TG	49.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:49:35:18,17:0.485714:49,0,64; ```; note that that is the single-base del and the insertion that occurs in only 1 read. Since the mom's is the same, maybe there's something akin to realignment going on, but; by contrast, here is the kid's (seemingly more sensible) VCF for that region:; ```; chr8	75144981	.	T	A	71.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:66:27:11,16:0.592593:71,0,67; chr8	75144982	.	A	T	63.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:61:27:11,16:0.592593:63,0,63; chr8	75144983	.	T	G	67.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:66:27:11,16:0.592593:67,0,71; ```; here is the content of the gvcf for dad:; ```; chr8	75144980	.	CT	C,<*>	44.7	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:44:34:16,18,0:0.529412,0:44,0,53,990,990,990; chr8	75144982	.	A	<*>	0	.	END=75144982	GT:GQ:MIN_DP:PL	0/0:48:16:0,48,479; chr8	75144983	.	T	TG,<*>	49.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:49:35:18,17,0:0.485714,0:49,0,64,990,990,990; chr8	75144984	.	G	<*>	0	.	END=75145000	GT:GQ:MIN_DP:PL	0/0:50:31:0,105,1049; ```; and kid:; ```; chr8	75144981	.	T	A,<*>	71.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:66:27:11,16,0:0.592593,0:71,0,67,990,990,990; chr8	75144982	.	A	T,<*>	63.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:61:27:11,16,0:0.592593,0:63,0,63,990,990,990; chr8	75144983	.	T	G,<*>	67.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:66:27:11,16,0:0.592593,0:67,0,71,990,990,990; chr8	75144984	.	G	<*>	0	.	END=75145000	GT:GQ:MIN_DP:PL	0/0:50:25:0,75,749; ```; I am attaching a small sam for kid and dad aligned to hg38; [kid.sam.gz](https://github.com/google/deepvariant/files/4206576/kid.sam.gz); [dad.sam.gz](https://github.com/google/deepvariant/files/4206577/dad.sam.gz). I have other scenarios, but this one is one that seems clearly a deep variant issue and not a problem with glnexus.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272
https://github.com/google/deepvariant/issues/273:2468,Availability,checkpoint,checkpoint,2468,"on()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session; config=config); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s; user	0m9.233s; sys	0m4.817s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1; ```; However, the direc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/273
https://github.com/google/deepvariant/issues/273:3351,Availability,checkpoint,checkpoint,3351,"ine 1268, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s; user	0m9.233s; sys	0m4.817s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1; ```; However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```; #!/usr/bin/zsh; OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model""; mkdir -p ""${OUTPUT_DIR}""; INPUT_DIR=""${PWD}""; BIN_VERSION=""0.9.0""; N_SHARDS=20; LOG_DIR=""${OUTPUT_DIR}/logs"" ; mkdir -p ""${LOG_DIR}"" ; #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3); #for SAMPLE in ""${decade[@]}""; #do; # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz; #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam""",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/273
https://github.com/google/deepvariant/issues/273:2089,Modifiability,config,config,2089,"riod_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session; config=config); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s; user	0m9.233s; sys	0m4.817s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise Ca",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/273
https://github.com/google/deepvariant/issues/273:2096,Modifiability,config,config,2096,"riod_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session; config=config); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s; user	0m9.233s; sys	0m4.817s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise Ca",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/273
https://github.com/google/deepvariant/issues/273:757,Safety,predict,prediction,757,"Hello, a following-up thread of a previous issue. I don't know if you get notified of a reply in a closed issue so I am opening a new one. ; I have tried your flag but it doesn't work. . Thank you. ```. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict; hooks=all_hooks) as mon_sess:; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/273
https://github.com/google/deepvariant/issues/273:775,Safety,predict,predictions,775,"Hello, a following-up thread of a previous issue. I don't know if you get notified of a reply in a closed issue so I am opening a new one. ; I have tried your flag but it doesn't work. . Thank you. ```. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict; hooks=all_hooks) as mon_sess:; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/273
https://github.com/google/deepvariant/issues/273:900,Safety,predict,predict,900,"Hello, a following-up thread of a previous issue. I don't know if you get notified of a reply in a closed issue so I am opening a new one. ; I have tried your flag but it doesn't work. . Thank you. ```. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict; hooks=all_hooks) as mon_sess:; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/273
https://github.com/google/deepvariant/issues/273:3900,Testability,log,logs,3900,"sl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1; ```; However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```; #!/usr/bin/zsh; OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model""; mkdir -p ""${OUTPUT_DIR}""; INPUT_DIR=""${PWD}""; BIN_VERSION=""0.9.0""; N_SHARDS=20; LOG_DIR=""${OUTPUT_DIR}/logs"" ; mkdir -p ""${LOG_DIR}"" ; #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3); #for SAMPLE in ""${decade[@]}""; #do; # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz; #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log""; #done; ```. _Originally posted by @aderzelle in https://github.com/google/deepvariant/issues/268#issuecomment-586584341_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/273
https://github.com/google/deepvariant/issues/273:4601,Testability,log,log,4601,"sl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1; ```; However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```; #!/usr/bin/zsh; OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model""; mkdir -p ""${OUTPUT_DIR}""; INPUT_DIR=""${PWD}""; BIN_VERSION=""0.9.0""; N_SHARDS=20; LOG_DIR=""${OUTPUT_DIR}/logs"" ; mkdir -p ""${LOG_DIR}"" ; #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3); #for SAMPLE in ""${decade[@]}""; #do; # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz; #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log""; #done; ```. _Originally posted by @aderzelle in https://github.com/google/deepvariant/issues/268#issuecomment-586584341_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/273
https://github.com/google/deepvariant/issues/274:403,Modifiability,config,config,403,"Hello, follow-up ... I was not able to make it work, also we noticed, even when it's not taking all the threads, when it's running with the numbers of threads given by the shards argument, other processes (so, not DeepVariant) are on S as shown in htop. Is it that DeepVariant co-opt all the threads even when it doesn't currently need them? . Got it. Thanks for the context! If you end up tweaking the config, let me know whether it works for you or not.; I'll close this issue now. _Originally posted by @pichuan in https://github.com/google/deepvariant/issues/271#issuecomment-586523576_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274
https://github.com/google/deepvariant/issues/275:565,Integrability,wrap,wrapper,565,"Hi,; I have a question about the information used by DeepVariant (v0.9.0) with the model `PACBIO`: does this model rely on/benefit from the additional quality information contained in ""PacBio-native"" BAM files? In other words, are the variant calls identical for a dataset that is processed (i) using the PacBio-native BAM as input, requiring the alignment to be done with pbmm2 to keep said information intact; and (ii), using FASTQ as input (w/o the additional quality information), and the alignment is performed with minimap2 (since pbmm2 is essentially just a wrapper around minimap2, let's assume the resulting alignments are identical)? Thanks for the clarification.; Best,; Peter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/issues/275:506,Performance,perform,performed,506,"Hi,; I have a question about the information used by DeepVariant (v0.9.0) with the model `PACBIO`: does this model rely on/benefit from the additional quality information contained in ""PacBio-native"" BAM files? In other words, are the variant calls identical for a dataset that is processed (i) using the PacBio-native BAM as input, requiring the alignment to be done with pbmm2 to keep said information intact; and (ii), using FASTQ as input (w/o the additional quality information), and the alignment is performed with minimap2 (since pbmm2 is essentially just a wrapper around minimap2, let's assume the resulting alignments are identical)? Thanks for the clarification.; Best,; Peter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275
https://github.com/google/deepvariant/pull/276:15,Deployability,update,update,15,This is a blog update by the DeepVariant team. Note: we are not taking pull requests at this time.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/276
https://github.com/google/deepvariant/issues/281:587,Availability,error,error,587,"Hi, I run this command to call variant on my bam file:; ```; sudo docker run \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/home/thanh/ref37/hg19.fa \; --reads=/home/thanh/Procedure_practice/17D2627041/qualification/17D2627041_hg19_arr.bam \; --regions=$/home/thanh/RB1_Project/VBDI-VAR/RB1_padding_100.bed \; --output_vcf=/home/thanh/Procedure_practice/17D2627041/VCF/17D2627041_De_hg19.vcf \; --output_gvcf=/home/thanh/Procedure_practice/17D2627041/VCF/17D2627041_De_hg19.g.vcf \; --num_shards=4 ; ```; it resulted in this error; ```; ValueError: Not found: Could not open /home/thanh/Procedure_practice/17D2627041/qualification/17D2627041_hg19_arr.bam; I0305 09:55:16.252675 139778511243008 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. [E::hts_open_format] Failed to open file /home/thanh/Procedure_practice/17D2627041/qualification/17D2627041_hg19_arr.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_3IsReI/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1461, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_3IsReI/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1399, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_3IsReI/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 388, in default_options; with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:; File ""/tmp/Bazel.runfiles_3IsReI/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_3IsReI/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return Native",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/281
https://github.com/google/deepvariant/issues/285:28,Deployability,upgrade,upgrade,28,"Hi,. Are there any plans to upgrade to python3? Py2 has been deprecated, which makes deepvariant kind of obsolete.. Thanks; M",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/285
https://github.com/google/deepvariant/issues/287:402,Availability,Down,Download,402,"Given that academic HPCs do not have root privileges it is not possible to run docker and one needs to use singularity. I think it would helpful to your user-base if you added a section on singularity to either the `deepvariant-quick-start.md` or `README.md`. . I had never used containers before today so the following would have been very helpful for me getting your tool up-and-running quicker:. ## Download an existing container from Docker Hub; ```; singularity build deepvariant.simg docker://google/deepvariant; ```. ## Run Deep Variant with Singularity in one command; ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. Respectfully,; Matthew",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/288:1264,Availability,checkpoint,checkpoint,1264,"UT_DIR}:/input \; -v ${OUTPUT_DIR}:/output \; google/deepvariant:latest-gpu \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,000,100"" \; --output_vcf=/dev/stdout \; --num_shards=4 \; 2> stderr.txt; ```; it could work, but print some debug information to stdout and pollute the result; ```; ***** Running the command:*****; time seq 0 3 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --regions ""chr20:10,000,000-10,000,100"" --task {}. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.; For more information, please see:; * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md; * https://github.com/tensorflow/addons; If you depend on functionality not listed there, please file an issue. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --outfile ""/dev/stdout"". ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##FILTER=<ID=RefCall,Description=""Genotyping model thinks this site is reference."">; ##FILTER=<ID=LowQual,Description=""Confidence in this variant being real is below calling threshold."">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End position (for use with symbolic alleles)"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=G",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/288
https://github.com/google/deepvariant/issues/288:2947,Availability,error,errors,2947,"cription=""Genotyping model thinks this site is reference."">; ##FILTER=<ID=LowQual,Description=""Confidence in this variant being real is below calling threshold."">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End position (for use with symbolic alleles)"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">; ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">; ##contig=<ID=chr20,length=63025520>; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	default; ```. I then tried explicitly using /dev/stdin, this time I got errors; ```bash; cat ${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam | \; docker run \; --gpus all \; --rm \; -i \; -v ${INPUT_DIR}:/input \; -v ${OUTPUT_DIR}:/output \; google/deepvariant:latest-gpu \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/dev/stdin \; --regions ""chr20:10,000,000-10,000,100"" \; --output_vcf=/output/output.vcf \; --num_shards=4 ; ```. ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_O8GZSR/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1461, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_O8GZSR/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1451, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_O8GZSR/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1325, in make_exampl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/288
https://github.com/google/deepvariant/issues/288:77,Deployability,pipeline,pipeline,77,"Hi, I want to know is there any way to make deepvariant support command line pipeline, I'm expecting it could accept read from stdin and pipe result to stdout. I tried explicitly using /dev/stdout to pipe result to stdout, ; ```bash; docker run \; --gpus all \; --rm \; -v ${INPUT_DIR}:/input \; -v ${OUTPUT_DIR}:/output \; google/deepvariant:latest-gpu \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,000,100"" \; --output_vcf=/dev/stdout \; --num_shards=4 \; 2> stderr.txt; ```; it could work, but print some debug information to stdout and pollute the result; ```; ***** Running the command:*****; time seq 0 3 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --regions ""chr20:10,000,000-10,000,100"" --task {}. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.; For more information, please see:; * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md; * https://github.com/tensorflow/addons; If you depend on functionality not listed there, please file an issue. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --outfile ""/dev/stdout"". ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##FILTER=<ID=RefCall,Description=""Genotyping model thinks this site is reference."">; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/288
https://github.com/google/deepvariant/issues/288:1554,Integrability,depend,depend,1554,"dev/stdout \; --num_shards=4 \; 2> stderr.txt; ```; it could work, but print some debug information to stdout and pollute the result; ```; ***** Running the command:*****; time seq 0 3 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --regions ""chr20:10,000,000-10,000,100"" --task {}. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.; For more information, please see:; * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md; * https://github.com/tensorflow/addons; If you depend on functionality not listed there, please file an issue. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --outfile ""/dev/stdout"". ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##FILTER=<ID=RefCall,Description=""Genotyping model thinks this site is reference."">; ##FILTER=<ID=LowQual,Description=""Confidence in this variant being real is below calling threshold."">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End position (for use with symbolic alleles)"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/288
https://github.com/google/deepvariant/issues/289:99,Availability,error,error,99,Hello. ; Thanks for long awaited update to Python3.; I'm trying to update our pipeline and getting error when installing CLIF.; Looks like https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-18.latest.tgz is missing while https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-16.latest.tgz exists.; Can you fix this please?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/289
https://github.com/google/deepvariant/issues/289:33,Deployability,update,update,33,Hello. ; Thanks for long awaited update to Python3.; I'm trying to update our pipeline and getting error when installing CLIF.; Looks like https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-18.latest.tgz is missing while https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-16.latest.tgz exists.; Can you fix this please?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/289
https://github.com/google/deepvariant/issues/289:67,Deployability,update,update,67,Hello. ; Thanks for long awaited update to Python3.; I'm trying to update our pipeline and getting error when installing CLIF.; Looks like https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-18.latest.tgz is missing while https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-16.latest.tgz exists.; Can you fix this please?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/289
https://github.com/google/deepvariant/issues/289:78,Deployability,pipeline,pipeline,78,Hello. ; Thanks for long awaited update to Python3.; I'm trying to update our pipeline and getting error when installing CLIF.; Looks like https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-18.latest.tgz is missing while https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-16.latest.tgz exists.; Can you fix this please?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/289
https://github.com/google/deepvariant/issues/289:110,Deployability,install,installing,110,Hello. ; Thanks for long awaited update to Python3.; I'm trying to update our pipeline and getting error when installing CLIF.; Looks like https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-18.latest.tgz is missing while https://storage.googleapis.com/deepvariant/packages/oss_clif_py3/oss_clif.ubuntu-16.latest.tgz exists.; Can you fix this please?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/289
https://github.com/google/deepvariant/issues/290:107,Deployability,pipeline,pipeline,107,"Hi. . I recently ran deep variant with a collection of WGS samples. It seems to have run through the whole pipeline producing the g.vcf.gz and vcf.gz files, however there was no visual_report.html file? . /nrnb/opt/singularity-3.3.0/bin/singularity run --nv /nrnb/opt/singularity-containers/deepvariant_gpu_0.8.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref --reads=$bam --regions $chr --output_vcf=$x.$chr.vcf.gz --output_gvcf=$x.$chr.g.vcf.gz --num_shards=3. Do I have to include a specific output tag in order to get the file? Based on the docs, it should automatically be produced?. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/291:260,Availability,down,download,260,"Hi,. I am trying to train the DeepVariant model in my local machine (presumably not using docker as well). Is there a specific guide on doing that? or do I simply follow https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md, download the binaries and change all output bucket references in the guide to local directories? . Also, if I want to change the structure and parameters of the neural network for training, where can I impose those changes? (for example reducing the image size from 221x100 to 101x100). . Thank you very much for your help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:127,Usability,guid,guide,127,"Hi,. I am trying to train the DeepVariant model in my local machine (presumably not using docker as well). Is there a specific guide on doing that? or do I simply follow https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md, download the binaries and change all output bucket references in the guide to local directories? . Also, if I want to change the structure and parameters of the neural network for training, where can I impose those changes? (for example reducing the image size from 221x100 to 101x100). . Thank you very much for your help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:156,Usability,simpl,simply,156,"Hi,. I am trying to train the DeepVariant model in my local machine (presumably not using docker as well). Is there a specific guide on doing that? or do I simply follow https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md, download the binaries and change all output bucket references in the guide to local directories? . Also, if I want to change the structure and parameters of the neural network for training, where can I impose those changes? (for example reducing the image size from 221x100 to 101x100). . Thank you very much for your help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:329,Usability,guid,guide,329,"Hi,. I am trying to train the DeepVariant model in my local machine (presumably not using docker as well). Is there a specific guide on doing that? or do I simply follow https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md, download the binaries and change all output bucket references in the guide to local directories? . Also, if I want to change the structure and parameters of the neural network for training, where can I impose those changes? (for example reducing the image size from 221x100 to 101x100). . Thank you very much for your help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/292:193,Availability,error,error,193,"I've had success following the **Getting started guide** with both CPU and GPU on the example datasets and now I'm trying to run the CPU version on my own data, _C. elegans_, but am getting an error:. ## Submission script for example. ```; #!/bin/bash; #SBATCH --job-name=example_DV; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; sin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:514,Availability,error,error,514,"I've had success following the **Getting started guide** with both CPU and GPU on the example datasets and now I'm trying to run the CPU version on my own data, _C. elegans_, but am getting an error:. ## Submission script for example. ```; #!/bin/bash; #SBATCH --job-name=example_DV; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; sin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:1662,Availability,error,error,1662,"0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/c_elegans.PRJEB28388.WS274.genomic.fa \; --reads=""${INPUT_DIR}""/maddog_bam_trim_bwaMEM_sort_dedupped.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. The error looks like:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; time=""2020-03-31T17:40:13-07:00"" level=warning msg=""\""/run/user/3019658\"" dire",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:2494,Availability,error,error,2494,"ATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/c_elegans.PRJEB28388.WS274.genomic.fa \; --reads=""${INPUT_DIR}""/maddog_bam_trim_bwaMEM_sort_dedupped.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. The error looks like:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; time=""2020-03-31T17:40:13-07:00"" level=warning msg=""\""/run/user/3019658\"" directory set by $XDG_RUNTIME_DIR does not exist. Either create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such file or directory: Trying to pull image in the event that it is a public image.""; I0331 17:40:16.049175 47823917316800 run_deepvariant.py:241] Re-using the directory for intermediate results in /tmp/tmpl3fvinw4; I0331 17:40:24.867229 47384002755264 make_examples.py:386] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I0331 17:40:25.244051 47384002755264 genomics_reader.py:223] Reading /scratch/moldach/bin/DEEPVARIANT/MADDOG/maddog_bam_trim_bwaMEM_sort_dedupped.bam with NativeSamReader; I0331 17:40:25.256583 47384002755264 make_examples.py:535] Preparing inputs; I0331 17:40:25.453527 47384002755264 genomics_re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:7540,Availability,error,error,7540,")); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/MADDOG/c_elegans.PRJEB28388.WS274.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/MADDOG/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmpl3fvinw4/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpl3fvinw4/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 1. ```. This is what my input directory looks like:. ```; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; ```. I noticed there are a few more input files in the sample example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_eleg",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:8276,Availability,down,download,8276,88.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; ```. I noticed there are a few more input files in the sample example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:8795,Availability,down,download,8795,.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527.impute.vcf.gz; WI.20180527.impute.vcf.gz.tbi; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz; c_elegans.PRJEB28388.WS274.genomic.fa.gz.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz.gzi; ```. Now that I think I have all the appropriate input files in my `INPUT_DIR` I will try to run the code again:. ```; [31mFATAL: ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:8472,Deployability,release,releases,8472,le example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527.impute.vcf.gz; WI.20180527.impute.vcf.gz.tbi; c_elegans.PRJEB28388.WS274.geno,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:8855,Deployability,release,release,8855,"20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527.impute.vcf.gz; WI.20180527.impute.vcf.gz.tbi; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz; c_elegans.PRJEB28388.WS274.genomic.fa.gz.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz.gzi; ```. Now that I think I have all the appropriate input files in my `INPUT_DIR` I will try to run the code again:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:9055,Deployability,release,releases,9055,"gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527.impute.vcf.gz; WI.20180527.impute.vcf.gz.tbi; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz; c_elegans.PRJEB28388.WS274.genomic.fa.gz.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz.gzi; ```. Now that I think I have all the appropriate input files in my `INPUT_DIR` I will try to run the code again:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; time=""2020-03-31T18:35:24-07:00"" level=warning msg=""\""/run/user/3019658\"" directory set by $XDG_RUNTIME_DIR does not exist. Either create the directory or unset $XDG_RUNTIME_DIR.: sta",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:655,Performance,load,load,655,"I've had success following the **Getting started guide** with both CPU and GPU on the example datasets and now I'm trying to run the CPU version on my own data, _C. elegans_, but am getting an error:. ## Submission script for example. ```; #!/bin/bash; #SBATCH --job-name=example_DV; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; sin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:1804,Performance,load,load,1804,"u-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/c_elegans.PRJEB28388.WS274.genomic.fa \; --reads=""${INPUT_DIR}""/maddog_bam_trim_bwaMEM_sort_dedupped.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. The error looks like:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; time=""2020-03-31T17:40:13-07:00"" level=warning msg=""\""/run/user/3019658\"" directory set by $XDG_RUNTIME_DIR does not exist. Either create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/30196",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:8093,Performance,load,load,8093,"xamples.tfrecord@1.gz"" --gvcf ""/tmp/tmpl3fvinw4/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 1. ```. This is what my input directory looks like:. ```; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; ```. I noticed there are a few more input files in the sample example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:8120,Performance,load,load,8120,"vcf ""/tmp/tmpl3fvinw4/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 1. ```. This is what my input directory looks like:. ```; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; ```. I noticed there are a few more input files in the sample example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module loa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:8143,Performance,load,load,8143,".tfrecord@1.gz"" --task {}' returned non-zero exit status 1. ```. This is what my input directory looks like:. ```; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; ```. I noticed there are a few more input files in the sample example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module lo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:8361,Performance,load,load,8361,MEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; ```. I noticed there are a few more input files in the sample example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_ele,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:8388,Performance,load,load,8388,dog_bam_trim_bwaMEM_sort_dedupped.bam.bai; ```. I noticed there are a few more input files in the sample example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.ann,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:8411,Performance,load,load,8411,dedupped.bam.bai; ```. I noticed there are a few more input files in the sample example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:9117,Performance,load,load,9117," load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527.impute.vcf.gz; WI.20180527.impute.vcf.gz.tbi; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz; c_elegans.PRJEB28388.WS274.genomic.fa.gz.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz.gzi; ```. Now that I think I have all the appropriate input files in my `INPUT_DIR` I will try to run the code again:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; time=""2020-03-31T18:35:24-07:00"" level=warning msg=""\""/run/user/3019658\"" directory set by $XDG_RUNTIME_DIR does not exist. Either create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such file or directory: Try",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:9144,Performance,load,load,9144,"oad samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527.impute.vcf.gz; WI.20180527.impute.vcf.gz.tbi; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz; c_elegans.PRJEB28388.WS274.genomic.fa.gz.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz.gzi; ```. Now that I think I have all the appropriate input files in my `INPUT_DIR` I will try to run the code again:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; time=""2020-03-31T18:35:24-07:00"" level=warning msg=""\""/run/user/3019658\"" directory set by $XDG_RUNTIME_DIR does not exist. Either create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such file or directory: Trying to pull image in the",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:9167,Performance,load,load,9167,"_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527.impute.vcf.gz; WI.20180527.impute.vcf.gz.tbi; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz; c_elegans.PRJEB28388.WS274.genomic.fa.gz.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz.gzi; ```. Now that I think I have all the appropriate input files in my `INPUT_DIR` I will try to run the code again:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; time=""2020-03-31T18:35:24-07:00"" level=warning msg=""\""/run/user/3019658\"" directory set by $XDG_RUNTIME_DIR does not exist. Either create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such file or directory: Trying to pull image in the event that it is a publi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:475,Testability,log,logs,475,"I've had success following the **Getting started guide** with both CPU and GPU on the example datasets and now I'm trying to run the CPU version on my own data, _C. elegans_, but am getting an error:. ## Submission script for example. ```; #!/bin/bash; #SBATCH --job-name=example_DV; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; sin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:553,Testability,log,logs,553,"I've had success following the **Getting started guide** with both CPU and GPU on the example datasets and now I'm trying to run the CPU version on my own data, _C. elegans_, but am getting an error:. ## Submission script for example. ```; #!/bin/bash; #SBATCH --job-name=example_DV; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; sin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:750,Testability,test,testdata,750,"I've had success following the **Getting started guide** with both CPU and GPU on the example datasets and now I'm trying to run the CPU version on my own data, _C. elegans_, but am getting an error:. ## Submission script for example. ```; #!/bin/bash; #SBATCH --job-name=example_DV; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; sin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:1622,Testability,log,logs,1622," --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/c_elegans.PRJEB28388.WS274.genomic.fa \; --reads=""${INPUT_DIR}""/maddog_bam_trim_bwaMEM_sort_dedupped.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. The error looks like:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:1701,Testability,log,logs,1701,"0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/c_elegans.PRJEB28388.WS274.genomic.fa \; --reads=""${INPUT_DIR}""/maddog_bam_trim_bwaMEM_sort_dedupped.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. The error looks like:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; time=""2020-03-31T17:40:13-07:00"" level=warning msg=""\""/run/user/3019658\"" dire",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:49,Usability,guid,guide,49,"I've had success following the **Getting started guide** with both CPU and GPU on the example datasets and now I'm trying to run the CPU version on my own data, _C. elegans_, but am getting an error:. ## Submission script for example. ```; #!/bin/bash; #SBATCH --job-name=example_DV; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; sin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/293:22,Performance,perform,perform,22,"Hi,; Will DeepVariant perform well with PacBio CCS smaller insert reads? (i.e. < 5 kb); Thanks,; Gilad",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/293
https://github.com/google/deepvariant/issues/294:28,Performance,perform,perform,28,"Hi,; Will DeepVariant still perform on very low coverage (1x, 2x, 3x) highly accurate PacBio CCS reads? Or would GATK be better for lower coverage libraries?. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/294
https://github.com/google/deepvariant/issues/296:100,Availability,error,error,100,Hi. I started using the new version of deepvariant (0:10:0-gpu) and have been getting the following error: . parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. It seems like running deepvariant leaves behind temporary files that are taking up a lot of disk space and I was wondering if there is a post run cleaning you recommend to remove these sort of items or some flags to do it?. Thanks for the help!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:119,Availability,Error,Error,119,Hi. I started using the new version of deepvariant (0:10:0-gpu) and have been getting the following error: . parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. It seems like running deepvariant leaves behind temporary files that are taking up a lot of disk space and I was wondering if there is a post run cleaning you recommend to remove these sort of items or some flags to do it?. Thanks for the help!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:215,Availability,Error,Error,215,Hi. I started using the new version of deepvariant (0:10:0-gpu) and have been getting the following error: . parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. It seems like running deepvariant leaves behind temporary files that are taking up a lot of disk space and I was wondering if there is a post run cleaning you recommend to remove these sort of items or some flags to do it?. Thanks for the help!,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/298:57,Deployability,pipeline,pipeline,57,"Hi, . Would it be possible to use Deepvariant's training pipeline to create a model that is able to call larger variants (SVs), or is there something that would fundamentally limit the use of Deepvariant's algorithm for this case? . Thanks a lot in advance for any comments!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/298
https://github.com/google/deepvariant/issues/301:534,Integrability,depend,dependencies,534,"I'm using google/deepvariant-0.10.0 through docker on AVX-512 instruction capable Intel skylake processor. But the docker image ""google/deepvariant-0.10.0"" binaries are not built for AVX-512. Here are the warnings for the same:. ` I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA `. Where can I get deepvariant or tensorflow binaries with AVX-512 optimzation? I tried to build deepvariant from source, but couldn't due to lot of dependencies. . Please let me know if there is any way to get AVX-512 optimized binaries for deepvariant/tensorflow.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:604,Performance,optimiz,optimized,604,"I'm using google/deepvariant-0.10.0 through docker on AVX-512 instruction capable Intel skylake processor. But the docker image ""google/deepvariant-0.10.0"" binaries are not built for AVX-512. Here are the warnings for the same:. ` I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA `. Where can I get deepvariant or tensorflow binaries with AVX-512 optimzation? I tried to build deepvariant from source, but couldn't due to lot of dependencies. . Please let me know if there is any way to get AVX-512 optimized binaries for deepvariant/tensorflow.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/302:35,Safety,detect,detect,35,"Hi,. I used DeepVariant+GLnexus to detect the mutations.; In the GT area of the VCF file, there were descriptions such as "" . /0"" , "" . /1"" or "" . /2"". I've never seen a pattern like "" . /0"" or "" . /1"" when using gatk's haplotypecaller.; Basically, I recognized "" . "" as a missing value. What do the missing values and the heterogeneity of ALT mean?; Does this mean that it is different from the heterozygous mutation of ""REF/ALT"", which is different from "" 0/0"" or "" 0/1""??. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/302
https://github.com/google/deepvariant/issues/303:81,Performance,perform,performed,81,"Hello, ; I used DeepVariant with the mosquito trained model, on 11 samples, then performed joint calling with GLnexus. In the following track, in the first sample it calls 2 SNP G/A and T/C (corresponding to the 2 last blue box) in the D2A1 sample. However, in the D5B3 sample the sites are called as 0/0. There is no ""RefCall"" either so I think it means it did not even generate candidates. ; Point to note: I have no idea if there are, or not, SNPs at those 2 loci. But from the alignment, it is not at clear to me why in one case it thought there were SNPs, and in the other case it thought not. . For the same sites, GATK joint caller decided the site was 0/0 for all samples (again, I don't know which one is the true genotype there). The bam I am showing here are the diagnostic bams emitted by DeepVariant, so my understanding is that I see what it saw. ![igv_snapshot](https://user-images.githubusercontent.com/23341393/80101296-7e995c80-8571-11ea-8e3d-37e306442888.png). As you might notice, the coverage depth across my samples is not always identical. So I wonder if it's not simply a question of coverage (though my lowest average coverage value is 30, which is ok I think). . Would you have any clue of what might be happening? . Thank a lot. EDIT: I can get rid of those regions by filtering on QUAL on the GLnexus pVCF, however. But I am still curious, as I might want to keep them and filter them otherwise (not easy to reach a spot where you get read of FP without removing all the TP).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/303
https://github.com/google/deepvariant/issues/303:505,Usability,clear,clear,505,"Hello, ; I used DeepVariant with the mosquito trained model, on 11 samples, then performed joint calling with GLnexus. In the following track, in the first sample it calls 2 SNP G/A and T/C (corresponding to the 2 last blue box) in the D2A1 sample. However, in the D5B3 sample the sites are called as 0/0. There is no ""RefCall"" either so I think it means it did not even generate candidates. ; Point to note: I have no idea if there are, or not, SNPs at those 2 loci. But from the alignment, it is not at clear to me why in one case it thought there were SNPs, and in the other case it thought not. . For the same sites, GATK joint caller decided the site was 0/0 for all samples (again, I don't know which one is the true genotype there). The bam I am showing here are the diagnostic bams emitted by DeepVariant, so my understanding is that I see what it saw. ![igv_snapshot](https://user-images.githubusercontent.com/23341393/80101296-7e995c80-8571-11ea-8e3d-37e306442888.png). As you might notice, the coverage depth across my samples is not always identical. So I wonder if it's not simply a question of coverage (though my lowest average coverage value is 30, which is ok I think). . Would you have any clue of what might be happening? . Thank a lot. EDIT: I can get rid of those regions by filtering on QUAL on the GLnexus pVCF, however. But I am still curious, as I might want to keep them and filter them otherwise (not easy to reach a spot where you get read of FP without removing all the TP).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/303
https://github.com/google/deepvariant/issues/303:1087,Usability,simpl,simply,1087,"Hello, ; I used DeepVariant with the mosquito trained model, on 11 samples, then performed joint calling with GLnexus. In the following track, in the first sample it calls 2 SNP G/A and T/C (corresponding to the 2 last blue box) in the D2A1 sample. However, in the D5B3 sample the sites are called as 0/0. There is no ""RefCall"" either so I think it means it did not even generate candidates. ; Point to note: I have no idea if there are, or not, SNPs at those 2 loci. But from the alignment, it is not at clear to me why in one case it thought there were SNPs, and in the other case it thought not. . For the same sites, GATK joint caller decided the site was 0/0 for all samples (again, I don't know which one is the true genotype there). The bam I am showing here are the diagnostic bams emitted by DeepVariant, so my understanding is that I see what it saw. ![igv_snapshot](https://user-images.githubusercontent.com/23341393/80101296-7e995c80-8571-11ea-8e3d-37e306442888.png). As you might notice, the coverage depth across my samples is not always identical. So I wonder if it's not simply a question of coverage (though my lowest average coverage value is 30, which is ok I think). . Would you have any clue of what might be happening? . Thank a lot. EDIT: I can get rid of those regions by filtering on QUAL on the GLnexus pVCF, however. But I am still curious, as I might want to keep them and filter them otherwise (not easy to reach a spot where you get read of FP without removing all the TP).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/303
https://github.com/google/deepvariant/issues/304:409,Availability,checkpoint,checkpoint,409,"Not sure what is causing the issue but upon reaching this step DeepVariant failed. Any thoughts on how to fix? I tired to run it in a python2.7 environment and still it somehow is pulling from python 3.6 it seems. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp9_28zx5u/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp9_28zx5u/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". I0424 15:59:50.266534 139872277903104 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-04-24 15:59:50.321136: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-04-24 15:59:50.376605: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz; 2020-04-24 15:59:50.378224: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56a1fd0 executing computations on platform Host. Devices:; 2020-04-24 15:59:50.378283: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-04-24 15:59:50.380979: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I0424 15:59:50.447775 139872277903104 modeling.py:563] Initializing model with random parameters; W0424 15:59:50.449538 139872277903104 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp3bl4tsmc; I0424 15:59:50.450443 139872277903104 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp3bl4tsmc', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_cou",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:6641,Availability,checkpoint,checkpoint,6641,"903104 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0424 16:00:02.682547 139872277903104 estimator.py:1149] Done calling model_fn.; I0424 16:00:06.021238 139872277903104 monitored_session.py:240] Graph was finalized.; I0424 16:00:06.037272 139872277903104 saver.py:1284] Restoring parameters from /opt/models/wes/model.ckpt; I0424 16:00:10.817819 139872277903104 session_manager.py:500] Running local_init_op.; I0424 16:00:11.060626 139872277903104 session_manager.py:502] Done running local_init_op.; I0424 16:00:12.403780 139872277903104 modeling.py:413] Reloading EMA...; I0424 16:00:12.405867 139872277903104 saver.py:1284] Restoring parameters from /opt/models/wes/model.ckpt; I0424 16:00:48.634510 139872277903104 call_variants.py:402] Processed 1 examples in 1 batches [5816.472 sec per 100]. real	4m2.970s; user	5m54.674s; sys	1m14.107s; I0424 16:03:48.557898 140277446174464 run_deepvariant.py:321] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp9_28zx5u/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp9_28zx5u/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wes/model.ckpt""' returned non-zero exit status 247.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:1733,Modifiability,config,config,1733,"4: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56a1fd0 executing computations on platform Host. Devices:; 2020-04-24 15:59:50.378283: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-04-24 15:59:50.380979: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I0424 15:59:50.447775 139872277903104 modeling.py:563] Initializing model with random parameters; W0424 15:59:50.449538 139872277903104 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp3bl4tsmc; I0424 15:59:50.450443 139872277903104 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp3bl4tsmc', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3659263518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0424 15:59:50.451262 139872277903104 call_variants.py:384] Writing calls to /tmp/tmp9_28zx5u/call_variants_output.tfrecord.gz; W0424 15:59:50.467876 139872277903104 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.; Instructions for updating:; I",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:3042,Modifiability,layers,layers,3042,"ps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3659263518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0424 15:59:50.451262 139872277903104 call_variants.py:384] Writing calls to /tmp/tmp9_28zx5u/call_variants_output.tfrecord.gz; W0424 15:59:50.467876 139872277903104 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.; Instructions for updating:; If using Keras pass *_constraint arguments to layers.; I0424 15:59:50.501495 139872277903104 data_providers.py:369] self.input_read_threads=8; W0424 15:59:50.501965 139872277903104 deprecation.py:323] From /tmp/Bazel.runfiles_sszxydhb/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; I0424 15:59:50.681574 139872277903104 data_providers.py:376] self.input_map_threads=48; W0424 15:59:50.681832 139872277903104 deprecation.py:323] From /tmp/Bazel.runfiles_sszxydhb/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:4795,Modifiability,layers,layers,4795,"72277903104 deprecation.py:323] From /tmp/Bazel.runfiles_sszxydhb/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0424 15:59:51.794167 139872277903104 estimator.py:1147] Calling model_fn.; W0424 15:59:51.800228 139872277903104 deprecation.py:323] From /tmp/Bazel.runfiles_sszxydhb/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0424 15:59:51.806498 139872277903104 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0424 16:00:02.682547 139872277903104 estimator.py:1149] Done calling model_fn.; I0424 16:00:06.021238 139872277903104 monitored_session.py:240] Graph was finalized.; I0424 16:00:06.037272 139872277903104 saver.py:1284] Restoring parameters from /opt/models/wes/model.ckpt; I0424 16:00:10.817819 139872277903104 session_manager.py:500] Running local_init_op.; I0424 16:00:11.060626 139872277903104 session_manager.py:502] Done running local_init_op.; I0424 16:00:12.403780 139872277903104 modeling.py:413] Reloading EMA...; I0424 16:00:12.405867 139872277903104 saver.py:1284] Restoring parameters from /opt/models/wes/model.ckpt; I0424 16:00:48.634510 139872277903104 call_variants.py:402] Processed 1 examples in 1 batches [5816.472 sec per 100]. real	4m2.970s; user	5m54.674s; sy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:4802,Modifiability,layers,layers,4802,"72277903104 deprecation.py:323] From /tmp/Bazel.runfiles_sszxydhb/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0424 15:59:51.794167 139872277903104 estimator.py:1147] Calling model_fn.; W0424 15:59:51.800228 139872277903104 deprecation.py:323] From /tmp/Bazel.runfiles_sszxydhb/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0424 15:59:51.806498 139872277903104 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0424 16:00:02.682547 139872277903104 estimator.py:1149] Done calling model_fn.; I0424 16:00:06.021238 139872277903104 monitored_session.py:240] Graph was finalized.; I0424 16:00:06.037272 139872277903104 saver.py:1284] Restoring parameters from /opt/models/wes/model.ckpt; I0424 16:00:10.817819 139872277903104 session_manager.py:500] Running local_init_op.; I0424 16:00:11.060626 139872277903104 session_manager.py:502] Done running local_init_op.; I0424 16:00:12.403780 139872277903104 modeling.py:413] Reloading EMA...; I0424 16:00:12.405867 139872277903104 saver.py:1284] Restoring parameters from /opt/models/wes/model.ckpt; I0424 16:00:48.634510 139872277903104 call_variants.py:402] Processed 1 examples in 1 batches [5816.472 sec per 100]. real	4m2.970s; user	5m54.674s; sy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:641,Performance,optimiz,optimized,641,"Not sure what is causing the issue but upon reaching this step DeepVariant failed. Any thoughts on how to fix? I tired to run it in a python2.7 environment and still it somehow is pulling from python 3.6 it seems. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp9_28zx5u/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp9_28zx5u/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". I0424 15:59:50.266534 139872277903104 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-04-24 15:59:50.321136: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-04-24 15:59:50.376605: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz; 2020-04-24 15:59:50.378224: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56a1fd0 executing computations on platform Host. Devices:; 2020-04-24 15:59:50.378283: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-04-24 15:59:50.380979: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I0424 15:59:50.447775 139872277903104 modeling.py:563] Initializing model with random parameters; W0424 15:59:50.449538 139872277903104 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp3bl4tsmc; I0424 15:59:50.450443 139872277903104 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp3bl4tsmc', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_cou",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:714,Performance,perform,performance,714,"Not sure what is causing the issue but upon reaching this step DeepVariant failed. Any thoughts on how to fix? I tired to run it in a python2.7 environment and still it somehow is pulling from python 3.6 it seems. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp9_28zx5u/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp9_28zx5u/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". I0424 15:59:50.266534 139872277903104 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-04-24 15:59:50.321136: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-04-24 15:59:50.376605: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz; 2020-04-24 15:59:50.378224: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56a1fd0 executing computations on platform Host. Devices:; 2020-04-24 15:59:50.378283: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-04-24 15:59:50.380979: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I0424 15:59:50.447775 139872277903104 modeling.py:563] Initializing model with random parameters; W0424 15:59:50.449538 139872277903104 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp3bl4tsmc; I0424 15:59:50.450443 139872277903104 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp3bl4tsmc', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_cou",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:1392,Performance,Tune,Tune,1392,"opt/models/wes/model.ckpt"". I0424 15:59:50.266534 139872277903104 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-04-24 15:59:50.321136: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-04-24 15:59:50.376605: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz; 2020-04-24 15:59:50.378224: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56a1fd0 executing computations on platform Host. Devices:; 2020-04-24 15:59:50.378283: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-04-24 15:59:50.380979: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I0424 15:59:50.447775 139872277903104 modeling.py:563] Initializing model with random parameters; W0424 15:59:50.449538 139872277903104 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp3bl4tsmc; I0424 15:59:50.450443 139872277903104 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp3bl4tsmc', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3659263518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '',",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:1441,Performance,perform,performance,1441,"opt/models/wes/model.ckpt"". I0424 15:59:50.266534 139872277903104 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-04-24 15:59:50.321136: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-04-24 15:59:50.376605: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz; 2020-04-24 15:59:50.378224: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56a1fd0 executing computations on platform Host. Devices:; 2020-04-24 15:59:50.378283: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-04-24 15:59:50.380979: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I0424 15:59:50.447775 139872277903104 modeling.py:563] Initializing model with random parameters; W0424 15:59:50.449538 139872277903104 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp3bl4tsmc; I0424 15:59:50.450443 139872277903104 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp3bl4tsmc', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3659263518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '',",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:4213,Performance,optimiz,optimizations,4213,"m_google_deepvariant/deepvariant/data_providers.py:374: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; I0424 15:59:50.681574 139872277903104 data_providers.py:376] self.input_map_threads=48; W0424 15:59:50.681832 139872277903104 deprecation.py:323] From /tmp/Bazel.runfiles_sszxydhb/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0424 15:59:51.794167 139872277903104 estimator.py:1147] Calling model_fn.; W0424 15:59:51.800228 139872277903104 deprecation.py:323] From /tmp/Bazel.runfiles_sszxydhb/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0424 15:59:51.806498 139872277903104 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0424 16:00:02.682547 139872277903104 estimator.py:1149] Done calling model_fn.; I0424 16:00:06.021238 139872277903104 monitored_session.py:240] Graph was finalized.; I0424 16:00:06.037272 139872277903104 saver.py:1284] Restoring para",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/305:332,Availability,error,error,332,"Hi, I am trying to call some targeted mutations from multiple targeted regions and chromosomes by Deepvariant. I followed the manual of make_examples.py to separate the multiple regions by space(https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/make_examples.py#L116), but it showed an error like the following:; sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/Homo_sapiens_assembly19.fasta --reads=/input/proper.bam ; --regions=""3:178936057-178936106 3:178952054-178952106"" ; --output_vcf=/output/outputch3.vcf.gz --output_gvcf=/output/outputchr3.g.vcf.gz ; --num_shards=1 --make_examples_extra_args vsc_min_fraction_snps=0.004; E0416 22:33:24.439428 140413429749504 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xs3AuO/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"".; I wonder whether this issue is due to the chromosome index of my sample is 1-22 without chr and how to run it with multiple regions and multiple chromosomes. Another issue is that when I changed the region to the bed file which includes all targeted regions, the variants called are all false-positive and none true variants included. . Furthermore, my sample is from deep sequencing on targeted regions. I have followed issue #62 to change the pileup_image_hight and downsample_fraction options, but the results are not improved. Thanks a lot!; Best regards,; Weiwei",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:839,Availability,error,errors,839,"Hi, I am trying to call some targeted mutations from multiple targeted regions and chromosomes by Deepvariant. I followed the manual of make_examples.py to separate the multiple regions by space(https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/make_examples.py#L116), but it showed an error like the following:; sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/Homo_sapiens_assembly19.fasta --reads=/input/proper.bam ; --regions=""3:178936057-178936106 3:178952054-178952106"" ; --output_vcf=/output/outputch3.vcf.gz --output_gvcf=/output/outputchr3.g.vcf.gz ; --num_shards=1 --make_examples_extra_args vsc_min_fraction_snps=0.004; E0416 22:33:24.439428 140413429749504 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xs3AuO/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"".; I wonder whether this issue is due to the chromosome index of my sample is 1-22 without chr and how to run it with multiple regions and multiple chromosomes. Another issue is that when I changed the region to the bed file which includes all targeted regions, the variants called are all false-positive and none true variants included. . Furthermore, my sample is from deep sequencing on targeted regions. I have followed issue #62 to change the pileup_image_hight and downsample_fraction options, but the results are not improved. Thanks a lot!; Best regards,; Weiwei",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:874,Availability,failure,failure,874,"Hi, I am trying to call some targeted mutations from multiple targeted regions and chromosomes by Deepvariant. I followed the manual of make_examples.py to separate the multiple regions by space(https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/make_examples.py#L116), but it showed an error like the following:; sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/Homo_sapiens_assembly19.fasta --reads=/input/proper.bam ; --regions=""3:178936057-178936106 3:178952054-178952106"" ; --output_vcf=/output/outputch3.vcf.gz --output_gvcf=/output/outputchr3.g.vcf.gz ; --num_shards=1 --make_examples_extra_args vsc_min_fraction_snps=0.004; E0416 22:33:24.439428 140413429749504 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xs3AuO/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"".; I wonder whether this issue is due to the chromosome index of my sample is 1-22 without chr and how to run it with multiple regions and multiple chromosomes. Another issue is that when I changed the region to the bed file which includes all targeted regions, the variants called are all false-positive and none true variants included. . Furthermore, my sample is from deep sequencing on targeted regions. I have followed issue #62 to change the pileup_image_hight and downsample_fraction options, but the results are not improved. Thanks a lot!; Best regards,; Weiwei",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/306:244,Deployability,continuous,continuous,244,"Hi, I'm trying to visualize the pileup images generated by DeepVariant. The images for SNP sites and deletions seem to be straightforward, but I found those for insertions are rather confusing. The reference lines for insertion sites are still continuous, and at the point where the insertion happens, the bases on the sequenced reads are set to 0. Here's part of an example of a homozygous ""A->AATAAAAT"" variant, the top 5 lines are the reference lines. 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250. The problem is these images are not presenting detailed infomation for the inserted sequence, and on sites where multiple insertions happen, the ""supports variant"" channel might become the only useful infomation to distinguish them.; Also, on the ""base quality"" channel, the qualities for these 0-bases are not zeros, how are these values determined?. I'm wondering if other structures of pileup images on these sites can achieve better performance, like adding 0s on the reference lines?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/306
https://github.com/google/deepvariant/issues/306:1244,Performance,perform,performance,1244,"Hi, I'm trying to visualize the pileup images generated by DeepVariant. The images for SNP sites and deletions seem to be straightforward, but I found those for insertions are rather confusing. The reference lines for insertion sites are still continuous, and at the point where the insertion happens, the bases on the sequenced reads are set to 0. Here's part of an example of a homozygous ""A->AATAAAAT"" variant, the top 5 lines are the reference lines. 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 250 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250; 250 | 30 | 180 | 0 | 250 | 100 | 250. The problem is these images are not presenting detailed infomation for the inserted sequence, and on sites where multiple insertions happen, the ""supports variant"" channel might become the only useful infomation to distinguish them.; Also, on the ""base quality"" channel, the qualities for these 0-bases are not zeros, how are these values determined?. I'm wondering if other structures of pileup images on these sites can achieve better performance, like adding 0s on the reference lines?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/306
https://github.com/google/deepvariant/issues/307:341,Availability,error,error,341,"I'm currently attempting to run Deepvariant 0.10.0 utilizing a cram file with a reference file. Everything else appears to be properly coded but it seems to fail upon reading the fa file. After failing it tries to fallback to an fai file that doesn't exist. I've tried it both compressed and uncompressed and it still seems to give the same error. Any ideas on what I should do here? . For reference Here is the command i'm running from Docker; sudo docker run \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --num_shards=$(nproc); ; and here is the error i'm seeing right before crashing:; I0509 06:55:24.592304 140264323651328 genomics_reader.py:223] Reading /input/2009617.cram with NativeSamReader; I0509 06:55:24.597789 140264323651328 make_examples.py:535] Task 2/16: Preparing inputs; [E::fai_load3_core] Failed to open FASTA index /input/hg19.fa.gz.fai: No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1358, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1264, in processing_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:812,Availability,error,error,812,"I'm currently attempting to run Deepvariant 0.10.0 utilizing a cram file with a reference file. Everything else appears to be properly coded but it seems to fail upon reading the fa file. After failing it tries to fallback to an fai file that doesn't exist. I've tried it both compressed and uncompressed and it still seems to give the same error. Any ideas on what I should do here? . For reference Here is the command i'm running from Docker; sudo docker run \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --num_shards=$(nproc); ; and here is the error i'm seeing right before crashing:; I0509 06:55:24.592304 140264323651328 genomics_reader.py:223] Reading /input/2009617.cram with NativeSamReader; I0509 06:55:24.597789 140264323651328 make_examples.py:535] Task 2/16: Preparing inputs; [E::fai_load3_core] Failed to open FASTA index /input/hg19.fa.gz.fai: No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1358, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1264, in processing_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:2256,Performance,load,load,2256,"com_google_deepvariant/deepvariant/make_examples.py"", line 1510, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1358, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1264, in processing_regions_from_options; options.reference_filename).header.contigs; File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__; fasta_path, fai_path, options); ValueError: Not found: could not load fasta and/or fai for fasta /input/hg19.fa.gz; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/hg19.fa.gz --reads /input/2009617.cram --examples /tmp/tmpr2hdz82_/make_examples.tfrecord@16.gz --gvcf /tmp/tmpr2hdz82_/gvcf.tfrecord; @16.gz --task 1; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/hg19.fa.gz --reads /input/2009617.cram --examples /tmp/tmpr2hdz82_/make_examples.tfrecord@16.gz --gvcf /tmp/tmpr2hdz82_/gvcf.tfrecord; @16.gz --task 4; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/hg19.fa.gz --reads /input/2009617.cram --examples /tmp/tmpr2hdz82_/make_examples.tfrecord@16.gz --gvcf /tmp/tmpr2hdz82_/gvcf.tfrecord; @16.gz --task 7; real 0m5.299s; user 0m12.412s; sys 0m3.014s; I0509 06:55:25.059437 140033813915392 run_deepvariant.py:321] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/308:613,Energy Efficiency,adapt,adapted,613,"Hi all,; I have read the discussion about deep sequencing on issue #62. I have tried to modify three options, downsample_fraction, pileup_image_height, and vsc_min_fraction_snps for our deep sequencing data but it didn't work and output many false-positive calls. Here I want to train a new model for deep sequencing data with rare somatic mutation(MAF~1%) and there is some confusion.; 1) There is a maximum threshold for pileup_height of 362, can I modify it?; 2) There is only one training tutorial with Google cloud platform, is there any guideline for training with the Linux system?; 3) Can Deepvariant be adapted to a somatic mutation caller?; Thanks a lot!; Best regards,; Weiwei",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:613,Modifiability,adapt,adapted,613,"Hi all,; I have read the discussion about deep sequencing on issue #62. I have tried to modify three options, downsample_fraction, pileup_image_height, and vsc_min_fraction_snps for our deep sequencing data but it didn't work and output many false-positive calls. Here I want to train a new model for deep sequencing data with rare somatic mutation(MAF~1%) and there is some confusion.; 1) There is a maximum threshold for pileup_height of 362, can I modify it?; 2) There is only one training tutorial with Google cloud platform, is there any guideline for training with the Linux system?; 3) Can Deepvariant be adapted to a somatic mutation caller?; Thanks a lot!; Best regards,; Weiwei",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:544,Usability,guid,guideline,544,"Hi all,; I have read the discussion about deep sequencing on issue #62. I have tried to modify three options, downsample_fraction, pileup_image_height, and vsc_min_fraction_snps for our deep sequencing data but it didn't work and output many false-positive calls. Here I want to train a new model for deep sequencing data with rare somatic mutation(MAF~1%) and there is some confusion.; 1) There is a maximum threshold for pileup_height of 362, can I modify it?; 2) There is only one training tutorial with Google cloud platform, is there any guideline for training with the Linux system?; 3) Can Deepvariant be adapted to a somatic mutation caller?; Thanks a lot!; Best regards,; Weiwei",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/309:98,Availability,error,errors,98,"Hi. I am trying to apply variant calling on CCS reads of 16 samples. I am getting the following 2 errors:. 1. `[E::hts_open_format] Failed to open file /scratch/041-Melon-Reseq/Pacbio_targeted/alignedBams/ARUM_R--ARUM_R_vsMelonv4.MD.bam`; 2. `/var/log/slurm/log_slurmd//job2563083/slurm_script: lnea 45: --num_shards=4: command not found; `. Below I present the defined variables and the command. `INPUT_DIR=""/scratch/041-Melon-Reseq/Pacbio_targeted/alignedBams""`; `OUTPUT_DIR=""/scratch/041-Melon-Reseq/Pacbio_targeted/DeepVariant""`; `NCPUS=4`; `BIN_VERSION=""0.10.0""`. singularity run; -B /usr/lib/locale/:/usr/lib/locale/ \; 	-B /home/kalexiou/PacBio/ \; 	docker://google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type=PACBIO \; 	--ref=/home/kalexiou/PacBio/Melon_v4.0_PacBio.fasta \; 	--reads=""${INPUT_DIR}""/${base} \; 	--regions ""chr05:24760000-27020000"" \; 	--output_vcf=""${OUTPUT_DIR}""/${name}.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/${name}.g.vcf.gz \ ; 	--num_shards=""${NCPUS}"". Any help will be appreciated!. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/309
https://github.com/google/deepvariant/issues/309:371,Modifiability,variab,variables,371,"Hi. I am trying to apply variant calling on CCS reads of 16 samples. I am getting the following 2 errors:. 1. `[E::hts_open_format] Failed to open file /scratch/041-Melon-Reseq/Pacbio_targeted/alignedBams/ARUM_R--ARUM_R_vsMelonv4.MD.bam`; 2. `/var/log/slurm/log_slurmd//job2563083/slurm_script: lnea 45: --num_shards=4: command not found; `. Below I present the defined variables and the command. `INPUT_DIR=""/scratch/041-Melon-Reseq/Pacbio_targeted/alignedBams""`; `OUTPUT_DIR=""/scratch/041-Melon-Reseq/Pacbio_targeted/DeepVariant""`; `NCPUS=4`; `BIN_VERSION=""0.10.0""`. singularity run; -B /usr/lib/locale/:/usr/lib/locale/ \; 	-B /home/kalexiou/PacBio/ \; 	docker://google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type=PACBIO \; 	--ref=/home/kalexiou/PacBio/Melon_v4.0_PacBio.fasta \; 	--reads=""${INPUT_DIR}""/${base} \; 	--regions ""chr05:24760000-27020000"" \; 	--output_vcf=""${OUTPUT_DIR}""/${name}.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/${name}.g.vcf.gz \ ; 	--num_shards=""${NCPUS}"". Any help will be appreciated!. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/309
https://github.com/google/deepvariant/issues/309:248,Testability,log,log,248,"Hi. I am trying to apply variant calling on CCS reads of 16 samples. I am getting the following 2 errors:. 1. `[E::hts_open_format] Failed to open file /scratch/041-Melon-Reseq/Pacbio_targeted/alignedBams/ARUM_R--ARUM_R_vsMelonv4.MD.bam`; 2. `/var/log/slurm/log_slurmd//job2563083/slurm_script: lnea 45: --num_shards=4: command not found; `. Below I present the defined variables and the command. `INPUT_DIR=""/scratch/041-Melon-Reseq/Pacbio_targeted/alignedBams""`; `OUTPUT_DIR=""/scratch/041-Melon-Reseq/Pacbio_targeted/DeepVariant""`; `NCPUS=4`; `BIN_VERSION=""0.10.0""`. singularity run; -B /usr/lib/locale/:/usr/lib/locale/ \; 	-B /home/kalexiou/PacBio/ \; 	docker://google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type=PACBIO \; 	--ref=/home/kalexiou/PacBio/Melon_v4.0_PacBio.fasta \; 	--reads=""${INPUT_DIR}""/${base} \; 	--regions ""chr05:24760000-27020000"" \; 	--output_vcf=""${OUTPUT_DIR}""/${name}.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/${name}.g.vcf.gz \ ; 	--num_shards=""${NCPUS}"". Any help will be appreciated!. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/309
https://github.com/google/deepvariant/issues/310:17,Availability,error,error,17,"Hello,. I get an error about a missing index file for the used fasta. But it is available in the specified folder. BIN_VERSION=""0.10.0"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/hg19.fasta --reads=/input/input.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=15. I0529 12:24:27.041167 139828907894528 make_examples.py:535] Task 5/15: Preparing inputs; [E::fai_load3_core] Failed to open FASTA index /input/hg19.fasta.fai: No such file or directory. Do you have any idea why this happens?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:80,Availability,avail,available,80,"Hello,. I get an error about a missing index file for the used fasta. But it is available in the specified folder. BIN_VERSION=""0.10.0"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/hg19.fasta --reads=/input/input.bam --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=15. I0529 12:24:27.041167 139828907894528 make_examples.py:535] Task 5/15: Preparing inputs; [E::fai_load3_core] Failed to open FASTA index /input/hg19.fasta.fai: No such file or directory. Do you have any idea why this happens?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/311:83,Deployability,pipeline,pipeline,83,"I recently took four cram files (one family) and put them through the deep variant pipeline in accordance with the documentation. I then followed the instructions for ""Best practices for multi-sample variant calling with DeepVariant (WES trio demonstration)"" and tried two sets of of ""trios"" and one comparison containing all four. Each of them however came up with a substanial amount of mendelian constraints which I'll list below:. Checking: /home/username/deepvariant-run/output/RBAs.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2115432]; 12 non-pass records were skipped; Concordance 2115432: F:127216/127862 (99.49%) M:121292/121882 (99.52%) F+M:79650/80917 (98.43%); Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 4249/338863 (1.25%) records did not conform to expected call ploidy; 228759/338863 (67.51%) records were variant in at least 1 family member and checked for Mendelian constraints; 147067/228759 (64.29%) records had indeterminate consistency status due to incomplete calls; 1838/228759 (0.80%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNs.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2009617]; 18 non-pass records were skipped; Concordance 2009617: F:124581/125097 (99.59%) M:120027/120545 (99.57%) F+M:79289/80523 (98.47%); Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 3705/295693 (1.25%) records did not conform to expected call ploidy; 184648/295693 (62.45%) records were variant in at least 1 family member and checked for Mendelian constraints; 103541/184648 (56.07%) records had indeterminate consistency status due to incomplete calls; 1603/184648 (0.87%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNAs.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2009617, 2115432]; 24 non-pass records were ski",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/312:739,Availability,down,downsampling,739,"Hello, after some quite impressive results applying HiSeq-trained DeepVariant on MGISEQ-2000 data, I've been working on achieving even better performance by retraining DeepVariant specifically for the MGISEQ-2000. To do this I've been broadly following the sketch at https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md. I now have unshuffled tfrecords for six reference samples, and have some questions about next steps. Because it might be relevant to the questions, my data are structured as follows:; * 6 samples (4xHG001/NA12878, 2xHG005/NA24631), each with:; * 25 tfrecord shards (00000-00024) of chr1, for tuning (perhaps over-optimistically) ; * 247 tfrecord shards (00000-00246) of chr2-19, no downsampling, for training; * 247 tfrecord shards (00000-00246) of chr2-19, 50% downsampling, for training; * 17 tfrecord shards (00000-00016) of chr20-22, for validation. My questions are:; 1. Is it necessary to shuffle the training data? I ask as it's proving to be a bit laborious to set up, and so I'm hoping that I can get around it. Given I have so many shards, if I just shuffle the order of the chr2-19 shards when I supply them to the training loop, will this be almost as good as shuffling the whole dataset?; 2. Is it necessary to shuffle the validation data? The tutorial does this, but I'm not sure why.; 3. How can I supply multiple datasets to the training loop (here effectively 12 datasets: 6 samples x 2 downsampling settings)? In the tutorial, `model_train` is supplied a wildcard path of `validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz`, which seems like it would only work for a single sample, and I'm not sure how this will work with multiple samples.; 4. Have there been any changes to the code base to better support warmstarting, or is the advice at https://github.com/google/deepvariant/issues/185 still the best approach to fine-tuning the model?. DeepVariant is a fantastic tool and I'm very much looking forward to see",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:819,Availability,down,downsampling,819,"Hello, after some quite impressive results applying HiSeq-trained DeepVariant on MGISEQ-2000 data, I've been working on achieving even better performance by retraining DeepVariant specifically for the MGISEQ-2000. To do this I've been broadly following the sketch at https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md. I now have unshuffled tfrecords for six reference samples, and have some questions about next steps. Because it might be relevant to the questions, my data are structured as follows:; * 6 samples (4xHG001/NA12878, 2xHG005/NA24631), each with:; * 25 tfrecord shards (00000-00024) of chr1, for tuning (perhaps over-optimistically) ; * 247 tfrecord shards (00000-00246) of chr2-19, no downsampling, for training; * 247 tfrecord shards (00000-00246) of chr2-19, 50% downsampling, for training; * 17 tfrecord shards (00000-00016) of chr20-22, for validation. My questions are:; 1. Is it necessary to shuffle the training data? I ask as it's proving to be a bit laborious to set up, and so I'm hoping that I can get around it. Given I have so many shards, if I just shuffle the order of the chr2-19 shards when I supply them to the training loop, will this be almost as good as shuffling the whole dataset?; 2. Is it necessary to shuffle the validation data? The tutorial does this, but I'm not sure why.; 3. How can I supply multiple datasets to the training loop (here effectively 12 datasets: 6 samples x 2 downsampling settings)? In the tutorial, `model_train` is supplied a wildcard path of `validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz`, which seems like it would only work for a single sample, and I'm not sure how this will work with multiple samples.; 4. Have there been any changes to the code base to better support warmstarting, or is the advice at https://github.com/google/deepvariant/issues/185 still the best approach to fine-tuning the model?. DeepVariant is a fantastic tool and I'm very much looking forward to see",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1461,Availability,down,downsampling,1461,"ed DeepVariant on MGISEQ-2000 data, I've been working on achieving even better performance by retraining DeepVariant specifically for the MGISEQ-2000. To do this I've been broadly following the sketch at https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md. I now have unshuffled tfrecords for six reference samples, and have some questions about next steps. Because it might be relevant to the questions, my data are structured as follows:; * 6 samples (4xHG001/NA12878, 2xHG005/NA24631), each with:; * 25 tfrecord shards (00000-00024) of chr1, for tuning (perhaps over-optimistically) ; * 247 tfrecord shards (00000-00246) of chr2-19, no downsampling, for training; * 247 tfrecord shards (00000-00246) of chr2-19, 50% downsampling, for training; * 17 tfrecord shards (00000-00016) of chr20-22, for validation. My questions are:; 1. Is it necessary to shuffle the training data? I ask as it's proving to be a bit laborious to set up, and so I'm hoping that I can get around it. Given I have so many shards, if I just shuffle the order of the chr2-19 shards when I supply them to the training loop, will this be almost as good as shuffling the whole dataset?; 2. Is it necessary to shuffle the validation data? The tutorial does this, but I'm not sure why.; 3. How can I supply multiple datasets to the training loop (here effectively 12 datasets: 6 samples x 2 downsampling settings)? In the tutorial, `model_train` is supplied a wildcard path of `validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz`, which seems like it would only work for a single sample, and I'm not sure how this will work with multiple samples.; 4. Have there been any changes to the code base to better support warmstarting, or is the advice at https://github.com/google/deepvariant/issues/185 still the best approach to fine-tuning the model?. DeepVariant is a fantastic tool and I'm very much looking forward to seeing what it can do with these data, so many thanks in advance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:142,Performance,perform,performance,142,"Hello, after some quite impressive results applying HiSeq-trained DeepVariant on MGISEQ-2000 data, I've been working on achieving even better performance by retraining DeepVariant specifically for the MGISEQ-2000. To do this I've been broadly following the sketch at https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md. I now have unshuffled tfrecords for six reference samples, and have some questions about next steps. Because it might be relevant to the questions, my data are structured as follows:; * 6 samples (4xHG001/NA12878, 2xHG005/NA24631), each with:; * 25 tfrecord shards (00000-00024) of chr1, for tuning (perhaps over-optimistically) ; * 247 tfrecord shards (00000-00246) of chr2-19, no downsampling, for training; * 247 tfrecord shards (00000-00246) of chr2-19, 50% downsampling, for training; * 17 tfrecord shards (00000-00016) of chr20-22, for validation. My questions are:; 1. Is it necessary to shuffle the training data? I ask as it's proving to be a bit laborious to set up, and so I'm hoping that I can get around it. Given I have so many shards, if I just shuffle the order of the chr2-19 shards when I supply them to the training loop, will this be almost as good as shuffling the whole dataset?; 2. Is it necessary to shuffle the validation data? The tutorial does this, but I'm not sure why.; 3. How can I supply multiple datasets to the training loop (here effectively 12 datasets: 6 samples x 2 downsampling settings)? In the tutorial, `model_train` is supplied a wildcard path of `validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz`, which seems like it would only work for a single sample, and I'm not sure how this will work with multiple samples.; 4. Have there been any changes to the code base to better support warmstarting, or is the advice at https://github.com/google/deepvariant/issues/185 still the best approach to fine-tuning the model?. DeepVariant is a fantastic tool and I'm very much looking forward to see",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:899,Security,validat,validation,899,"Hello, after some quite impressive results applying HiSeq-trained DeepVariant on MGISEQ-2000 data, I've been working on achieving even better performance by retraining DeepVariant specifically for the MGISEQ-2000. To do this I've been broadly following the sketch at https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md. I now have unshuffled tfrecords for six reference samples, and have some questions about next steps. Because it might be relevant to the questions, my data are structured as follows:; * 6 samples (4xHG001/NA12878, 2xHG005/NA24631), each with:; * 25 tfrecord shards (00000-00024) of chr1, for tuning (perhaps over-optimistically) ; * 247 tfrecord shards (00000-00246) of chr2-19, no downsampling, for training; * 247 tfrecord shards (00000-00246) of chr2-19, 50% downsampling, for training; * 17 tfrecord shards (00000-00016) of chr20-22, for validation. My questions are:; 1. Is it necessary to shuffle the training data? I ask as it's proving to be a bit laborious to set up, and so I'm hoping that I can get around it. Given I have so many shards, if I just shuffle the order of the chr2-19 shards when I supply them to the training loop, will this be almost as good as shuffling the whole dataset?; 2. Is it necessary to shuffle the validation data? The tutorial does this, but I'm not sure why.; 3. How can I supply multiple datasets to the training loop (here effectively 12 datasets: 6 samples x 2 downsampling settings)? In the tutorial, `model_train` is supplied a wildcard path of `validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz`, which seems like it would only work for a single sample, and I'm not sure how this will work with multiple samples.; 4. Have there been any changes to the code base to better support warmstarting, or is the advice at https://github.com/google/deepvariant/issues/185 still the best approach to fine-tuning the model?. DeepVariant is a fantastic tool and I'm very much looking forward to see",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1293,Security,validat,validation,1293,"ed DeepVariant on MGISEQ-2000 data, I've been working on achieving even better performance by retraining DeepVariant specifically for the MGISEQ-2000. To do this I've been broadly following the sketch at https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md. I now have unshuffled tfrecords for six reference samples, and have some questions about next steps. Because it might be relevant to the questions, my data are structured as follows:; * 6 samples (4xHG001/NA12878, 2xHG005/NA24631), each with:; * 25 tfrecord shards (00000-00024) of chr1, for tuning (perhaps over-optimistically) ; * 247 tfrecord shards (00000-00246) of chr2-19, no downsampling, for training; * 247 tfrecord shards (00000-00246) of chr2-19, 50% downsampling, for training; * 17 tfrecord shards (00000-00016) of chr20-22, for validation. My questions are:; 1. Is it necessary to shuffle the training data? I ask as it's proving to be a bit laborious to set up, and so I'm hoping that I can get around it. Given I have so many shards, if I just shuffle the order of the chr2-19 shards when I supply them to the training loop, will this be almost as good as shuffling the whole dataset?; 2. Is it necessary to shuffle the validation data? The tutorial does this, but I'm not sure why.; 3. How can I supply multiple datasets to the training loop (here effectively 12 datasets: 6 samples x 2 downsampling settings)? In the tutorial, `model_train` is supplied a wildcard path of `validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz`, which seems like it would only work for a single sample, and I'm not sure how this will work with multiple samples.; 4. Have there been any changes to the code base to better support warmstarting, or is the advice at https://github.com/google/deepvariant/issues/185 still the best approach to fine-tuning the model?. DeepVariant is a fantastic tool and I'm very much looking forward to seeing what it can do with these data, so many thanks in advance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/313:163,Availability,fault,fault,163,"Dear deepvariant developers,. I'm using deepvariant (0.10.0) to call variants in HG002 human genome using Pacbio hifi sequencing data. However, I am getting a seg-fault error in the process. . ```; REF=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; BAM=output.primary.bam; MODEL=""/opt/models/pacbio/model.ckpt""; N_SHARDS=24; CALL_VARIANTS_OUTPUT=""output/call_variants_output.tfrecord.gz"". mkdir input output logs. /usr/bin/time seq 0 $((N_SHARDS-1)) \; | parallel -P ${SLURM_CPUS_PER_TASK} --eta --halt 2 \; --joblog ""logs/log"" --res ""logs"" \; make_examples --mode calling \; --ref ""${REF}"" \; --reads ""${wd}/${BAM}"" \; --examples output/examples.tfrecord@${N_SHARDS}.gz\; --task {} \; || exit 1; ```. Here is the [output log file](https://github.com/google/deepvariant/files/4714025/slurm-58983130.log). Please let me know if I can share more details.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:169,Availability,error,error,169,"Dear deepvariant developers,. I'm using deepvariant (0.10.0) to call variants in HG002 human genome using Pacbio hifi sequencing data. However, I am getting a seg-fault error in the process. . ```; REF=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; BAM=output.primary.bam; MODEL=""/opt/models/pacbio/model.ckpt""; N_SHARDS=24; CALL_VARIANTS_OUTPUT=""output/call_variants_output.tfrecord.gz"". mkdir input output logs. /usr/bin/time seq 0 $((N_SHARDS-1)) \; | parallel -P ${SLURM_CPUS_PER_TASK} --eta --halt 2 \; --joblog ""logs/log"" --res ""logs"" \; make_examples --mode calling \; --ref ""${REF}"" \; --reads ""${wd}/${BAM}"" \; --examples output/examples.tfrecord@${N_SHARDS}.gz\; --task {} \; || exit 1; ```. Here is the [output log file](https://github.com/google/deepvariant/files/4714025/slurm-58983130.log). Please let me know if I can share more details.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:410,Testability,log,logs,410,"Dear deepvariant developers,. I'm using deepvariant (0.10.0) to call variants in HG002 human genome using Pacbio hifi sequencing data. However, I am getting a seg-fault error in the process. . ```; REF=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; BAM=output.primary.bam; MODEL=""/opt/models/pacbio/model.ckpt""; N_SHARDS=24; CALL_VARIANTS_OUTPUT=""output/call_variants_output.tfrecord.gz"". mkdir input output logs. /usr/bin/time seq 0 $((N_SHARDS-1)) \; | parallel -P ${SLURM_CPUS_PER_TASK} --eta --halt 2 \; --joblog ""logs/log"" --res ""logs"" \; make_examples --mode calling \; --ref ""${REF}"" \; --reads ""${wd}/${BAM}"" \; --examples output/examples.tfrecord@${N_SHARDS}.gz\; --task {} \; || exit 1; ```. Here is the [output log file](https://github.com/google/deepvariant/files/4714025/slurm-58983130.log). Please let me know if I can share more details.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:520,Testability,log,logs,520,"Dear deepvariant developers,. I'm using deepvariant (0.10.0) to call variants in HG002 human genome using Pacbio hifi sequencing data. However, I am getting a seg-fault error in the process. . ```; REF=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; BAM=output.primary.bam; MODEL=""/opt/models/pacbio/model.ckpt""; N_SHARDS=24; CALL_VARIANTS_OUTPUT=""output/call_variants_output.tfrecord.gz"". mkdir input output logs. /usr/bin/time seq 0 $((N_SHARDS-1)) \; | parallel -P ${SLURM_CPUS_PER_TASK} --eta --halt 2 \; --joblog ""logs/log"" --res ""logs"" \; make_examples --mode calling \; --ref ""${REF}"" \; --reads ""${wd}/${BAM}"" \; --examples output/examples.tfrecord@${N_SHARDS}.gz\; --task {} \; || exit 1; ```. Here is the [output log file](https://github.com/google/deepvariant/files/4714025/slurm-58983130.log). Please let me know if I can share more details.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:525,Testability,log,log,525,"Dear deepvariant developers,. I'm using deepvariant (0.10.0) to call variants in HG002 human genome using Pacbio hifi sequencing data. However, I am getting a seg-fault error in the process. . ```; REF=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; BAM=output.primary.bam; MODEL=""/opt/models/pacbio/model.ckpt""; N_SHARDS=24; CALL_VARIANTS_OUTPUT=""output/call_variants_output.tfrecord.gz"". mkdir input output logs. /usr/bin/time seq 0 $((N_SHARDS-1)) \; | parallel -P ${SLURM_CPUS_PER_TASK} --eta --halt 2 \; --joblog ""logs/log"" --res ""logs"" \; make_examples --mode calling \; --ref ""${REF}"" \; --reads ""${wd}/${BAM}"" \; --examples output/examples.tfrecord@${N_SHARDS}.gz\; --task {} \; || exit 1; ```. Here is the [output log file](https://github.com/google/deepvariant/files/4714025/slurm-58983130.log). Please let me know if I can share more details.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:537,Testability,log,logs,537,"Dear deepvariant developers,. I'm using deepvariant (0.10.0) to call variants in HG002 human genome using Pacbio hifi sequencing data. However, I am getting a seg-fault error in the process. . ```; REF=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; BAM=output.primary.bam; MODEL=""/opt/models/pacbio/model.ckpt""; N_SHARDS=24; CALL_VARIANTS_OUTPUT=""output/call_variants_output.tfrecord.gz"". mkdir input output logs. /usr/bin/time seq 0 $((N_SHARDS-1)) \; | parallel -P ${SLURM_CPUS_PER_TASK} --eta --halt 2 \; --joblog ""logs/log"" --res ""logs"" \; make_examples --mode calling \; --ref ""${REF}"" \; --reads ""${wd}/${BAM}"" \; --examples output/examples.tfrecord@${N_SHARDS}.gz\; --task {} \; || exit 1; ```. Here is the [output log file](https://github.com/google/deepvariant/files/4714025/slurm-58983130.log). Please let me know if I can share more details.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:724,Testability,log,log,724,"Dear deepvariant developers,. I'm using deepvariant (0.10.0) to call variants in HG002 human genome using Pacbio hifi sequencing data. However, I am getting a seg-fault error in the process. . ```; REF=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; BAM=output.primary.bam; MODEL=""/opt/models/pacbio/model.ckpt""; N_SHARDS=24; CALL_VARIANTS_OUTPUT=""output/call_variants_output.tfrecord.gz"". mkdir input output logs. /usr/bin/time seq 0 $((N_SHARDS-1)) \; | parallel -P ${SLURM_CPUS_PER_TASK} --eta --halt 2 \; --joblog ""logs/log"" --res ""logs"" \; make_examples --mode calling \; --ref ""${REF}"" \; --reads ""${wd}/${BAM}"" \; --examples output/examples.tfrecord@${N_SHARDS}.gz\; --task {} \; || exit 1; ```. Here is the [output log file](https://github.com/google/deepvariant/files/4714025/slurm-58983130.log). Please let me know if I can share more details.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:801,Testability,log,log,801,"Dear deepvariant developers,. I'm using deepvariant (0.10.0) to call variants in HG002 human genome using Pacbio hifi sequencing data. However, I am getting a seg-fault error in the process. . ```; REF=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; BAM=output.primary.bam; MODEL=""/opt/models/pacbio/model.ckpt""; N_SHARDS=24; CALL_VARIANTS_OUTPUT=""output/call_variants_output.tfrecord.gz"". mkdir input output logs. /usr/bin/time seq 0 $((N_SHARDS-1)) \; | parallel -P ${SLURM_CPUS_PER_TASK} --eta --halt 2 \; --joblog ""logs/log"" --res ""logs"" \; make_examples --mode calling \; --ref ""${REF}"" \; --reads ""${wd}/${BAM}"" \; --examples output/examples.tfrecord@${N_SHARDS}.gz\; --task {} \; || exit 1; ```. Here is the [output log file](https://github.com/google/deepvariant/files/4714025/slurm-58983130.log). Please let me know if I can share more details.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/314:229,Availability,error,error,229,"Hi, . I had trouble running Deepvariant using conda. I ran the following command.; ```; dv_make_examples.py --sample {MY_SAMPLE} --ref {MY_FASTA}.fasta --reads {MY_BAM}.bam --logdir ./log/ --examples examples/; ```. and I got an error like this:; ```; ETA: 0s Left: 1 AVG: 0.00s local:1/0/100%/0.0s sh: 1: unzip: not found; Traceback (most recent call last):; File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 252, in <module>; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 187, in Main; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 138, in GetRepositoriesImports; FileNotFoundError: [Errno 2] No such file or directory: '/tmp/Bazel.runfiles_qwsw52c7/runfiles'; parallel: This job failed:; /opt/conda/envs/deepvariant/bin/python /opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip --mode calling --ref E.coli_K12_MG1655.fa --reads SRR1770413.bam --examples examples//SRR1770413.tfrecord@1.gz --task 0; ```. When I installed unzip by `conda install`, the command worked fine. When using conda to install deepvariant, should it not have to be installed together?. Thanks,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:1499,Deployability,install,installed,1499,"Hi, . I had trouble running Deepvariant using conda. I ran the following command.; ```; dv_make_examples.py --sample {MY_SAMPLE} --ref {MY_FASTA}.fasta --reads {MY_BAM}.bam --logdir ./log/ --examples examples/; ```. and I got an error like this:; ```; ETA: 0s Left: 1 AVG: 0.00s local:1/0/100%/0.0s sh: 1: unzip: not found; Traceback (most recent call last):; File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 252, in <module>; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 187, in Main; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 138, in GetRepositoriesImports; FileNotFoundError: [Errno 2] No such file or directory: '/tmp/Bazel.runfiles_qwsw52c7/runfiles'; parallel: This job failed:; /opt/conda/envs/deepvariant/bin/python /opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip --mode calling --ref E.coli_K12_MG1655.fa --reads SRR1770413.bam --examples examples//SRR1770413.tfrecord@1.gz --task 0; ```. When I installed unzip by `conda install`, the command worked fine. When using conda to install deepvariant, should it not have to be installed together?. Thanks,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:1525,Deployability,install,install,1525,"Hi, . I had trouble running Deepvariant using conda. I ran the following command.; ```; dv_make_examples.py --sample {MY_SAMPLE} --ref {MY_FASTA}.fasta --reads {MY_BAM}.bam --logdir ./log/ --examples examples/; ```. and I got an error like this:; ```; ETA: 0s Left: 1 AVG: 0.00s local:1/0/100%/0.0s sh: 1: unzip: not found; Traceback (most recent call last):; File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 252, in <module>; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 187, in Main; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 138, in GetRepositoriesImports; FileNotFoundError: [Errno 2] No such file or directory: '/tmp/Bazel.runfiles_qwsw52c7/runfiles'; parallel: This job failed:; /opt/conda/envs/deepvariant/bin/python /opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip --mode calling --ref E.coli_K12_MG1655.fa --reads SRR1770413.bam --examples examples//SRR1770413.tfrecord@1.gz --task 0; ```. When I installed unzip by `conda install`, the command worked fine. When using conda to install deepvariant, should it not have to be installed together?. Thanks,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:1580,Deployability,install,install,1580,"Hi, . I had trouble running Deepvariant using conda. I ran the following command.; ```; dv_make_examples.py --sample {MY_SAMPLE} --ref {MY_FASTA}.fasta --reads {MY_BAM}.bam --logdir ./log/ --examples examples/; ```. and I got an error like this:; ```; ETA: 0s Left: 1 AVG: 0.00s local:1/0/100%/0.0s sh: 1: unzip: not found; Traceback (most recent call last):; File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 252, in <module>; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 187, in Main; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 138, in GetRepositoriesImports; FileNotFoundError: [Errno 2] No such file or directory: '/tmp/Bazel.runfiles_qwsw52c7/runfiles'; parallel: This job failed:; /opt/conda/envs/deepvariant/bin/python /opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip --mode calling --ref E.coli_K12_MG1655.fa --reads SRR1770413.bam --examples examples//SRR1770413.tfrecord@1.gz --task 0; ```. When I installed unzip by `conda install`, the command worked fine. When using conda to install deepvariant, should it not have to be installed together?. Thanks,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:1626,Deployability,install,installed,1626,"Hi, . I had trouble running Deepvariant using conda. I ran the following command.; ```; dv_make_examples.py --sample {MY_SAMPLE} --ref {MY_FASTA}.fasta --reads {MY_BAM}.bam --logdir ./log/ --examples examples/; ```. and I got an error like this:; ```; ETA: 0s Left: 1 AVG: 0.00s local:1/0/100%/0.0s sh: 1: unzip: not found; Traceback (most recent call last):; File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 252, in <module>; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 187, in Main; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 138, in GetRepositoriesImports; FileNotFoundError: [Errno 2] No such file or directory: '/tmp/Bazel.runfiles_qwsw52c7/runfiles'; parallel: This job failed:; /opt/conda/envs/deepvariant/bin/python /opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip --mode calling --ref E.coli_K12_MG1655.fa --reads SRR1770413.bam --examples examples//SRR1770413.tfrecord@1.gz --task 0; ```. When I installed unzip by `conda install`, the command worked fine. When using conda to install deepvariant, should it not have to be installed together?. Thanks,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:175,Testability,log,logdir,175,"Hi, . I had trouble running Deepvariant using conda. I ran the following command.; ```; dv_make_examples.py --sample {MY_SAMPLE} --ref {MY_FASTA}.fasta --reads {MY_BAM}.bam --logdir ./log/ --examples examples/; ```. and I got an error like this:; ```; ETA: 0s Left: 1 AVG: 0.00s local:1/0/100%/0.0s sh: 1: unzip: not found; Traceback (most recent call last):; File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 252, in <module>; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 187, in Main; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 138, in GetRepositoriesImports; FileNotFoundError: [Errno 2] No such file or directory: '/tmp/Bazel.runfiles_qwsw52c7/runfiles'; parallel: This job failed:; /opt/conda/envs/deepvariant/bin/python /opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip --mode calling --ref E.coli_K12_MG1655.fa --reads SRR1770413.bam --examples examples//SRR1770413.tfrecord@1.gz --task 0; ```. When I installed unzip by `conda install`, the command worked fine. When using conda to install deepvariant, should it not have to be installed together?. Thanks,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:184,Testability,log,log,184,"Hi, . I had trouble running Deepvariant using conda. I ran the following command.; ```; dv_make_examples.py --sample {MY_SAMPLE} --ref {MY_FASTA}.fasta --reads {MY_BAM}.bam --logdir ./log/ --examples examples/; ```. and I got an error like this:; ```; ETA: 0s Left: 1 AVG: 0.00s local:1/0/100%/0.0s sh: 1: unzip: not found; Traceback (most recent call last):; File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 252, in <module>; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 187, in Main; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 138, in GetRepositoriesImports; FileNotFoundError: [Errno 2] No such file or directory: '/tmp/Bazel.runfiles_qwsw52c7/runfiles'; parallel: This job failed:; /opt/conda/envs/deepvariant/bin/python /opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip --mode calling --ref E.coli_K12_MG1655.fa --reads SRR1770413.bam --examples examples//SRR1770413.tfrecord@1.gz --task 0; ```. When I installed unzip by `conda install`, the command worked fine. When using conda to install deepvariant, should it not have to be installed together?. Thanks,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/315:318,Usability,guid,guidance,318,"To whom it may concern,. I have created an openstack instance to run deepvariant 0.9.0 as detailed below. I had no problem running deepvariant in another openstack instance, but I cannot figure out why it will not work in the instance that I newly created. I was wondering if deepvariant developers could provide some guidance towards running deepvariant again. Regards,; Sangjin. ```; ***** Running the command:*****; time seq 0 54 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}. real 0m1.608s; user 0m25.676s; sys 0m23.860s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 54 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 55; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/316:352,Testability,log,logdir,352,"Hi, @chapmanb; Sorry for asking so many questions. I looked for the `run_deepvariant` command in the conda package, but the I couldn't find it.; So to generate the vcf file, I created the following code. ```; dv_make_examples.py \; --cores {threads} \; --ref {ref} \; --reads {bam} \; --sample {samplename} \; --examples {output_dir}/{samplename} \; --logdir {log_dir} . dv_call_variants.py \; --cores {threads} \; --outfile {output_dir}/{samplename}.tmp \; --sample {samplename} \; --examples {output_dir}/{samplename} \; --model {model} . dv_postprocess_variants.py \; --ref {ref} \; --infile {output_dir}/{samplename}.tmp \; --outfile {vcf}; ```. Or is there a way to run the `run_deepvariant` command that I am just not aware of?; I'd like to generate a gvcf file if possible.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/316
https://github.com/google/deepvariant/issues/317:172,Availability,error,errors,172,"Hi. I tried to investigate the ADs and VAFs of sites with label '0' in the training data.; In most cases, hom-ref sites have very low ADs, which is generated by sequencing errors. However, about 3% of the total examples (about 20000) have AD values greater than 10, yet still labeled as hom-ref. For example, in the BAM I'm studying, the following site. 4:39708091-39708091 C->CT DP:AD:VAF 46:28,15:0.326087. is found, and labeled as hom-ref. Most of these sites (about 98% of which) are short-tandem-repeats (STR), the reference bases are ; CTTTTTTTTTTTTTTTTTCCCAGATGGAAT; at the mentioned site. It seems the GIAB high-confidence VCFs have missed quite a few STR sites (which are within the high-confidence regions). My question is, would these mislabeled sites pollute the training set of DeepVariant, and lead to a higher FN rate? Could this be fixed by, for example, adding an STR/non-STR channel?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/318:841,Availability,avail,available,841,"I have a few queries about your output:-. 1) Can we generate VCF file with each position information?. 2) Is it possible to convert g.VCF to VCF since commonly used software used VCF format? What does QUAL ""0"" mean in VCF? Some FILTER column with QUAL 0 is marked as ""RefCall"" while some as ""."" Can you please explain how is filter decided in VCF?. 3); #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 6 | 1 | . | A | <*> | 0 | . | END=587 | GT:GQ:MIN_DP:PL | 0/0:1:0:0,0,0; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 6 | 588 | . | A | <*> | 0 | . | END=588 | GT:GQ:MIN_DP:PL | ./.:0:1:29,3,0. Both the row has QUAL value ""0"". Then why is first row has given homozygous call and the second row with now call?. 4) I have observed that there are **three options available in filter column**. RefCall:- Which would mean Homozygous reference ; PASS:- Can be homozygous/Heterozygous; . :- what does this mean?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/319:1248,Modifiability,variab,variables,1248,"i have used this command to run deep variant and generate VCF file. sudo docker run -v `pwd`:`pwd` -w `pwd` google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=${base_path}/${ref_file_name}.fasta --reads=${base_path}/base_recalib/$i\_vqsr.bam --output_vcf=deep_variant_results/$i\.vcf.gz --output_gvcf=deep_variant_results/$i\.g.vcf.gz. I want to run post-process variants but cannot get it from the above command. 1) Is there some way to add parameters to above command?. 2) I found two links related to it:-; a) [https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md); GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`. There is no make_examples.zip in the bin directory and what should be supplied to this parameter --examples. Can you please give more details about variables?. b) [https://github.com/google/deepvariant/issues/103](https://github.com/google/deepvariant/issues/103); sudo docker run -v ${HOME}:${HOME} gcr.io/deepvariant-docker/deepvariant:0.7.0 /opt/deepvariant/bin/postprocess_variants \; --ref ${OUTDIR}/data/hg19.fa \; --infile ${OUTDIR}/output/cvo.tfrecord.gz \; --outfile ${OUTDIR}/output/output.vcf.gz \; --nonvariant_site_tfrecord_path ${OUTDIR}/output/gvcf.tfrecord@8.gz \; --gvcf_outfile ${OUTDIR}/output/output.gvcf.gz. This is another way and parameters are also different. how to define the infile here?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:866,Testability,log,log,866,"i have used this command to run deep variant and generate VCF file. sudo docker run -v `pwd`:`pwd` -w `pwd` google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=${base_path}/${ref_file_name}.fasta --reads=${base_path}/base_recalib/$i\_vqsr.bam --output_vcf=deep_variant_results/$i\.vcf.gz --output_gvcf=deep_variant_results/$i\.g.vcf.gz. I want to run post-process variants but cannot get it from the above command. 1) Is there some way to add parameters to above command?. 2) I found two links related to it:-; a) [https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md); GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`. There is no make_examples.zip in the bin directory and what should be supplied to this parameter --examples. Can you please give more details about variables?. b) [https://github.com/google/deepvariant/issues/103](https://github.com/google/deepvariant/issues/103); sudo docker run -v ${HOME}:${HOME} gcr.io/deepvariant-docker/deepvariant:0.7.0 /opt/deepvariant/bin/postprocess_variants \; --ref ${OUTDIR}/data/hg19.fa \; --infile ${OUTDIR}/output/cvo.tfrecord.gz \; --outfile ${OUTDIR}/output/output.vcf.gz \; --nonvariant_site_tfrecord_path ${OUTDIR}/output/gvcf.tfrecord@8.gz \; --gvcf_outfile ${OUTDIR}/output/output.gvcf.gz. This is another way and parameters are also different. how to define the infile here?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:1088,Testability,log,log,1088,"i have used this command to run deep variant and generate VCF file. sudo docker run -v `pwd`:`pwd` -w `pwd` google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=${base_path}/${ref_file_name}.fasta --reads=${base_path}/base_recalib/$i\_vqsr.bam --output_vcf=deep_variant_results/$i\.vcf.gz --output_gvcf=deep_variant_results/$i\.g.vcf.gz. I want to run post-process variants but cannot get it from the above command. 1) Is there some way to add parameters to above command?. 2) I found two links related to it:-; a) [https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md); GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`. There is no make_examples.zip in the bin directory and what should be supplied to this parameter --examples. Can you please give more details about variables?. b) [https://github.com/google/deepvariant/issues/103](https://github.com/google/deepvariant/issues/103); sudo docker run -v ${HOME}:${HOME} gcr.io/deepvariant-docker/deepvariant:0.7.0 /opt/deepvariant/bin/postprocess_variants \; --ref ${OUTDIR}/data/hg19.fa \; --infile ${OUTDIR}/output/cvo.tfrecord.gz \; --outfile ${OUTDIR}/output/output.vcf.gz \; --nonvariant_site_tfrecord_path ${OUTDIR}/output/gvcf.tfrecord@8.gz \; --gvcf_outfile ${OUTDIR}/output/output.gvcf.gz. This is another way and parameters are also different. how to define the infile here?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/321:140,Availability,error,errors,140,"Dear deepvariant team,. we are currently playing a bit around with the GPU docker version of deepvariant 0.10.0. I've noticed the following errors in the ""call_variants"" step and I'm not sure if the GPU is used correctly in this step (""cannot find working devices"" ???) or if this is just a warning because the calling went through without any abort:; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp7l6e69ft/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp7l6e69ft/make_examples.tfrecord@25.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0703 17:18:45.593843 140322304501504 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-07-03 17:18:45.625121: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA; 2020-07-03 17:18:45.661204: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2800000000 Hz; 2020-07-03 17:18:45.667182: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4f919f0 executing computations on platform Host. Devices:; 2020-07-03 17:18:45.667206: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-07-03 17:18:45.674475: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1; 2020-07-03 17:18:45.680312: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; 2020-07-03 17:18:45.680339: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 7c1895dbad7c; 2020-07-03 17:18:45.680346: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 7c1895dbad7c; 2020-07-03 17:18:45.680397: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 410.129.0; 2020-07-03",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:553,Availability,checkpoint,checkpoint,553,"Dear deepvariant team,. we are currently playing a bit around with the GPU docker version of deepvariant 0.10.0. I've noticed the following errors in the ""call_variants"" step and I'm not sure if the GPU is used correctly in this step (""cannot find working devices"" ???) or if this is just a warning because the calling went through without any abort:; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp7l6e69ft/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp7l6e69ft/make_examples.tfrecord@25.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0703 17:18:45.593843 140322304501504 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-07-03 17:18:45.625121: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA; 2020-07-03 17:18:45.661204: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2800000000 Hz; 2020-07-03 17:18:45.667182: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4f919f0 executing computations on platform Host. Devices:; 2020-07-03 17:18:45.667206: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-07-03 17:18:45.674475: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1; 2020-07-03 17:18:45.680312: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; 2020-07-03 17:18:45.680339: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 7c1895dbad7c; 2020-07-03 17:18:45.680346: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 7c1895dbad7c; 2020-07-03 17:18:45.680397: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 410.129.0; 2020-07-03",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:2305,Deployability,configurat,configuration,2305,"ream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1; 2020-07-03 17:18:45.680312: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; 2020-07-03 17:18:45.680339: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 7c1895dbad7c; 2020-07-03 17:18:45.680346: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 7c1895dbad7c; 2020-07-03 17:18:45.680397: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 410.129.0; 2020-07-03 17:18:45.680416: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.113.0; 2020-07-03 17:18:45.680422: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 418.113.0 does not match DSO version 410.129.0 -- cannot find working devices in this configuration; I0703 17:18:45.713418 140322304501504 modeling.py:563] Initializing model with random parameters; W0703 17:18:45.713950 140322304501504 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp7amyb_ws; I0703 17:18:45.714213 140322304501504 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp7amyb_ws', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9f0f102630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:6242,Deployability,install,installed,6242,"Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; I0703 17:18:45.810746 140322304501504 data_providers.py:376] self.input_map_threads=48; W0703 17:18:45.810852 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0703 17:18:46.328745 140322304501504 estimator.py:1147] Calling model_fn.; W0703 17:18:46.330687 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0703 17:18:46.333346 140322304501504 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0703 17:18:50.712157 140322304501504 estimator.py:1149] Done calling model_fn.; I0703 17:18:51.788142 140322304501504 monitored_session.py:240] Graph was finalized.; ```; We are on Docker 19.03.11 on Debian 10 and executed deepvariant with ```docker run --gpus all ....``` (proposed way of using nvidia docker for Docker version >19.03 in the nvidia docker docs). Our installed Nvidia driver version is 418.113. Thanks in advance for your help,. Sebastian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:2305,Modifiability,config,configuration,2305,"ream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1; 2020-07-03 17:18:45.680312: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; 2020-07-03 17:18:45.680339: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 7c1895dbad7c; 2020-07-03 17:18:45.680346: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 7c1895dbad7c; 2020-07-03 17:18:45.680397: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 410.129.0; 2020-07-03 17:18:45.680416: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.113.0; 2020-07-03 17:18:45.680422: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 418.113.0 does not match DSO version 410.129.0 -- cannot find working devices in this configuration; I0703 17:18:45.713418 140322304501504 modeling.py:563] Initializing model with random parameters; W0703 17:18:45.713950 140322304501504 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp7amyb_ws; I0703 17:18:45.714213 140322304501504 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp7amyb_ws', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9f0f102630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:2598,Modifiability,config,config,2598,"; 2020-07-03 17:18:45.680397: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 410.129.0; 2020-07-03 17:18:45.680416: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.113.0; 2020-07-03 17:18:45.680422: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 418.113.0 does not match DSO version 410.129.0 -- cannot find working devices in this configuration; I0703 17:18:45.713418 140322304501504 modeling.py:563] Initializing model with random parameters; W0703 17:18:45.713950 140322304501504 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp7amyb_ws; I0703 17:18:45.714213 140322304501504 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp7amyb_ws', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9f0f102630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0703 17:18:45.714398 140322304501504 call_variants.py:384] Writing calls to /tmp/tmp7l6e69ft/call_variants_output.tfrecord.gz; W0703 17:18:45.719665 140322304501504 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.; Instructions for updating:; I",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:3907,Modifiability,layers,layers,3907,"ps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9f0f102630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0703 17:18:45.714398 140322304501504 call_variants.py:384] Writing calls to /tmp/tmp7l6e69ft/call_variants_output.tfrecord.gz; W0703 17:18:45.719665 140322304501504 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.; Instructions for updating:; If using Keras pass *_constraint arguments to layers.; I0703 17:18:45.730541 140322304501504 data_providers.py:369] self.input_read_threads=8; W0703 17:18:45.730644 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; I0703 17:18:45.810746 140322304501504 data_providers.py:376] self.input_map_threads=48; W0703 17:18:45.810852 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:5660,Modifiability,layers,layers,5660,"Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; I0703 17:18:45.810746 140322304501504 data_providers.py:376] self.input_map_threads=48; W0703 17:18:45.810852 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0703 17:18:46.328745 140322304501504 estimator.py:1147] Calling model_fn.; W0703 17:18:46.330687 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0703 17:18:46.333346 140322304501504 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0703 17:18:50.712157 140322304501504 estimator.py:1149] Done calling model_fn.; I0703 17:18:51.788142 140322304501504 monitored_session.py:240] Graph was finalized.; ```; We are on Docker 19.03.11 on Debian 10 and executed deepvariant with ```docker run --gpus all ....``` (proposed way of using nvidia docker for Docker version >19.03 in the nvidia docker docs). Our installed Nvidia driver version is 418.113. Thanks in advance for your help,. Sebastian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:5667,Modifiability,layers,layers,5667,"Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; I0703 17:18:45.810746 140322304501504 data_providers.py:376] self.input_map_threads=48; W0703 17:18:45.810852 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0703 17:18:46.328745 140322304501504 estimator.py:1147] Calling model_fn.; W0703 17:18:46.330687 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0703 17:18:46.333346 140322304501504 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0703 17:18:50.712157 140322304501504 estimator.py:1149] Done calling model_fn.; I0703 17:18:51.788142 140322304501504 monitored_session.py:240] Graph was finalized.; ```; We are on Docker 19.03.11 on Debian 10 and executed deepvariant with ```docker run --gpus all ....``` (proposed way of using nvidia docker for Docker version >19.03 in the nvidia docker docs). Our installed Nvidia driver version is 418.113. Thanks in advance for your help,. Sebastian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:5078,Performance,optimiz,optimizations,5078,"m_google_deepvariant/deepvariant/data_providers.py:374: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; I0703 17:18:45.810746 140322304501504 data_providers.py:376] self.input_map_threads=48; W0703 17:18:45.810852 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0703 17:18:46.328745 140322304501504 estimator.py:1147] Calling model_fn.; W0703 17:18:46.330687 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0703 17:18:46.333346 140322304501504 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0703 17:18:50.712157 140322304501504 estimator.py:1149] Done calling model_fn.; I0703 17:18:51.788142 140322304501504 monitored_session.py:240] Graph was finalized.; ```; We are on Docker 19.03.11 on Debian 10 and executed deepvarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:344,Safety,abort,abort,344,"Dear deepvariant team,. we are currently playing a bit around with the GPU docker version of deepvariant 0.10.0. I've noticed the following errors in the ""call_variants"" step and I'm not sure if the GPU is used correctly in this step (""cannot find working devices"" ???) or if this is just a warning because the calling went through without any abort:; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp7l6e69ft/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp7l6e69ft/make_examples.tfrecord@25.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0703 17:18:45.593843 140322304501504 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-07-03 17:18:45.625121: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA; 2020-07-03 17:18:45.661204: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2800000000 Hz; 2020-07-03 17:18:45.667182: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4f919f0 executing computations on platform Host. Devices:; 2020-07-03 17:18:45.667206: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-07-03 17:18:45.674475: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1; 2020-07-03 17:18:45.680312: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; 2020-07-03 17:18:45.680339: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 7c1895dbad7c; 2020-07-03 17:18:45.680346: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 7c1895dbad7c; 2020-07-03 17:18:45.680397: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 410.129.0; 2020-07-03",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/322:657,Availability,error,error,657,"Dear,. I tried to use the singularity to run the DeepVariant docker with the following command:. BIN_VERSION=""0.10.0""; OUTPUT_DIR=$fpath; INPUT_DIR=$fpath; REF=$refpath ;. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ -B ${INPUT_DIR}:/input -B ${OUTPUT_DIR}:/output -B ${REF}:/ref \; 		docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; 		/opt/deepvariant/bin/run_deepvariant \; 		--model_type=WES \ ; 		--ref=/ref/human_g1k_v37.fasta \; 		--reads=/input/""${fname}.bam"" \; 		--output_vcf=/output/""${fname}.vcf.gz"" \; 		--output_gvcf=/output/""${fname}.g.vcf.gz"" \; 		--num_shards=150 ;. unfortunately, it's not working and give me the following error:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; line 82: --ref=/ref/human_g1k_v37.fasta: No such file or directory. FYI: Same Structure fro the Docker itself is working.; 	BIN_VERSION=""0.10.0""; 		OUTPUT_DIR=$fpath; 		INPUT_DIR=$fpath; 		REF=$refpath ;; 		sudo docker run -v ${INPUT_DIR}:/input -v ${OUTPUT_DIR}:/output -v ${REF}:/ref \; 		gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; 		/opt/deepvariant/bin/run_deepvariant \; 		--model_type=WES \; 		--ref=/ref/human_g1k_v37.fasta \; 		--reads=/input/""${fname}.bam"" \; 		--output_vcf=/output/""${fname}.vcf.gz"" \; 		--output_gvcf=/output/""${fname}.g.vcf.gz"" \; 		--num_shards=45 ;; 	. Kind regards ; Amin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/322
https://github.com/google/deepvariant/issues/322:685,Availability,error,error,685,"Dear,. I tried to use the singularity to run the DeepVariant docker with the following command:. BIN_VERSION=""0.10.0""; OUTPUT_DIR=$fpath; INPUT_DIR=$fpath; REF=$refpath ;. singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ -B ${INPUT_DIR}:/input -B ${OUTPUT_DIR}:/output -B ${REF}:/ref \; 		docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; 		/opt/deepvariant/bin/run_deepvariant \; 		--model_type=WES \ ; 		--ref=/ref/human_g1k_v37.fasta \; 		--reads=/input/""${fname}.bam"" \; 		--output_vcf=/output/""${fname}.vcf.gz"" \; 		--output_gvcf=/output/""${fname}.g.vcf.gz"" \; 		--num_shards=150 ;. unfortunately, it's not working and give me the following error:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; line 82: --ref=/ref/human_g1k_v37.fasta: No such file or directory. FYI: Same Structure fro the Docker itself is working.; 	BIN_VERSION=""0.10.0""; 		OUTPUT_DIR=$fpath; 		INPUT_DIR=$fpath; 		REF=$refpath ;; 		sudo docker run -v ${INPUT_DIR}:/input -v ${OUTPUT_DIR}:/output -v ${REF}:/ref \; 		gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; 		/opt/deepvariant/bin/run_deepvariant \; 		--model_type=WES \; 		--ref=/ref/human_g1k_v37.fasta \; 		--reads=/input/""${fname}.bam"" \; 		--output_vcf=/output/""${fname}.vcf.gz"" \; 		--output_gvcf=/output/""${fname}.g.vcf.gz"" \; 		--num_shards=45 ;; 	. Kind regards ; Amin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/322
https://github.com/google/deepvariant/issues/324:26,Performance,perform,perform,26,"Hello,. There is a way to perform some kind of VQSR or use truth sets for calling on Deepvariant?. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/325:17,Availability,error,error,17,"Dears,. I get an error trying to reproduce the test example on my Bio-Linux Ubuntu 14.04.6 LTS where I run a 1.6.2 docker. The docker installation was successful:. ```; REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE; google/deepvariant 0.10.0 4745891a5ab0 3 months ago 3.866 GB; google/deepvariant latest 4745891a5ab0 3 months ago 3.866 GB; ```. But I get this error:. ```; I0715 10:39:51.140211 139624775427840 run_deepvariant.py:241] Re-using the directory for intermediate results in /tmp/tmpsowmvllp. ***** Intermediate results will be written to /tmp/tmpsowmvllp in docker. ****. ***** Running the command:*****; time seq 0 31 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmpsowmvllp/make_examples.tfrecord@32.gz"" --gvcf ""/tmp/tmpsowmvllp/gvcf.tfrecord@32.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 2; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 0; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 3. real 0m20.988s; user 0m7.822s; sys 3m7.414s; I0715 10:40:12.133007 139624775427840 run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:359,Availability,error,error,359,"Dears,. I get an error trying to reproduce the test example on my Bio-Linux Ubuntu 14.04.6 LTS where I run a 1.6.2 docker. The docker installation was successful:. ```; REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE; google/deepvariant 0.10.0 4745891a5ab0 3 months ago 3.866 GB; google/deepvariant latest 4745891a5ab0 3 months ago 3.866 GB; ```. But I get this error:. ```; I0715 10:39:51.140211 139624775427840 run_deepvariant.py:241] Re-using the directory for intermediate results in /tmp/tmpsowmvllp. ***** Intermediate results will be written to /tmp/tmpsowmvllp in docker. ****. ***** Running the command:*****; time seq 0 31 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmpsowmvllp/make_examples.tfrecord@32.gz"" --gvcf ""/tmp/tmpsowmvllp/gvcf.tfrecord@32.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 2; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 0; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 3. real 0m20.988s; user 0m7.822s; sys 3m7.414s; I0715 10:40:12.133007 139624775427840 run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:134,Deployability,install,installation,134,"Dears,. I get an error trying to reproduce the test example on my Bio-Linux Ubuntu 14.04.6 LTS where I run a 1.6.2 docker. The docker installation was successful:. ```; REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE; google/deepvariant 0.10.0 4745891a5ab0 3 months ago 3.866 GB; google/deepvariant latest 4745891a5ab0 3 months ago 3.866 GB; ```. But I get this error:. ```; I0715 10:39:51.140211 139624775427840 run_deepvariant.py:241] Re-using the directory for intermediate results in /tmp/tmpsowmvllp. ***** Intermediate results will be written to /tmp/tmpsowmvllp in docker. ****. ***** Running the command:*****; time seq 0 31 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmpsowmvllp/make_examples.tfrecord@32.gz"" --gvcf ""/tmp/tmpsowmvllp/gvcf.tfrecord@32.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 2; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 0; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 3. real 0m20.988s; user 0m7.822s; sys 3m7.414s; I0715 10:40:12.133007 139624775427840 run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:47,Testability,test,test,47,"Dears,. I get an error trying to reproduce the test example on my Bio-Linux Ubuntu 14.04.6 LTS where I run a 1.6.2 docker. The docker installation was successful:. ```; REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE; google/deepvariant 0.10.0 4745891a5ab0 3 months ago 3.866 GB; google/deepvariant latest 4745891a5ab0 3 months ago 3.866 GB; ```. But I get this error:. ```; I0715 10:39:51.140211 139624775427840 run_deepvariant.py:241] Re-using the directory for intermediate results in /tmp/tmpsowmvllp. ***** Intermediate results will be written to /tmp/tmpsowmvllp in docker. ****. ***** Running the command:*****; time seq 0 31 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmpsowmvllp/make_examples.tfrecord@32.gz"" --gvcf ""/tmp/tmpsowmvllp/gvcf.tfrecord@32.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 2; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 0; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 3. real 0m20.988s; user 0m7.822s; sys 3m7.414s; I0715 10:40:12.133007 139624775427840 run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:3119,Testability,test,testdata,3119,":40:12.133007 139624775427840 run_deepvariant.py:321] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 31 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmpsowmvllp/make_examples.tfrecord@32.gz"" --gvcf ""/tmp/tmpsowmvllp/gvcf.tfrecord@32.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252.; ```; The command I run is the following:. ```; :~# cat command.sh; docker run -v ""/root/quickstart-testdata"":""/input"" -v ""/root/quickstart-output"":""/output"" google/deepvariant:latest /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/ucsc.hg19.chr20.unittest.fasta --reads=/input/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=32; ```; And the content of ```testdata``` dir is:. ```; :~# ls quickstart-testdata/; NA12878_S1.chr20.10_10p1mb.bam ucsc.hg19.chr20.unittest.fasta; NA12878_S1.chr20.10_10p1mb.bam.bai ucsc.hg19.chr20.unittest.fasta.fai; test_nist.b37_chr20_100kbp_at_10mb.bed ucsc.hg19.chr20.unittest.fasta.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz ucsc.hg19.chr20.unittest.fasta.gz.fai; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Thanks a lot for any help!; -A",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:3504,Testability,test,testdata,3504,":40:12.133007 139624775427840 run_deepvariant.py:321] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 31 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmpsowmvllp/make_examples.tfrecord@32.gz"" --gvcf ""/tmp/tmpsowmvllp/gvcf.tfrecord@32.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252.; ```; The command I run is the following:. ```; :~# cat command.sh; docker run -v ""/root/quickstart-testdata"":""/input"" -v ""/root/quickstart-output"":""/output"" google/deepvariant:latest /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/ucsc.hg19.chr20.unittest.fasta --reads=/input/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=32; ```; And the content of ```testdata``` dir is:. ```; :~# ls quickstart-testdata/; NA12878_S1.chr20.10_10p1mb.bam ucsc.hg19.chr20.unittest.fasta; NA12878_S1.chr20.10_10p1mb.bam.bai ucsc.hg19.chr20.unittest.fasta.fai; test_nist.b37_chr20_100kbp_at_10mb.bed ucsc.hg19.chr20.unittest.fasta.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz ucsc.hg19.chr20.unittest.fasta.gz.fai; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Thanks a lot for any help!; -A",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:3548,Testability,test,testdata,3548,":40:12.133007 139624775427840 run_deepvariant.py:321] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 31 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmpsowmvllp/make_examples.tfrecord@32.gz"" --gvcf ""/tmp/tmpsowmvllp/gvcf.tfrecord@32.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252.; ```; The command I run is the following:. ```; :~# cat command.sh; docker run -v ""/root/quickstart-testdata"":""/input"" -v ""/root/quickstart-output"":""/output"" google/deepvariant:latest /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/ucsc.hg19.chr20.unittest.fasta --reads=/input/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=32; ```; And the content of ```testdata``` dir is:. ```; :~# ls quickstart-testdata/; NA12878_S1.chr20.10_10p1mb.bam ucsc.hg19.chr20.unittest.fasta; NA12878_S1.chr20.10_10p1mb.bam.bai ucsc.hg19.chr20.unittest.fasta.fai; test_nist.b37_chr20_100kbp_at_10mb.bed ucsc.hg19.chr20.unittest.fasta.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz ucsc.hg19.chr20.unittest.fasta.gz.fai; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Thanks a lot for any help!; -A",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/326:835,Availability,error,errors,835,"Hello! ; so some quick background, I am interested in SNP that are unique (= private) to each sample of my cohort. I ran DeepVariant on each sample, then GLnexus as per the best practices recommendations. I extracted unique variants using bcftools --private option. Then, I wanted to do some filtering on GQ. . Here is the GQ distribution on one of the individual vcf file (so before joint-calling), I don't have much to say about it, it makes sense ; ![GQ](https://user-images.githubusercontent.com/23341393/87663191-bb858180-c763-11ea-9579-22ab38004f05.PNG). However, here is the GQ distribution of the --private SNPs ; ![GQ_private](https://user-images.githubusercontent.com/23341393/87665589-6fd4d700-c767-11ea-99db-19a2cbbd6e4a.PNG). First, there are some values that are NA due to SNPs that have . as GQ value. That's all right, errors in sequencing / mapping I guess. In fact, my reasoning is that the --private option will enrich the SNP set in all the errors that are unique to each sequencing data set. Therefore, I was expecting that the GQ distribution would be shifted to the left. However, what we see is that, not only is it shifted to the left but the shape of the distribution is also changed. ; So I would like to know more about how GQ is exactly computed by DeepVariant. And why does the GQ seems to abruptly peak at 11-12. I also read on your blog that you consider ""high quality variants"" as the ones with a GQ of 20. Of course, owing to the distribution of GQ for the private set, setting a GQ threshold at 20 will make a big difference, as seen on this plot ; ![GQ_range_subs_cropped_for_github](https://user-images.githubusercontent.com/23341393/87666119-57b18780-c768-11ea-96ea-90e7f3b58191.PNG). Thanks a lot for your insight!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:961,Availability,error,errors,961,"Hello! ; so some quick background, I am interested in SNP that are unique (= private) to each sample of my cohort. I ran DeepVariant on each sample, then GLnexus as per the best practices recommendations. I extracted unique variants using bcftools --private option. Then, I wanted to do some filtering on GQ. . Here is the GQ distribution on one of the individual vcf file (so before joint-calling), I don't have much to say about it, it makes sense ; ![GQ](https://user-images.githubusercontent.com/23341393/87663191-bb858180-c763-11ea-9579-22ab38004f05.PNG). However, here is the GQ distribution of the --private SNPs ; ![GQ_private](https://user-images.githubusercontent.com/23341393/87665589-6fd4d700-c767-11ea-99db-19a2cbbd6e4a.PNG). First, there are some values that are NA due to SNPs that have . as GQ value. That's all right, errors in sequencing / mapping I guess. In fact, my reasoning is that the --private option will enrich the SNP set in all the errors that are unique to each sequencing data set. Therefore, I was expecting that the GQ distribution would be shifted to the left. However, what we see is that, not only is it shifted to the left but the shape of the distribution is also changed. ; So I would like to know more about how GQ is exactly computed by DeepVariant. And why does the GQ seems to abruptly peak at 11-12. I also read on your blog that you consider ""high quality variants"" as the ones with a GQ of 20. Of course, owing to the distribution of GQ for the private set, setting a GQ threshold at 20 will make a big difference, as seen on this plot ; ![GQ_range_subs_cropped_for_github](https://user-images.githubusercontent.com/23341393/87666119-57b18780-c768-11ea-96ea-90e7f3b58191.PNG). Thanks a lot for your insight!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/328:25,Safety,detect,detection,25,"Hi!; I am new in variant detection, especially detection in deep learning. I want to implement the process from bam file to output file for deepening the knowledge . but it's difficlut to find the definite network structure in the code, so, is there a cnn structure like the function,""model.summary()"", result in tensorflow2? That will help me a lot!; thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:47,Safety,detect,detection,47,"Hi!; I am new in variant detection, especially detection in deep learning. I want to implement the process from bam file to output file for deepening the knowledge . but it's difficlut to find the definite network structure in the code, so, is there a cnn structure like the function,""model.summary()"", result in tensorflow2? That will help me a lot!; thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:65,Usability,learn,learning,65,"Hi!; I am new in variant detection, especially detection in deep learning. I want to implement the process from bam file to output file for deepening the knowledge . but it's difficlut to find the definite network structure in the code, so, is there a cnn structure like the function,""model.summary()"", result in tensorflow2? That will help me a lot!; thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/329:21,Availability,error,error,21,"I wonder what is the error model difference between WGS and WES. Is it simply the way coverage varies, or is there any difference in the error rates/types?. Also for the open training [data](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md), I noticed that the BAM files are named *deduplicated.bam. What is the method used to do mark duplication? Is it GATK?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:137,Availability,error,error,137,"I wonder what is the error model difference between WGS and WES. Is it simply the way coverage varies, or is there any difference in the error rates/types?. Also for the open training [data](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md), I noticed that the BAM files are named *deduplicated.bam. What is the method used to do mark duplication? Is it GATK?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:71,Usability,simpl,simply,71,"I wonder what is the error model difference between WGS and WES. Is it simply the way coverage varies, or is there any difference in the error rates/types?. Also for the open training [data](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md), I noticed that the BAM files are named *deduplicated.bam. What is the method used to do mark duplication? Is it GATK?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/332:317,Testability,log,logging,317,"**Describe the issue:**; I have been using DV via the official Docker container; and I have not found a way yet to make DV tell me its version. The Docker container is versioned, obviously, but the included tools have no ""--version"" flag - which makes it a bit tricky to dump out version information at run time (for logging purposes, for example). . **Setup**; Any system would have this issue, I think. **Steps to reproduce:**; N/A",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/333:446,Availability,reliab,reliable,446,"**Describe the issue:**; Apparently DeepVariant will not call variants on certain regions, irrespective of the ""calling intervals"" I pass it via BED file. . First of all, I only found this out after googling it and coming across a closed issue. This seems like it is ""important"" information. I spent a fair amount of time trying to figure out why my calls were missing MT information... Secondly, while I ""get"" that the results may not be highly reliable, MT variant calling is still useful (and commonly done) for some applications; so if I pass the Mitochondrion as a calling target, I would expect to get MT variant calls. This is a bit of an odd behavior, I think. . Solutions: Clearly document this on github (sorry if I didn't see it, if it is already there). And maybe allow users to overwrite this through their BED file targets - maybe with a warning (unless MT variants are never trained so the algorithm is simply unable to call them). . **Setup**; Any. **Steps to reproduce:**; N/A",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/333
https://github.com/google/deepvariant/issues/333:682,Usability,Clear,Clearly,682,"**Describe the issue:**; Apparently DeepVariant will not call variants on certain regions, irrespective of the ""calling intervals"" I pass it via BED file. . First of all, I only found this out after googling it and coming across a closed issue. This seems like it is ""important"" information. I spent a fair amount of time trying to figure out why my calls were missing MT information... Secondly, while I ""get"" that the results may not be highly reliable, MT variant calling is still useful (and commonly done) for some applications; so if I pass the Mitochondrion as a calling target, I would expect to get MT variant calls. This is a bit of an odd behavior, I think. . Solutions: Clearly document this on github (sorry if I didn't see it, if it is already there). And maybe allow users to overwrite this through their BED file targets - maybe with a warning (unless MT variants are never trained so the algorithm is simply unable to call them). . **Setup**; Any. **Steps to reproduce:**; N/A",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/333
https://github.com/google/deepvariant/issues/333:918,Usability,simpl,simply,918,"**Describe the issue:**; Apparently DeepVariant will not call variants on certain regions, irrespective of the ""calling intervals"" I pass it via BED file. . First of all, I only found this out after googling it and coming across a closed issue. This seems like it is ""important"" information. I spent a fair amount of time trying to figure out why my calls were missing MT information... Secondly, while I ""get"" that the results may not be highly reliable, MT variant calling is still useful (and commonly done) for some applications; so if I pass the Mitochondrion as a calling target, I would expect to get MT variant calls. This is a bit of an odd behavior, I think. . Solutions: Clearly document this on github (sorry if I didn't see it, if it is already there). And maybe allow users to overwrite this through their BED file targets - maybe with a warning (unless MT variants are never trained so the algorithm is simply unable to call them). . **Setup**; Any. **Steps to reproduce:**; N/A",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/333
https://github.com/google/deepvariant/issues/334:839,Availability,down,downstream,839,"**Describe the issue:**; DeepVariant currently outputs [`default` as the sample name](https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/postprocess_variants.py#L967) for empty VCFs (related to the fix in #186 ). Ideally, the `sample_name` should be transferred from the original BAM file, but if that is too difficult to implement (since there are no examples), I think it would also be ok if we let the user customize this ID. One suggestion is that it can use the value provided by the [`--sample_name`](https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/scripts/run_deepvariant.py#L89) flag (currently, used for `make_examples`, but I think it can also be reused for `postprocess_variants`). Using `default` causes issues in pipelines where the VCF is used downstream of DeepVariant (e.g. merging the VCF with other callers; or even within DeepVariant in a pipeline that scatters calling across multiple chromosomes and tries to gather them and some of those VCFs are empty). **Setup**; - DeepVariant version: v0.10.0; - Installation method (Docker, built from source, etc.): docker",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:807,Deployability,pipeline,pipelines,807,"**Describe the issue:**; DeepVariant currently outputs [`default` as the sample name](https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/postprocess_variants.py#L967) for empty VCFs (related to the fix in #186 ). Ideally, the `sample_name` should be transferred from the original BAM file, but if that is too difficult to implement (since there are no examples), I think it would also be ok if we let the user customize this ID. One suggestion is that it can use the value provided by the [`--sample_name`](https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/scripts/run_deepvariant.py#L89) flag (currently, used for `make_examples`, but I think it can also be reused for `postprocess_variants`). Using `default` causes issues in pipelines where the VCF is used downstream of DeepVariant (e.g. merging the VCF with other callers; or even within DeepVariant in a pipeline that scatters calling across multiple chromosomes and tries to gather them and some of those VCFs are empty). **Setup**; - DeepVariant version: v0.10.0; - Installation method (Docker, built from source, etc.): docker",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:939,Deployability,pipeline,pipeline,939,"**Describe the issue:**; DeepVariant currently outputs [`default` as the sample name](https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/postprocess_variants.py#L967) for empty VCFs (related to the fix in #186 ). Ideally, the `sample_name` should be transferred from the original BAM file, but if that is too difficult to implement (since there are no examples), I think it would also be ok if we let the user customize this ID. One suggestion is that it can use the value provided by the [`--sample_name`](https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/scripts/run_deepvariant.py#L89) flag (currently, used for `make_examples`, but I think it can also be reused for `postprocess_variants`). Using `default` causes issues in pipelines where the VCF is used downstream of DeepVariant (e.g. merging the VCF with other callers; or even within DeepVariant in a pipeline that scatters calling across multiple chromosomes and tries to gather them and some of those VCFs are empty). **Setup**; - DeepVariant version: v0.10.0; - Installation method (Docker, built from source, etc.): docker",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:1103,Deployability,Install,Installation,1103,"**Describe the issue:**; DeepVariant currently outputs [`default` as the sample name](https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/postprocess_variants.py#L967) for empty VCFs (related to the fix in #186 ). Ideally, the `sample_name` should be transferred from the original BAM file, but if that is too difficult to implement (since there are no examples), I think it would also be ok if we let the user customize this ID. One suggestion is that it can use the value provided by the [`--sample_name`](https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/scripts/run_deepvariant.py#L89) flag (currently, used for `make_examples`, but I think it can also be reused for `postprocess_variants`). Using `default` causes issues in pipelines where the VCF is used downstream of DeepVariant (e.g. merging the VCF with other callers; or even within DeepVariant in a pipeline that scatters calling across multiple chromosomes and tries to gather them and some of those VCFs are empty). **Setup**; - DeepVariant version: v0.10.0; - Installation method (Docker, built from source, etc.): docker",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/335:28,Usability,clear,clear,28,"**Describe the issue:**; (A clear and concise description of what the issue is.); I have no ""sudo"" authority; I think deepvariant tool have to run ""sudo"" authority ; and I did this command; `sudo docker pull google/deepvariant:""0.10.0""`. as far as I understand, this next step is ; `sudo docker run \; -v ""${INPUT_DIR}"":""/3.Sort"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${ref_fasta}"" \; --reads=""/3.Sort/$1_Markdup_sort.bam"" \; --output_vcf=/output/$1_Deepvariant.output.vcf.gz \; --output_gvcf=/output/$1_Deepvariant.output.g.vcf.gz \; --num_shards=${N_SHARDS}; `; is that right?; But I can't use ""sudo"" every time. how can I run 'deepvariant' without 'sudo' ?? . **Setup**; - Operating system: Ubuntu; - DeepVariant version:0.10.0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335
https://github.com/google/deepvariant/issues/336:20,Deployability,release,release,20,Is there a document release for DeepVariant used in PrecisionFDA v2 (the hybrid version)?. https://precision.fda.gov/challenges/10/view/results,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/336
https://github.com/google/deepvariant/issues/337:44,Availability,error,error,44,"Hi I want to run DeepVariant but i faced an error . **Describe the issue:**; `#!/bin/bash; BIN_VERSION=""0.10.0""; N_SHARDS=""4""; BASE=""/tera/home/phswin92/WGS/Variant_Call/DeepVariant""; INPUT_DIR=""$BASE/input""; mkdir $BASE/output; OUTPUT_DIR=""$BASE/output"". docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$INPUT_DIR/GRCh38_full_analysis_set_plus_decoy_hla.fa \; --reads=$INPUT_DIR/$2_Markdup_sort.bam \; --output_vcf=$OUTPUT_DIR/$1_$2_Deepvariant.output.vcf.gz \; --output_gvcf=$OUTPUT_DIR/$1_$2_Deepvariant.output.g.vcf.gz \; --num_shards=$N_SHARDS; `; this is my code. ![image](https://user-images.githubusercontent.com/68986997/91021146-4f355200-e62e-11ea-96bd-40bbe6a53c94.png); and this is my file in 'input' folder. error; `***** Running the command:*****; time seq 0 3 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/BWA_Markdup_sort.bam"" --examples ""/tmp/tmpwgmyf_jr/make_examples.tfrecord@4.gz"" --gvcf ""/tmp/tmpwgmyf_jr/gvcf.tfrecord@4.gz"" --task {}. I0824 08:09:22.428504 140501281453824 make_examples.py:386] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. [E::hts_open_format] Failed to open file /tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/BWA_Markdup_sort.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_ujzt8s5j/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_ujzt8s5j/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_ujzt8s5j/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:871,Availability,error,error,871,"Hi I want to run DeepVariant but i faced an error . **Describe the issue:**; `#!/bin/bash; BIN_VERSION=""0.10.0""; N_SHARDS=""4""; BASE=""/tera/home/phswin92/WGS/Variant_Call/DeepVariant""; INPUT_DIR=""$BASE/input""; mkdir $BASE/output; OUTPUT_DIR=""$BASE/output"". docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$INPUT_DIR/GRCh38_full_analysis_set_plus_decoy_hla.fa \; --reads=$INPUT_DIR/$2_Markdup_sort.bam \; --output_vcf=$OUTPUT_DIR/$1_$2_Deepvariant.output.vcf.gz \; --output_gvcf=$OUTPUT_DIR/$1_$2_Deepvariant.output.g.vcf.gz \; --num_shards=$N_SHARDS; `; this is my code. ![image](https://user-images.githubusercontent.com/68986997/91021146-4f355200-e62e-11ea-96bd-40bbe6a53c94.png); and this is my file in 'input' folder. error; `***** Running the command:*****; time seq 0 3 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/BWA_Markdup_sort.bam"" --examples ""/tmp/tmpwgmyf_jr/make_examples.tfrecord@4.gz"" --gvcf ""/tmp/tmpwgmyf_jr/gvcf.tfrecord@4.gz"" --task {}. I0824 08:09:22.428504 140501281453824 make_examples.py:386] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. [E::hts_open_format] Failed to open file /tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/BWA_Markdup_sort.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_ujzt8s5j/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_ujzt8s5j/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_ujzt8s5j/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:9770,Availability,error,error,9770,"oogle_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open /tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/BWA_Markdup_sort.bam; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/BWA_Markdup_sort.bam --examples /tmp/tmpwgmyf_jr/make_examples.tfrecord@4.gz --gvcf /tmp/tmpwgmyf_jr/gvcf.tfrecord@4.gz --task 3. real	0m2.438s; user	0m2.289s; sys	0m3.833s; I0824 08:09:22.720529 140047104423680 run_deepvariant.py:321] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 3 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/BWA_Markdup_sort.bam"" --examples ""/tmp/tmpwgmyf_jr/make_examples.tfrecord@4.gz"" --gvcf ""/tmp/tmpwgmyf_jr/gvcf.tfrecord@4.gz"" --task {}' returned non-zero exit status 1.; Done...; `; this is my error when I run deepvariant script. - Operating system: CentOS; - DeepVariant version: 0.10.0; - Installation method (Docker, built from source, etc.): Docker. how can I solve this problem",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:9868,Deployability,Install,Installation,9868,"oogle_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open /tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/BWA_Markdup_sort.bam; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/BWA_Markdup_sort.bam --examples /tmp/tmpwgmyf_jr/make_examples.tfrecord@4.gz --gvcf /tmp/tmpwgmyf_jr/gvcf.tfrecord@4.gz --task 3. real	0m2.438s; user	0m2.289s; sys	0m3.833s; I0824 08:09:22.720529 140047104423680 run_deepvariant.py:321] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 3 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/tera/home/phswin92/WGS/Variant_Call/DeepVariant/input/BWA_Markdup_sort.bam"" --examples ""/tmp/tmpwgmyf_jr/make_examples.tfrecord@4.gz"" --gvcf ""/tmp/tmpwgmyf_jr/gvcf.tfrecord@4.gz"" --task {}' returned non-zero exit status 1.; Done...; `; this is my error when I run deepvariant script. - Operating system: CentOS; - DeepVariant version: 0.10.0; - Installation method (Docker, built from source, etc.): Docker. how can I solve this problem",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/338:411,Availability,error,error,411,"I have WGS data (about 200x) and WES data (about 1000x) of the same individual.; Ideally I would like to merge the 2 datasets and run DeepVariant with --model_type=WGS on the merged data and obtain one VCF file. Or is the model behind ""--model_type=WES"" really a different machine learning model (ML) trained on real Exome data?; I could imagine that such a ML model would learn a slightly different sequencing error model specific for sequencing data derived from target enrichment (hybridization probes) as the ones used for WES. Thank you for your advice. **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: r0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: Illumina WGS and WES data",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:636,Deployability,Install,Installation,636,"I have WGS data (about 200x) and WES data (about 1000x) of the same individual.; Ideally I would like to merge the 2 datasets and run DeepVariant with --model_type=WGS on the merged data and obtain one VCF file. Or is the model behind ""--model_type=WES"" really a different machine learning model (ML) trained on real Exome data?; I could imagine that such a ML model would learn a slightly different sequencing error model specific for sequencing data derived from target enrichment (hybridization probes) as the ones used for WES. Thank you for your advice. **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: r0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: Illumina WGS and WES data",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:281,Usability,learn,learning,281,"I have WGS data (about 200x) and WES data (about 1000x) of the same individual.; Ideally I would like to merge the 2 datasets and run DeepVariant with --model_type=WGS on the merged data and obtain one VCF file. Or is the model behind ""--model_type=WES"" really a different machine learning model (ML) trained on real Exome data?; I could imagine that such a ML model would learn a slightly different sequencing error model specific for sequencing data derived from target enrichment (hybridization probes) as the ones used for WES. Thank you for your advice. **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: r0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: Illumina WGS and WES data",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:373,Usability,learn,learn,373,"I have WGS data (about 200x) and WES data (about 1000x) of the same individual.; Ideally I would like to merge the 2 datasets and run DeepVariant with --model_type=WGS on the merged data and obtain one VCF file. Or is the model behind ""--model_type=WES"" really a different machine learning model (ML) trained on real Exome data?; I could imagine that such a ML model would learn a slightly different sequencing error model specific for sequencing data derived from target enrichment (hybridization probes) as the ones used for WES. Thank you for your advice. **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: r0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: Illumina WGS and WES data",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/339:89,Availability,checkpoint,checkpoint,89,"**Describe the issue:**; I Build the docker image; Inside Docker image: I am reading the checkpoint files to create a frozen graph; When doing ""import_meta_graph"" I get the error. Below is the stack trace; `tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`. **Setup**; - Operating system: Ubuntu 18.04 on Intel i7 CPU (no GPU or TPU); - DeepVariant version: r-0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - Error trace: ; `2020-08-26 18:04:05.695108: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; Traceback (most recent call last):; File ""tf2_mipso_convert.py"", line 35, in <module>; saver = tf.compat.v1.train.import_meta_graph(meta_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1453, in import_meta_graph; **kwargs)[0]; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:173,Availability,error,error,173,"**Describe the issue:**; I Build the docker image; Inside Docker image: I am reading the checkpoint files to create a frozen graph; When doing ""import_meta_graph"" I get the error. Below is the stack trace; `tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`. **Setup**; - Operating system: Ubuntu 18.04 on Intel i7 CPU (no GPU or TPU); - DeepVariant version: r-0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - Error trace: ; `2020-08-26 18:04:05.695108: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; Traceback (most recent call last):; File ""tf2_mipso_convert.py"", line 35, in <module>; saver = tf.compat.v1.train.import_meta_graph(meta_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1453, in import_meta_graph; **kwargs)[0]; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:993,Availability,Error,Error,993,"**Describe the issue:**; I Build the docker image; Inside Docker image: I am reading the checkpoint files to create a frozen graph; When doing ""import_meta_graph"" I get the error. Below is the stack trace; `tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`. **Setup**; - Operating system: Ubuntu 18.04 on Intel i7 CPU (no GPU or TPU); - DeepVariant version: r-0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - Error trace: ; `2020-08-26 18:04:05.695108: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; Traceback (most recent call last):; File ""tf2_mipso_convert.py"", line 35, in <module>; saver = tf.compat.v1.train.import_meta_graph(meta_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1453, in import_meta_graph; **kwargs)[0]; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:780,Deployability,Install,Installation,780,"**Describe the issue:**; I Build the docker image; Inside Docker image: I am reading the checkpoint files to create a frozen graph; When doing ""import_meta_graph"" I get the error. Below is the stack trace; `tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`. **Setup**; - Operating system: Ubuntu 18.04 on Intel i7 CPU (no GPU or TPU); - DeepVariant version: r-0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - Error trace: ; `2020-08-26 18:04:05.695108: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; Traceback (most recent call last):; File ""tf2_mipso_convert.py"", line 35, in <module>; saver = tf.compat.v1.train.import_meta_graph(meta_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1453, in import_meta_graph; **kwargs)[0]; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:459,Performance,load,loading,459,"**Describe the issue:**; I Build the docker image; Inside Docker image: I am reading the checkpoint files to create a frozen graph; When doing ""import_meta_graph"" I get the error. Below is the stack trace; `tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`. **Setup**; - Operating system: Ubuntu 18.04 on Intel i7 CPU (no GPU or TPU); - DeepVariant version: r-0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - Error trace: ; `2020-08-26 18:04:05.695108: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; Traceback (most recent call last):; File ""tf2_mipso_convert.py"", line 35, in <module>; saver = tf.compat.v1.train.import_meta_graph(meta_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1453, in import_meta_graph; **kwargs)[0]; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:1150,Performance,Tune,Tune,1150,"Below is the stack trace; `tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`. **Setup**; - Operating system: Ubuntu 18.04 on Intel i7 CPU (no GPU or TPU); - DeepVariant version: r-0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - Error trace: ; `2020-08-26 18:04:05.695108: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; Traceback (most recent call last):; File ""tf2_mipso_convert.py"", line 35, in <module>; saver = tf.compat.v1.train.import_meta_graph(meta_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1453, in import_meta_graph; **kwargs)[0]; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def; producer_op_list=producer_op_list); File ""/usr/local/lib/python3.6/dist-package",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:1199,Performance,perform,performance,1199,"Below is the stack trace; `tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`. **Setup**; - Operating system: Ubuntu 18.04 on Intel i7 CPU (no GPU or TPU); - DeepVariant version: r-0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - Error trace: ; `2020-08-26 18:04:05.695108: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; Traceback (most recent call last):; File ""tf2_mipso_convert.py"", line 35, in <module>; saver = tf.compat.v1.train.import_meta_graph(meta_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1453, in import_meta_graph; **kwargs)[0]; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def; producer_op_list=producer_op_list); File ""/usr/local/lib/python3.6/dist-package",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:2595,Performance,load,loading,2595,"ver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def; producer_op_list=producer_op_list); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 501, in _import_graph_def_internal; graph._c_graph, serialized, options) # pylint: disable=protected-access; tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed`. **Does the quick start test work on your system?** ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Quick start works on my system -- I can perform make_examples, call_variants and post_processing. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`). Code snippet :; `import tensorflow as tf. meta_path = '/opt/models/wgs/model.ckpt.meta'. ckpt_folder = '/opt/models/wgs'. with tf.compat.v1.Session() as sess:. saver = tf.compat.v1.train.import_meta_graph(meta_path). print(""\n**Import Sucessful\n**""). saver.restore(sess,tf.compat.v1.train.latest_checkpoint(ckpt_folder)); `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:3065,Performance,perform,perform,3065,"ver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def; producer_op_list=producer_op_list); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 501, in _import_graph_def_internal; graph._c_graph, serialized, options) # pylint: disable=protected-access; tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed`. **Does the quick start test work on your system?** ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Quick start works on my system -- I can perform make_examples, call_variants and post_processing. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`). Code snippet :; `import tensorflow as tf. meta_path = '/opt/models/wgs/model.ckpt.meta'. ckpt_folder = '/opt/models/wgs'. with tf.compat.v1.Session() as sess:. saver = tf.compat.v1.train.import_meta_graph(meta_path). print(""\n**Import Sucessful\n**""). saver.restore(sess,tf.compat.v1.train.latest_checkpoint(ckpt_folder)); `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:513,Security,access,accessing,513,"**Describe the issue:**; I Build the docker image; Inside Docker image: I am reading the checkpoint files to create a frozen graph; When doing ""import_meta_graph"" I get the error. Below is the stack trace; `tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`. **Setup**; - Operating system: Ubuntu 18.04 on Intel i7 CPU (no GPU or TPU); - DeepVariant version: r-0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - Error trace: ; `2020-08-26 18:04:05.695108: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; Traceback (most recent call last):; File ""tf2_mipso_convert.py"", line 35, in <module>; saver = tf.compat.v1.train.import_meta_graph(meta_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1453, in import_meta_graph; **kwargs)[0]; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:658,Security,access,accessed,658,"**Describe the issue:**; I Build the docker image; Inside Docker image: I am reading the checkpoint files to create a frozen graph; When doing ""import_meta_graph"" I get the error. Below is the stack trace; `tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`. **Setup**; - Operating system: Ubuntu 18.04 on Intel i7 CPU (no GPU or TPU); - DeepVariant version: r-0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - Error trace: ; `2020-08-26 18:04:05.695108: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; Traceback (most recent call last):; File ""tf2_mipso_convert.py"", line 35, in <module>; saver = tf.compat.v1.train.import_meta_graph(meta_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1453, in import_meta_graph; **kwargs)[0]; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:2335,Security,access,access,2335,"at.v1.train.import_meta_graph(meta_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1453, in import_meta_graph; **kwargs)[0]; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def; producer_op_list=producer_op_list); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 501, in _import_graph_def_internal; graph._c_graph, serialized, options) # pylint: disable=protected-access; tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed`. **Does the quick start test work on your system?** ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Quick start works on my system -- I can perform make_examples, call_variants and post_processing. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`). Code snippet :; `import tensorflow as tf. meta_path = '/opt/models/w",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:2649,Security,access,accessing,2649,"ver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def; producer_op_list=producer_op_list); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 501, in _import_graph_def_internal; graph._c_graph, serialized, options) # pylint: disable=protected-access; tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed`. **Does the quick start test work on your system?** ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Quick start works on my system -- I can perform make_examples, call_variants and post_processing. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`). Code snippet :; `import tensorflow as tf. meta_path = '/opt/models/wgs/model.ckpt.meta'. ckpt_folder = '/opt/models/wgs'. with tf.compat.v1.Session() as sess:. saver = tf.compat.v1.train.import_meta_graph(meta_path). print(""\n**Import Sucessful\n**""). saver.restore(sess,tf.compat.v1.train.latest_checkpoint(ckpt_folder)); `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:2794,Security,access,accessed,2794,"ver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def; producer_op_list=producer_op_list); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 501, in _import_graph_def_internal; graph._c_graph, serialized, options) # pylint: disable=protected-access; tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed`. **Does the quick start test work on your system?** ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Quick start works on my system -- I can perform make_examples, call_variants and post_processing. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`). Code snippet :; `import tensorflow as tf. meta_path = '/opt/models/wgs/model.ckpt.meta'. ckpt_folder = '/opt/models/wgs'. with tf.compat.v1.Session() as sess:. saver = tf.compat.v1.train.import_meta_graph(meta_path). print(""\n**Import Sucessful\n**""). saver.restore(sess,tf.compat.v1.train.latest_checkpoint(ckpt_folder)); `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:2828,Testability,test,test,2828,"ver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def; producer_op_list=producer_op_list); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 501, in _import_graph_def_internal; graph._c_graph, serialized, options) # pylint: disable=protected-access; tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed`. **Does the quick start test work on your system?** ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Quick start works on my system -- I can perform make_examples, call_variants and post_processing. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`). Code snippet :; `import tensorflow as tf. meta_path = '/opt/models/wgs/model.ckpt.meta'. ckpt_folder = '/opt/models/wgs'. with tf.compat.v1.Session() as sess:. saver = tf.compat.v1.train.import_meta_graph(meta_path). print(""\n**Import Sucessful\n**""). saver.restore(sess,tf.compat.v1.train.latest_checkpoint(ckpt_folder)); `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:2865,Testability,test,test,2865,"ver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def; producer_op_list=producer_op_list); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 501, in _import_graph_def_internal; graph._c_graph, serialized, options) # pylint: disable=protected-access; tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed`. **Does the quick start test work on your system?** ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Quick start works on my system -- I can perform make_examples, call_variants and post_processing. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`). Code snippet :; `import tensorflow as tf. meta_path = '/opt/models/wgs/model.ckpt.meta'. ckpt_folder = '/opt/models/wgs'. with tf.compat.v1.Session() as sess:. saver = tf.compat.v1.train.import_meta_graph(meta_path). print(""\n**Import Sucessful\n**""). saver.restore(sess,tf.compat.v1.train.latest_checkpoint(ckpt_folder)); `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/340:445,Availability,Error,Error,445,"**Describe the issue:**; Hello, I want to know what is an efficient way to build and run locally. My intent: make a change in call_variant.py and observe the effect. ; Do I have to always build the docker? ; OR which shell scripts can I use to achieve my purpose?. **Setup**; - Operating system: Ubuntu 18.04 LTS; - DeepVariant version: 0.8.0; - Installation method: build from source; - Type of data: NA. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/340
https://github.com/google/deepvariant/issues/340:346,Deployability,Install,Installation,346,"**Describe the issue:**; Hello, I want to know what is an efficient way to build and run locally. My intent: make a change in call_variant.py and observe the effect. ; Do I have to always build the docker? ; OR which shell scripts can I use to achieve my purpose?. **Setup**; - Operating system: Ubuntu 18.04 LTS; - DeepVariant version: 0.8.0; - Installation method: build from source; - Type of data: NA. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/340
https://github.com/google/deepvariant/issues/340:58,Energy Efficiency,efficient,efficient,58,"**Describe the issue:**; Hello, I want to know what is an efficient way to build and run locally. My intent: make a change in call_variant.py and observe the effect. ; Do I have to always build the docker? ; OR which shell scripts can I use to achieve my purpose?. **Setup**; - Operating system: Ubuntu 18.04 LTS; - DeepVariant version: 0.8.0; - Installation method: build from source; - Type of data: NA. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/340
https://github.com/google/deepvariant/issues/340:498,Testability,test,test,498,"**Describe the issue:**; Hello, I want to know what is an efficient way to build and run locally. My intent: make a change in call_variant.py and observe the effect. ; Do I have to always build the docker? ; OR which shell scripts can I use to achieve my purpose?. **Setup**; - Operating system: Ubuntu 18.04 LTS; - DeepVariant version: 0.8.0; - Installation method: build from source; - Type of data: NA. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/340
https://github.com/google/deepvariant/issues/340:534,Testability,test,test,534,"**Describe the issue:**; Hello, I want to know what is an efficient way to build and run locally. My intent: make a change in call_variant.py and observe the effect. ; Do I have to always build the docker? ; OR which shell scripts can I use to achieve my purpose?. **Setup**; - Operating system: Ubuntu 18.04 LTS; - DeepVariant version: 0.8.0; - Installation method: build from source; - Type of data: NA. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/340
https://github.com/google/deepvariant/issues/341:552,Availability,error,error,552,"**Describe the issue:**; When doing the variant calling with a VCF file of proposed variants (`variant_caller=vcf_candidate_importer,proposed_variants=${VCF_JOIN}`) and specifying GVCF output, ref call sites end up with only 2 entries in the FORMAT field AD while the ALT field contain also 2 entries which should result in 3 AD entries. ; Example: `chr1 11391 . T A,<*> 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:24:0:0,0:0:0,26,27,990,990,990`. Here the AD field has 2 entries but should have 3. Thereafter, the merging with GLNexus fails with the following error `Failed to genotype: Invalid: genotyper: VCF allele depth FORMAT field is malformed (sample <0>:11391-11391 (AD))`. . **Setup**; - Operating system: Linux; - DeepVariant version: 0.10.0; - Installation method (Docker, built from source, etc.): Singularity. **Steps to reproduce:**; - Command: ` singularity exec -B /usr/lib/locale/:/usr/lib/locale/ ${SINGULARITY_IMG} bash /opt/deepvariant/bin/run_deepvariant --model_type WGS --customized_model ${MODEL} --ref ${REF} --reads ${IN_BAM} --output_vcf ${OUT_VCF}.2.vcf.gz --output_gvcf ${OUT_VCF}.2.g.vcf.gz --num_shards 24 --make_examples_extra_args=\""realign_reads=false,min_mapping_quality=${MIN_MAPQ},min_base_quality=${MIN_QS},variant_caller=vcf_candidate_importer,proposed_variants=${VCF_JOIN}\"" --sample_name ${IN_PN} ${BED_ARG};""; `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:747,Deployability,Install,Installation,747,"**Describe the issue:**; When doing the variant calling with a VCF file of proposed variants (`variant_caller=vcf_candidate_importer,proposed_variants=${VCF_JOIN}`) and specifying GVCF output, ref call sites end up with only 2 entries in the FORMAT field AD while the ALT field contain also 2 entries which should result in 3 AD entries. ; Example: `chr1 11391 . T A,<*> 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:24:0:0,0:0:0,26,27,990,990,990`. Here the AD field has 2 entries but should have 3. Thereafter, the merging with GLNexus fails with the following error `Failed to genotype: Invalid: genotyper: VCF allele depth FORMAT field is malformed (sample <0>:11391-11391 (AD))`. . **Setup**; - Operating system: Linux; - DeepVariant version: 0.10.0; - Installation method (Docker, built from source, etc.): Singularity. **Steps to reproduce:**; - Command: ` singularity exec -B /usr/lib/locale/:/usr/lib/locale/ ${SINGULARITY_IMG} bash /opt/deepvariant/bin/run_deepvariant --model_type WGS --customized_model ${MODEL} --ref ${REF} --reads ${IN_BAM} --output_vcf ${OUT_VCF}.2.vcf.gz --output_gvcf ${OUT_VCF}.2.g.vcf.gz --num_shards 24 --make_examples_extra_args=\""realign_reads=false,min_mapping_quality=${MIN_MAPQ},min_base_quality=${MIN_QS},variant_caller=vcf_candidate_importer,proposed_variants=${VCF_JOIN}\"" --sample_name ${IN_PN} ${BED_ARG};""; `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/342:268,Availability,error,errors,268,"**Describe the issue:**. - I setup version 0.8. ( I have to stick to TF1.x ); - Modified the run-prereq.sh : to install some dependencies before installing tensor2tensor; - Modified the setings.sh : ""DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=1.14"" (the 1.13.1 was giving errors); - Followed [instruction ](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) to setup the INPUT_DIR & OUTPUT_DIR. ; - Then built the docker using ""docker run ."" ; - When inside docker, I run : . /opt/deepvariant/bin/make_examples --mode calling --ref ""/INPUT/ucsc.hg19.chr20.unittest.fasta"" --reads ""/INPUT/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/OUTPUT/tmp0cv1ybnt/make_examples.tfrecord@8.gz"" --gvcf ""/OUTPUT/tmp0cv1ybnt/gvcf.tfrecord@8.gz"" --regions ""chr20:10,000,000-10,010,000"". But, I get error shown in stack trace section (below). I can manually import tensorflow and print version on the terminal. . Please help! . **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 0.8; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 28, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 52, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 17, in <module>; from tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2; File ""/usr/local/lib/python2.7/dist-packages/te",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:807,Availability,error,error,807,"**Describe the issue:**. - I setup version 0.8. ( I have to stick to TF1.x ); - Modified the run-prereq.sh : to install some dependencies before installing tensor2tensor; - Modified the setings.sh : ""DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=1.14"" (the 1.13.1 was giving errors); - Followed [instruction ](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) to setup the INPUT_DIR & OUTPUT_DIR. ; - Then built the docker using ""docker run ."" ; - When inside docker, I run : . /opt/deepvariant/bin/make_examples --mode calling --ref ""/INPUT/ucsc.hg19.chr20.unittest.fasta"" --reads ""/INPUT/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/OUTPUT/tmp0cv1ybnt/make_examples.tfrecord@8.gz"" --gvcf ""/OUTPUT/tmp0cv1ybnt/gvcf.tfrecord@8.gz"" --regions ""chr20:10,000,000-10,010,000"". But, I get error shown in stack trace section (below). I can manually import tensorflow and print version on the terminal. . Please help! . **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 0.8; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 28, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 52, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 17, in <module>; from tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2; File ""/usr/local/lib/python2.7/dist-packages/te",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:1223,Availability,Error,Error,1223,"ON=1.14"" (the 1.13.1 was giving errors); - Followed [instruction ](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) to setup the INPUT_DIR & OUTPUT_DIR. ; - Then built the docker using ""docker run ."" ; - When inside docker, I run : . /opt/deepvariant/bin/make_examples --mode calling --ref ""/INPUT/ucsc.hg19.chr20.unittest.fasta"" --reads ""/INPUT/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/OUTPUT/tmp0cv1ybnt/make_examples.tfrecord@8.gz"" --gvcf ""/OUTPUT/tmp0cv1ybnt/gvcf.tfrecord@8.gz"" --regions ""chr20:10,000,000-10,010,000"". But, I get error shown in stack trace section (below). I can manually import tensorflow and print version on the terminal. . Please help! . **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 0.8; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 28, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 52, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 17, in <module>; from tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/function_pb2.py"", line 18, in <module>; from tensorflow.core.framework import op_def_pb2 as tensorflow_dot_core_dot_framework_dot_op__def__pb2; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framewo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:112,Deployability,install,install,112,"**Describe the issue:**. - I setup version 0.8. ( I have to stick to TF1.x ); - Modified the run-prereq.sh : to install some dependencies before installing tensor2tensor; - Modified the setings.sh : ""DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=1.14"" (the 1.13.1 was giving errors); - Followed [instruction ](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) to setup the INPUT_DIR & OUTPUT_DIR. ; - Then built the docker using ""docker run ."" ; - When inside docker, I run : . /opt/deepvariant/bin/make_examples --mode calling --ref ""/INPUT/ucsc.hg19.chr20.unittest.fasta"" --reads ""/INPUT/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/OUTPUT/tmp0cv1ybnt/make_examples.tfrecord@8.gz"" --gvcf ""/OUTPUT/tmp0cv1ybnt/gvcf.tfrecord@8.gz"" --regions ""chr20:10,000,000-10,010,000"". But, I get error shown in stack trace section (below). I can manually import tensorflow and print version on the terminal. . Please help! . **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 0.8; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 28, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 52, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 17, in <module>; from tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2; File ""/usr/local/lib/python2.7/dist-packages/te",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:145,Deployability,install,installing,145,"**Describe the issue:**. - I setup version 0.8. ( I have to stick to TF1.x ); - Modified the run-prereq.sh : to install some dependencies before installing tensor2tensor; - Modified the setings.sh : ""DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=1.14"" (the 1.13.1 was giving errors); - Followed [instruction ](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) to setup the INPUT_DIR & OUTPUT_DIR. ; - Then built the docker using ""docker run ."" ; - When inside docker, I run : . /opt/deepvariant/bin/make_examples --mode calling --ref ""/INPUT/ucsc.hg19.chr20.unittest.fasta"" --reads ""/INPUT/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/OUTPUT/tmp0cv1ybnt/make_examples.tfrecord@8.gz"" --gvcf ""/OUTPUT/tmp0cv1ybnt/gvcf.tfrecord@8.gz"" --regions ""chr20:10,000,000-10,010,000"". But, I get error shown in stack trace section (below). I can manually import tensorflow and print version on the terminal. . Please help! . **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 0.8; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 28, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 52, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 17, in <module>; from tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2; File ""/usr/local/lib/python2.7/dist-packages/te",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:1011,Deployability,Install,Installation,1011," version 0.8. ( I have to stick to TF1.x ); - Modified the run-prereq.sh : to install some dependencies before installing tensor2tensor; - Modified the setings.sh : ""DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=1.14"" (the 1.13.1 was giving errors); - Followed [instruction ](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) to setup the INPUT_DIR & OUTPUT_DIR. ; - Then built the docker using ""docker run ."" ; - When inside docker, I run : . /opt/deepvariant/bin/make_examples --mode calling --ref ""/INPUT/ucsc.hg19.chr20.unittest.fasta"" --reads ""/INPUT/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/OUTPUT/tmp0cv1ybnt/make_examples.tfrecord@8.gz"" --gvcf ""/OUTPUT/tmp0cv1ybnt/gvcf.tfrecord@8.gz"" --regions ""chr20:10,000,000-10,010,000"". But, I get error shown in stack trace section (below). I can manually import tensorflow and print version on the terminal. . Please help! . **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 0.8; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 28, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 52, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 17, in <module>; from tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/function_p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:125,Integrability,depend,dependencies,125,"**Describe the issue:**. - I setup version 0.8. ( I have to stick to TF1.x ); - Modified the run-prereq.sh : to install some dependencies before installing tensor2tensor; - Modified the setings.sh : ""DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=1.14"" (the 1.13.1 was giving errors); - Followed [instruction ](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) to setup the INPUT_DIR & OUTPUT_DIR. ; - Then built the docker using ""docker run ."" ; - When inside docker, I run : . /opt/deepvariant/bin/make_examples --mode calling --ref ""/INPUT/ucsc.hg19.chr20.unittest.fasta"" --reads ""/INPUT/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/OUTPUT/tmp0cv1ybnt/make_examples.tfrecord@8.gz"" --gvcf ""/OUTPUT/tmp0cv1ybnt/gvcf.tfrecord@8.gz"" --regions ""chr20:10,000,000-10,010,000"". But, I get error shown in stack trace section (below). I can manually import tensorflow and print version on the terminal. . Please help! . **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 0.8; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 28, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 52, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 17, in <module>; from tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2; File ""/usr/local/lib/python2.7/dist-packages/te",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:2588,Testability,test,test,2588,"he terminal. . Please help! . **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 0.8; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 28, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 52, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 17, in <module>; from tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/function_pb2.py"", line 18, in <module>; from tensorflow.core.framework import op_def_pb2 as tensorflow_dot_core_dot_framework_dot_op__def__pb2; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/op_def_pb2.py"", line 210, in <module>; serialized_options=None, file=DESCRIPTOR),; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 534, in __new__; return _message.default_pool.FindFieldByName(full_name); KeyError: ""Couldn't find field tensorflow.OpDef.control_output"". **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:2624,Testability,test,test,2624,"he terminal. . Please help! . **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 0.8; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 28, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 52, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 17, in <module>; from tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/function_pb2.py"", line 18, in <module>; from tensorflow.core.framework import op_def_pb2 as tensorflow_dot_core_dot_framework_dot_op__def__pb2; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/op_def_pb2.py"", line 210, in <module>; serialized_options=None, file=DESCRIPTOR),; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 534, in __new__; return _message.default_pool.FindFieldByName(full_name); KeyError: ""Couldn't find field tensorflow.OpDef.control_output"". **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/345:73,Availability,error,error,73,"Hi~; I tried to run Deepvariant v1.0.0 by docker image.; But it returned error message when I run test dataset. Here is the code:. ```sh; # BIN_VERSION=""1.0.0""; # INPUT_DIR=""${PWD}/quickstart-testdata""; # OUTPUT_DIR=""${PWD}/quickstart-output"". # ls quickstart-testdata; NA12878_S1.chr20.10_10p1mb.bam ; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi ; ucsc.hg19.chr20.unittest.fasta.gz.fai; NA12878_S1.chr20.10_10p1mb.bam.bai ; ucsc.hg19.chr20.unittest.fasta ; ucsc.hg19.chr20.unittest.fasta.gz.gzi; test_nist.b37_chr20_100kbp_at_10mb.bed ; ucsc.hg19.chr20.unittest.fasta.fai; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz ; ucsc.hg19.chr20.unittest.fasta.gz. # /home/d008/data/covid19/deepvarient/test# sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; ```. And here is the error message from docker. ```sh; ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0907 09:04:08.296450 140053878712064 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unitt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1199,Availability,error,error,1199,"# OUTPUT_DIR=""${PWD}/quickstart-output"". # ls quickstart-testdata; NA12878_S1.chr20.10_10p1mb.bam ; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi ; ucsc.hg19.chr20.unittest.fasta.gz.fai; NA12878_S1.chr20.10_10p1mb.bam.bai ; ucsc.hg19.chr20.unittest.fasta ; ucsc.hg19.chr20.unittest.fasta.gz.gzi; test_nist.b37_chr20_100kbp_at_10mb.bed ; ucsc.hg19.chr20.unittest.fasta.fai; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz ; ucsc.hg19.chr20.unittest.fasta.gz. # /home/d008/data/covid19/deepvarient/test# sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; ```. And here is the error message from docker. ```sh; ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0907 09:04:08.296450 140053878712064 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4537,Availability,error,error,4537,"ain(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. Here is the supported CPU instructions of host:. ```sh; # cat /proc/cpuinfo; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 30; model name : Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz; stepping : 5; microcode : 0xa; cpu MHz : 1197.018; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 11; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs; bogomips : 5851.92; clflush size : 64; cache_alignment : 64; address sizes : 36 bits physical, 48 bits virtual; power management:; ```. I also tried create a env & install on host by `conda install -c bioconda deepvariant`, but it pop-up the same error.; And Deepvariant v0.10.0 also have the same error. Please kindly give me some advice about this thank you. Best,; Jerry",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4588,Availability,error,error,4588,"ain(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. Here is the supported CPU instructions of host:. ```sh; # cat /proc/cpuinfo; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 30; model name : Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz; stepping : 5; microcode : 0xa; cpu MHz : 1197.018; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 11; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs; bogomips : 5851.92; clflush size : 64; cache_alignment : 64; address sizes : 36 bits physical, 48 bits virtual; power management:; ```. I also tried create a env & install on host by `conda install -c bioconda deepvariant`, but it pop-up the same error.; And Deepvariant v0.10.0 also have the same error. Please kindly give me some advice about this thank you. Best,; Jerry",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4454,Deployability,install,install,4454,"ain(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. Here is the supported CPU instructions of host:. ```sh; # cat /proc/cpuinfo; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 30; model name : Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz; stepping : 5; microcode : 0xa; cpu MHz : 1197.018; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 11; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs; bogomips : 5851.92; clflush size : 64; cache_alignment : 64; address sizes : 36 bits physical, 48 bits virtual; power management:; ```. I also tried create a env & install on host by `conda install -c bioconda deepvariant`, but it pop-up the same error.; And Deepvariant v0.10.0 also have the same error. Please kindly give me some advice about this thank you. Best,; Jerry",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4480,Deployability,install,install,4480,"ain(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. Here is the supported CPU instructions of host:. ```sh; # cat /proc/cpuinfo; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 30; model name : Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz; stepping : 5; microcode : 0xa; cpu MHz : 1197.018; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 11; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs; bogomips : 5851.92; clflush size : 64; cache_alignment : 64; address sizes : 36 bits physical, 48 bits virtual; power management:; ```. I also tried create a env & install on host by `conda install -c bioconda deepvariant`, but it pop-up the same error.; And Deepvariant v0.10.0 also have the same error. Please kindly give me some advice about this thank you. Best,; Jerry",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4047,Energy Efficiency,monitor,monitor,4047,"ain(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. Here is the supported CPU instructions of host:. ```sh; # cat /proc/cpuinfo; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 30; model name : Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz; stepping : 5; microcode : 0xa; cpu MHz : 1197.018; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 11; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs; bogomips : 5851.92; clflush size : 64; cache_alignment : 64; address sizes : 36 bits physical, 48 bits virtual; power management:; ```. I also tried create a env & install on host by `conda install -c bioconda deepvariant`, but it pop-up the same error.; And Deepvariant v0.10.0 also have the same error. Please kindly give me some advice about this thank you. Best,; Jerry",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4402,Energy Efficiency,power,power,4402,"ain(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. Here is the supported CPU instructions of host:. ```sh; # cat /proc/cpuinfo; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 30; model name : Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz; stepping : 5; microcode : 0xa; cpu MHz : 1197.018; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 11; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs; bogomips : 5851.92; clflush size : 64; cache_alignment : 64; address sizes : 36 bits physical, 48 bits virtual; power management:; ```. I also tried create a env & install on host by `conda install -c bioconda deepvariant`, but it pop-up the same error.; And Deepvariant v0.10.0 also have the same error. Please kindly give me some advice about this thank you. Best,; Jerry",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:79,Integrability,message,message,79,"Hi~; I tried to run Deepvariant v1.0.0 by docker image.; But it returned error message when I run test dataset. Here is the code:. ```sh; # BIN_VERSION=""1.0.0""; # INPUT_DIR=""${PWD}/quickstart-testdata""; # OUTPUT_DIR=""${PWD}/quickstart-output"". # ls quickstart-testdata; NA12878_S1.chr20.10_10p1mb.bam ; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi ; ucsc.hg19.chr20.unittest.fasta.gz.fai; NA12878_S1.chr20.10_10p1mb.bam.bai ; ucsc.hg19.chr20.unittest.fasta ; ucsc.hg19.chr20.unittest.fasta.gz.gzi; test_nist.b37_chr20_100kbp_at_10mb.bed ; ucsc.hg19.chr20.unittest.fasta.fai; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz ; ucsc.hg19.chr20.unittest.fasta.gz. # /home/d008/data/covid19/deepvarient/test# sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; ```. And here is the error message from docker. ```sh; ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0907 09:04:08.296450 140053878712064 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unitt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1205,Integrability,message,message,1205,"# OUTPUT_DIR=""${PWD}/quickstart-output"". # ls quickstart-testdata; NA12878_S1.chr20.10_10p1mb.bam ; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi ; ucsc.hg19.chr20.unittest.fasta.gz.fai; NA12878_S1.chr20.10_10p1mb.bam.bai ; ucsc.hg19.chr20.unittest.fasta ; ucsc.hg19.chr20.unittest.fasta.gz.gzi; test_nist.b37_chr20_100kbp_at_10mb.bed ; ucsc.hg19.chr20.unittest.fasta.fai; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz ; ucsc.hg19.chr20.unittest.fasta.gz. # /home/d008/data/covid19/deepvarient/test# sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; ```. And here is the error message from docker. ```sh; ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0907 09:04:08.296450 140053878712064 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3626,Performance,cache,cache,3626,"ain(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. Here is the supported CPU instructions of host:. ```sh; # cat /proc/cpuinfo; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 30; model name : Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz; stepping : 5; microcode : 0xa; cpu MHz : 1197.018; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 11; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs; bogomips : 5851.92; clflush size : 64; cache_alignment : 64; address sizes : 36 bits physical, 48 bits virtual; power management:; ```. I also tried create a env & install on host by `conda install -c bioconda deepvariant`, but it pop-up the same error.; And Deepvariant v0.10.0 also have the same error. Please kindly give me some advice about this thank you. Best,; Jerry",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:98,Testability,test,test,98,"Hi~; I tried to run Deepvariant v1.0.0 by docker image.; But it returned error message when I run test dataset. Here is the code:. ```sh; # BIN_VERSION=""1.0.0""; # INPUT_DIR=""${PWD}/quickstart-testdata""; # OUTPUT_DIR=""${PWD}/quickstart-output"". # ls quickstart-testdata; NA12878_S1.chr20.10_10p1mb.bam ; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi ; ucsc.hg19.chr20.unittest.fasta.gz.fai; NA12878_S1.chr20.10_10p1mb.bam.bai ; ucsc.hg19.chr20.unittest.fasta ; ucsc.hg19.chr20.unittest.fasta.gz.gzi; test_nist.b37_chr20_100kbp_at_10mb.bed ; ucsc.hg19.chr20.unittest.fasta.fai; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz ; ucsc.hg19.chr20.unittest.fasta.gz. # /home/d008/data/covid19/deepvarient/test# sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; ```. And here is the error message from docker. ```sh; ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0907 09:04:08.296450 140053878712064 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unitt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:192,Testability,test,testdata,192,"Hi~; I tried to run Deepvariant v1.0.0 by docker image.; But it returned error message when I run test dataset. Here is the code:. ```sh; # BIN_VERSION=""1.0.0""; # INPUT_DIR=""${PWD}/quickstart-testdata""; # OUTPUT_DIR=""${PWD}/quickstart-output"". # ls quickstart-testdata; NA12878_S1.chr20.10_10p1mb.bam ; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi ; ucsc.hg19.chr20.unittest.fasta.gz.fai; NA12878_S1.chr20.10_10p1mb.bam.bai ; ucsc.hg19.chr20.unittest.fasta ; ucsc.hg19.chr20.unittest.fasta.gz.gzi; test_nist.b37_chr20_100kbp_at_10mb.bed ; ucsc.hg19.chr20.unittest.fasta.fai; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz ; ucsc.hg19.chr20.unittest.fasta.gz. # /home/d008/data/covid19/deepvarient/test# sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; ```. And here is the error message from docker. ```sh; ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0907 09:04:08.296450 140053878712064 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unitt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:260,Testability,test,testdata,260,"Hi~; I tried to run Deepvariant v1.0.0 by docker image.; But it returned error message when I run test dataset. Here is the code:. ```sh; # BIN_VERSION=""1.0.0""; # INPUT_DIR=""${PWD}/quickstart-testdata""; # OUTPUT_DIR=""${PWD}/quickstart-output"". # ls quickstart-testdata; NA12878_S1.chr20.10_10p1mb.bam ; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi ; ucsc.hg19.chr20.unittest.fasta.gz.fai; NA12878_S1.chr20.10_10p1mb.bam.bai ; ucsc.hg19.chr20.unittest.fasta ; ucsc.hg19.chr20.unittest.fasta.gz.gzi; test_nist.b37_chr20_100kbp_at_10mb.bed ; ucsc.hg19.chr20.unittest.fasta.fai; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz ; ucsc.hg19.chr20.unittest.fasta.gz. # /home/d008/data/covid19/deepvarient/test# sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; ```. And here is the error message from docker. ```sh; ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0907 09:04:08.296450 140053878712064 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unitt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:693,Testability,test,test,693,"Hi~; I tried to run Deepvariant v1.0.0 by docker image.; But it returned error message when I run test dataset. Here is the code:. ```sh; # BIN_VERSION=""1.0.0""; # INPUT_DIR=""${PWD}/quickstart-testdata""; # OUTPUT_DIR=""${PWD}/quickstart-output"". # ls quickstart-testdata; NA12878_S1.chr20.10_10p1mb.bam ; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi ; ucsc.hg19.chr20.unittest.fasta.gz.fai; NA12878_S1.chr20.10_10p1mb.bam.bai ; ucsc.hg19.chr20.unittest.fasta ; ucsc.hg19.chr20.unittest.fasta.gz.gzi; test_nist.b37_chr20_100kbp_at_10mb.bed ; ucsc.hg19.chr20.unittest.fasta.fai; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz ; ucsc.hg19.chr20.unittest.fasta.gz. # /home/d008/data/covid19/deepvarient/test# sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; ```. And here is the error message from docker. ```sh; ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0907 09:04:08.296450 140053878712064 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unitt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/346:581,Availability,error,error,581,"Hello,. I'm trying to apply deepvariant v1.0.0 to a small cohort to generate multi-sample VCF. I've followed the steps described in your tutorial on multi-sample calling, so:; 1. generate single sample g.vcf with deepvariant; 2. merge g.vcf using GLnexus. However, I'm confused by the output in the multi-sample VCF. Looking at the variants I've found several positions where one or more of the samples are reported as `0/0` genotype, but with DP = 0. In this case, I assume the position has no reads in the data and so I'm expecting a missing genotype (`./.`) not a hom ref. This error can really mess up segregation analysis.; Trying to understand what is going on, I've looked at the single g.vcf generated by deepvariant for some of this position and I've noticed that the errors seem related to variants output as a hom ref block with MIN_DP zero in the g.vcf file. See the following example:. **multi-sample VCF:**; ```; chr1 72787 chr1_72787_C_T C T 18 . AF=0.5;AQ=18 GT:DP:AD:GQ:PL:RNC 0/0:0:0,0:1:0,3,29:.. 1/1:2:0,2:6:18,9,0:..; ```; **g.vcf sample1:**; ```; chr1 72121 . A <*> 0 . END=73000 GT:GQ:MIN_DP:PL 0/0:1:0:0,3,29; ```; **g.vcf sample2:**; ```; chr1 72787 . C T,<*> 18.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:10:2:0,2,0:1,0:18,9,0,990,990,990; ```. Instead, if the g.vcf line is like the one below the genotype is correctly reported as missing in the multi-sample VCF.; ```; chr1 20595 . A <*> 0 . END=20595 GT:GQ:MIN_DP:PL ./.:0:4:17,0,77; ```. So a couple of questions:; 1. Is this behavior expected for deepvariant or is it a kind of bug?; 2. How to interpret a g.vcf block like the one above with MIN_DP zero? Does it mean that some of the positions in this block have actually non-zero coverage or all of them have zero? In the first case, I suggest splitting the block when a position has zero coverage; in the second case, it is probably better to output the block with `./.` genotype.; 3. How can I interpret `0/0` genotypes with zero DP in the multi-sample VCF? Currently, I assume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:777,Availability,error,errors,777,"Hello,. I'm trying to apply deepvariant v1.0.0 to a small cohort to generate multi-sample VCF. I've followed the steps described in your tutorial on multi-sample calling, so:; 1. generate single sample g.vcf with deepvariant; 2. merge g.vcf using GLnexus. However, I'm confused by the output in the multi-sample VCF. Looking at the variants I've found several positions where one or more of the samples are reported as `0/0` genotype, but with DP = 0. In this case, I assume the position has no reads in the data and so I'm expecting a missing genotype (`./.`) not a hom ref. This error can really mess up segregation analysis.; Trying to understand what is going on, I've looked at the single g.vcf generated by deepvariant for some of this position and I've noticed that the errors seem related to variants output as a hom ref block with MIN_DP zero in the g.vcf file. See the following example:. **multi-sample VCF:**; ```; chr1 72787 chr1_72787_C_T C T 18 . AF=0.5;AQ=18 GT:DP:AD:GQ:PL:RNC 0/0:0:0,0:1:0,3,29:.. 1/1:2:0,2:6:18,9,0:..; ```; **g.vcf sample1:**; ```; chr1 72121 . A <*> 0 . END=73000 GT:GQ:MIN_DP:PL 0/0:1:0:0,3,29; ```; **g.vcf sample2:**; ```; chr1 72787 . C T,<*> 18.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:10:2:0,2,0:1,0:18,9,0,990,990,990; ```. Instead, if the g.vcf line is like the one below the genotype is correctly reported as missing in the multi-sample VCF.; ```; chr1 20595 . A <*> 0 . END=20595 GT:GQ:MIN_DP:PL ./.:0:4:17,0,77; ```. So a couple of questions:; 1. Is this behavior expected for deepvariant or is it a kind of bug?; 2. How to interpret a g.vcf block like the one above with MIN_DP zero? Does it mean that some of the positions in this block have actually non-zero coverage or all of them have zero? In the first case, I suggest splitting the block when a position has zero coverage; in the second case, it is probably better to output the block with `./.` genotype.; 3. How can I interpret `0/0` genotypes with zero DP in the multi-sample VCF? Currently, I assume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:2247,Safety,safe,safely,2247,". In this case, I assume the position has no reads in the data and so I'm expecting a missing genotype (`./.`) not a hom ref. This error can really mess up segregation analysis.; Trying to understand what is going on, I've looked at the single g.vcf generated by deepvariant for some of this position and I've noticed that the errors seem related to variants output as a hom ref block with MIN_DP zero in the g.vcf file. See the following example:. **multi-sample VCF:**; ```; chr1 72787 chr1_72787_C_T C T 18 . AF=0.5;AQ=18 GT:DP:AD:GQ:PL:RNC 0/0:0:0,0:1:0,3,29:.. 1/1:2:0,2:6:18,9,0:..; ```; **g.vcf sample1:**; ```; chr1 72121 . A <*> 0 . END=73000 GT:GQ:MIN_DP:PL 0/0:1:0:0,3,29; ```; **g.vcf sample2:**; ```; chr1 72787 . C T,<*> 18.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:10:2:0,2,0:1,0:18,9,0,990,990,990; ```. Instead, if the g.vcf line is like the one below the genotype is correctly reported as missing in the multi-sample VCF.; ```; chr1 20595 . A <*> 0 . END=20595 GT:GQ:MIN_DP:PL ./.:0:4:17,0,77; ```. So a couple of questions:; 1. Is this behavior expected for deepvariant or is it a kind of bug?; 2. How to interpret a g.vcf block like the one above with MIN_DP zero? Does it mean that some of the positions in this block have actually non-zero coverage or all of them have zero? In the first case, I suggest splitting the block when a position has zero coverage; in the second case, it is probably better to output the block with `./.` genotype.; 3. How can I interpret `0/0` genotypes with zero DP in the multi-sample VCF? Currently, I assume that the DP notation is correct and thus convert all these ones to missing genotypes. Or is it possible that some of them actually have reads covering the position? This latter case would be particularly problematic since I would not be able to safely distinguish between an actual home ref and a missing call due to zero coverage.; 4. Is there any setting I can change in GLnexus to have positions with DP zero outputted as missing?. Thanks a lot!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/351:9,Deployability,release,release,9,"With the release of version 1.0.0, it is stated that DeepVariant will not support somatic variant calling because the only genotypes supported are hom-alt, het, and hom-ref. . Is there the potential for a deep variant somatic variant caller in the future? ; or ; Could individuals produce somatic variants using a matched-normal approach? ie by calling variants on the germline and tumor(s) and extracting variants found in the tumor only?; Thank you for your time",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/352:405,Availability,Error,Error,405,"Deepvariant 1.0 is still not considering polyploid polyclonal tumor data analysis in consideration. Anything in the pipeline or any plug in?. **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352
https://github.com/google/deepvariant/issues/352:116,Deployability,pipeline,pipeline,116,"Deepvariant 1.0 is still not considering polyploid polyclonal tumor data analysis in consideration. Anything in the pipeline or any plug in?. **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352
https://github.com/google/deepvariant/issues/352:200,Deployability,Install,Installation,200,"Deepvariant 1.0 is still not considering polyploid polyclonal tumor data analysis in consideration. Anything in the pipeline or any plug in?. **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352
https://github.com/google/deepvariant/issues/352:458,Testability,test,test,458,"Deepvariant 1.0 is still not considering polyploid polyclonal tumor data analysis in consideration. Anything in the pipeline or any plug in?. **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352
https://github.com/google/deepvariant/issues/352:494,Testability,test,test,494,"Deepvariant 1.0 is still not considering polyploid polyclonal tumor data analysis in consideration. Anything in the pipeline or any plug in?. **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352
https://github.com/google/deepvariant/issues/353:67,Deployability,update,updated,67,"Hi,. The singularity containers are on version 0.9.0, can these be updated to 1.0.0?. Cheers,. Max H.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/353
https://github.com/google/deepvariant/issues/354:294,Availability,down,downsampling,294,"**Describe the issue:**; `/opt/deepvariant/bin/run_deepvariant` crashes when there are no variants, because a VCF file gets written with 'default' as the sample name'. This happens because I use targetted sequencing, and when the capture fails, I get hardly any reads. This can be simulated by downsampling the quickstart data to 0.1%, see below. **Setup**; - HPC; - google/deepvariant:0.10.0; - Docker; - Targetted PacBio sequencing, aligned against HG38. **Does the quick start test work on your system?**; Yes. **Workaround, for version 1.0.0 only**; This error does not occur when using version 1.0.0, and explicitly passing the `--sample_name` flag to `run_deepvariant`. . **Steps to reproduce, using the quickstart data:**; ```bash; MODEL_TYPE=PACBIO; NUM_SHARDS=4; READS=NA12878_0.1_percent.bam. # Downsample the reads to 0.1%; samtools view -s 0.001 -b NA12878_S1.chr20.10_10p1mb.bam -o ${READS} --write-index. docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${MODEL_TYPE} \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/${READS} \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=${NUM_SHARDS}; ```. **Error trace**; ```bash; $ docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=${MODEL_TYPE} --ref=/input/ucsc.hg19.chr20.unittest.fasta --reads=/input/${READS} --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --intermediate_results_dir /output/intermediate_results_dir --num_shards=${NUM_SHARDS}; I0921 06:50:39.795207 140086398105344 run_deepvariant.py:241] Re-using the directory for intermediate results in /output/intermediate_results",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354
https://github.com/google/deepvariant/issues/354:559,Availability,error,error,559,"**Describe the issue:**; `/opt/deepvariant/bin/run_deepvariant` crashes when there are no variants, because a VCF file gets written with 'default' as the sample name'. This happens because I use targetted sequencing, and when the capture fails, I get hardly any reads. This can be simulated by downsampling the quickstart data to 0.1%, see below. **Setup**; - HPC; - google/deepvariant:0.10.0; - Docker; - Targetted PacBio sequencing, aligned against HG38. **Does the quick start test work on your system?**; Yes. **Workaround, for version 1.0.0 only**; This error does not occur when using version 1.0.0, and explicitly passing the `--sample_name` flag to `run_deepvariant`. . **Steps to reproduce, using the quickstart data:**; ```bash; MODEL_TYPE=PACBIO; NUM_SHARDS=4; READS=NA12878_0.1_percent.bam. # Downsample the reads to 0.1%; samtools view -s 0.001 -b NA12878_S1.chr20.10_10p1mb.bam -o ${READS} --write-index. docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${MODEL_TYPE} \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/${READS} \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=${NUM_SHARDS}; ```. **Error trace**; ```bash; $ docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=${MODEL_TYPE} --ref=/input/ucsc.hg19.chr20.unittest.fasta --reads=/input/${READS} --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --intermediate_results_dir /output/intermediate_results_dir --num_shards=${NUM_SHARDS}; I0921 06:50:39.795207 140086398105344 run_deepvariant.py:241] Re-using the directory for intermediate results in /output/intermediate_results",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354
https://github.com/google/deepvariant/issues/354:805,Availability,Down,Downsample,805,"**Describe the issue:**; `/opt/deepvariant/bin/run_deepvariant` crashes when there are no variants, because a VCF file gets written with 'default' as the sample name'. This happens because I use targetted sequencing, and when the capture fails, I get hardly any reads. This can be simulated by downsampling the quickstart data to 0.1%, see below. **Setup**; - HPC; - google/deepvariant:0.10.0; - Docker; - Targetted PacBio sequencing, aligned against HG38. **Does the quick start test work on your system?**; Yes. **Workaround, for version 1.0.0 only**; This error does not occur when using version 1.0.0, and explicitly passing the `--sample_name` flag to `run_deepvariant`. . **Steps to reproduce, using the quickstart data:**; ```bash; MODEL_TYPE=PACBIO; NUM_SHARDS=4; READS=NA12878_0.1_percent.bam. # Downsample the reads to 0.1%; samtools view -s 0.001 -b NA12878_S1.chr20.10_10p1mb.bam -o ${READS} --write-index. docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${MODEL_TYPE} \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/${READS} \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=${NUM_SHARDS}; ```. **Error trace**; ```bash; $ docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=${MODEL_TYPE} --ref=/input/ucsc.hg19.chr20.unittest.fasta --reads=/input/${READS} --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --intermediate_results_dir /output/intermediate_results_dir --num_shards=${NUM_SHARDS}; I0921 06:50:39.795207 140086398105344 run_deepvariant.py:241] Re-using the directory for intermediate results in /output/intermediate_results",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354
https://github.com/google/deepvariant/issues/354:1398,Availability,Error,Error,1398,"em?**; Yes. **Workaround, for version 1.0.0 only**; This error does not occur when using version 1.0.0, and explicitly passing the `--sample_name` flag to `run_deepvariant`. . **Steps to reproduce, using the quickstart data:**; ```bash; MODEL_TYPE=PACBIO; NUM_SHARDS=4; READS=NA12878_0.1_percent.bam. # Downsample the reads to 0.1%; samtools view -s 0.001 -b NA12878_S1.chr20.10_10p1mb.bam -o ${READS} --write-index. docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${MODEL_TYPE} \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/${READS} \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=${NUM_SHARDS}; ```. **Error trace**; ```bash; $ docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=${MODEL_TYPE} --ref=/input/ucsc.hg19.chr20.unittest.fasta --reads=/input/${READS} --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --intermediate_results_dir /output/intermediate_results_dir --num_shards=${NUM_SHARDS}; I0921 06:50:39.795207 140086398105344 run_deepvariant.py:241] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 3 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_0.1_percent.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@4.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@4.gz"" --norealign_reads --regions ""chr20:10,000,000-1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354
https://github.com/google/deepvariant/issues/354:9644,Availability,checkpoint,checkpoint,9644,"0 candidate variants; I0921 06:50:42.154923 139913398449920 make_examples.py:535] Task 1/4: Created 0 examples; I0921 06:50:42.137424 140575591024384 make_examples.py:535] Task 3/4: 0 candidates (0 examples) [0.11s elapsed]; I0921 06:50:42.147503 140575591024384 make_examples.py:535] Task 3/4: Found 0 candidate variants; I0921 06:50:42.147607 140575591024384 make_examples.py:535] Task 3/4: Created 0 examples; I0921 06:50:42.173398 140201192457984 make_examples.py:535] Task 0/4: 0 candidates (0 examples) [0.14s elapsed]; I0921 06:50:42.195395 140201192457984 make_examples.py:535] Task 0/4: Found 0 candidate variants; I0921 06:50:42.195529 140201192457984 make_examples.py:535] Task 0/4: Created 0 examples. real	0m2.637s; user	0m8.272s; sys	0m1.419s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@4.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". I0921 06:50:44.216155 140117089298176 call_variants.py:316] Set KMP_BLOCKTIME to 0; W0921 06:50:44.234806 140117089298176 call_variants.py:323] Unable to read any records from /output/intermediate_results_dir/make_examples.tfrecord@4.gz. Output will contain zero records. real	0m1.979s; user	0m1.868s; sys	0m0.327s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@4.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". I0921 06:50:46.249888 139970848761600 postprocess_variants.py:966] call_variants_output is empty. Writing out empty VCF.; I0921 06:50:46.250269 139970848761600 postprocess_variants.py:1004] Merging and writing variants to VCF and gVCF.; I0921 06:50:46.250618 139970848761600 genomics_writer.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354
https://github.com/google/deepvariant/issues/354:480,Testability,test,test,480,"**Describe the issue:**; `/opt/deepvariant/bin/run_deepvariant` crashes when there are no variants, because a VCF file gets written with 'default' as the sample name'. This happens because I use targetted sequencing, and when the capture fails, I get hardly any reads. This can be simulated by downsampling the quickstart data to 0.1%, see below. **Setup**; - HPC; - google/deepvariant:0.10.0; - Docker; - Targetted PacBio sequencing, aligned against HG38. **Does the quick start test work on your system?**; Yes. **Workaround, for version 1.0.0 only**; This error does not occur when using version 1.0.0, and explicitly passing the `--sample_name` flag to `run_deepvariant`. . **Steps to reproduce, using the quickstart data:**; ```bash; MODEL_TYPE=PACBIO; NUM_SHARDS=4; READS=NA12878_0.1_percent.bam. # Downsample the reads to 0.1%; samtools view -s 0.001 -b NA12878_S1.chr20.10_10p1mb.bam -o ${READS} --write-index. docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${MODEL_TYPE} \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/${READS} \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=${NUM_SHARDS}; ```. **Error trace**; ```bash; $ docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=${MODEL_TYPE} --ref=/input/ucsc.hg19.chr20.unittest.fasta --reads=/input/${READS} --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --intermediate_results_dir /output/intermediate_results_dir --num_shards=${NUM_SHARDS}; I0921 06:50:39.795207 140086398105344 run_deepvariant.py:241] Re-using the directory for intermediate results in /output/intermediate_results",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354
https://github.com/google/deepvariant/issues/355:171,Availability,ERROR,ERROR,171,"Hi, I am trying to build DeepVariant from source, and I encounter the following issue in build_and_test. I have bazel 0.26.1 compiled from source as well. ```; (16:39:00) ERROR: Analysis of target '//deepvariant:make_examples_utils_test' failed; build aborted: . /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/bazel_tools/tools/jdk/BUILD:487:14: Configurable attribute ""actual"" doesn't match this configuration: Could not find a JDK for host execution environment, please explicitly provide one using `--host_javabase.`; ```; I tried passing the argument ""--host_javabase=@local_jdk//:jdk"" to bazel to no avail. Java:; ```; # java -version; openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:631,Availability,avail,avail,631,"Hi, I am trying to build DeepVariant from source, and I encounter the following issue in build_and_test. I have bazel 0.26.1 compiled from source as well. ```; (16:39:00) ERROR: Analysis of target '//deepvariant:make_examples_utils_test' failed; build aborted: . /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/bazel_tools/tools/jdk/BUILD:487:14: Configurable attribute ""actual"" doesn't match this configuration: Could not find a JDK for host execution environment, please explicitly provide one using `--host_javabase.`; ```; I tried passing the argument ""--host_javabase=@local_jdk//:jdk"" to bazel to no avail. Java:; ```; # java -version; openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:423,Deployability,configurat,configuration,423,"Hi, I am trying to build DeepVariant from source, and I encounter the following issue in build_and_test. I have bazel 0.26.1 compiled from source as well. ```; (16:39:00) ERROR: Analysis of target '//deepvariant:make_examples_utils_test' failed; build aborted: . /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/bazel_tools/tools/jdk/BUILD:487:14: Configurable attribute ""actual"" doesn't match this configuration: Could not find a JDK for host execution environment, please explicitly provide one using `--host_javabase.`; ```; I tried passing the argument ""--host_javabase=@local_jdk//:jdk"" to bazel to no avail. Java:; ```; # java -version; openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:372,Modifiability,Config,Configurable,372,"Hi, I am trying to build DeepVariant from source, and I encounter the following issue in build_and_test. I have bazel 0.26.1 compiled from source as well. ```; (16:39:00) ERROR: Analysis of target '//deepvariant:make_examples_utils_test' failed; build aborted: . /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/bazel_tools/tools/jdk/BUILD:487:14: Configurable attribute ""actual"" doesn't match this configuration: Could not find a JDK for host execution environment, please explicitly provide one using `--host_javabase.`; ```; I tried passing the argument ""--host_javabase=@local_jdk//:jdk"" to bazel to no avail. Java:; ```; # java -version; openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:423,Modifiability,config,configuration,423,"Hi, I am trying to build DeepVariant from source, and I encounter the following issue in build_and_test. I have bazel 0.26.1 compiled from source as well. ```; (16:39:00) ERROR: Analysis of target '//deepvariant:make_examples_utils_test' failed; build aborted: . /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/bazel_tools/tools/jdk/BUILD:487:14: Configurable attribute ""actual"" doesn't match this configuration: Could not find a JDK for host execution environment, please explicitly provide one using `--host_javabase.`; ```; I tried passing the argument ""--host_javabase=@local_jdk//:jdk"" to bazel to no avail. Java:; ```; # java -version; openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:270,Performance,cache,cache,270,"Hi, I am trying to build DeepVariant from source, and I encounter the following issue in build_and_test. I have bazel 0.26.1 compiled from source as well. ```; (16:39:00) ERROR: Analysis of target '//deepvariant:make_examples_utils_test' failed; build aborted: . /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/bazel_tools/tools/jdk/BUILD:487:14: Configurable attribute ""actual"" doesn't match this configuration: Could not find a JDK for host execution environment, please explicitly provide one using `--host_javabase.`; ```; I tried passing the argument ""--host_javabase=@local_jdk//:jdk"" to bazel to no avail. Java:; ```; # java -version; openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:252,Safety,abort,aborted,252,"Hi, I am trying to build DeepVariant from source, and I encounter the following issue in build_and_test. I have bazel 0.26.1 compiled from source as well. ```; (16:39:00) ERROR: Analysis of target '//deepvariant:make_examples_utils_test' failed; build aborted: . /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/bazel_tools/tools/jdk/BUILD:487:14: Configurable attribute ""actual"" doesn't match this configuration: Could not find a JDK for host execution environment, please explicitly provide one using `--host_javabase.`; ```; I tried passing the argument ""--host_javabase=@local_jdk//:jdk"" to bazel to no avail. Java:; ```; # java -version; openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/356:1349,Availability,Error,Error,1349,"on. I am not an expert in bazel by any means, so any help in how to get around this issue is greatly appreciated. Here is the command used for build (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PYTHON_BIN_PATH=/opt/at11.0/bin/python \; PYTHON_LIB_PATH=/opt/at11.0/lib64/python3.6/site-packages \; TF_CONFIGURE_IOS=0 \; TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.0,7.0 \; TF_CUDA_VERSION=10.0 \; TF_CUDNN_VERSION=7 \; TF_NEED_CUDA=1 \; /bin/bash -c 'source external/bazel_tools/tools/genrule/genrul",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:1379,Availability,ERROR,ERROR,1379,"on. I am not an expert in bazel by any means, so any help in how to get around this issue is greatly appreciated. Here is the command used for build (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PYTHON_BIN_PATH=/opt/at11.0/bin/python \; PYTHON_LIB_PATH=/opt/at11.0/lib64/python3.6/site-packages \; TF_CONFIGURE_IOS=0 \; TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.0,7.0 \; TF_CUDA_VERSION=10.0 \; TF_CUDNN_VERSION=7 \; TF_NEED_CUDA=1 \; /bin/bash -c 'source external/bazel_tools/tools/genrule/genrul",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:1602,Availability,error,error,1602,"ld (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PYTHON_BIN_PATH=/opt/at11.0/bin/python \; PYTHON_LIB_PATH=/opt/at11.0/lib64/python3.6/site-packages \; TF_CONFIGURE_IOS=0 \; TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.0,7.0 \; TF_CUDA_VERSION=10.0 \; TF_CUDNN_VERSION=7 \; TF_NEED_CUDA=1 \; /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/external/org_tensorflow/tensorflow/tools/git/gen_git_source --generate external/local_config_git/gen/spec.json ext",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:82,Deployability,install,installation,82,"I am trying to build DeepVariant from source, and **trying to use a custom python installation rather than the standard one.** However, ```bazel test ``` fails because it tries to use the standard library python. The requisite python is accessible as ""python"" because it is in the PATH variable, but bazel seems to ignore that and looks for python in the standard location. I am not an expert in bazel by any means, so any help in how to get around this issue is greatly appreciated. Here is the command used for build (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/loca",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:618,Deployability,install,installed,618,"I am trying to build DeepVariant from source, and **trying to use a custom python installation rather than the standard one.** However, ```bazel test ``` fails because it tries to use the standard library python. The requisite python is accessible as ""python"" because it is in the PATH variable, but bazel seems to ignore that and looks for python in the standard location. I am not an expert in bazel by any means, so any help in how to get around this issue is greatly appreciated. Here is the command used for build (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/loca",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:3410,Deployability,release,release,3410,"22bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PYTHON_BIN_PATH=/opt/at11.0/bin/python \; PYTHON_LIB_PATH=/opt/at11.0/lib64/python3.6/site-packages \; TF_CONFIGURE_IOS=0 \; TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.0,7.0 \; TF_CUDA_VERSION=10.0 \; TF_CUDNN_VERSION=7 \; TF_NEED_CUDA=1 \; /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/external/org_tensorflow/tensorflow/tools/git/gen_git_source --generate external/local_config_git/gen/spec.json external/local_config_git/gen/head external/local_config_git/gen/branch_ref ""bazel-out/ppc-opt/bin/external/org_tensorflow/tensorflow/core/util/version_info.cc"" --git_tag_override=${GIT_TAG_OVERRIDE:-}'); Execution platform: @bazel_tools//platforms:host_platform; Traceback (most recent call last):; File ""bazel-out/host/bin/external/org_tensorflow/tensorflow/tools/git/gen_git_source"", line 252, in <module>; Main(); File ""bazel-out/host/bin/external/org_tensorflow/tensorflow/tools/git/gen_git_source"", line 242, in Main; os.execv(args[0], args); FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3.6'; (15:44:57) INFO: Elapsed time: 34.327s, Critical Path: 15.32s; (15:44:57) INFO: 910 processes: 910 local.; (15:44:57) FAILED: Build did NOT complete successfully; ```. I am running on a CentOS 7 docker container. I am trying to build DeepVariant 1.0 (the current github release).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:286,Modifiability,variab,variable,286,"I am trying to build DeepVariant from source, and **trying to use a custom python installation rather than the standard one.** However, ```bazel test ``` fails because it tries to use the standard library python. The requisite python is accessible as ""python"" because it is in the PATH variable, but bazel seems to ignore that and looks for python in the standard location. I am not an expert in bazel by any means, so any help in how to get around this issue is greatly appreciated. Here is the command used for build (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/loca",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:1393,Performance,cache,cache,1393,"ld (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PYTHON_BIN_PATH=/opt/at11.0/bin/python \; PYTHON_LIB_PATH=/opt/at11.0/lib64/python3.6/site-packages \; TF_CONFIGURE_IOS=0 \; TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.0,7.0 \; TF_CUDA_VERSION=10.0 \; TF_CUDNN_VERSION=7 \; TF_NEED_CUDA=1 \; /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/external/org_tensorflow/tensorflow/tools/git/gen_git_source --generate external/local_config_git/gen/spec.json ext",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:1639,Performance,cache,cache,1639,"test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PYTHON_BIN_PATH=/opt/at11.0/bin/python \; PYTHON_LIB_PATH=/opt/at11.0/lib64/python3.6/site-packages \; TF_CONFIGURE_IOS=0 \; TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.0,7.0 \; TF_CUDA_VERSION=10.0 \; TF_CUDNN_VERSION=7 \; TF_NEED_CUDA=1 \; /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/external/org_tensorflow/tensorflow/tools/git/gen_git_source --generate external/local_config_git/gen/spec.json external/local_config_git/gen/head external/local_config_git/gen/branch_ref ""bazel-out/ppc-opt/bin/external/org_tensorflow/tensorflow/core/util/version_info.cc"" --git_tag_override=${GIT_TAG_OVERRIDE:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:237,Security,access,accessible,237,"I am trying to build DeepVariant from source, and **trying to use a custom python installation rather than the standard one.** However, ```bazel test ``` fails because it tries to use the standard library python. The requisite python is accessible as ""python"" because it is in the PATH variable, but bazel seems to ignore that and looks for python in the standard location. I am not an expert in bazel by any means, so any help in how to get around this issue is greatly appreciated. Here is the command used for build (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/loca",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:145,Testability,test,test,145,"I am trying to build DeepVariant from source, and **trying to use a custom python installation rather than the standard one.** However, ```bazel test ``` fails because it tries to use the standard library python. The requisite python is accessible as ""python"" because it is in the PATH variable, but bazel seems to ignore that and looks for python in the standard location. I am not an expert in bazel by any means, so any help in how to get around this issue is greatly appreciated. Here is the command used for build (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/loca",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:757,Testability,test,test,757,"I am trying to build DeepVariant from source, and **trying to use a custom python installation rather than the standard one.** However, ```bazel test ``` fails because it tries to use the standard library python. The requisite python is accessible as ""python"" because it is in the PATH variable, but bazel seems to ignore that and looks for python in the standard location. I am not an expert in bazel by any means, so any help in how to get around this issue is greatly appreciated. Here is the command used for build (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/loca",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/357:18,Performance,load,load,18,"Hi,. I have a big load of data to genotype and I did some tests using subsets of my data. My question is if will work use DV default parameter to a Plant genome. I have no gold set for training model. There is any suggestions?. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:58,Testability,test,tests,58,"Hi,. I have a big load of data to genotype and I did some tests using subsets of my data. My question is if will work use DV default parameter to a Plant genome. I have no gold set for training model. There is any suggestions?. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/358:1431,Availability,Error,Error,1431,"51.06 CUDA Version: 11.0 ; GeForce RTX 2070 super. **Workaround**; Apparently the gpu module is consuming all my memmory (8gb), possilbe "" config.gpu_options.allow_growth = True"" not present in the script?. **Command line**. `BIN_VERSION=""1.0.0""`; `BASE=""${PWD}/deepvariant-run""`; `INPUT_DIR=""${BASE}/input""`; `REF=""10consensus.fasta""`; `REF2=""reftst.fa""`; `BAM=""268_041_m10.sorted.bam""`; `BAM2=""tst.sorted.bam""`; `OUTPUT_DIR=""${BASE}/output""`; `DATA_DIR=""${INPUT_DIR}/data""`; `OUTPUT_VCF=""M10.output.vcf.gz""`; `OUTPUT_VCF2=""TST.output.vcf.gz""`; `OUTPUT_GVCF=""M10.output.g.vcf.gz""`; `OUTPUT_GVCF2=""TST.output.g.vcf.gz""`; `sudo docker run --gpus 1 -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}-gpu"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""/input/${REF2}"" --reads=""/input/${BAM2}"" --output_vcf=/output/${OUTPUT_VCF} --output_gvcf=/output/${OUTPUT_GVCF} --intermediate_results_dir /output/intermediate_results_dir --num_shards=30`. **Error trace**; ................ 2020-09-24 03:47:35.386802: W third_party/nucleus/io/sam_reader.cc:534] Could not read base quality scores GWNJ-1012:204:GW191209000:1:1101:22544:2049: Not found: Could not read base quality scores; I0924 03:47:35.394492 139826099087104 make_examples.py:587] Task 28/30: Found 88 candidate variants; I0924 03:47:35.394706 139826099087104 make_examples.py:587] Task 28/30: Created 88 examples; I0924 03:47:35.416212 139915800631040 make_examples.py:587] Task 9/30: Found 74 candidate variants; I0924 03:47:35.416471 139915800631040 make_examples.py:587] Task 9/30: Created 76 examples; I0924 03:47:35.441959 139746083813120 make_examples.py:587] Task 29/30: Found 78 candidate variants; I0924 03:47:35.442209 139746083813120 make_examples.py:587] Task 29/30: Created 78 examples. real	0m5.429s; user	2m1.568s; sys	0m23.089s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:2515,Availability,checkpoint,checkpoint,2515,":534] Could not read base quality scores GWNJ-1012:204:GW191209000:1:1101:22544:2049: Not found: Could not read base quality scores; I0924 03:47:35.394492 139826099087104 make_examples.py:587] Task 28/30: Found 88 candidate variants; I0924 03:47:35.394706 139826099087104 make_examples.py:587] Task 28/30: Created 88 examples; I0924 03:47:35.416212 139915800631040 make_examples.py:587] Task 9/30: Found 74 candidate variants; I0924 03:47:35.416471 139915800631040 make_examples.py:587] Task 9/30: Created 76 examples; I0924 03:47:35.441959 139746083813120 make_examples.py:587] Task 29/30: Found 78 candidate variants; I0924 03:47:35.442209 139746083813120 make_examples.py:587] Task 29/30: Created 78 examples. real	0m5.429s; user	2m1.568s; sys	0m23.089s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0924 03:47:37.408303 140325876573952 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-09-24 03:47:37.413854: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA; 2020-09-24 03:47:37.437208: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz; 2020-09-24 03:47:37.440001: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e41920 executing computations on platform Host. Devices:; 2020-09-24 03:47:37.440048: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-09-24 03:47:37.444991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1; 2020-09-24 03:47:37.554617: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5ea0f10 executing computations on platform CUDA. Devices:; 20",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:12321,Availability,error,error,12321,"3952 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt; 2020-09-24 03:47:45.236844: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7; 2020-09-24 03:47:45.652085: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] **Could not create cudnn handle:** CUDNN_STATUS_INTERNAL_ERROR; 2020-09-24 03:47:45.654628: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call; return fn(*args); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn; target_list, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun; run_metadata); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Ba",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:12866,Availability,error,errors,12866,"al/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call; return fn(*args); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn; target_list, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun; run_metadata); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.6/dist-packages/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:15616,Availability,error,error,15616,"ng/monitored_session.py"", line 1345, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1418, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1176, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 956, in run; run_metadata_ptr); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run; feed_dict_tensor, options, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run; run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/di",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:16347,Availability,error,errors,16347,"lient/session.py"", line 1359, in _do_run; run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:22602,Availability,checkpoint,checkpoint,22602,"10, in conv2d; name=name); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py"", line 1071, in conv2d; data_format=data_format, dilations=dilations, name=name); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py"", line 793, in _apply_op_helper; op_def=op_def); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py"", line 3360, in create_op; attrs, op_def, compute_device); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py"", line 3429, in _create_op_internal; op_def=op_def); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py"", line 1751, in __init__; self._traceback = tf_stack.extract_stack(). real	0m10.613s; user	0m11.112s; sys	0m4.718s; I0924 03:47:46.482943 140410383501056 run_deepvariant.py:364] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@30.gz"" --checkpoint ""/opt/models/wgs/model.ckpt""' returned non-zero exit status 1. falllowing my ndvida-smi it consumes all the memmory, htere is a way to limit memmory?. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:360,Deployability,release,release,360,"**Describe the issue:**; /opt/deepvariant/bin/run_deepvariant crashes when start the GPU stage of call variants. **Setup**; google/deepvariant:0.10.0; Docker; subset of illumina resequencing data; $nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2017 NVIDIA Corporation; Built on Fri_Nov__3_21:07:56_CDT_2017; Cuda compilation tools, release 9.1, V9.1.85; $ nvidia-smi; NVIDIA-SMI 450.51.06 Driver Version: 450.51.06 CUDA Version: 11.0 ; GeForce RTX 2070 super. **Workaround**; Apparently the gpu module is consuming all my memmory (8gb), possilbe "" config.gpu_options.allow_growth = True"" not present in the script?. **Command line**. `BIN_VERSION=""1.0.0""`; `BASE=""${PWD}/deepvariant-run""`; `INPUT_DIR=""${BASE}/input""`; `REF=""10consensus.fasta""`; `REF2=""reftst.fa""`; `BAM=""268_041_m10.sorted.bam""`; `BAM2=""tst.sorted.bam""`; `OUTPUT_DIR=""${BASE}/output""`; `DATA_DIR=""${INPUT_DIR}/data""`; `OUTPUT_VCF=""M10.output.vcf.gz""`; `OUTPUT_VCF2=""TST.output.vcf.gz""`; `OUTPUT_GVCF=""M10.output.g.vcf.gz""`; `OUTPUT_GVCF2=""TST.output.g.vcf.gz""`; `sudo docker run --gpus 1 -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}-gpu"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""/input/${REF2}"" --reads=""/input/${BAM2}"" --output_vcf=/output/${OUTPUT_VCF} --output_gvcf=/output/${OUTPUT_GVCF} --intermediate_results_dir /output/intermediate_results_dir --num_shards=30`. **Error trace**; ................ 2020-09-24 03:47:35.386802: W third_party/nucleus/io/sam_reader.cc:534] Could not read base quality scores GWNJ-1012:204:GW191209000:1:1101:22544:2049: Not found: Could not read base quality scores; I0924 03:47:35.394492 139826099087104 make_examples.py:587] Task 28/30: Found 88 candidate variants; I0924 03:47:35.394706 139826099087104 make_examples.py:587] Task 28/30: Created 88 examples; I0924 03:47:35.416212 139915800631040 make_examples.py:587] Task 9/30: Found 74 candidate variants; I0924 03:47:35.416471 139915800631040 make_ex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:12480,Integrability,message,message,12480,"or/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7; 2020-09-24 03:47:45.652085: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] **Could not create cudnn handle:** CUDNN_STATUS_INTERNAL_ERROR; 2020-09-24 03:47:45.654628: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call; return fn(*args); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn; target_list, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun; run_metadata); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_dgqnmzud/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:12711,Integrability,message,message,12711," 2020-09-24 03:47:45.654628: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call; return fn(*args); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn; target_list, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun; run_metadata); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Ba",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:15545,Integrability,message,message,15545,"aise value; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1345, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1418, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1176, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 956, in run; run_metadata_ptr); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run; feed_dict_tensor, options, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run; run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:15775,Integrability,message,message,15775,"s/tensorflow_core/python/training/monitored_session.py"", line 1418, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1176, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 956, in run; run_metadata_ptr); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run; feed_dict_tensor, options, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run; run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolera",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:16099,Integrability,message,message,16099,"python/client/session.py"", line 956, in run; run_metadata_ptr); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run; feed_dict_tensor, options, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run; run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:19729,Integrability,wrap,wrapper,19729,"lim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1089, in convolution; outputs = layer.apply(inputs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 324, in new_func; return func(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 1695, in apply; return self.__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/base.py"", line 548, in __call__; outputs = super(Layer, self).__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 847, in __call__; outputs = call_fn(cast_inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 234, in wrapper; return converted_call(f, options, args, kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 439, in converted_call; return _call_unconverted(f, args, kwargs, options); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 330, in _call_unconverted; return f(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/convolutional.py"", line 197, in call; outputs = self._convolution_op(inputs, self.kernel); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py"", line 1134, in __call__; return self.conv_op(inp, filter); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py"", line 639, in __call__; return self.call(inp, filter); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py"", line 238, in __call__; name=self.name); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:576,Modifiability,config,config,576,"**Describe the issue:**; /opt/deepvariant/bin/run_deepvariant crashes when start the GPU stage of call variants. **Setup**; google/deepvariant:0.10.0; Docker; subset of illumina resequencing data; $nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2017 NVIDIA Corporation; Built on Fri_Nov__3_21:07:56_CDT_2017; Cuda compilation tools, release 9.1, V9.1.85; $ nvidia-smi; NVIDIA-SMI 450.51.06 Driver Version: 450.51.06 CUDA Version: 11.0 ; GeForce RTX 2070 super. **Workaround**; Apparently the gpu module is consuming all my memmory (8gb), possilbe "" config.gpu_options.allow_growth = True"" not present in the script?. **Command line**. `BIN_VERSION=""1.0.0""`; `BASE=""${PWD}/deepvariant-run""`; `INPUT_DIR=""${BASE}/input""`; `REF=""10consensus.fasta""`; `REF2=""reftst.fa""`; `BAM=""268_041_m10.sorted.bam""`; `BAM2=""tst.sorted.bam""`; `OUTPUT_DIR=""${BASE}/output""`; `DATA_DIR=""${INPUT_DIR}/data""`; `OUTPUT_VCF=""M10.output.vcf.gz""`; `OUTPUT_VCF2=""TST.output.vcf.gz""`; `OUTPUT_GVCF=""M10.output.g.vcf.gz""`; `OUTPUT_GVCF2=""TST.output.g.vcf.gz""`; `sudo docker run --gpus 1 -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}-gpu"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""/input/${REF2}"" --reads=""/input/${BAM2}"" --output_vcf=/output/${OUTPUT_VCF} --output_gvcf=/output/${OUTPUT_GVCF} --intermediate_results_dir /output/intermediate_results_dir --num_shards=30`. **Error trace**; ................ 2020-09-24 03:47:35.386802: W third_party/nucleus/io/sam_reader.cc:534] Could not read base quality scores GWNJ-1012:204:GW191209000:1:1101:22544:2049: Not found: Could not read base quality scores; I0924 03:47:35.394492 139826099087104 make_examples.py:587] Task 28/30: Found 88 candidate variants; I0924 03:47:35.394706 139826099087104 make_examples.py:587] Task 28/30: Created 88 examples; I0924 03:47:35.416212 139915800631040 make_examples.py:587] Task 9/30: Found 74 candidate variants; I0924 03:47:35.416471 139915800631040 make_ex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:5992,Modifiability,config,config,5992,"u_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:; 2020-09-24 03:47:37.577462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165] 0 ; 2020-09-24 03:47:37.577470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0: N ; 2020-09-24 03:47:37.578993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 **with 6199 MB memory**) -> physical GPU (device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:21:00.0, compute capability: 7.5); W0924 03:47:37.676500 140325876573952 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp3gvrq0ei; I0924 03:47:37.676881 140325876573952 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp3gvrq0ei', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9f898d3630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0924 03:47:37.677164 140325876573952 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz; W0924 03:47:37.681965 140325876573952 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.; Instructions ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:7317,Modifiability,layers,layers,7317,"n_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9f898d3630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0924 03:47:37.677164 140325876573952 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz; W0924 03:47:37.681965 140325876573952 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.; Instructions for updating:; If using Keras pass *_constraint arguments to layers.; W0924 03:47:37.690693 140325876573952 deprecation.py:323] From /tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; W0924 03:47:37.814187 140325876573952 deprecation.py:323] From /tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:8563,Modifiability,layers,layers,8563,"ave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; W0924 03:47:37.814187 140325876573952 deprecation.py:323] From /tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0924 03:47:38.164505 140325876573952 estimator.py:1147] Calling model_fn.; W0924 03:47:38.168455 140325876573952 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0924 03:47:41.667636 140325876573952 estimator.py:1149] Done calling model_fn.; I0924 03:47:42.548214 140325876573952 monitored_session.py:240] Graph was finalized.; 2020-09-24 03:47:42.549039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: ; name: GeForce RTX 2070 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.77; pciBusID: 0000:21:00.0; 2020-09-24 03:47:42.549107: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0; 2020-09-24 03:47:42.549121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0; 2020-09-24 03:47:42.549131: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfull",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:8570,Modifiability,layers,layers,8570,"ave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; W0924 03:47:37.814187 140325876573952 deprecation.py:323] From /tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0924 03:47:38.164505 140325876573952 estimator.py:1147] Calling model_fn.; W0924 03:47:38.168455 140325876573952 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0924 03:47:41.667636 140325876573952 estimator.py:1149] Done calling model_fn.; I0924 03:47:42.548214 140325876573952 monitored_session.py:240] Graph was finalized.; 2020-09-24 03:47:42.549039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: ; name: GeForce RTX 2070 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.77; pciBusID: 0000:21:00.0; 2020-09-24 03:47:42.549107: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0; 2020-09-24 03:47:42.549121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0; 2020-09-24 03:47:42.549131: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfull",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:17425,Modifiability,config,config,17425,"tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 622, in predict; features, None, ModeKeys.PREDICT, self.config); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1148, in _call_model_fn; model_fn_results = self._model_fn(features=features, **kwargs); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 914, in model_fn; is_training=mode == tf.estimator.ModeKeys.TRAIN); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 744, in create; return self._create(images, num_classes, is_training); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 1122, in _create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 587, in inception_v3; depth_multiplier=depth_multiplier); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 117, in inception_v3_base; net = layers.conv2d(inputs, depth(32), [3, 3], stride=",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:18396,Modifiability,layers,layers,18396,"redict; features, None, ModeKeys.PREDICT, self.config); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1148, in _call_model_fn; model_fn_results = self._model_fn(features=features, **kwargs); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 914, in model_fn; is_training=mode == tf.estimator.ModeKeys.TRAIN); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 744, in create; return self._create(images, num_classes, is_training); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 1122, in _create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 587, in inception_v3; depth_multiplier=depth_multiplier); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 117, in inception_v3_base; net = layers.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1191, in convolution2d; conv_dims=2); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1089, in convolution; outputs = layer.apply(inputs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 324, in new_func; return func(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 1695, in apply; return self.__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/base.py"", line 548",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:18653,Modifiability,layers,layers,18653,"s_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 914, in model_fn; is_training=mode == tf.estimator.ModeKeys.TRAIN); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 744, in create; return self._create(images, num_classes, is_training); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 1122, in _create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 587, in inception_v3; depth_multiplier=depth_multiplier); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 117, in inception_v3_base; net = layers.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1191, in convolution2d; conv_dims=2); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1089, in convolution; outputs = layer.apply(inputs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 324, in new_func; return func(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 1695, in apply; return self.__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/base.py"", line 548, in __call__; outputs = super(Layer, self).__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 847, in __call__; outputs = call_fn(cast_inputs, *args, **kwargs); File ""usr/local/lib/pyt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:18660,Modifiability,layers,layers,18660,"s_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 914, in model_fn; is_training=mode == tf.estimator.ModeKeys.TRAIN); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 744, in create; return self._create(images, num_classes, is_training); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 1122, in _create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 587, in inception_v3; depth_multiplier=depth_multiplier); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 117, in inception_v3_base; net = layers.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1191, in convolution2d; conv_dims=2); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1089, in convolution; outputs = layer.apply(inputs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 324, in new_func; return func(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 1695, in apply; return self.__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/base.py"", line 548, in __call__; outputs = super(Layer, self).__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 847, in __call__; outputs = call_fn(cast_inputs, *args, **kwargs); File ""usr/local/lib/pyt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:18903,Modifiability,layers,layers,18903,"ate; return self._create(images, num_classes, is_training); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 1122, in _create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 587, in inception_v3; depth_multiplier=depth_multiplier); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 117, in inception_v3_base; net = layers.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1191, in convolution2d; conv_dims=2); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1089, in convolution; outputs = layer.apply(inputs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 324, in new_func; return func(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 1695, in apply; return self.__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/base.py"", line 548, in __call__; outputs = super(Layer, self).__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 847, in __call__; outputs = call_fn(cast_inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 234, in wrapper; return converted_call(f, options, args, kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 439, in convert",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:18910,Modifiability,layers,layers,18910,"ate; return self._create(images, num_classes, is_training); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 1122, in _create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 587, in inception_v3; depth_multiplier=depth_multiplier); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 117, in inception_v3_base; net = layers.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1191, in convolution2d; conv_dims=2); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1089, in convolution; outputs = layer.apply(inputs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 324, in new_func; return func(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 1695, in apply; return self.__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/base.py"", line 548, in __call__; outputs = super(Layer, self).__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 847, in __call__; outputs = call_fn(cast_inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 234, in wrapper; return converted_call(f, options, args, kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 439, in convert",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:19353,Modifiability,layers,layers,19353,"/inception_v3.py"", line 117, in inception_v3_base; net = layers.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1191, in convolution2d; conv_dims=2); File ""usr/local/lib/python3.6/dist-packages/tf_slim/ops/arg_scope.py"", line 184, in func_with_args; return func(*args, **current_args); File ""usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py"", line 1089, in convolution; outputs = layer.apply(inputs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 324, in new_func; return func(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 1695, in apply; return self.__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/base.py"", line 548, in __call__; outputs = super(Layer, self).__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 847, in __call__; outputs = call_fn(cast_inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 234, in wrapper; return converted_call(f, options, args, kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 439, in converted_call; return _call_unconverted(f, args, kwargs, options); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 330, in _call_unconverted; return f(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/convolutional.py"", line 197, in call; outputs = self._convolution_op(inputs, self.kernel); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:20182,Modifiability,layers,layers,20182,"orflow_core/python/keras/engine/base_layer.py"", line 1695, in apply; return self.__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/base.py"", line 548, in __call__; outputs = super(Layer, self).__call__(inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 847, in __call__; outputs = call_fn(cast_inputs, *args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 234, in wrapper; return converted_call(f, options, args, kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 439, in converted_call; return _call_unconverted(f, args, kwargs, options); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py"", line 330, in _call_unconverted; return f(*args, **kwargs); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/convolutional.py"", line 197, in call; outputs = self._convolution_op(inputs, self.kernel); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py"", line 1134, in __call__; return self.conv_op(inp, filter); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py"", line 639, in __call__; return self.call(inp, filter); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py"", line 238, in __call__; name=self.name); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py"", line 2010, in conv2d; name=name); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py"", line 1071, in conv2d; data_format=data_format, dilations=dilations, name=name); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py"", line 793, in _apply_op_helper; op_def=op_def); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:8312,Performance,optimiz,optimizations,8312,"690693 140325876573952 deprecation.py:323] From /tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; W0924 03:47:37.814187 140325876573952 deprecation.py:323] From /tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0924 03:47:38.164505 140325876573952 estimator.py:1147] Calling model_fn.; W0924 03:47:38.168455 140325876573952 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0924 03:47:41.667636 140325876573952 estimator.py:1149] Done calling model_fn.; I0924 03:47:42.548214 140325876573952 monitored_session.py:240] Graph was finalized.; 2020-09-24 03:47:42.549039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: ; name: GeForce RTX 2070 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.77; pciBusID: 0000:21:00.0; 2020-09-24 03:47:42.549107: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0; 2020-09-24 03:47:42.549121: I ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:13791,Safety,predict,prediction,13791,"oftmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 640, in predict; preds_evaluated = mon_sess.run(predictions); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 754, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1259, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1360, in run; raise six.reraise(*original_exc_info); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/six_archive/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1345, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 14",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:13809,Safety,predict,predictions,13809,"oftmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 640, in predict; preds_evaluated = mon_sess.run(predictions); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 754, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1259, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1360, in run; raise six.reraise(*original_exc_info); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/six_archive/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1345, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 14",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:13934,Safety,predict,predict,13934,"urred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 640, in predict; preds_evaluated = mon_sess.run(predictions); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 754, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1259, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1360, in run; raise six.reraise(*original_exc_info); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/six_archive/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1345, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1418, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py""",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:13974,Safety,predict,predictions,13974,"/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 640, in predict; preds_evaluated = mon_sess.run(predictions); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 754, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1259, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1360, in run; raise six.reraise(*original_exc_info); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/six_archive/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1345, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1418, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1176, in run; return self._sess.run(*args, *",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:17235,Safety,predict,prediction,17235,"ython/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 622, in predict; features, None, ModeKeys.PREDICT, self.config); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1148, in _call_model_fn; model_fn_results = self._model_fn(features=features, **kwargs); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 914, in model_fn; is_training=mode == tf.estimator.ModeKeys.TRAIN); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 744, in create; return self._create(images, num_classes, is_training); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 1122, in _create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 587, in inception_v3; de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:17253,Safety,predict,predictions,17253,"ython/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 622, in predict; features, None, ModeKeys.PREDICT, self.config); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1148, in _call_model_fn; model_fn_results = self._model_fn(features=features, **kwargs); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 914, in model_fn; is_training=mode == tf.estimator.ModeKeys.TRAIN); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 744, in create; return self._create(images, num_classes, is_training); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 1122, in _create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 587, in inception_v3; de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:17377,Safety,predict,predict,17377,"or 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 622, in predict; features, None, ModeKeys.PREDICT, self.config); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1148, in _call_model_fn; model_fn_results = self._model_fn(features=features, **kwargs); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 914, in model_fn; is_training=mode == tf.estimator.ModeKeys.TRAIN); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 744, in create; return self._create(images, num_classes, is_training); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 1122, in _create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 587, in inception_v3; depth_multiplier=depth_multiplier); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 117, in inception_v3_b",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:17411,Safety,PREDICT,PREDICT,17411,"d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 622, in predict; features, None, ModeKeys.PREDICT, self.config); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1148, in _call_model_fn; model_fn_results = self._model_fn(features=features, **kwargs); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 914, in model_fn; is_training=mode == tf.estimator.ModeKeys.TRAIN); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 744, in create; return self._create(images, num_classes, is_training); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 1122, in _create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 587, in inception_v3; depth_multiplier=depth_multiplier); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 117, in inception_v3_base; net = layers.conv2d(inputs, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:12476,Testability,log,log,12476,"or/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7; 2020-09-24 03:47:45.652085: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] **Could not create cudnn handle:** CUDNN_STATUS_INTERNAL_ERROR; 2020-09-24 03:47:45.654628: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call; return fn(*args); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn; target_list, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun; run_metadata); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_dgqnmzud/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:12707,Testability,log,log,12707," 2020-09-24 03:47:45.654628: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call; return fn(*args); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn; target_list, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun; run_metadata); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Ba",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:15771,Testability,log,log,15771,"s/tensorflow_core/python/training/monitored_session.py"", line 1418, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1176, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 956, in run; run_metadata_ptr); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run; feed_dict_tensor, options, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run; run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolera",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:16095,Testability,log,log,16095,"python/client/session.py"", line 956, in run; run_metadata_ptr); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run; feed_dict_tensor, options, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run; run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/359:1029,Deployability,Install,Installation,1029,"**Issue**; I am using the docker you provided, while working on a remote machine.; Using Pycharm Professional's Services tab, I configured my interpreter to run the code I have on my local clone of the entire git. ; I am trying to run the ""make_examples.py"" file line-by-line, to understand it better. The entire clone is on my remote machine, and it runs with the docker container's interpreter. When I debug the code, there are many unresolved references. ; Some examples are:; `from third_party.nucleus.protos import reads_pb2`; `from deepvariant.protos import deepvariant_pb2`; `from deepvariant.python import pileup_image_native`; `from deepvariant.protos import deepvariant_pb2`; `from deepvariant.python import allelecounter`; `from third_party.nucleus.io.python import hts_verbose`; ...; I looked for these files, and they aren't there.; I understand there is something very basic that I misunderstand, so thanks in advance for your patience!. **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: irrelevant. **Does the quick start test work on your system?**; I have succeeded in running the quick start example on the remote machine, through the terminal.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:128,Modifiability,config,configured,128,"**Issue**; I am using the docker you provided, while working on a remote machine.; Using Pycharm Professional's Services tab, I configured my interpreter to run the code I have on my local clone of the entire git. ; I am trying to run the ""make_examples.py"" file line-by-line, to understand it better. The entire clone is on my remote machine, and it runs with the docker container's interpreter. When I debug the code, there are many unresolved references. ; Some examples are:; `from third_party.nucleus.protos import reads_pb2`; `from deepvariant.protos import deepvariant_pb2`; `from deepvariant.python import pileup_image_native`; `from deepvariant.protos import deepvariant_pb2`; `from deepvariant.python import allelecounter`; `from third_party.nucleus.io.python import hts_verbose`; ...; I looked for these files, and they aren't there.; I understand there is something very basic that I misunderstand, so thanks in advance for your patience!. **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: irrelevant. **Does the quick start test work on your system?**; I have succeeded in running the quick start example on the remote machine, through the terminal.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:1143,Testability,test,test,1143,"**Issue**; I am using the docker you provided, while working on a remote machine.; Using Pycharm Professional's Services tab, I configured my interpreter to run the code I have on my local clone of the entire git. ; I am trying to run the ""make_examples.py"" file line-by-line, to understand it better. The entire clone is on my remote machine, and it runs with the docker container's interpreter. When I debug the code, there are many unresolved references. ; Some examples are:; `from third_party.nucleus.protos import reads_pb2`; `from deepvariant.protos import deepvariant_pb2`; `from deepvariant.python import pileup_image_native`; `from deepvariant.protos import deepvariant_pb2`; `from deepvariant.python import allelecounter`; `from third_party.nucleus.io.python import hts_verbose`; ...; I looked for these files, and they aren't there.; I understand there is something very basic that I misunderstand, so thanks in advance for your patience!. **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: irrelevant. **Does the quick start test work on your system?**; I have succeeded in running the quick start example on the remote machine, through the terminal.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/360:906,Deployability,Install,Installation,906,"**Describe the issue:**; Shuffle script for tfrecords (https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-training-case-study.md) runs out of memory when using a training set from multiple BAM files. This is what I followed:; - Run make_examples for each BAM file to obtain tfrecords; - Run shuffle script (https://raw.githubusercontent.com/google/deepvariant/r1.0/tools/shuffle_tfrecords_beam.py) on all the records from all the BAM files. This requires over 230 GB of CPU RAM, and the process is eventually killed. I do not know whether the memory requirement will keep growing beyond this point. Is there another way to deal with this situation? For example, it would be possible to run shuffling for data from each bam file independently. However, I am not sure what the flow would look like after that point. **Setup**; - Operating system: Ubuntu Bionic; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/362:2,Availability,down,downloaded,2,"I downloaded your docker ( `docker pull google/deepvariant:1.0.0` ) and it runs perfectly and I am really happy with it. However, I wanted to check out some options, so I run your readme command:; `docker run google/deepvariant:1.0.0 --help`; which lead to following error:; ```; docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""exec: \""--help\"": executable file not found in $PATH"": unknown.; ERRO[0000] error waiting for container: context canceled; ```; should I change the command or is it an error with the image ? . do not worry it is not urgent, but I would be really keen on checking out more options.; thank you !",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362
https://github.com/google/deepvariant/issues/362:267,Availability,error,error,267,"I downloaded your docker ( `docker pull google/deepvariant:1.0.0` ) and it runs perfectly and I am really happy with it. However, I wanted to check out some options, so I run your readme command:; `docker run google/deepvariant:1.0.0 --help`; which lead to following error:; ```; docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""exec: \""--help\"": executable file not found in $PATH"": unknown.; ERRO[0000] error waiting for container: context canceled; ```; should I change the command or is it an error with the image ? . do not worry it is not urgent, but I would be really keen on checking out more options.; thank you !",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362
https://github.com/google/deepvariant/issues/362:288,Availability,Error,Error,288,"I downloaded your docker ( `docker pull google/deepvariant:1.0.0` ) and it runs perfectly and I am really happy with it. However, I wanted to check out some options, so I run your readme command:; `docker run google/deepvariant:1.0.0 --help`; which lead to following error:; ```; docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""exec: \""--help\"": executable file not found in $PATH"": unknown.; ERRO[0000] error waiting for container: context canceled; ```; should I change the command or is it an error with the image ? . do not worry it is not urgent, but I would be really keen on checking out more options.; thank you !",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362
https://github.com/google/deepvariant/issues/362:478,Availability,error,error,478,"I downloaded your docker ( `docker pull google/deepvariant:1.0.0` ) and it runs perfectly and I am really happy with it. However, I wanted to check out some options, so I run your readme command:; `docker run google/deepvariant:1.0.0 --help`; which lead to following error:; ```; docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""exec: \""--help\"": executable file not found in $PATH"": unknown.; ERRO[0000] error waiting for container: context canceled; ```; should I change the command or is it an error with the image ? . do not worry it is not urgent, but I would be really keen on checking out more options.; thank you !",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362
https://github.com/google/deepvariant/issues/362:570,Availability,error,error,570,"I downloaded your docker ( `docker pull google/deepvariant:1.0.0` ) and it runs perfectly and I am really happy with it. However, I wanted to check out some options, so I run your readme command:; `docker run google/deepvariant:1.0.0 --help`; which lead to following error:; ```; docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""exec: \""--help\"": executable file not found in $PATH"": unknown.; ERRO[0000] error waiting for container: context canceled; ```; should I change the command or is it an error with the image ? . do not worry it is not urgent, but I would be really keen on checking out more options.; thank you !",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362
https://github.com/google/deepvariant/pull/363:169,Testability,test,testdata,169,"* Build Docker image with OpenVINO support; ```; docker build -t deepvariant . --build-arg DV_OPENVINO_BUILD=1; ```. * Run; ```bash; export INPUT_DIR=""${PWD}/quickstart-testdata""; export OUTPUT_DIR=""${PWD}/quickstart-output"". docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; deepvariant \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --call_variants_extra_args=""use_openvino=True"" \; --num_shards=1; ```. (added extra flag `--call_variants_extra_args=""use_openvino=True""` comparing to original Getting Started)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363
https://github.com/google/deepvariant/issues/364:696,Safety,sanity check,sanity check,696,"Hi,. I am running the shuffle script (https://raw.githubusercontent.com/google/deepvariant/r1.0/tools/shuffle_tfrecords_beam.py) on some training data. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as [well](https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi)). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/364
https://github.com/google/deepvariant/issues/364:674,Usability,simpl,simply,674,"Hi,. I am running the shuffle script (https://raw.githubusercontent.com/google/deepvariant/r1.0/tools/shuffle_tfrecords_beam.py) on some training data. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as [well](https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi)). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/364
https://github.com/google/deepvariant/issues/364:746,Usability,simpl,simply,746,"Hi,. I am running the shuffle script (https://raw.githubusercontent.com/google/deepvariant/r1.0/tools/shuffle_tfrecords_beam.py) on some training data. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as [well](https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi)). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/364
https://github.com/google/deepvariant/issues/364:810,Usability,simpl,simply,810,"Hi,. I am running the shuffle script (https://raw.githubusercontent.com/google/deepvariant/r1.0/tools/shuffle_tfrecords_beam.py) on some training data. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as [well](https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi)). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/364
https://github.com/google/deepvariant/pull/365:193,Modifiability,Portab,PortableRunner,193,"I understand that PRs are not performed on github. So, I just wanted to recommend/discuss some potential changes for the shuffle_tfrecords_beam.py script to enable running it with SparkRunner (PortableRunner). Without these changes the script works only in LOOPBACK mode (which is a testing mode where the actual work is performed on the submitting host).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:30,Performance,perform,performed,30,"I understand that PRs are not performed on github. So, I just wanted to recommend/discuss some potential changes for the shuffle_tfrecords_beam.py script to enable running it with SparkRunner (PortableRunner). Without these changes the script works only in LOOPBACK mode (which is a testing mode where the actual work is performed on the submitting host).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:321,Performance,perform,performed,321,"I understand that PRs are not performed on github. So, I just wanted to recommend/discuss some potential changes for the shuffle_tfrecords_beam.py script to enable running it with SparkRunner (PortableRunner). Without these changes the script works only in LOOPBACK mode (which is a testing mode where the actual work is performed on the submitting host).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/pull/365:283,Testability,test,testing,283,"I understand that PRs are not performed on github. So, I just wanted to recommend/discuss some potential changes for the shuffle_tfrecords_beam.py script to enable running it with SparkRunner (PortableRunner). Without these changes the script works only in LOOPBACK mode (which is a testing mode where the actual work is performed on the submitting host).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365
https://github.com/google/deepvariant/issues/367:879,Availability,Error,Error,879,"**Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: Ubuntu; - DeepVariant version: 0.9.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: sample: CHM13, instrument: PacBio Sequel CCS sequencing, reference genome: CHM13 draft genome from T2T project. **Steps to reproduce:**; - Command:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/chm13.draft_v1.0.fasta \; --reads=/input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam \; --output_vcf=/output/CHM13.CHM13.minimap2_asm20.deepvariant_0.9.0.vcf.gz \; --output_gvcf=/output/CHM13.CHM13.minimap2_asm20.deepvariant_0.9.0.g.vcf.gz \; --num_shards=29; ```. - Error trace: (if applicable). ```; ***** Running the command:*****; time seq 0 28 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/chm13.draft_v1.0.fasta"" --reads ""/input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp. I1023 11:00:14.182121 140022713169664 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1023 11:00:14.268690 140022713169664 genomics_reader.py:223] Reading /input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam with NativeSamReader; I1023 11:00:14.297683 140022713169664 make_examples.py:1324] Preparing inputs; I1023 11:00:14.382807 140022713169664 genomics_reader.py:223] Reading /input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam with NativeSamReader; I1023 11:00:14.425673 140022713169664 make_examples.py:1248] Common contigs are [u'chr1', u'chr2', u'chr3', u'chr4', u'chr5', u'chr6', u'chr7', u'chr8', u'chr9', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'ch; I1023 11:00:14.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/367:153,Deployability,Install,Installation,153,"**Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: Ubuntu; - DeepVariant version: 0.9.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: sample: CHM13, instrument: PacBio Sequel CCS sequencing, reference genome: CHM13 draft genome from T2T project. **Steps to reproduce:**; - Command:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/chm13.draft_v1.0.fasta \; --reads=/input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam \; --output_vcf=/output/CHM13.CHM13.minimap2_asm20.deepvariant_0.9.0.vcf.gz \; --output_gvcf=/output/CHM13.CHM13.minimap2_asm20.deepvariant_0.9.0.g.vcf.gz \; --num_shards=29; ```. - Error trace: (if applicable). ```; ***** Running the command:*****; time seq 0 28 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/chm13.draft_v1.0.fasta"" --reads ""/input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp. I1023 11:00:14.182121 140022713169664 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1023 11:00:14.268690 140022713169664 genomics_reader.py:223] Reading /input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam with NativeSamReader; I1023 11:00:14.297683 140022713169664 make_examples.py:1324] Preparing inputs; I1023 11:00:14.382807 140022713169664 genomics_reader.py:223] Reading /input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam with NativeSamReader; I1023 11:00:14.425673 140022713169664 make_examples.py:1248] Common contigs are [u'chr1', u'chr2', u'chr3', u'chr4', u'chr5', u'chr6', u'chr7', u'chr8', u'chr9', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'ch; I1023 11:00:14.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/367:4178,Testability,test,test,4178,"M13.minimap2_asm20.primary_alignments.sorted.bam with NativeSamReader; W1023 11:00:14.858257 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:9; W1023 11:00:14.858535 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:30; W1023 11:00:14.858640 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:63; W1023 11:00:14.858738 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:107; W1023 11:00:14.858865 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:109; I1023 11:00:14.943413 140022713169664 make_examples.py:1363] Task 0: 26 candidates (21 examples) [0.51s elapsed]; I1023 11:00:34.047770 140022713169664 make_examples.py:1363] Task 0: 100 candidates (123 examples) [19.10s elapsed]; I1023 11:00:53.403723 140022713169664 make_examples.py:1363] Task 0: 200 candidates (255 examples) [19.36s elapsed]; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2020-10-23 11:01:17.042772: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 26013000 end: 26014000; I1023 11:00:13.973792 139819486582528 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes . **Any additional context:** . Deepvariant was successful on grch37 and grch38 reference genomes. I assume that the PacBio CCS model was trained on grch37 and grch38 reference genome and might not be applicable to other reference genomes. Can someone recommend a tweak to run deepvariant on CHM13 draft genome?. Regards,; Sangjin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/367:4214,Testability,test,test,4214,"M13.minimap2_asm20.primary_alignments.sorted.bam with NativeSamReader; W1023 11:00:14.858257 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:9; W1023 11:00:14.858535 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:30; W1023 11:00:14.858640 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:63; W1023 11:00:14.858738 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:107; W1023 11:00:14.858865 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:109; I1023 11:00:14.943413 140022713169664 make_examples.py:1363] Task 0: 26 candidates (21 examples) [0.51s elapsed]; I1023 11:00:34.047770 140022713169664 make_examples.py:1363] Task 0: 100 candidates (123 examples) [19.10s elapsed]; I1023 11:00:53.403723 140022713169664 make_examples.py:1363] Task 0: 200 candidates (255 examples) [19.36s elapsed]; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2020-10-23 11:01:17.042772: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 26013000 end: 26014000; I1023 11:00:13.973792 139819486582528 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes . **Any additional context:** . Deepvariant was successful on grch37 and grch38 reference genomes. I assume that the PacBio CCS model was trained on grch37 and grch38 reference genome and might not be applicable to other reference genomes. Can someone recommend a tweak to run deepvariant on CHM13 draft genome?. Regards,; Sangjin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/367:28,Usability,clear,clear,28,"**Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: Ubuntu; - DeepVariant version: 0.9.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: sample: CHM13, instrument: PacBio Sequel CCS sequencing, reference genome: CHM13 draft genome from T2T project. **Steps to reproduce:**; - Command:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/chm13.draft_v1.0.fasta \; --reads=/input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam \; --output_vcf=/output/CHM13.CHM13.minimap2_asm20.deepvariant_0.9.0.vcf.gz \; --output_gvcf=/output/CHM13.CHM13.minimap2_asm20.deepvariant_0.9.0.g.vcf.gz \; --num_shards=29; ```. - Error trace: (if applicable). ```; ***** Running the command:*****; time seq 0 28 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/chm13.draft_v1.0.fasta"" --reads ""/input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp. I1023 11:00:14.182121 140022713169664 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1023 11:00:14.268690 140022713169664 genomics_reader.py:223] Reading /input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam with NativeSamReader; I1023 11:00:14.297683 140022713169664 make_examples.py:1324] Preparing inputs; I1023 11:00:14.382807 140022713169664 genomics_reader.py:223] Reading /input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam with NativeSamReader; I1023 11:00:14.425673 140022713169664 make_examples.py:1248] Common contigs are [u'chr1', u'chr2', u'chr3', u'chr4', u'chr5', u'chr6', u'chr7', u'chr8', u'chr9', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'ch; I1023 11:00:14.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/367
https://github.com/google/deepvariant/issues/370:347,Availability,avail,available,347,"I'm attempting to write a bam file of the realigned reads, as I'm seeing ADs in the vcf that do not line up with what is present in the input bam file. I'm mainly concerned with two specific locations in the genome. The full genome is approx 11 Gbp, the input bam file is about 190 GB, and the drive I'm attempting to output to has more than 4 TB available. When using `-emit_realigned_reads` and using `-realigner_diagnostics` to provide an output directory, the log file tells me that deepvariant attempts to write more than seven million bam files, making it impossible to access the directory before crashing due to running out of disk space. Is there some way of getting around this? I'm thinking either a more storage efficient way of getting the entire bam file, or a way of getting the bam file of the specific regions I'm interested in. I'm using deepvariant 1.0 from docker on Ubuntu 18.04. The data is short read Illumina data. . The command I'm using to run this:. ```; seq 0 $((60-1)) |\; parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples \; --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \; --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \; --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord@60.gz \; --sample_name sample --task {} 2> results/sample/deepvariant/tmp/make_examples.log ; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/370
https://github.com/google/deepvariant/issues/370:724,Energy Efficiency,efficient,efficient,724,"I'm attempting to write a bam file of the realigned reads, as I'm seeing ADs in the vcf that do not line up with what is present in the input bam file. I'm mainly concerned with two specific locations in the genome. The full genome is approx 11 Gbp, the input bam file is about 190 GB, and the drive I'm attempting to output to has more than 4 TB available. When using `-emit_realigned_reads` and using `-realigner_diagnostics` to provide an output directory, the log file tells me that deepvariant attempts to write more than seven million bam files, making it impossible to access the directory before crashing due to running out of disk space. Is there some way of getting around this? I'm thinking either a more storage efficient way of getting the entire bam file, or a way of getting the bam file of the specific regions I'm interested in. I'm using deepvariant 1.0 from docker on Ubuntu 18.04. The data is short read Illumina data. . The command I'm using to run this:. ```; seq 0 $((60-1)) |\; parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples \; --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \; --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \; --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord@60.gz \; --sample_name sample --task {} 2> results/sample/deepvariant/tmp/make_examples.log ; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/370
https://github.com/google/deepvariant/issues/370:576,Security,access,access,576,"I'm attempting to write a bam file of the realigned reads, as I'm seeing ADs in the vcf that do not line up with what is present in the input bam file. I'm mainly concerned with two specific locations in the genome. The full genome is approx 11 Gbp, the input bam file is about 190 GB, and the drive I'm attempting to output to has more than 4 TB available. When using `-emit_realigned_reads` and using `-realigner_diagnostics` to provide an output directory, the log file tells me that deepvariant attempts to write more than seven million bam files, making it impossible to access the directory before crashing due to running out of disk space. Is there some way of getting around this? I'm thinking either a more storage efficient way of getting the entire bam file, or a way of getting the bam file of the specific regions I'm interested in. I'm using deepvariant 1.0 from docker on Ubuntu 18.04. The data is short read Illumina data. . The command I'm using to run this:. ```; seq 0 $((60-1)) |\; parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples \; --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \; --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \; --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord@60.gz \; --sample_name sample --task {} 2> results/sample/deepvariant/tmp/make_examples.log ; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/370
https://github.com/google/deepvariant/issues/370:464,Testability,log,log,464,"I'm attempting to write a bam file of the realigned reads, as I'm seeing ADs in the vcf that do not line up with what is present in the input bam file. I'm mainly concerned with two specific locations in the genome. The full genome is approx 11 Gbp, the input bam file is about 190 GB, and the drive I'm attempting to output to has more than 4 TB available. When using `-emit_realigned_reads` and using `-realigner_diagnostics` to provide an output directory, the log file tells me that deepvariant attempts to write more than seven million bam files, making it impossible to access the directory before crashing due to running out of disk space. Is there some way of getting around this? I'm thinking either a more storage efficient way of getting the entire bam file, or a way of getting the bam file of the specific regions I'm interested in. I'm using deepvariant 1.0 from docker on Ubuntu 18.04. The data is short read Illumina data. . The command I'm using to run this:. ```; seq 0 $((60-1)) |\; parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples \; --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \; --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \; --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord@60.gz \; --sample_name sample --task {} 2> results/sample/deepvariant/tmp/make_examples.log ; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/370
https://github.com/google/deepvariant/issues/370:1420,Testability,log,log,1420,"I'm attempting to write a bam file of the realigned reads, as I'm seeing ADs in the vcf that do not line up with what is present in the input bam file. I'm mainly concerned with two specific locations in the genome. The full genome is approx 11 Gbp, the input bam file is about 190 GB, and the drive I'm attempting to output to has more than 4 TB available. When using `-emit_realigned_reads` and using `-realigner_diagnostics` to provide an output directory, the log file tells me that deepvariant attempts to write more than seven million bam files, making it impossible to access the directory before crashing due to running out of disk space. Is there some way of getting around this? I'm thinking either a more storage efficient way of getting the entire bam file, or a way of getting the bam file of the specific regions I'm interested in. I'm using deepvariant 1.0 from docker on Ubuntu 18.04. The data is short read Illumina data. . The command I'm using to run this:. ```; seq 0 $((60-1)) |\; parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples \; --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \; --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \; --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord@60.gz \; --sample_name sample --task {} 2> results/sample/deepvariant/tmp/make_examples.log ; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/370
https://github.com/google/deepvariant/issues/371:44,Usability,learn,learning,44,"My goal is to use DeepVariant in a transfer learning application. Is it possible to get the last layer as an embedding output?. If yes:; Is it possible to use it as a Python module to get these embeddings?. Also, is it possible to run DeepVariant in with a VCF input?. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/371
https://github.com/google/deepvariant/issues/372:28,Availability,error,error,28,"I've never encountered this error previously and I cannot figure out what is causing the issue. Looks like something to do with the reference file?. user@node1784:~/MyData$ /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=./hg19.fa --reads=./NA12878_S1.bam --output_vcf=./NA12878_DeepVariant_output.vcf.gz --num_shards=1 ; I1027 14:35:49.384760 139774463268608 run_deepvariant.py:273] Re-using the directory for intermediate results in /tmp/tmps6oyff7s. ***** Intermediate results will be written to /tmp/tmps6oyff7s in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""./hg19.fa"" --reads ""./NA12878_S1.bam"" --examples ""/tmp/tmps6oyff7s/make_examples.tfrecord@1.gz"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I1027 14:35:51.541710 140702132172544 genomics_reader.py:223] Reading ./NA12878_S1.bam with NativeSamReader; I1027 14:35:51.552782 140702132172544 make_examples.py:587] Preparing inputs; I1027 14:35:51.576705 140702132172544 genomics_reader.py:223] Reading ./NA12878_S1.bam with NativeSamReader; I1027 14:35:51.590540 140702132172544 make_examples.py:587] Common contigs are ['chrM', 'chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY']; I1027 14:35:56.576697 140702132172544 make_examples.py:587] Writing examples to /tmp/tmps6oyff7s/make_examples.tfrecord-00000-of-00001.g",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/372
https://github.com/google/deepvariant/issues/372:1003,Energy Efficiency,Power,Power,1003," encountered this error previously and I cannot figure out what is causing the issue. Looks like something to do with the reference file?. user@node1784:~/MyData$ /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=./hg19.fa --reads=./NA12878_S1.bam --output_vcf=./NA12878_DeepVariant_output.vcf.gz --num_shards=1 ; I1027 14:35:49.384760 139774463268608 run_deepvariant.py:273] Re-using the directory for intermediate results in /tmp/tmps6oyff7s. ***** Intermediate results will be written to /tmp/tmps6oyff7s in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""./hg19.fa"" --reads ""./NA12878_S1.bam"" --examples ""/tmp/tmps6oyff7s/make_examples.tfrecord@1.gz"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I1027 14:35:51.541710 140702132172544 genomics_reader.py:223] Reading ./NA12878_S1.bam with NativeSamReader; I1027 14:35:51.552782 140702132172544 make_examples.py:587] Preparing inputs; I1027 14:35:51.576705 140702132172544 genomics_reader.py:223] Reading ./NA12878_S1.bam with NativeSamReader; I1027 14:35:51.590540 140702132172544 make_examples.py:587] Common contigs are ['chrM', 'chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY']; I1027 14:35:56.576697 140702132172544 make_examples.py:587] Writing examples to /tmp/tmps6oyff7s/make_examples.tfrecord-00000-of-00001.gz; I1027 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/372
https://github.com/google/deepvariant/issues/372:1017,Testability,log,login,1017," encountered this error previously and I cannot figure out what is causing the issue. Looks like something to do with the reference file?. user@node1784:~/MyData$ /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=./hg19.fa --reads=./NA12878_S1.bam --output_vcf=./NA12878_DeepVariant_output.vcf.gz --num_shards=1 ; I1027 14:35:49.384760 139774463268608 run_deepvariant.py:273] Re-using the directory for intermediate results in /tmp/tmps6oyff7s. ***** Intermediate results will be written to /tmp/tmps6oyff7s in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""./hg19.fa"" --reads ""./NA12878_S1.bam"" --examples ""/tmp/tmps6oyff7s/make_examples.tfrecord@1.gz"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I1027 14:35:51.541710 140702132172544 genomics_reader.py:223] Reading ./NA12878_S1.bam with NativeSamReader; I1027 14:35:51.552782 140702132172544 make_examples.py:587] Preparing inputs; I1027 14:35:51.576705 140702132172544 genomics_reader.py:223] Reading ./NA12878_S1.bam with NativeSamReader; I1027 14:35:51.590540 140702132172544 make_examples.py:587] Common contigs are ['chrM', 'chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY']; I1027 14:35:56.576697 140702132172544 make_examples.py:587] Writing examples to /tmp/tmps6oyff7s/make_examples.tfrecord-00000-of-00001.gz; I1027 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/372
https://github.com/google/deepvariant/issues/374:584,Availability,Error,Error,584,"**Describe the issue:**; Make_Example fail because of bed.file. **Setup**; - Operating system:ubuntu18.04; - DeepVariant version:v1.0.0; - Installation method (Docker, built from source, etc.):Docker; - Type of data: same as case study. **Steps to reproduce:**; - Command:; /usr/local/seqslab/deepvariant/bazel-bin/deepvariant/make_examples \; --mode calling \; --ref /opt/command/test_dir/ref.fa \; --reads /opt/command/test_dir/0-0.bam \; --regions /opt/command/test_dir/part_0.bed \; --examples /opt/command/test_dir/expl_tfrecord \; --gvcf /opt/command/test_dir/gvcf_tfrecord ; - Error trace: (if applicable); [E::idx_find_and_load] Could not retrieve index file for '/opt/command/test_dir/0-0.bam'; I1028 05:56:59.842471 139811846457152 genomics_reader.py:223] Reading /opt/command/test_dir/0-0.bam with NativeSamReader; I1028 05:56:59.844546 139811846457152 make_examples.py:587] Preparing inputs; [E::idx_find_and_load] Could not retrieve index file for '/opt/command/test_dir/0-0.bam'; I1028 05:56:59.845231 139811846457152 genomics_reader.py:223] Reading /opt/command/test_dir/0-0.bam with NativeSamReader; I1028 05:56:59.854553 139811846457152 make_examples.py:587] Common contigs are ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', 'X', 'Y', 'MT']; [E::hts_hopen] Failed to open file /opt/command/test_dir/part_0.bed; [E::hts_open_format] Failed to open file ""/opt/command/test_dir/part_0.bed"" : Exec format error; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2126",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:1495,Availability,error,error,1495,"_dir/expl_tfrecord \; --gvcf /opt/command/test_dir/gvcf_tfrecord ; - Error trace: (if applicable); [E::idx_find_and_load] Could not retrieve index file for '/opt/command/test_dir/0-0.bam'; I1028 05:56:59.842471 139811846457152 genomics_reader.py:223] Reading /opt/command/test_dir/0-0.bam with NativeSamReader; I1028 05:56:59.844546 139811846457152 make_examples.py:587] Preparing inputs; [E::idx_find_and_load] Could not retrieve index file for '/opt/command/test_dir/0-0.bam'; I1028 05:56:59.845231 139811846457152 genomics_reader.py:223] Reading /opt/command/test_dir/0-0.bam with NativeSamReader; I1028 05:56:59.854553 139811846457152 make_examples.py:587] Common contigs are ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', 'X', 'Y', 'MT']; [E::hts_hopen] Failed to open file /opt/command/test_dir/part_0.bed; [E::hts_open_format] Failed to open file ""/opt/command/test_dir/part_0.bed"" : Exec format error; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2126, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1984, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1879, in processing_regions_from_options; options.exclude_calling_regions); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1065,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:4203,Availability,error,error,4203,"regions_from_options; options.exclude_calling_regions); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1065, in build_calling_regions; ranges.RangeSet.from_regions(regions_to_include, contig_dict)); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions; return cls(ranges=from_regions(regions, contig_map=contig_map)); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__; for i, range_ in enumerate(ranges):; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 493, in from_regions; for elt in reader(region):; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 458, in bed_parser; with bed.BedReader(filename) as fin:; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: Not found: Could not open /opt/command/test_dir/part_0.bed; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; My part_0.bed has 5 columns and goes error; chrm start end name (option); 1 0 1005000 0 5000; But when I change my part_0.bed to 4 columns , it works; chrm start end name ; 1 0 1005000 0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:139,Deployability,Install,Installation,139,"**Describe the issue:**; Make_Example fail because of bed.file. **Setup**; - Operating system:ubuntu18.04; - DeepVariant version:v1.0.0; - Installation method (Docker, built from source, etc.):Docker; - Type of data: same as case study. **Steps to reproduce:**; - Command:; /usr/local/seqslab/deepvariant/bazel-bin/deepvariant/make_examples \; --mode calling \; --ref /opt/command/test_dir/ref.fa \; --reads /opt/command/test_dir/0-0.bam \; --regions /opt/command/test_dir/part_0.bed \; --examples /opt/command/test_dir/expl_tfrecord \; --gvcf /opt/command/test_dir/gvcf_tfrecord ; - Error trace: (if applicable); [E::idx_find_and_load] Could not retrieve index file for '/opt/command/test_dir/0-0.bam'; I1028 05:56:59.842471 139811846457152 genomics_reader.py:223] Reading /opt/command/test_dir/0-0.bam with NativeSamReader; I1028 05:56:59.844546 139811846457152 make_examples.py:587] Preparing inputs; [E::idx_find_and_load] Could not retrieve index file for '/opt/command/test_dir/0-0.bam'; I1028 05:56:59.845231 139811846457152 genomics_reader.py:223] Reading /opt/command/test_dir/0-0.bam with NativeSamReader; I1028 05:56:59.854553 139811846457152 make_examples.py:587] Common contigs are ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', 'X', 'Y', 'MT']; [E::hts_hopen] Failed to open file /opt/command/test_dir/part_0.bed; [E::hts_open_format] Failed to open file ""/opt/command/test_dir/part_0.bed"" : Exec format error; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2126",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:3942,Testability,test,test,3942,"regions_from_options; options.exclude_calling_regions); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1065, in build_calling_regions; ranges.RangeSet.from_regions(regions_to_include, contig_dict)); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions; return cls(ranges=from_regions(regions, contig_map=contig_map)); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__; for i, range_ in enumerate(ranges):; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 493, in from_regions; for elt in reader(region):; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 458, in bed_parser; with bed.BedReader(filename) as fin:; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: Not found: Could not open /opt/command/test_dir/part_0.bed; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; My part_0.bed has 5 columns and goes error; chrm start end name (option); 1 0 1005000 0 5000; But when I change my part_0.bed to 4 columns , it works; chrm start end name ; 1 0 1005000 0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/374:3978,Testability,test,test,3978,"regions_from_options; options.exclude_calling_regions); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1065, in build_calling_regions; ranges.RangeSet.from_regions(regions_to_include, contig_dict)); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions; return cls(ranges=from_regions(regions, contig_map=contig_map)); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__; for i, range_ in enumerate(ranges):; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 493, in from_regions; for elt in reader(region):; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 458, in bed_parser; with bed.BedReader(filename) as fin:; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: Not found: Could not open /opt/command/test_dir/part_0.bed; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; My part_0.bed has 5 columns and goes error; chrm start end name (option); 1 0 1005000 0 5000; But when I change my part_0.bed to 4 columns , it works; chrm start end name ; 1 0 1005000 0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/374
https://github.com/google/deepvariant/issues/377:957,Deployability,Install,Installation,957,"**_Describe the issue:_**; I was wondering if the authors have recommended method for generating a set of high-confidence de novo germline mutations from trio-deepvariant vcf files. I have used DeepVariant 0.9.0 version for calling the germline mutations and I have merged HG002, HG003 and HG004 .g.vcf.gz files using GLnexus. If there isn't a recommendation calling de novo germline mutations from trio-deepvariant vcf files, I am hoping to implement the method described in DeNovoGear: de novo indel and point mutation discovery and phasing. _**Miscellaneous**_; - I was hoping to use GATK GenotypeGVCFs for merging the HG002, HG003 and HG004 .g.vcf.gz files, but deepvariant .g.vcf.gz file is not compatible with GATK GenotypeGVCFs. ; - I was hoping to use the HG002-HG003-HG004 trio vcf file as input towards strelka2 de novo mutation caller, but the vcf file was again not compatible. **_Setup_**. Operating system: Ubuntu; DeepVariant version: 0.9.0; Installation method (Docker, built from source, etc.): Docker; Type of data: sample: HG002, HG003 and HG004 PacBio CCS fastq. **_Steps to reproduce:_**. **_Any additional context:_**. Regards,; Sangjin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/377
https://github.com/google/deepvariant/issues/378:128,Availability,checkpoint,checkpoint,128,"I launched a training run, but the evaluation run wasn't launched concurrently. When I launch it, it simply evaluates the final checkpoint, not all the checkpoints in between. Is there an option force evaluation of all checkpoints in model_eval?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:152,Availability,checkpoint,checkpoints,152,"I launched a training run, but the evaluation run wasn't launched concurrently. When I launch it, it simply evaluates the final checkpoint, not all the checkpoints in between. Is there an option force evaluation of all checkpoints in model_eval?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:219,Availability,checkpoint,checkpoints,219,"I launched a training run, but the evaluation run wasn't launched concurrently. When I launch it, it simply evaluates the final checkpoint, not all the checkpoints in between. Is there an option force evaluation of all checkpoints in model_eval?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:66,Performance,concurren,concurrently,66,"I launched a training run, but the evaluation run wasn't launched concurrently. When I launch it, it simply evaluates the final checkpoint, not all the checkpoints in between. Is there an option force evaluation of all checkpoints in model_eval?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/378:101,Usability,simpl,simply,101,"I launched a training run, but the evaluation run wasn't launched concurrently. When I launch it, it simply evaluates the final checkpoint, not all the checkpoints in between. Is there an option force evaluation of all checkpoints in model_eval?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/378
https://github.com/google/deepvariant/issues/379:542,Testability,test,testdata,542,"Hi,; I am trying to deepen my understanding of DeepVariant (pun unintended), similar to what is written here (https://github.com/google/deepvariant/issues/306).; If I understand the discussion above correctly, whenever there is an insertion, the insertion itself will not be present in the pileup image, and the preceding pixel will be zero'd out. So if we have a cigar 6M3I1M such as:; ref: ACGCCT T; alt: ACGCCTCCCT; this will actually be represented in the pileup as:; ref: ACGCCTT; alt: ACGCC0T. I visualized the examples you provided in testdata, and on the 3rd example [fig1] (chr20:10001436-10001436) there is a case in which an insertion occurs right after the variant's position. Following the above logic, the center column should be zero'd out. However, it seems to not be the case. What happens instead is that the last channels center column indicates a diversion from the reference genome (a pixel is 'lit' up). So it looks like the bam read is a basic SNP, even though it actually is a length 5 insertion (the bam is an indel and the variant in the vcf is an snp).; In fact, I haven't seen any case of zero'd out pixels which aren't deletions.; Some options are:. 1. Did I not understand the insertions logic correctly? Whas it changed since the above git issue in April?; 2. Am I possibly working with the wrong files? I use the NA12878_S1.chr20.10_10p1mb.bam/.bai bam file with the golden.training_examples.tfrecord for your examples.; Sorry for the long post, I hope I was clear but let me know if more information/explanations are needed.; ![fig1](https://user-images.githubusercontent.com/52149642/98434660-6ab2e380-20da-11eb-887c-4683bb759d1c.png)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:709,Testability,log,logic,709,"Hi,; I am trying to deepen my understanding of DeepVariant (pun unintended), similar to what is written here (https://github.com/google/deepvariant/issues/306).; If I understand the discussion above correctly, whenever there is an insertion, the insertion itself will not be present in the pileup image, and the preceding pixel will be zero'd out. So if we have a cigar 6M3I1M such as:; ref: ACGCCT T; alt: ACGCCTCCCT; this will actually be represented in the pileup as:; ref: ACGCCTT; alt: ACGCC0T. I visualized the examples you provided in testdata, and on the 3rd example [fig1] (chr20:10001436-10001436) there is a case in which an insertion occurs right after the variant's position. Following the above logic, the center column should be zero'd out. However, it seems to not be the case. What happens instead is that the last channels center column indicates a diversion from the reference genome (a pixel is 'lit' up). So it looks like the bam read is a basic SNP, even though it actually is a length 5 insertion (the bam is an indel and the variant in the vcf is an snp).; In fact, I haven't seen any case of zero'd out pixels which aren't deletions.; Some options are:. 1. Did I not understand the insertions logic correctly? Whas it changed since the above git issue in April?; 2. Am I possibly working with the wrong files? I use the NA12878_S1.chr20.10_10p1mb.bam/.bai bam file with the golden.training_examples.tfrecord for your examples.; Sorry for the long post, I hope I was clear but let me know if more information/explanations are needed.; ![fig1](https://user-images.githubusercontent.com/52149642/98434660-6ab2e380-20da-11eb-887c-4683bb759d1c.png)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:1218,Testability,log,logic,1218,"Hi,; I am trying to deepen my understanding of DeepVariant (pun unintended), similar to what is written here (https://github.com/google/deepvariant/issues/306).; If I understand the discussion above correctly, whenever there is an insertion, the insertion itself will not be present in the pileup image, and the preceding pixel will be zero'd out. So if we have a cigar 6M3I1M such as:; ref: ACGCCT T; alt: ACGCCTCCCT; this will actually be represented in the pileup as:; ref: ACGCCTT; alt: ACGCC0T. I visualized the examples you provided in testdata, and on the 3rd example [fig1] (chr20:10001436-10001436) there is a case in which an insertion occurs right after the variant's position. Following the above logic, the center column should be zero'd out. However, it seems to not be the case. What happens instead is that the last channels center column indicates a diversion from the reference genome (a pixel is 'lit' up). So it looks like the bam read is a basic SNP, even though it actually is a length 5 insertion (the bam is an indel and the variant in the vcf is an snp).; In fact, I haven't seen any case of zero'd out pixels which aren't deletions.; Some options are:. 1. Did I not understand the insertions logic correctly? Whas it changed since the above git issue in April?; 2. Am I possibly working with the wrong files? I use the NA12878_S1.chr20.10_10p1mb.bam/.bai bam file with the golden.training_examples.tfrecord for your examples.; Sorry for the long post, I hope I was clear but let me know if more information/explanations are needed.; ![fig1](https://user-images.githubusercontent.com/52149642/98434660-6ab2e380-20da-11eb-887c-4683bb759d1c.png)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/379:1491,Usability,clear,clear,1491,"Hi,; I am trying to deepen my understanding of DeepVariant (pun unintended), similar to what is written here (https://github.com/google/deepvariant/issues/306).; If I understand the discussion above correctly, whenever there is an insertion, the insertion itself will not be present in the pileup image, and the preceding pixel will be zero'd out. So if we have a cigar 6M3I1M such as:; ref: ACGCCT T; alt: ACGCCTCCCT; this will actually be represented in the pileup as:; ref: ACGCCTT; alt: ACGCC0T. I visualized the examples you provided in testdata, and on the 3rd example [fig1] (chr20:10001436-10001436) there is a case in which an insertion occurs right after the variant's position. Following the above logic, the center column should be zero'd out. However, it seems to not be the case. What happens instead is that the last channels center column indicates a diversion from the reference genome (a pixel is 'lit' up). So it looks like the bam read is a basic SNP, even though it actually is a length 5 insertion (the bam is an indel and the variant in the vcf is an snp).; In fact, I haven't seen any case of zero'd out pixels which aren't deletions.; Some options are:. 1. Did I not understand the insertions logic correctly? Whas it changed since the above git issue in April?; 2. Am I possibly working with the wrong files? I use the NA12878_S1.chr20.10_10p1mb.bam/.bai bam file with the golden.training_examples.tfrecord for your examples.; Sorry for the long post, I hope I was clear but let me know if more information/explanations are needed.; ![fig1](https://user-images.githubusercontent.com/52149642/98434660-6ab2e380-20da-11eb-887c-4683bb759d1c.png)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/379
https://github.com/google/deepvariant/issues/380:370,Availability,error,error,370,"**Describe the issue:**. I am trying to build deepvariant on my machine that has Centos 7 and it seems there are no instructions to do that. All the instructions are based on Ubuntu operation system. I install all the dependencies but it seems there is no version of CLIF for Centos 7. So I tried building CLIF on my machine using clang/llvm 11.0.0. I get the following error during installation of that. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm::GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequen",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/380
https://github.com/google/deepvariant/issues/380:494,Availability,error,error,494,"**Describe the issue:**. I am trying to build deepvariant on my machine that has Centos 7 and it seems there are no instructions to do that. All the instructions are based on Ubuntu operation system. I install all the dependencies but it seems there is no version of CLIF for Centos 7. So I tried building CLIF on my machine using clang/llvm 11.0.0. I get the following error during installation of that. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm::GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequen",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/380
https://github.com/google/deepvariant/issues/380:522,Availability,error,error,522,"**Describe the issue:**. I am trying to build deepvariant on my machine that has Centos 7 and it seems there are no instructions to do that. All the instructions are based on Ubuntu operation system. I install all the dependencies but it seems there is no version of CLIF for Centos 7. So I tried building CLIF on my machine using clang/llvm 11.0.0. I get the following error during installation of that. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm::GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequen",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/380
https://github.com/google/deepvariant/issues/380:2127,Availability,Error,Error,2127,"t. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm::GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/380
https://github.com/google/deepvariant/issues/380:202,Deployability,install,install,202,"**Describe the issue:**. I am trying to build deepvariant on my machine that has Centos 7 and it seems there are no instructions to do that. All the instructions are based on Ubuntu operation system. I install all the dependencies but it seems there is no version of CLIF for Centos 7. So I tried building CLIF on my machine using clang/llvm 11.0.0. I get the following error during installation of that. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm::GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequen",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/380
https://github.com/google/deepvariant/issues/380:383,Deployability,install,installation,383,"**Describe the issue:**. I am trying to build deepvariant on my machine that has Centos 7 and it seems there are no instructions to do that. All the instructions are based on Ubuntu operation system. I install all the dependencies but it seems there is no version of CLIF for Centos 7. So I tried building CLIF on my machine using clang/llvm 11.0.0. I get the following error during installation of that. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm::GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequen",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/380
https://github.com/google/deepvariant/issues/380:535,Deployability,install,installation,535,"**Describe the issue:**. I am trying to build deepvariant on my machine that has Centos 7 and it seems there are no instructions to do that. All the instructions are based on Ubuntu operation system. I install all the dependencies but it seems there is no version of CLIF for Centos 7. So I tried building CLIF on my machine using clang/llvm 11.0.0. I get the following error during installation of that. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm::GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequen",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/380
https://github.com/google/deepvariant/issues/380:556,Deployability,INSTALL,INSTALL,556,"**Describe the issue:**. I am trying to build deepvariant on my machine that has Centos 7 and it seems there are no instructions to do that. All the instructions are based on Ubuntu operation system. I install all the dependencies but it seems there is no version of CLIF for Centos 7. So I tried building CLIF on my machine using clang/llvm 11.0.0. I get the following error during installation of that. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm::GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequen",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/380
https://github.com/google/deepvariant/issues/380:1901,Deployability,Install,Installation,1901,"t. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm::GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/380
https://github.com/google/deepvariant/issues/380:218,Integrability,depend,dependencies,218,"**Describe the issue:**. I am trying to build deepvariant on my machine that has Centos 7 and it seems there are no instructions to do that. All the instructions are based on Ubuntu operation system. I install all the dependencies but it seems there is no version of CLIF for Centos 7. So I tried building CLIF on my machine using clang/llvm 11.0.0. I get the following error during installation of that. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm::GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequen",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/380
https://github.com/google/deepvariant/issues/380:579,Integrability,depend,dependencies,579,"**Describe the issue:**. I am trying to build deepvariant on my machine that has Centos 7 and it seems there are no instructions to do that. All the instructions are based on Ubuntu operation system. I install all the dependencies but it seems there is no version of CLIF for Centos 7. So I tried building CLIF on my machine using clang/llvm 11.0.0. I get the following error during installation of that. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm::GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequen",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/380
https://github.com/google/deepvariant/issues/380:2180,Testability,test,test,2180,"t. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm::GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/380
https://github.com/google/deepvariant/issues/380:2216,Testability,test,test,2216,"t. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm::GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/380
https://github.com/google/deepvariant/issues/381:250,Availability,avail,available,250,"Hi, is it possible to get some information regarding training of default PacBio models in v1.0.0? Specifically,. 1. What genomes were used in training?; 2. What was the source of BAM or FASTQ files of genomes used for training, and are they publicly available?; 3. What ground truth variant call set was used for training? If GIAB benchmark variants were used, then which version was used?. I see this information is available [here ](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md)for Illumina models but not for PacBio. Also, can you please tell us which is the latest PacBio model trained on v3.3.2 of GIAB benchmark variant calls?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:417,Availability,avail,available,417,"Hi, is it possible to get some information regarding training of default PacBio models in v1.0.0? Specifically,. 1. What genomes were used in training?; 2. What was the source of BAM or FASTQ files of genomes used for training, and are they publicly available?; 3. What ground truth variant call set was used for training? If GIAB benchmark variants were used, then which version was used?. I see this information is available [here ](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md)for Illumina models but not for PacBio. Also, can you please tell us which is the latest PacBio model trained on v3.3.2 of GIAB benchmark variant calls?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:331,Testability,benchmark,benchmark,331,"Hi, is it possible to get some information regarding training of default PacBio models in v1.0.0? Specifically,. 1. What genomes were used in training?; 2. What was the source of BAM or FASTQ files of genomes used for training, and are they publicly available?; 3. What ground truth variant call set was used for training? If GIAB benchmark variants were used, then which version was used?. I see this information is available [here ](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md)for Illumina models but not for PacBio. Also, can you please tell us which is the latest PacBio model trained on v3.3.2 of GIAB benchmark variant calls?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/381:654,Testability,benchmark,benchmark,654,"Hi, is it possible to get some information regarding training of default PacBio models in v1.0.0? Specifically,. 1. What genomes were used in training?; 2. What was the source of BAM or FASTQ files of genomes used for training, and are they publicly available?; 3. What ground truth variant call set was used for training? If GIAB benchmark variants were used, then which version was used?. I see this information is available [here ](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md)for Illumina models but not for PacBio. Also, can you please tell us which is the latest PacBio model trained on v3.3.2 of GIAB benchmark variant calls?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/381
https://github.com/google/deepvariant/issues/383:59,Availability,checkpoint,checkpoint,59,"I ran training for a number of epochs and obtained a model checkpoint (lets call it `checkpoint-first`) with accuracy > 0.99 (F1/All). Then I launched training again with `--start_from_checkpoint=checkpoint-first`. I expected `model-ckpt-0` for the second training run to show the same high accuracy as `checkpoint-first`. But it shows very low accuracy instead (F1/All is 0.85 or so). However the next checkpoint after `model-ckpt-0` (lets call it `model-ckpt-N`) shows high accuracy. Does this mean `model-ckpt-0` is dumped before loading parameters from `checkpoint-first`, and `model-ckpt-N` is the first checkpoint I should be looking at for meaningful results for the second training run?. I used Google Cloud TPU for both training runs. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/383:85,Availability,checkpoint,checkpoint-first,85,"I ran training for a number of epochs and obtained a model checkpoint (lets call it `checkpoint-first`) with accuracy > 0.99 (F1/All). Then I launched training again with `--start_from_checkpoint=checkpoint-first`. I expected `model-ckpt-0` for the second training run to show the same high accuracy as `checkpoint-first`. But it shows very low accuracy instead (F1/All is 0.85 or so). However the next checkpoint after `model-ckpt-0` (lets call it `model-ckpt-N`) shows high accuracy. Does this mean `model-ckpt-0` is dumped before loading parameters from `checkpoint-first`, and `model-ckpt-N` is the first checkpoint I should be looking at for meaningful results for the second training run?. I used Google Cloud TPU for both training runs. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/383:196,Availability,checkpoint,checkpoint-first,196,"I ran training for a number of epochs and obtained a model checkpoint (lets call it `checkpoint-first`) with accuracy > 0.99 (F1/All). Then I launched training again with `--start_from_checkpoint=checkpoint-first`. I expected `model-ckpt-0` for the second training run to show the same high accuracy as `checkpoint-first`. But it shows very low accuracy instead (F1/All is 0.85 or so). However the next checkpoint after `model-ckpt-0` (lets call it `model-ckpt-N`) shows high accuracy. Does this mean `model-ckpt-0` is dumped before loading parameters from `checkpoint-first`, and `model-ckpt-N` is the first checkpoint I should be looking at for meaningful results for the second training run?. I used Google Cloud TPU for both training runs. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/383:304,Availability,checkpoint,checkpoint-first,304,"I ran training for a number of epochs and obtained a model checkpoint (lets call it `checkpoint-first`) with accuracy > 0.99 (F1/All). Then I launched training again with `--start_from_checkpoint=checkpoint-first`. I expected `model-ckpt-0` for the second training run to show the same high accuracy as `checkpoint-first`. But it shows very low accuracy instead (F1/All is 0.85 or so). However the next checkpoint after `model-ckpt-0` (lets call it `model-ckpt-N`) shows high accuracy. Does this mean `model-ckpt-0` is dumped before loading parameters from `checkpoint-first`, and `model-ckpt-N` is the first checkpoint I should be looking at for meaningful results for the second training run?. I used Google Cloud TPU for both training runs. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/383:403,Availability,checkpoint,checkpoint,403,"I ran training for a number of epochs and obtained a model checkpoint (lets call it `checkpoint-first`) with accuracy > 0.99 (F1/All). Then I launched training again with `--start_from_checkpoint=checkpoint-first`. I expected `model-ckpt-0` for the second training run to show the same high accuracy as `checkpoint-first`. But it shows very low accuracy instead (F1/All is 0.85 or so). However the next checkpoint after `model-ckpt-0` (lets call it `model-ckpt-N`) shows high accuracy. Does this mean `model-ckpt-0` is dumped before loading parameters from `checkpoint-first`, and `model-ckpt-N` is the first checkpoint I should be looking at for meaningful results for the second training run?. I used Google Cloud TPU for both training runs. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/383:558,Availability,checkpoint,checkpoint-first,558,"I ran training for a number of epochs and obtained a model checkpoint (lets call it `checkpoint-first`) with accuracy > 0.99 (F1/All). Then I launched training again with `--start_from_checkpoint=checkpoint-first`. I expected `model-ckpt-0` for the second training run to show the same high accuracy as `checkpoint-first`. But it shows very low accuracy instead (F1/All is 0.85 or so). However the next checkpoint after `model-ckpt-0` (lets call it `model-ckpt-N`) shows high accuracy. Does this mean `model-ckpt-0` is dumped before loading parameters from `checkpoint-first`, and `model-ckpt-N` is the first checkpoint I should be looking at for meaningful results for the second training run?. I used Google Cloud TPU for both training runs. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/383:609,Availability,checkpoint,checkpoint,609,"I ran training for a number of epochs and obtained a model checkpoint (lets call it `checkpoint-first`) with accuracy > 0.99 (F1/All). Then I launched training again with `--start_from_checkpoint=checkpoint-first`. I expected `model-ckpt-0` for the second training run to show the same high accuracy as `checkpoint-first`. But it shows very low accuracy instead (F1/All is 0.85 or so). However the next checkpoint after `model-ckpt-0` (lets call it `model-ckpt-N`) shows high accuracy. Does this mean `model-ckpt-0` is dumped before loading parameters from `checkpoint-first`, and `model-ckpt-N` is the first checkpoint I should be looking at for meaningful results for the second training run?. I used Google Cloud TPU for both training runs. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/383:533,Performance,load,loading,533,"I ran training for a number of epochs and obtained a model checkpoint (lets call it `checkpoint-first`) with accuracy > 0.99 (F1/All). Then I launched training again with `--start_from_checkpoint=checkpoint-first`. I expected `model-ckpt-0` for the second training run to show the same high accuracy as `checkpoint-first`. But it shows very low accuracy instead (F1/All is 0.85 or so). However the next checkpoint after `model-ckpt-0` (lets call it `model-ckpt-N`) shows high accuracy. Does this mean `model-ckpt-0` is dumped before loading parameters from `checkpoint-first`, and `model-ckpt-N` is the first checkpoint I should be looking at for meaningful results for the second training run?. I used Google Cloud TPU for both training runs. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/383
https://github.com/google/deepvariant/issues/385:78,Availability,error,error,78,"Hi,; I'm using the docker Deepvariant 1.0.0 via singularity, and getting this error:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_nug2d3l9/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 47, in <module>; import tensorflow as tf; File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow/__init__.py"", line 101, in <module>; from tensorflow_core import *; File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/tensorflow_core/__init__.py"", line 33, in <module>; import distutils as _distutils; File ""/usr/local/lib/python3.6/dist-packages/_distutils_hack/__init__.py"", line 82, in create_module; return importlib.import_module('._distutils', 'setuptools'); File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module; return _bootstrap._gcd_import(name[level:], package, level); File ""/gpfs/home/evrong01/.local/lib/python3.6/site-packages/setuptools/__init__.py"", line 6, in <module>; import distutils.core; File ""/usr/local/lib/python3.6/dist-packages/_distutils_hack/__init__.py"", line 82, in create_module; return importlib.import_module('._distutils', 'setuptools'); File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module; return _bootstrap._gcd_import(name[level:], package, level); ModuleNotFoundError: No module named 'setuptools._distutils'",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/385
https://github.com/google/deepvariant/issues/388:44,Safety,detect,detected,44,"Hi, The attached variant was missed. It was detected in 39 / 39 reads (100%) at the locus, and all reads have high MAPQ. Very strange, but warrants an explanation. It was not even a RefCall variant. Appreciate any advice.; <img width=""943"" alt=""Screen Shot 2020-11-24 at 4 31 05 PM"" src=""https://user-images.githubusercontent.com/47928628/100153865-bd2e2700-2e72-11eb-9af1-c182b3fd57ee.png"">",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/388
https://github.com/google/deepvariant/issues/389:358,Availability,down,download,358,"hi ; i want to cast model in /opt/models/wgs/model.ckpt to fp16 to satisfy my poor machine; but can not reload the model from model.ckpt.meta.; I try to get graph from source code ,so that i can restore and save a new model,but the model i saved did not math the model tensorname in the source code.; how can i convert trained model to fp16? or where i can download a fp16 model",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/389
https://github.com/google/deepvariant/issues/391:87,Deployability,update,update,87,"Hello,; first of all ,i do not have the permission to run docker on my machine or sudo update or install. And as i work on cluster service,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/391:97,Deployability,install,install,97,"Hello,; first of all ,i do not have the permission to run docker on my machine or sudo update or install. And as i work on cluster service,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/391:188,Deployability,update,update,188,"Hello,; first of all ,i do not have the permission to run docker on my machine or sudo update or install. And as i work on cluster service,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/391:244,Deployability,update,update,244,"Hello,; first of all ,i do not have the permission to run docker on my machine or sudo update or install. And as i work on cluster service,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/391:399,Deployability,update,update,399,"Hello,; first of all ,i do not have the permission to run docker on my machine or sudo update or install. And as i work on cluster service,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/391:513,Deployability,install,install,513,"Hello,; first of all ,i do not have the permission to run docker on my machine or sudo update or install. And as i work on cluster service,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/391:1012,Deployability,update,update,1012,"Hello,; first of all ,i do not have the permission to run docker on my machine or sudo update or install. And as i work on cluster service,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/391:1121,Deployability,update,update,1121,"work on cluster service,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) ) #1 SMP Sun Jul 26 15:27:06 UTC 2020; conda 4.9.2; Python 3.7.6. i really hope ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/391:1246,Deployability,install,install,1246,"ervice,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) ) #1 SMP Sun Jul 26 15:27:06 UTC 2020; conda 4.9.2; Python 3.7.6. i really hope you can help me.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/391:1351,Deployability,update,update,1351,"ervice,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) ) #1 SMP Sun Jul 26 15:27:06 UTC 2020; conda 4.9.2; Python 3.7.6. i really hope you can help me.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/391:1947,Testability,mock,mockbuild,1947,"ervice,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) ) #1 SMP Sun Jul 26 15:27:06 UTC 2020; conda 4.9.2; Python 3.7.6. i really hope you can help me.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/391
https://github.com/google/deepvariant/issues/392:526,Availability,checkpoint,checkpoint,526,"Hello, when running Deepvariant with the following command: . ```bash; BIN_VERSION=""1.0.0""`; INPUT_DIR=""${PWD}/data""; OUTPUT_DIR=""${PWD}/output""; LOGDIR=""${PWD}/log""; N_SHARDS=$( /bin/ls output/ | wc -l ); ; sudo docker run --gpus 1 \; -v ${HOME}:${HOME} \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""gs://deepvariant/models/DeepVariant/1.0.0/DeepVariant-inception_v3-1.0.0+data-pacbio_standard/model.ckpt""; ```. the following error occurs:. ```; I1203 17:49:21.931325 140389904897792 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-12-03 17:49:32.284722: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:664,Availability,error,error,664,"Hello, when running Deepvariant with the following command: . ```bash; BIN_VERSION=""1.0.0""`; INPUT_DIR=""${PWD}/data""; OUTPUT_DIR=""${PWD}/output""; LOGDIR=""${PWD}/log""; N_SHARDS=$( /bin/ls output/ | wc -l ); ; sudo docker run --gpus 1 \; -v ${HOME}:${HOME} \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""gs://deepvariant/models/DeepVariant/1.0.0/DeepVariant-inception_v3-1.0.0+data-pacbio_standard/model.ckpt""; ```. the following error occurs:. ```; I1203 17:49:21.931325 140389904897792 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-12-03 17:49:32.284722: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:1145,Availability,failure,failure,1145,"r run --gpus 1 \; -v ${HOME}:${HOME} \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""gs://deepvariant/models/DeepVariant/1.0.0/DeepVariant-inception_v3-1.0.0+data-pacbio_standard/model.ckpt""; ```. the following error occurs:. ```; I1203 17:49:21.931325 140389904897792 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-12-03 17:49:32.284722: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 348, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); ValueError: The numbe",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:1167,Availability,Error,Error,1167,"r run --gpus 1 \; -v ${HOME}:${HOME} \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""gs://deepvariant/models/DeepVariant/1.0.0/DeepVariant-inception_v3-1.0.0+data-pacbio_standard/model.ckpt""; ```. the following error occurs:. ```; I1203 17:49:21.931325 140389904897792 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-12-03 17:49:32.284722: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 348, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); ValueError: The numbe",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:1253,Availability,error,error,1253,"r run --gpus 1 \; -v ${HOME}:${HOME} \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""gs://deepvariant/models/DeepVariant/1.0.0/DeepVariant-inception_v3-1.0.0+data-pacbio_standard/model.ckpt""; ```. the following error occurs:. ```; I1203 17:49:21.931325 140389904897792 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-12-03 17:49:32.284722: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 348, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); ValueError: The numbe",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:2249,Availability,checkpoint,checkpoint,2249,"ut.tfrecord.gz"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""gs://deepvariant/models/DeepVariant/1.0.0/DeepVariant-inception_v3-1.0.0+data-pacbio_standard/model.ckpt""; ```. the following error occurs:. ```; I1203 17:49:21.931325 140389904897792 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-12-03 17:49:32.284722: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 348, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 8 channels while the examples have 6.; ```. Running with the checkpoint for 0.9.0 works with this command. Is the model on the GS bucket correct?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:2282,Availability,checkpoint,checkpoint,2282,"ut.tfrecord.gz"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""gs://deepvariant/models/DeepVariant/1.0.0/DeepVariant-inception_v3-1.0.0+data-pacbio_standard/model.ckpt""; ```. the following error occurs:. ```; I1203 17:49:21.931325 140389904897792 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-12-03 17:49:32.284722: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 348, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 8 channels while the examples have 6.; ```. Running with the checkpoint for 0.9.0 works with this command. Is the model on the GS bucket correct?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:2358,Availability,checkpoint,checkpoint,2358,"ut.tfrecord.gz"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""gs://deepvariant/models/DeepVariant/1.0.0/DeepVariant-inception_v3-1.0.0+data-pacbio_standard/model.ckpt""; ```. the following error occurs:. ```; I1203 17:49:21.931325 140389904897792 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-12-03 17:49:32.284722: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 348, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 8 channels while the examples have 6.; ```. Running with the checkpoint for 0.9.0 works with this command. Is the model on the GS bucket correct?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:1097,Safety,Abort,Aborted,1097,"UT_DIR=""${PWD}/data""; OUTPUT_DIR=""${PWD}/output""; LOGDIR=""${PWD}/log""; N_SHARDS=$( /bin/ls output/ | wc -l ); ; sudo docker run --gpus 1 \; -v ${HOME}:${HOME} \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""gs://deepvariant/models/DeepVariant/1.0.0/DeepVariant-inception_v3-1.0.0+data-pacbio_standard/model.ckpt""; ```. the following error occurs:. ```; I1203 17:49:21.931325 140389904897792 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-12-03 17:49:32.284722: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:903,Security,authenticat,authentication,903,"Hello, when running Deepvariant with the following command: . ```bash; BIN_VERSION=""1.0.0""`; INPUT_DIR=""${PWD}/data""; OUTPUT_DIR=""${PWD}/output""; LOGDIR=""${PWD}/log""; N_SHARDS=$( /bin/ls output/ | wc -l ); ; sudo docker run --gpus 1 \; -v ${HOME}:${HOME} \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""gs://deepvariant/models/DeepVariant/1.0.0/DeepVariant-inception_v3-1.0.0+data-pacbio_standard/model.ckpt""; ```. the following error occurs:. ```; I1203 17:49:21.931325 140389904897792 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-12-03 17:49:32.284722: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:146,Testability,LOG,LOGDIR,146,"Hello, when running Deepvariant with the following command: . ```bash; BIN_VERSION=""1.0.0""`; INPUT_DIR=""${PWD}/data""; OUTPUT_DIR=""${PWD}/output""; LOGDIR=""${PWD}/log""; N_SHARDS=$( /bin/ls output/ | wc -l ); ; sudo docker run --gpus 1 \; -v ${HOME}:${HOME} \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""gs://deepvariant/models/DeepVariant/1.0.0/DeepVariant-inception_v3-1.0.0+data-pacbio_standard/model.ckpt""; ```. the following error occurs:. ```; I1203 17:49:21.931325 140389904897792 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-12-03 17:49:32.284722: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/issues/392:161,Testability,log,log,161,"Hello, when running Deepvariant with the following command: . ```bash; BIN_VERSION=""1.0.0""`; INPUT_DIR=""${PWD}/data""; OUTPUT_DIR=""${PWD}/output""; LOGDIR=""${PWD}/log""; N_SHARDS=$( /bin/ls output/ | wc -l ); ; sudo docker run --gpus 1 \; -v ${HOME}:${HOME} \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""gs://deepvariant/models/DeepVariant/1.0.0/DeepVariant-inception_v3-1.0.0+data-pacbio_standard/model.ckpt""; ```. the following error occurs:. ```; I1203 17:49:21.931325 140389904897792 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-12-03 17:49:32.284722: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/392
https://github.com/google/deepvariant/pull/393:352,Deployability,patch,patch,352,Run OpenVINO processing in separate thread which let's to use common logging (https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 + https://github.com/google/deepvariant/pull/363/commits/9e69c4096fac8ddb788c3d29e4405fc50e85d1e3) . Please ignore test scripts from `.github/workflows` - they are not a part of patch but just used for validation.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:376,Security,validat,validation,376,Run OpenVINO processing in separate thread which let's to use common logging (https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 + https://github.com/google/deepvariant/pull/363/commits/9e69c4096fac8ddb788c3d29e4405fc50e85d1e3) . Please ignore test scripts from `.github/workflows` - they are not a part of patch but just used for validation.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:69,Testability,log,logging,69,Run OpenVINO processing in separate thread which let's to use common logging (https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 + https://github.com/google/deepvariant/pull/363/commits/9e69c4096fac8ddb788c3d29e4405fc50e85d1e3) . Please ignore test scripts from `.github/workflows` - they are not a part of patch but just used for validation.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/pull/393:289,Testability,test,test,289,Run OpenVINO processing in separate thread which let's to use common logging (https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 + https://github.com/google/deepvariant/pull/363/commits/9e69c4096fac8ddb788c3d29e4405fc50e85d1e3) . Please ignore test scripts from `.github/workflows` - they are not a part of patch but just used for validation.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/393
https://github.com/google/deepvariant/issues/394:311,Availability,error,error,311,"Hello DeepVariant team, thanks for great tool. After we tried to upgrade our deepvariant installation to the latest release we encountered a problem with `numpy` installation, which I described [here](https://github.com/pypa/wheel/issues/389). The problem is caused by `wheel` - not by `numpy` itself, but this error raised several questions for me:. - For now looks like installing deepvariant with https://github.com/google/deepvariant/blob/r1.1/build-prereq.sh will fail on non `Ubuntu 16.04` due to `numpy` problem. This problem won't be fixed in older versions of `numpy` I think - only `wheel` can fix it now. Or you can switch `numpy 1.18.5` for `1.19.3+`?. - Part of this problem origins from using Python 3.6 if I understand correctly [this](https://github.com/pypa/wheel/issues/331#issuecomment-579285573), maybe you should update version of Python installing by script?. - I've just tested your installation script with one change: I added `Ubuntu 18.04` to this [check](https://github.com/google/deepvariant/blob/r1.1/run-prereq.sh#L120) - looks like all good. If a problem was in `Ubuntu 14.04` - maybe wide this check a bit? Even `Ubuntu 20.04` is released half a year ago.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:65,Deployability,upgrade,upgrade,65,"Hello DeepVariant team, thanks for great tool. After we tried to upgrade our deepvariant installation to the latest release we encountered a problem with `numpy` installation, which I described [here](https://github.com/pypa/wheel/issues/389). The problem is caused by `wheel` - not by `numpy` itself, but this error raised several questions for me:. - For now looks like installing deepvariant with https://github.com/google/deepvariant/blob/r1.1/build-prereq.sh will fail on non `Ubuntu 16.04` due to `numpy` problem. This problem won't be fixed in older versions of `numpy` I think - only `wheel` can fix it now. Or you can switch `numpy 1.18.5` for `1.19.3+`?. - Part of this problem origins from using Python 3.6 if I understand correctly [this](https://github.com/pypa/wheel/issues/331#issuecomment-579285573), maybe you should update version of Python installing by script?. - I've just tested your installation script with one change: I added `Ubuntu 18.04` to this [check](https://github.com/google/deepvariant/blob/r1.1/run-prereq.sh#L120) - looks like all good. If a problem was in `Ubuntu 14.04` - maybe wide this check a bit? Even `Ubuntu 20.04` is released half a year ago.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:89,Deployability,install,installation,89,"Hello DeepVariant team, thanks for great tool. After we tried to upgrade our deepvariant installation to the latest release we encountered a problem with `numpy` installation, which I described [here](https://github.com/pypa/wheel/issues/389). The problem is caused by `wheel` - not by `numpy` itself, but this error raised several questions for me:. - For now looks like installing deepvariant with https://github.com/google/deepvariant/blob/r1.1/build-prereq.sh will fail on non `Ubuntu 16.04` due to `numpy` problem. This problem won't be fixed in older versions of `numpy` I think - only `wheel` can fix it now. Or you can switch `numpy 1.18.5` for `1.19.3+`?. - Part of this problem origins from using Python 3.6 if I understand correctly [this](https://github.com/pypa/wheel/issues/331#issuecomment-579285573), maybe you should update version of Python installing by script?. - I've just tested your installation script with one change: I added `Ubuntu 18.04` to this [check](https://github.com/google/deepvariant/blob/r1.1/run-prereq.sh#L120) - looks like all good. If a problem was in `Ubuntu 14.04` - maybe wide this check a bit? Even `Ubuntu 20.04` is released half a year ago.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:116,Deployability,release,release,116,"Hello DeepVariant team, thanks for great tool. After we tried to upgrade our deepvariant installation to the latest release we encountered a problem with `numpy` installation, which I described [here](https://github.com/pypa/wheel/issues/389). The problem is caused by `wheel` - not by `numpy` itself, but this error raised several questions for me:. - For now looks like installing deepvariant with https://github.com/google/deepvariant/blob/r1.1/build-prereq.sh will fail on non `Ubuntu 16.04` due to `numpy` problem. This problem won't be fixed in older versions of `numpy` I think - only `wheel` can fix it now. Or you can switch `numpy 1.18.5` for `1.19.3+`?. - Part of this problem origins from using Python 3.6 if I understand correctly [this](https://github.com/pypa/wheel/issues/331#issuecomment-579285573), maybe you should update version of Python installing by script?. - I've just tested your installation script with one change: I added `Ubuntu 18.04` to this [check](https://github.com/google/deepvariant/blob/r1.1/run-prereq.sh#L120) - looks like all good. If a problem was in `Ubuntu 14.04` - maybe wide this check a bit? Even `Ubuntu 20.04` is released half a year ago.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:162,Deployability,install,installation,162,"Hello DeepVariant team, thanks for great tool. After we tried to upgrade our deepvariant installation to the latest release we encountered a problem with `numpy` installation, which I described [here](https://github.com/pypa/wheel/issues/389). The problem is caused by `wheel` - not by `numpy` itself, but this error raised several questions for me:. - For now looks like installing deepvariant with https://github.com/google/deepvariant/blob/r1.1/build-prereq.sh will fail on non `Ubuntu 16.04` due to `numpy` problem. This problem won't be fixed in older versions of `numpy` I think - only `wheel` can fix it now. Or you can switch `numpy 1.18.5` for `1.19.3+`?. - Part of this problem origins from using Python 3.6 if I understand correctly [this](https://github.com/pypa/wheel/issues/331#issuecomment-579285573), maybe you should update version of Python installing by script?. - I've just tested your installation script with one change: I added `Ubuntu 18.04` to this [check](https://github.com/google/deepvariant/blob/r1.1/run-prereq.sh#L120) - looks like all good. If a problem was in `Ubuntu 14.04` - maybe wide this check a bit? Even `Ubuntu 20.04` is released half a year ago.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:372,Deployability,install,installing,372,"Hello DeepVariant team, thanks for great tool. After we tried to upgrade our deepvariant installation to the latest release we encountered a problem with `numpy` installation, which I described [here](https://github.com/pypa/wheel/issues/389). The problem is caused by `wheel` - not by `numpy` itself, but this error raised several questions for me:. - For now looks like installing deepvariant with https://github.com/google/deepvariant/blob/r1.1/build-prereq.sh will fail on non `Ubuntu 16.04` due to `numpy` problem. This problem won't be fixed in older versions of `numpy` I think - only `wheel` can fix it now. Or you can switch `numpy 1.18.5` for `1.19.3+`?. - Part of this problem origins from using Python 3.6 if I understand correctly [this](https://github.com/pypa/wheel/issues/331#issuecomment-579285573), maybe you should update version of Python installing by script?. - I've just tested your installation script with one change: I added `Ubuntu 18.04` to this [check](https://github.com/google/deepvariant/blob/r1.1/run-prereq.sh#L120) - looks like all good. If a problem was in `Ubuntu 14.04` - maybe wide this check a bit? Even `Ubuntu 20.04` is released half a year ago.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:834,Deployability,update,update,834,"Hello DeepVariant team, thanks for great tool. After we tried to upgrade our deepvariant installation to the latest release we encountered a problem with `numpy` installation, which I described [here](https://github.com/pypa/wheel/issues/389). The problem is caused by `wheel` - not by `numpy` itself, but this error raised several questions for me:. - For now looks like installing deepvariant with https://github.com/google/deepvariant/blob/r1.1/build-prereq.sh will fail on non `Ubuntu 16.04` due to `numpy` problem. This problem won't be fixed in older versions of `numpy` I think - only `wheel` can fix it now. Or you can switch `numpy 1.18.5` for `1.19.3+`?. - Part of this problem origins from using Python 3.6 if I understand correctly [this](https://github.com/pypa/wheel/issues/331#issuecomment-579285573), maybe you should update version of Python installing by script?. - I've just tested your installation script with one change: I added `Ubuntu 18.04` to this [check](https://github.com/google/deepvariant/blob/r1.1/run-prereq.sh#L120) - looks like all good. If a problem was in `Ubuntu 14.04` - maybe wide this check a bit? Even `Ubuntu 20.04` is released half a year ago.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:859,Deployability,install,installing,859,"Hello DeepVariant team, thanks for great tool. After we tried to upgrade our deepvariant installation to the latest release we encountered a problem with `numpy` installation, which I described [here](https://github.com/pypa/wheel/issues/389). The problem is caused by `wheel` - not by `numpy` itself, but this error raised several questions for me:. - For now looks like installing deepvariant with https://github.com/google/deepvariant/blob/r1.1/build-prereq.sh will fail on non `Ubuntu 16.04` due to `numpy` problem. This problem won't be fixed in older versions of `numpy` I think - only `wheel` can fix it now. Or you can switch `numpy 1.18.5` for `1.19.3+`?. - Part of this problem origins from using Python 3.6 if I understand correctly [this](https://github.com/pypa/wheel/issues/331#issuecomment-579285573), maybe you should update version of Python installing by script?. - I've just tested your installation script with one change: I added `Ubuntu 18.04` to this [check](https://github.com/google/deepvariant/blob/r1.1/run-prereq.sh#L120) - looks like all good. If a problem was in `Ubuntu 14.04` - maybe wide this check a bit? Even `Ubuntu 20.04` is released half a year ago.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:906,Deployability,install,installation,906,"Hello DeepVariant team, thanks for great tool. After we tried to upgrade our deepvariant installation to the latest release we encountered a problem with `numpy` installation, which I described [here](https://github.com/pypa/wheel/issues/389). The problem is caused by `wheel` - not by `numpy` itself, but this error raised several questions for me:. - For now looks like installing deepvariant with https://github.com/google/deepvariant/blob/r1.1/build-prereq.sh will fail on non `Ubuntu 16.04` due to `numpy` problem. This problem won't be fixed in older versions of `numpy` I think - only `wheel` can fix it now. Or you can switch `numpy 1.18.5` for `1.19.3+`?. - Part of this problem origins from using Python 3.6 if I understand correctly [this](https://github.com/pypa/wheel/issues/331#issuecomment-579285573), maybe you should update version of Python installing by script?. - I've just tested your installation script with one change: I added `Ubuntu 18.04` to this [check](https://github.com/google/deepvariant/blob/r1.1/run-prereq.sh#L120) - looks like all good. If a problem was in `Ubuntu 14.04` - maybe wide this check a bit? Even `Ubuntu 20.04` is released half a year ago.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:1162,Deployability,release,released,1162,"Hello DeepVariant team, thanks for great tool. After we tried to upgrade our deepvariant installation to the latest release we encountered a problem with `numpy` installation, which I described [here](https://github.com/pypa/wheel/issues/389). The problem is caused by `wheel` - not by `numpy` itself, but this error raised several questions for me:. - For now looks like installing deepvariant with https://github.com/google/deepvariant/blob/r1.1/build-prereq.sh will fail on non `Ubuntu 16.04` due to `numpy` problem. This problem won't be fixed in older versions of `numpy` I think - only `wheel` can fix it now. Or you can switch `numpy 1.18.5` for `1.19.3+`?. - Part of this problem origins from using Python 3.6 if I understand correctly [this](https://github.com/pypa/wheel/issues/331#issuecomment-579285573), maybe you should update version of Python installing by script?. - I've just tested your installation script with one change: I added `Ubuntu 18.04` to this [check](https://github.com/google/deepvariant/blob/r1.1/run-prereq.sh#L120) - looks like all good. If a problem was in `Ubuntu 14.04` - maybe wide this check a bit? Even `Ubuntu 20.04` is released half a year ago.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/394:894,Testability,test,tested,894,"Hello DeepVariant team, thanks for great tool. After we tried to upgrade our deepvariant installation to the latest release we encountered a problem with `numpy` installation, which I described [here](https://github.com/pypa/wheel/issues/389). The problem is caused by `wheel` - not by `numpy` itself, but this error raised several questions for me:. - For now looks like installing deepvariant with https://github.com/google/deepvariant/blob/r1.1/build-prereq.sh will fail on non `Ubuntu 16.04` due to `numpy` problem. This problem won't be fixed in older versions of `numpy` I think - only `wheel` can fix it now. Or you can switch `numpy 1.18.5` for `1.19.3+`?. - Part of this problem origins from using Python 3.6 if I understand correctly [this](https://github.com/pypa/wheel/issues/331#issuecomment-579285573), maybe you should update version of Python installing by script?. - I've just tested your installation script with one change: I added `Ubuntu 18.04` to this [check](https://github.com/google/deepvariant/blob/r1.1/run-prereq.sh#L120) - looks like all good. If a problem was in `Ubuntu 14.04` - maybe wide this check a bit? Even `Ubuntu 20.04` is released half a year ago.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/394
https://github.com/google/deepvariant/issues/395:22,Deployability,install,install,22,"Dear Authors,; I just install deepvariant using conda and prepare to test the software with HiFi data. I noticed in the recently published DipAsm [package](https://github.com/shilpagarg/DipAsm) the parameters 'model_type' was set as 'PACBIO' in 'dv.sh' file, but there are only two options supplied in conda-deepvariant(wgs & wes). Since the alleged HiFidata base accuracy is very close to Illumina data, I wonder can I use wgs model for now? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/395:69,Testability,test,test,69,"Dear Authors,; I just install deepvariant using conda and prepare to test the software with HiFi data. I noticed in the recently published DipAsm [package](https://github.com/shilpagarg/DipAsm) the parameters 'model_type' was set as 'PACBIO' in 'dv.sh' file, but there are only two options supplied in conda-deepvariant(wgs & wes). Since the alleged HiFidata base accuracy is very close to Illumina data, I wonder can I use wgs model for now? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/395
https://github.com/google/deepvariant/issues/396:721,Availability,error,error,721,"I am running on BAM generated with BWA. The fastq that was used had one reads file with less sequences. Docker latest image. . /opt/deepvariant/bin/run_deepvariant --model_type WGS --num_shards 5 --output_vcf 19CT030668_deepvariant.vcf --reads 19CT030668.bam --ref ../human_g1k_v37.fasta --sample_name 19CT030668. The processe starts but when it gets to end of the BAM it exits. . 2020-12-11 15:45:32.928477: W third_party/nucleus/io/sam_reader.cc:532] Could not read base quality scores A00215:130:HK3H2DSXX:2:2549:4869:24768: Not found: Could not read base quality scores; 2020-12-11 15:45:32.931412: F deepvariant/allelecounter.cc:103] Check failed: offset + len <= read.aligned_quality_size() (1 vs. 0); Fatal Python error: Aborted. Current thread 0x00007f38280a8700 (most recent call first):; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 76 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/absl_py/absl/app.py"", line 251 in _run_main; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>; p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396
https://github.com/google/deepvariant/issues/396:728,Safety,Abort,Aborted,728,"I am running on BAM generated with BWA. The fastq that was used had one reads file with less sequences. Docker latest image. . /opt/deepvariant/bin/run_deepvariant --model_type WGS --num_shards 5 --output_vcf 19CT030668_deepvariant.vcf --reads 19CT030668.bam --ref ../human_g1k_v37.fasta --sample_name 19CT030668. The processe starts but when it gets to end of the BAM it exits. . 2020-12-11 15:45:32.928477: W third_party/nucleus/io/sam_reader.cc:532] Could not read base quality scores A00215:130:HK3H2DSXX:2:2549:4869:24768: Not found: Could not read base quality scores; 2020-12-11 15:45:32.931412: F deepvariant/allelecounter.cc:103] Check failed: offset + len <= read.aligned_quality_size() (1 vs. 0); Fatal Python error: Aborted. Current thread 0x00007f38280a8700 (most recent call first):; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 76 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/absl_py/absl/app.py"", line 251 in _run_main; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>; p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/396
https://github.com/google/deepvariant/issues/398:371,Safety,safe,safe,371,"Hello! . not an issue, a simple suggestion: several persons I know are put off by DeepVariant because they can't get a gVCF with a line per base, basically they would like to deactivate the formation of blocks. Personally, blocks have never worried me and I guess computationally it has its advantages. . But just to tlet you know as a ""user experience"". Cheers and stay safe. DeepVariant is a very awesome tool.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/398
https://github.com/google/deepvariant/issues/398:25,Usability,simpl,simple,25,"Hello! . not an issue, a simple suggestion: several persons I know are put off by DeepVariant because they can't get a gVCF with a line per base, basically they would like to deactivate the formation of blocks. Personally, blocks have never worried me and I guess computationally it has its advantages. . But just to tlet you know as a ""user experience"". Cheers and stay safe. DeepVariant is a very awesome tool.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/398
https://github.com/google/deepvariant/issues/398:337,Usability,user experience,user experience,337,"Hello! . not an issue, a simple suggestion: several persons I know are put off by DeepVariant because they can't get a gVCF with a line per base, basically they would like to deactivate the formation of blocks. Personally, blocks have never worried me and I guess computationally it has its advantages. . But just to tlet you know as a ""user experience"". Cheers and stay safe. DeepVariant is a very awesome tool.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/398
https://github.com/google/deepvariant/issues/399:396,Availability,error,error,396,"Hi,. I don't know if this is the place to report issues with running the docker pipeline on the google cloud, I have been following instructions at [https://cloud.google.com/life-sciences/docs/tutorials/deepvariant#console_1](https://cloud.google.com/life-sciences/docs/tutorials/deepvariant#console_1) and it mostly works, but the second command within the big docker call dies with some python error. . That page indicates I should email google directly at google-genomics-contact@google.com, but this address bounces, which is why I came here. Anyway I have all the commands and error messages etc, so let me know if this is the right place for that and I will post. Thanks,; Ariel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:582,Availability,error,error,582,"Hi,. I don't know if this is the place to report issues with running the docker pipeline on the google cloud, I have been following instructions at [https://cloud.google.com/life-sciences/docs/tutorials/deepvariant#console_1](https://cloud.google.com/life-sciences/docs/tutorials/deepvariant#console_1) and it mostly works, but the second command within the big docker call dies with some python error. . That page indicates I should email google directly at google-genomics-contact@google.com, but this address bounces, which is why I came here. Anyway I have all the commands and error messages etc, so let me know if this is the right place for that and I will post. Thanks,; Ariel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:80,Deployability,pipeline,pipeline,80,"Hi,. I don't know if this is the place to report issues with running the docker pipeline on the google cloud, I have been following instructions at [https://cloud.google.com/life-sciences/docs/tutorials/deepvariant#console_1](https://cloud.google.com/life-sciences/docs/tutorials/deepvariant#console_1) and it mostly works, but the second command within the big docker call dies with some python error. . That page indicates I should email google directly at google-genomics-contact@google.com, but this address bounces, which is why I came here. Anyway I have all the commands and error messages etc, so let me know if this is the right place for that and I will post. Thanks,; Ariel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/399:588,Integrability,message,messages,588,"Hi,. I don't know if this is the place to report issues with running the docker pipeline on the google cloud, I have been following instructions at [https://cloud.google.com/life-sciences/docs/tutorials/deepvariant#console_1](https://cloud.google.com/life-sciences/docs/tutorials/deepvariant#console_1) and it mostly works, but the second command within the big docker call dies with some python error. . That page indicates I should email google directly at google-genomics-contact@google.com, but this address bounces, which is why I came here. Anyway I have all the commands and error messages etc, so let me know if this is the right place for that and I will post. Thanks,; Ariel",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/399
https://github.com/google/deepvariant/issues/400:773,Availability,Error,Error,773,"I am running the Docker google/deepvariant:latest-deeptrio. **Setup**; - Operating system: Docker Desktop 3.0 (Windows 10); - DeepVariant version: DeepTrio version 1.1.0. Trio WGS data HG37 (bams aligned with BWA MEM). Command: ; /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=human_g1k_v37.fasta --reads_child=DS187706-DS187706_39743_S1_261735.bam --reads_parent1=19CT021737-19CT021737-20190619_64683_S7_346963.bam --reads_parent2=19CT021740-19CT021740-20190619_64686_S8_346962.bam --output_vcf_child DS187706_proband.vcf --output_vcf_parent1 19CT021737_mother.vcf --output_vcf_parent2 19CT021740_father.vcf --sample_name_child 'DS187706_proband' --sample_name_parent1 '19CT021737_mother.vcf' --sample_name_parent2 '19CT021740_father' --num_shards 6. - Error trace: (if applicable); parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. real 438m45.227s; user 0m12.742s; sys 0m12.847s; I1220 22:21:36.502776 140568143795968 run_deeptrio.py:584] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 600, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 582, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 5 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""human_g1k_v37.fasta"" --reads_parent1 ""19CT021737-19CT021737-20190619_64683_S7_346963.bam""",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:813,Availability,Error,Error,813,"I am running the Docker google/deepvariant:latest-deeptrio. **Setup**; - Operating system: Docker Desktop 3.0 (Windows 10); - DeepVariant version: DeepTrio version 1.1.0. Trio WGS data HG37 (bams aligned with BWA MEM). Command: ; /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=human_g1k_v37.fasta --reads_child=DS187706-DS187706_39743_S1_261735.bam --reads_parent1=19CT021737-19CT021737-20190619_64683_S7_346963.bam --reads_parent2=19CT021740-19CT021740-20190619_64686_S8_346962.bam --output_vcf_child DS187706_proband.vcf --output_vcf_parent1 19CT021737_mother.vcf --output_vcf_parent2 19CT021740_father.vcf --sample_name_child 'DS187706_proband' --sample_name_parent1 '19CT021737_mother.vcf' --sample_name_parent2 '19CT021740_father' --num_shards 6. - Error trace: (if applicable); parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. real 438m45.227s; user 0m12.742s; sys 0m12.847s; I1220 22:21:36.502776 140568143795968 run_deeptrio.py:584] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 600, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 582, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 5 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""human_g1k_v37.fasta"" --reads_parent1 ""19CT021737-19CT021737-20190619_64683_S7_346963.bam""",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/400:909,Availability,Error,Error,909,"I am running the Docker google/deepvariant:latest-deeptrio. **Setup**; - Operating system: Docker Desktop 3.0 (Windows 10); - DeepVariant version: DeepTrio version 1.1.0. Trio WGS data HG37 (bams aligned with BWA MEM). Command: ; /opt/deepvariant/bin/deeptrio/run_deeptrio --model_type=WGS --ref=human_g1k_v37.fasta --reads_child=DS187706-DS187706_39743_S1_261735.bam --reads_parent1=19CT021737-19CT021737-20190619_64683_S7_346963.bam --reads_parent2=19CT021740-19CT021740-20190619_64686_S8_346962.bam --output_vcf_child DS187706_proband.vcf --output_vcf_parent1 19CT021737_mother.vcf --output_vcf_parent2 19CT021740_father.vcf --sample_name_child 'DS187706_proband' --sample_name_parent1 '19CT021737_mother.vcf' --sample_name_parent2 '19CT021740_father' --num_shards 6. - Error trace: (if applicable); parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. real 438m45.227s; user 0m12.742s; sys 0m12.847s; I1220 22:21:36.502776 140568143795968 run_deeptrio.py:584] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 600, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 582, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 5 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""human_g1k_v37.fasta"" --reads_parent1 ""19CT021737-19CT021737-20190619_64683_S7_346963.bam""",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/400
https://github.com/google/deepvariant/issues/402:1380,Availability,Error,Error,1380,"**Describe the issue:**; When I try to run DeepVariant using the examples in the quickstart document I receive the following output:. ```; INFO: Using cached SIF image; --ref is required.; Pass --helpshort or --helpfull to see help on flags.run_deepvariant.sh: line 13: --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory; ```. I am able to open the FASTA file at that path, so I know that it exists. The full script I am using is:. ```; #!/bin/sh. BIN_VERSION=""1.0.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. **Setup**; - Operating system: Linux, cluster; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker, through Singularity; - Type of data: The data from the quickstart . **Steps to reproduce:**; - Command: See above; - Error trace: See above. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. My issue is with the quickstart. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:1200,Deployability,Install,Installation,1200,"**Describe the issue:**; When I try to run DeepVariant using the examples in the quickstart document I receive the following output:. ```; INFO: Using cached SIF image; --ref is required.; Pass --helpshort or --helpfull to see help on flags.run_deepvariant.sh: line 13: --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory; ```. I am able to open the FASTA file at that path, so I know that it exists. The full script I am using is:. ```; #!/bin/sh. BIN_VERSION=""1.0.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. **Setup**; - Operating system: Linux, cluster; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker, through Singularity; - Type of data: The data from the quickstart . **Steps to reproduce:**; - Command: See above; - Error trace: See above. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. My issue is with the quickstart. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:151,Performance,cache,cached,151,"**Describe the issue:**; When I try to run DeepVariant using the examples in the quickstart document I receive the following output:. ```; INFO: Using cached SIF image; --ref is required.; Pass --helpshort or --helpfull to see help on flags.run_deepvariant.sh: line 13: --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory; ```. I am able to open the FASTA file at that path, so I know that it exists. The full script I am using is:. ```; #!/bin/sh. BIN_VERSION=""1.0.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. **Setup**; - Operating system: Linux, cluster; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker, through Singularity; - Type of data: The data from the quickstart . **Steps to reproduce:**; - Command: See above; - Error trace: See above. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. My issue is with the quickstart. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:310,Testability,test,testdata,310,"**Describe the issue:**; When I try to run DeepVariant using the examples in the quickstart document I receive the following output:. ```; INFO: Using cached SIF image; --ref is required.; Pass --helpshort or --helpfull to see help on flags.run_deepvariant.sh: line 13: --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory; ```. I am able to open the FASTA file at that path, so I know that it exists. The full script I am using is:. ```; #!/bin/sh. BIN_VERSION=""1.0.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. **Setup**; - Operating system: Linux, cluster; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker, through Singularity; - Type of data: The data from the quickstart . **Steps to reproduce:**; - Command: See above; - Error trace: See above. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. My issue is with the quickstart. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:554,Testability,test,testdata,554,"**Describe the issue:**; When I try to run DeepVariant using the examples in the quickstart document I receive the following output:. ```; INFO: Using cached SIF image; --ref is required.; Pass --helpshort or --helpfull to see help on flags.run_deepvariant.sh: line 13: --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory; ```. I am able to open the FASTA file at that path, so I know that it exists. The full script I am using is:. ```; #!/bin/sh. BIN_VERSION=""1.0.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. **Setup**; - Operating system: Linux, cluster; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker, through Singularity; - Type of data: The data from the quickstart . **Steps to reproduce:**; - Command: See above; - Error trace: See above. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. My issue is with the quickstart. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:1427,Testability,test,test,1427,"**Describe the issue:**; When I try to run DeepVariant using the examples in the quickstart document I receive the following output:. ```; INFO: Using cached SIF image; --ref is required.; Pass --helpshort or --helpfull to see help on flags.run_deepvariant.sh: line 13: --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory; ```. I am able to open the FASTA file at that path, so I know that it exists. The full script I am using is:. ```; #!/bin/sh. BIN_VERSION=""1.0.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. **Setup**; - Operating system: Linux, cluster; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker, through Singularity; - Type of data: The data from the quickstart . **Steps to reproduce:**; - Command: See above; - Error trace: See above. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. My issue is with the quickstart. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/402:1463,Testability,test,test,1463,"**Describe the issue:**; When I try to run DeepVariant using the examples in the quickstart document I receive the following output:. ```; INFO: Using cached SIF image; --ref is required.; Pass --helpshort or --helpfull to see help on flags.run_deepvariant.sh: line 13: --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory; ```. I am able to open the FASTA file at that path, so I know that it exists. The full script I am using is:. ```; #!/bin/sh. BIN_VERSION=""1.0.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. **Setup**; - Operating system: Linux, cluster; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker, through Singularity; - Type of data: The data from the quickstart . **Steps to reproduce:**; - Command: See above; - Error trace: See above. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. My issue is with the quickstart. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/402
https://github.com/google/deepvariant/issues/403:650,Usability,clear,clearly,650,"Hi,. So I am trying a little bit of a peculiar setup in which I am using ONT corrected reads (with [Ratatosk](https://github.com/DecodeGenetics/Ratatosk)) to call variants with DeepVariant 1.1 in difficult to map regions. For that, I am using the default PACBIO model which works pretty well in the non-difficult to map regions from what I've seen so far.; Now in the difficult to map regions, it is a little bit of a mixed bag which I am trying to understand. More specifically, I get variants in my GVCFs such as:; ```; chr1	26740	.	C	<*>	0	.	END=26740	GT:GQ:MIN_DP:PL	./.:0:7:38,0,128; ```; What I do not understand is that the PL values seems to clearly indicate 0/1, yet the genotype is undefined. Also, my understanding of GQ is that it is derived from the PL values as the difference between most likely and second most likely genotypes. Yet, the GQ is 0 in this example. . I would really appreciate if you could shed some light on this. Thank you very much.; Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/403
https://github.com/google/deepvariant/issues/404:334,Availability,error,error,334,"Hi,; I've been encountering an issue when using the `openvino` extra arg for calling variants. When I do not use this flag, things progress as expected. When using the extra flag, both via `--call_variants_extra_args ""use_openvino=true""` to *run_deepvariant* as well as the `--use_openvino` to *call_variants*, there is the following error about being unable to save the new model due to read-only system. This is running V1.1.0 via singularity on a linux system, using the pacbio hifi model. . ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Traceback (most recent call last):; File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph; f.write(graph_def.SerializeToString()); File ""/usr/local/lib/python3.6/dist-packages/te",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2509,Availability,checkpoint,checkpoint,2509,"File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph; f.write(graph_def.SerializeToString()); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write; self._prewrite_check(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check; compat.as_bytes(self.__name), compat.as_bytes(self.__mode)); tensorflow.python.framework.errors_impl.PermissionDeniedError: model.pb; Read-only file system; ```. I guess since the model resides in the container, it can't be updated as singularity is read-only without sudo. Would there be some way of passing the `--checkpoint ""/opt/models/pacbio/model.ckpt""` parameter, but allowing it to be saved elsewhere?. Thanks,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/404:2417,Deployability,update,updated,2417,"File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/scratch/13530063.tmpdir/Bazel.runfiles_69_p3nci/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 81, in freeze_graph; f.write(graph_def.SerializeToString()); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 101, in write; self._prewrite_check(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py"", line 87, in _prewrite_check; compat.as_bytes(self.__name), compat.as_bytes(self.__mode)); tensorflow.python.framework.errors_impl.PermissionDeniedError: model.pb; Read-only file system; ```. I guess since the model resides in the container, it can't be updated as singularity is read-only without sudo. Would there be some way of passing the `--checkpoint ""/opt/models/pacbio/model.ckpt""` parameter, but allowing it to be saved elsewhere?. Thanks,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/404
https://github.com/google/deepvariant/issues/405:228,Availability,avail,available,228,"Dear,. I tried the combination of the DeepVariant and GLnexus and they are working perfectly fine. I would like to know is there a way to incrementally add to the output of the pipeline? of course, if all the previous gvcfs are available in case of new variants. I mean something like the genomicsDB which is now being used by GATK. This will be a great help in case of a large cohort because it can save a lot of time and computation power to calculate frequencies. Kind regards; Amin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/405
https://github.com/google/deepvariant/issues/405:177,Deployability,pipeline,pipeline,177,"Dear,. I tried the combination of the DeepVariant and GLnexus and they are working perfectly fine. I would like to know is there a way to incrementally add to the output of the pipeline? of course, if all the previous gvcfs are available in case of new variants. I mean something like the genomicsDB which is now being used by GATK. This will be a great help in case of a large cohort because it can save a lot of time and computation power to calculate frequencies. Kind regards; Amin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/405
https://github.com/google/deepvariant/issues/405:435,Energy Efficiency,power,power,435,"Dear,. I tried the combination of the DeepVariant and GLnexus and they are working perfectly fine. I would like to know is there a way to incrementally add to the output of the pipeline? of course, if all the previous gvcfs are available in case of new variants. I mean something like the genomicsDB which is now being used by GATK. This will be a great help in case of a large cohort because it can save a lot of time and computation power to calculate frequencies. Kind regards; Amin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/405
https://github.com/google/deepvariant/pull/407:366,Security,access,access,366,Adds the ability to link to external posts. Add an `external_url` and `source` to the frontmatter of a post and it will use the external link and list the source like this:. ![image](https://user-images.githubusercontent.com/1536935/105360574-be57a800-5bc6-11eb-98f9-f3bccc3a1100.png). A blank page is created for the post but contains a redirect tag for users that access the post via RSS.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/407
https://github.com/google/deepvariant/issues/408:34,Performance,perform,performance,34,"I would like to clarify about the performance improvements re:--call_variants_extra_args ""use_openvino=true"". I am running the Docker Desktop container of the DeepVariant with the openvino feature on Windows 10. I am not sure what version deepvariant is at moment. Below is the output and how can I know if the performance boost is actually working. It seems to be taking just as long. the data is from WGS illumina. . docker stats. ```; CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS; 81c4c886a344 lucid_brown 499.35% 3.191GiB / 44.34GiB 7.20% 3.06kB / 0B 197MB / 0B 20; ```. ```; I0122 00:06:38.538676 140590877263616 make_examples.py:535] Task 2/5: 1501 candidates (1558 examples) [521.15s elapsed]; I0122 00:13:25.409308 139769488508672 make_examples.py:535] Task 1/5: 2300 candidates (2377 examples) [2030.76s elapsed]; I0122 00:14:08.258885 140590877263616 make_examples.py:535] Task 2/5: 1600 candidates (1665 examples) [449.72s elapsed]; I0122 00:17:39.217728 139769488508672 make_examples.py:535] Task 1/5: 2400 candidates (2477 examples) [253.81s elapsed]; I0122 00:24:45.306580 139769488508672 make_examples.py:535] Task 1/5: 2500 candidates (2579 examples) [426.09s elapsed]; I0122 00:33:59.351311 139769488508672 make_examples.py:535] Task 1/5: 2600 candidates (2685 examples) [554.04s elapsed]; I0122 00:36:39.138627 139769488508672 make_examples.py:535] Task 1/5: 2702 candidates (2796 examples) [159.79s elapsed]; I0122 00:44:51.534385 140120745805568 make_examples.py:535] Task 3/5: 1900 candidates (1966 examples) [4131.79s elapsed]; I0122 00:51:27.674227 140590877263616 make_examples.py:535] Task 2/5: 1700 candidates (1765 examples) [2239.42s elapsed]; I0122 01:07:23.046070 139769488508672 make_examples.py:535] Task 1/5: 2808 candidates (2902 examples) [1843.91s elapsed]; I0122 01:14:33.716036 140590877263616 make_examples.py:535] Task 2/5: 1800 candidates (1872 examples) [1386.04s elapsed]; I0122 01:20:36.708237 139769488508672 make_examples.py:535]",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/408:311,Performance,perform,performance,311,"I would like to clarify about the performance improvements re:--call_variants_extra_args ""use_openvino=true"". I am running the Docker Desktop container of the DeepVariant with the openvino feature on Windows 10. I am not sure what version deepvariant is at moment. Below is the output and how can I know if the performance boost is actually working. It seems to be taking just as long. the data is from WGS illumina. . docker stats. ```; CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS; 81c4c886a344 lucid_brown 499.35% 3.191GiB / 44.34GiB 7.20% 3.06kB / 0B 197MB / 0B 20; ```. ```; I0122 00:06:38.538676 140590877263616 make_examples.py:535] Task 2/5: 1501 candidates (1558 examples) [521.15s elapsed]; I0122 00:13:25.409308 139769488508672 make_examples.py:535] Task 1/5: 2300 candidates (2377 examples) [2030.76s elapsed]; I0122 00:14:08.258885 140590877263616 make_examples.py:535] Task 2/5: 1600 candidates (1665 examples) [449.72s elapsed]; I0122 00:17:39.217728 139769488508672 make_examples.py:535] Task 1/5: 2400 candidates (2477 examples) [253.81s elapsed]; I0122 00:24:45.306580 139769488508672 make_examples.py:535] Task 1/5: 2500 candidates (2579 examples) [426.09s elapsed]; I0122 00:33:59.351311 139769488508672 make_examples.py:535] Task 1/5: 2600 candidates (2685 examples) [554.04s elapsed]; I0122 00:36:39.138627 139769488508672 make_examples.py:535] Task 1/5: 2702 candidates (2796 examples) [159.79s elapsed]; I0122 00:44:51.534385 140120745805568 make_examples.py:535] Task 3/5: 1900 candidates (1966 examples) [4131.79s elapsed]; I0122 00:51:27.674227 140590877263616 make_examples.py:535] Task 2/5: 1700 candidates (1765 examples) [2239.42s elapsed]; I0122 01:07:23.046070 139769488508672 make_examples.py:535] Task 1/5: 2808 candidates (2902 examples) [1843.91s elapsed]; I0122 01:14:33.716036 140590877263616 make_examples.py:535] Task 2/5: 1800 candidates (1872 examples) [1386.04s elapsed]; I0122 01:20:36.708237 139769488508672 make_examples.py:535]",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/408
https://github.com/google/deepvariant/issues/410:57,Availability,down,download,57,"In the hybrid model case study, the example bam files to download [here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-hybrid-case-study.md#download-hg003-chr20-bam) are labeled `HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam`. Have these bam files been phased in the same way as in the pacbio case study? If so, is the merged bam file able to be phased via `whatshap`, or is the pacbio bam phased independently and then merged with the short read bam. Digging through the `run_deepvariant.py` script makes it seem like the phasing information only pertains to the pacbio model specifically, but just wanted to check. Thanks,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/410
https://github.com/google/deepvariant/issues/410:159,Availability,down,download-,159,"In the hybrid model case study, the example bam files to download [here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-hybrid-case-study.md#download-hg003-chr20-bam) are labeled `HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam`. Have these bam files been phased in the same way as in the pacbio case study? If so, is the merged bam file able to be phased via `whatshap`, or is the pacbio bam phased independently and then merged with the short read bam. Digging through the `run_deepvariant.py` script makes it seem like the phasing information only pertains to the pacbio model specifically, but just wanted to check. Thanks,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/410
https://github.com/google/deepvariant/issues/411:368,Availability,Error,Error,368,"**Describe the issue:**; In ```make_examples```: The middle base of reference sequence in the window doesn't match first character of variant.reference_bases. **Setup**; - Operating system: CentOS Linux v7; - DeepVariant version: 1.1.0; - Installation method: Docker; - Type of data: WGS (Illumina 150nt pairs from GIAB HG002). **Steps to reproduce:**; - Command: ; - Error trace: (if applicable). **Does the quick start test work on your system?** Yes, it does.; Is there any way to reproduce the issue by using the quick start? No. **Any additional context:**; The goal is to call SNPs and indels in GIAB HG002 WGS data, and to compare the results with a truthset. High-confidence intervals and the truthset are at https://github.com/genome-in-a-bottle/giab_latest_release. Please see the attached bash script (command line) and output files. Two questions:; - Is ```make_examples``` parameterized correctly (see attached script and output files)?; - Can someone please explain what this error message means and suggest an appropriate approach to troubleshooting and fixing it?. [vcall.log](https://github.com/google/deepvariant/files/5858295/vcall.log); [vcall.sh.txt](https://github.com/google/deepvariant/files/5858303/vcall.sh.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/411
https://github.com/google/deepvariant/issues/411:990,Availability,error,error,990,"**Describe the issue:**; In ```make_examples```: The middle base of reference sequence in the window doesn't match first character of variant.reference_bases. **Setup**; - Operating system: CentOS Linux v7; - DeepVariant version: 1.1.0; - Installation method: Docker; - Type of data: WGS (Illumina 150nt pairs from GIAB HG002). **Steps to reproduce:**; - Command: ; - Error trace: (if applicable). **Does the quick start test work on your system?** Yes, it does.; Is there any way to reproduce the issue by using the quick start? No. **Any additional context:**; The goal is to call SNPs and indels in GIAB HG002 WGS data, and to compare the results with a truthset. High-confidence intervals and the truthset are at https://github.com/genome-in-a-bottle/giab_latest_release. Please see the attached bash script (command line) and output files. Two questions:; - Is ```make_examples``` parameterized correctly (see attached script and output files)?; - Can someone please explain what this error message means and suggest an appropriate approach to troubleshooting and fixing it?. [vcall.log](https://github.com/google/deepvariant/files/5858295/vcall.log); [vcall.sh.txt](https://github.com/google/deepvariant/files/5858303/vcall.sh.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/411
https://github.com/google/deepvariant/issues/411:239,Deployability,Install,Installation,239,"**Describe the issue:**; In ```make_examples```: The middle base of reference sequence in the window doesn't match first character of variant.reference_bases. **Setup**; - Operating system: CentOS Linux v7; - DeepVariant version: 1.1.0; - Installation method: Docker; - Type of data: WGS (Illumina 150nt pairs from GIAB HG002). **Steps to reproduce:**; - Command: ; - Error trace: (if applicable). **Does the quick start test work on your system?** Yes, it does.; Is there any way to reproduce the issue by using the quick start? No. **Any additional context:**; The goal is to call SNPs and indels in GIAB HG002 WGS data, and to compare the results with a truthset. High-confidence intervals and the truthset are at https://github.com/genome-in-a-bottle/giab_latest_release. Please see the attached bash script (command line) and output files. Two questions:; - Is ```make_examples``` parameterized correctly (see attached script and output files)?; - Can someone please explain what this error message means and suggest an appropriate approach to troubleshooting and fixing it?. [vcall.log](https://github.com/google/deepvariant/files/5858295/vcall.log); [vcall.sh.txt](https://github.com/google/deepvariant/files/5858303/vcall.sh.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/411
https://github.com/google/deepvariant/issues/411:996,Integrability,message,message,996,"**Describe the issue:**; In ```make_examples```: The middle base of reference sequence in the window doesn't match first character of variant.reference_bases. **Setup**; - Operating system: CentOS Linux v7; - DeepVariant version: 1.1.0; - Installation method: Docker; - Type of data: WGS (Illumina 150nt pairs from GIAB HG002). **Steps to reproduce:**; - Command: ; - Error trace: (if applicable). **Does the quick start test work on your system?** Yes, it does.; Is there any way to reproduce the issue by using the quick start? No. **Any additional context:**; The goal is to call SNPs and indels in GIAB HG002 WGS data, and to compare the results with a truthset. High-confidence intervals and the truthset are at https://github.com/genome-in-a-bottle/giab_latest_release. Please see the attached bash script (command line) and output files. Two questions:; - Is ```make_examples``` parameterized correctly (see attached script and output files)?; - Can someone please explain what this error message means and suggest an appropriate approach to troubleshooting and fixing it?. [vcall.log](https://github.com/google/deepvariant/files/5858295/vcall.log); [vcall.sh.txt](https://github.com/google/deepvariant/files/5858303/vcall.sh.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/411
https://github.com/google/deepvariant/issues/411:886,Modifiability,parameteriz,parameterized,886,"**Describe the issue:**; In ```make_examples```: The middle base of reference sequence in the window doesn't match first character of variant.reference_bases. **Setup**; - Operating system: CentOS Linux v7; - DeepVariant version: 1.1.0; - Installation method: Docker; - Type of data: WGS (Illumina 150nt pairs from GIAB HG002). **Steps to reproduce:**; - Command: ; - Error trace: (if applicable). **Does the quick start test work on your system?** Yes, it does.; Is there any way to reproduce the issue by using the quick start? No. **Any additional context:**; The goal is to call SNPs and indels in GIAB HG002 WGS data, and to compare the results with a truthset. High-confidence intervals and the truthset are at https://github.com/genome-in-a-bottle/giab_latest_release. Please see the attached bash script (command line) and output files. Two questions:; - Is ```make_examples``` parameterized correctly (see attached script and output files)?; - Can someone please explain what this error message means and suggest an appropriate approach to troubleshooting and fixing it?. [vcall.log](https://github.com/google/deepvariant/files/5858295/vcall.log); [vcall.sh.txt](https://github.com/google/deepvariant/files/5858303/vcall.sh.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/411
https://github.com/google/deepvariant/issues/411:421,Testability,test,test,421,"**Describe the issue:**; In ```make_examples```: The middle base of reference sequence in the window doesn't match first character of variant.reference_bases. **Setup**; - Operating system: CentOS Linux v7; - DeepVariant version: 1.1.0; - Installation method: Docker; - Type of data: WGS (Illumina 150nt pairs from GIAB HG002). **Steps to reproduce:**; - Command: ; - Error trace: (if applicable). **Does the quick start test work on your system?** Yes, it does.; Is there any way to reproduce the issue by using the quick start? No. **Any additional context:**; The goal is to call SNPs and indels in GIAB HG002 WGS data, and to compare the results with a truthset. High-confidence intervals and the truthset are at https://github.com/genome-in-a-bottle/giab_latest_release. Please see the attached bash script (command line) and output files. Two questions:; - Is ```make_examples``` parameterized correctly (see attached script and output files)?; - Can someone please explain what this error message means and suggest an appropriate approach to troubleshooting and fixing it?. [vcall.log](https://github.com/google/deepvariant/files/5858295/vcall.log); [vcall.sh.txt](https://github.com/google/deepvariant/files/5858303/vcall.sh.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/411
https://github.com/google/deepvariant/issues/411:1088,Testability,log,log,1088,"**Describe the issue:**; In ```make_examples```: The middle base of reference sequence in the window doesn't match first character of variant.reference_bases. **Setup**; - Operating system: CentOS Linux v7; - DeepVariant version: 1.1.0; - Installation method: Docker; - Type of data: WGS (Illumina 150nt pairs from GIAB HG002). **Steps to reproduce:**; - Command: ; - Error trace: (if applicable). **Does the quick start test work on your system?** Yes, it does.; Is there any way to reproduce the issue by using the quick start? No. **Any additional context:**; The goal is to call SNPs and indels in GIAB HG002 WGS data, and to compare the results with a truthset. High-confidence intervals and the truthset are at https://github.com/genome-in-a-bottle/giab_latest_release. Please see the attached bash script (command line) and output files. Two questions:; - Is ```make_examples``` parameterized correctly (see attached script and output files)?; - Can someone please explain what this error message means and suggest an appropriate approach to troubleshooting and fixing it?. [vcall.log](https://github.com/google/deepvariant/files/5858295/vcall.log); [vcall.sh.txt](https://github.com/google/deepvariant/files/5858303/vcall.sh.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/411
https://github.com/google/deepvariant/issues/411:1151,Testability,log,log,1151,"**Describe the issue:**; In ```make_examples```: The middle base of reference sequence in the window doesn't match first character of variant.reference_bases. **Setup**; - Operating system: CentOS Linux v7; - DeepVariant version: 1.1.0; - Installation method: Docker; - Type of data: WGS (Illumina 150nt pairs from GIAB HG002). **Steps to reproduce:**; - Command: ; - Error trace: (if applicable). **Does the quick start test work on your system?** Yes, it does.; Is there any way to reproduce the issue by using the quick start? No. **Any additional context:**; The goal is to call SNPs and indels in GIAB HG002 WGS data, and to compare the results with a truthset. High-confidence intervals and the truthset are at https://github.com/genome-in-a-bottle/giab_latest_release. Please see the attached bash script (command line) and output files. Two questions:; - Is ```make_examples``` parameterized correctly (see attached script and output files)?; - Can someone please explain what this error message means and suggest an appropriate approach to troubleshooting and fixing it?. [vcall.log](https://github.com/google/deepvariant/files/5858295/vcall.log); [vcall.sh.txt](https://github.com/google/deepvariant/files/5858303/vcall.sh.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/411
https://github.com/google/deepvariant/issues/412:414,Availability,Error,Error,414,"I thought I should be able to call variants from short reads aligned to the human genome (hg19) with hisat2. So I gave it a try. Windows 10 Docker desktop. . `/opt/deepvariant/bin/run_deepvariant --model_type=WGS --output_vcf=CDAR-016_run0124_deepvariant.vcf --reads=CDAR-016_human37_align.bam --ref=./CFM032/human_g1k_v37.fasta --novcf_stats_report --num_shards=5 --call_variants_extra_args ""use_openvino=true""`. Error:; ```; I0123 08:06:11.771944 139769488508672 make_examples.py:535] Task 1/5: 9704 candidates (9978 examples) [651.40s elapsed]; I0123 08:07:25.050816 140120745805568 make_examples.py:535] Task 3/5: 9400 candidates (9710 examples) [246.46s elapsed]; I0123 08:09:37.633983 140665948894976 make_examples.py:535] Task 4/5: 6501 candidates (6671 examples) [1447.91s elapsed]; I0123 08:10:30.871732 140590877263616 make_examples.py:535] Task 2/5: 7503 candidates (7798 examples) [1754.74s elapsed]; I0123 08:14:47.526611 139645822764800 make_examples.py:535] Task 0/5: 8701 candidates (9015 examples) [4857.13s elapsed]; I0123 08:16:17.660391 140590877263616 make_examples.py:535] Task 2/5: 7600 candidates (7895 examples) [346.79s elapsed]; I0123 08:17:25.425076 140120745805568 make_examples.py:535] Task 3/5: 9501 candidates (9813 examples) [600.37s elapsed]; I0123 08:23:17.762356 140590877263616 make_examples.py:535] Task 2/5: 7700 candidates (8000 examples) [420.10s elapsed]; I0123 08:28:08.231647 140590877263616 make_examples.py:535] Task 2/5: 7800 candidates (8102 examples) [290.47s elapsed]; I0123 08:33:41.777438 139769488508672 make_examples.py:535] Task 1/5: 9800 candidates (10074 examples) [1650.01s elapsed]; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref ./CFM032/human_g1k_v37.fasta --reads run0124_lane9_indexUDI0001-UDI0001=CDAR-016_human37_align.bam --examples /tmp/tmpfh_b9v77/make_examples.tfrecord@5.gz --task 1. real 2480m8.473s; user 2463m44.983s; sys 2m30.474s; I0123 08:51:28.346467 139763485337344 run_deepvariant.py:3",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/412
https://github.com/google/deepvariant/issues/413:1231,Availability,Error,Error,1231,"ss The 2 different gvcf are name as followed:. SPLIT2.gvcf.tfrecord-00000-of-00030.gz; SPLIT.gvcf.tfrecord-00000-of-00030.gz. Both in the same directory. I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? . **Setup**; - Linux; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WGS from shallow resequencing data. **Steps to reproduce:**; - Command: "" /opt/deepvariant/bin/postprocess_variants --ref ""/ref/100kbPrad_v1_scaffolds.fasta"" --infile ""/input/call_variants_output.tfrecord.gz"" --outfile ""/output/MergedSplited.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" --gvcf_outfile ""/output/MergedSplited.output.g.vcf.gz"" --vcf_stats_report=False"" ; ; - Error trace: ; Singularity> time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/100kbPrad_v1_scaffolds.fasta"" --infile ""/input/call_variants_output.tfrecord.gz"" --outfile ""/output/MergedSplited.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" --gvcf_outfile ""/output/MergedSplited.output.g.vcf.gz"" --vcf_stats_report=False; 2021-01-26 15:54:17.883294: I deepvariant/postprocess_variants.cc:88] Read from: /input/call_variants_output.tfrecord.gz; 2021-01-26 16:15:59.645306: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 144590159; I0126 17:29:21.938455 140157300115200 postprocess_variants.py:1079] CVO sorting took 95.07083837985992 minutes; I0126 17:29:21.940265 140157300115200 postprocess_variants.py:1081] Transforming call_variants_output to variants.; I0126 17:29:22.052276 140157300115200 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF.; I0126 17:29:22.509974 140157300115200 genomics_writer.py:172] Writing /output/MergedSplited.ou",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:4199,Availability,Error,Error,4199,"] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main; vcf_writer, gvcf_writer); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 953, in merge_and_write_variants_and_nonvariants; nonvariant = next_or_none(nonvariant_iterable); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 948, in next_or_none; return next(iterable); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/third_party/nucleus/io/tfrecord.py"", line 139, in read_shard_sorted_tfrecords; protos = Reader(path, proto, compression_type=compression_type).iterate(); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/third_party/nucleus/io/tfrecord.py"", line 60, in Reader; path, proto, compression_type=compression_type); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 174, in __init__; 'Error trying to open %s for reading' % input_path); OSError: [Errno 5] Error trying to open /input/gvcf.tfrecord-00000-of-00030.gz for reading. real 95m15.549s; user 89m30.039s; sys 5m49.495s. **Does the quick start test work on your system?**; yes. **Any additional context:**. Appreciate any help. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:4270,Availability,Error,Error,4270,"] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main; vcf_writer, gvcf_writer); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 953, in merge_and_write_variants_and_nonvariants; nonvariant = next_or_none(nonvariant_iterable); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 948, in next_or_none; return next(iterable); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/third_party/nucleus/io/tfrecord.py"", line 139, in read_shard_sorted_tfrecords; protos = Reader(path, proto, compression_type=compression_type).iterate(); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/third_party/nucleus/io/tfrecord.py"", line 60, in Reader; path, proto, compression_type=compression_type); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 174, in __init__; 'Error trying to open %s for reading' % input_path); OSError: [Errno 5] Error trying to open /input/gvcf.tfrecord-00000-of-00030.gz for reading. real 95m15.549s; user 89m30.039s; sys 5m49.495s. **Does the quick start test work on your system?**; yes. **Any additional context:**. Appreciate any help. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:742,Deployability,Install,Installation,742,"**Describe the issue:**; I'm doing a series of test of how can I run DV faster with my resources. I'm trying splitting the bam file - run make examples on separate - run Call Variants of both at the same time and call variants. Everything goes fine until the postprocess The 2 different gvcf are name as followed:. SPLIT2.gvcf.tfrecord-00000-of-00030.gz; SPLIT.gvcf.tfrecord-00000-of-00030.gz. Both in the same directory. I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? . **Setup**; - Linux; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WGS from shallow resequencing data. **Steps to reproduce:**; - Command: "" /opt/deepvariant/bin/postprocess_variants --ref ""/ref/100kbPrad_v1_scaffolds.fasta"" --infile ""/input/call_variants_output.tfrecord.gz"" --outfile ""/output/MergedSplited.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" --gvcf_outfile ""/output/MergedSplited.output.g.vcf.gz"" --vcf_stats_report=False"" ; ; - Error trace: ; Singularity> time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/100kbPrad_v1_scaffolds.fasta"" --infile ""/input/call_variants_output.tfrecord.gz"" --outfile ""/output/MergedSplited.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" --gvcf_outfile ""/output/MergedSplited.output.g.vcf.gz"" --vcf_stats_report=False; 2021-01-26 15:54:17.883294: I deepvariant/postprocess_variants.cc:88] Read from: /input/call_variants_output.tfrecord.gz; 2021-01-26 16:15:59.645306: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 144590159; I0126 17:29:21.938455 140157300115200 postprocess_variants.py:1079] CVO sorting took 95.07083837985992 minutes; I0126 17:29:21.940265 140157300115200 postprocess_variants",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:47,Testability,test,test,47,"**Describe the issue:**; I'm doing a series of test of how can I run DV faster with my resources. I'm trying splitting the bam file - run make examples on separate - run Call Variants of both at the same time and call variants. Everything goes fine until the postprocess The 2 different gvcf are name as followed:. SPLIT2.gvcf.tfrecord-00000-of-00030.gz; SPLIT.gvcf.tfrecord-00000-of-00030.gz. Both in the same directory. I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? . **Setup**; - Linux; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WGS from shallow resequencing data. **Steps to reproduce:**; - Command: "" /opt/deepvariant/bin/postprocess_variants --ref ""/ref/100kbPrad_v1_scaffolds.fasta"" --infile ""/input/call_variants_output.tfrecord.gz"" --outfile ""/output/MergedSplited.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" --gvcf_outfile ""/output/MergedSplited.output.g.vcf.gz"" --vcf_stats_report=False"" ; ; - Error trace: ; Singularity> time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/100kbPrad_v1_scaffolds.fasta"" --infile ""/input/call_variants_output.tfrecord.gz"" --outfile ""/output/MergedSplited.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" --gvcf_outfile ""/output/MergedSplited.output.g.vcf.gz"" --vcf_stats_report=False; 2021-01-26 15:54:17.883294: I deepvariant/postprocess_variants.cc:88] Read from: /input/call_variants_output.tfrecord.gz; 2021-01-26 16:15:59.645306: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 144590159; I0126 17:29:21.938455 140157300115200 postprocess_variants.py:1079] CVO sorting took 95.07083837985992 minutes; I0126 17:29:21.940265 140157300115200 postprocess_variants",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/413:4415,Testability,test,test,4415,"] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main; vcf_writer, gvcf_writer); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 953, in merge_and_write_variants_and_nonvariants; nonvariant = next_or_none(nonvariant_iterable); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 948, in next_or_none; return next(iterable); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/third_party/nucleus/io/tfrecord.py"", line 139, in read_shard_sorted_tfrecords; protos = Reader(path, proto, compression_type=compression_type).iterate(); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/third_party/nucleus/io/tfrecord.py"", line 60, in Reader; path, proto, compression_type=compression_type); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 174, in __init__; 'Error trying to open %s for reading' % input_path); OSError: [Errno 5] Error trying to open /input/gvcf.tfrecord-00000-of-00030.gz for reading. real 95m15.549s; user 89m30.039s; sys 5m49.495s. **Does the quick start test work on your system?**; yes. **Any additional context:**. Appreciate any help. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/413
https://github.com/google/deepvariant/issues/416:116,Availability,error,error,116,"Hi,; This is all done using the singularity containers of the 1.1.0 version, on a linux system. . I encountered one error and one potential error when experimenting with DeepTrio. The first potential error is mismatched pileup heights, giving the following error when running the single command deeptrio under the PACBIO model; ` call_variants.py:355] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio).`. There are references to the pileup height defaulting to 0 and thus taking the value in **dv_constants.py**, but it appears in the **run_deeptrio.py** script [here](https://github.com/google/deepvariant/blob/5bee15713085316ae706c7f61274cd438b0006dc/scripts/run_deeptrio.py#L315), that the pileup is set to 60 and 40 **unless** being run in WES mode, then it is 100 each. I then reran the make_examples with explicit pileupheights for child and parent as 100, and the warning went away. The second issue is using openvino in deeptrio. When checking the **deeptrio_metric.md** doc, I didn't see any reference to openvino, so it may not be fully supported still, but it is a command line option. However, when using the same style of command that I got working in #404, I got the following error. ```; File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph; graph_def = optimize_for_inference_lib.optimize_for_inference(; NameError: name 'optimize_for_inference_lib' is not defined; ```. The error is a bit misleading, as that import is valid, but in a try/except block [here](https://github.com/google/deepvariant/blob/2dbeb",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:140,Availability,error,error,140,"Hi,; This is all done using the singularity containers of the 1.1.0 version, on a linux system. . I encountered one error and one potential error when experimenting with DeepTrio. The first potential error is mismatched pileup heights, giving the following error when running the single command deeptrio under the PACBIO model; ` call_variants.py:355] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio).`. There are references to the pileup height defaulting to 0 and thus taking the value in **dv_constants.py**, but it appears in the **run_deeptrio.py** script [here](https://github.com/google/deepvariant/blob/5bee15713085316ae706c7f61274cd438b0006dc/scripts/run_deeptrio.py#L315), that the pileup is set to 60 and 40 **unless** being run in WES mode, then it is 100 each. I then reran the make_examples with explicit pileupheights for child and parent as 100, and the warning went away. The second issue is using openvino in deeptrio. When checking the **deeptrio_metric.md** doc, I didn't see any reference to openvino, so it may not be fully supported still, but it is a command line option. However, when using the same style of command that I got working in #404, I got the following error. ```; File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph; graph_def = optimize_for_inference_lib.optimize_for_inference(; NameError: name 'optimize_for_inference_lib' is not defined; ```. The error is a bit misleading, as that import is valid, but in a try/except block [here](https://github.com/google/deepvariant/blob/2dbeb",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:200,Availability,error,error,200,"Hi,; This is all done using the singularity containers of the 1.1.0 version, on a linux system. . I encountered one error and one potential error when experimenting with DeepTrio. The first potential error is mismatched pileup heights, giving the following error when running the single command deeptrio under the PACBIO model; ` call_variants.py:355] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio).`. There are references to the pileup height defaulting to 0 and thus taking the value in **dv_constants.py**, but it appears in the **run_deeptrio.py** script [here](https://github.com/google/deepvariant/blob/5bee15713085316ae706c7f61274cd438b0006dc/scripts/run_deeptrio.py#L315), that the pileup is set to 60 and 40 **unless** being run in WES mode, then it is 100 each. I then reran the make_examples with explicit pileupheights for child and parent as 100, and the warning went away. The second issue is using openvino in deeptrio. When checking the **deeptrio_metric.md** doc, I didn't see any reference to openvino, so it may not be fully supported still, but it is a command line option. However, when using the same style of command that I got working in #404, I got the following error. ```; File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph; graph_def = optimize_for_inference_lib.optimize_for_inference(; NameError: name 'optimize_for_inference_lib' is not defined; ```. The error is a bit misleading, as that import is valid, but in a try/except block [here](https://github.com/google/deepvariant/blob/2dbeb",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:257,Availability,error,error,257,"Hi,; This is all done using the singularity containers of the 1.1.0 version, on a linux system. . I encountered one error and one potential error when experimenting with DeepTrio. The first potential error is mismatched pileup heights, giving the following error when running the single command deeptrio under the PACBIO model; ` call_variants.py:355] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio).`. There are references to the pileup height defaulting to 0 and thus taking the value in **dv_constants.py**, but it appears in the **run_deeptrio.py** script [here](https://github.com/google/deepvariant/blob/5bee15713085316ae706c7f61274cd438b0006dc/scripts/run_deeptrio.py#L315), that the pileup is set to 60 and 40 **unless** being run in WES mode, then it is 100 each. I then reran the make_examples with explicit pileupheights for child and parent as 100, and the warning went away. The second issue is using openvino in deeptrio. When checking the **deeptrio_metric.md** doc, I didn't see any reference to openvino, so it may not be fully supported still, but it is a command line option. However, when using the same style of command that I got working in #404, I got the following error. ```; File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph; graph_def = optimize_for_inference_lib.optimize_for_inference(; NameError: name 'optimize_for_inference_lib' is not defined; ```. The error is a bit misleading, as that import is valid, but in a try/except block [here](https://github.com/google/deepvariant/blob/2dbeb",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:1238,Availability,error,error,1238,"ential error is mismatched pileup heights, giving the following error when running the single command deeptrio under the PACBIO model; ` call_variants.py:355] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio).`. There are references to the pileup height defaulting to 0 and thus taking the value in **dv_constants.py**, but it appears in the **run_deeptrio.py** script [here](https://github.com/google/deepvariant/blob/5bee15713085316ae706c7f61274cd438b0006dc/scripts/run_deeptrio.py#L315), that the pileup is set to 60 and 40 **unless** being run in WES mode, then it is 100 each. I then reran the make_examples with explicit pileupheights for child and parent as 100, and the warning went away. The second issue is using openvino in deeptrio. When checking the **deeptrio_metric.md** doc, I didn't see any reference to openvino, so it may not be fully supported still, but it is a command line option. However, when using the same style of command that I got working in #404, I got the following error. ```; File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph; graph_def = optimize_for_inference_lib.optimize_for_inference(; NameError: name 'optimize_for_inference_lib' is not defined; ```. The error is a bit misleading, as that import is valid, but in a try/except block [here](https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L37) which actually fails because openvino is not installed in the container. . In the deepvariant image, there is `/opt/in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:1868,Availability,error,error,1868," single command deeptrio under the PACBIO model; ` call_variants.py:355] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio).`. There are references to the pileup height defaulting to 0 and thus taking the value in **dv_constants.py**, but it appears in the **run_deeptrio.py** script [here](https://github.com/google/deepvariant/blob/5bee15713085316ae706c7f61274cd438b0006dc/scripts/run_deeptrio.py#L315), that the pileup is set to 60 and 40 **unless** being run in WES mode, then it is 100 each. I then reran the make_examples with explicit pileupheights for child and parent as 100, and the warning went away. The second issue is using openvino in deeptrio. When checking the **deeptrio_metric.md** doc, I didn't see any reference to openvino, so it may not be fully supported still, but it is a command line option. However, when using the same style of command that I got working in #404, I got the following error. ```; File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph; graph_def = optimize_for_inference_lib.optimize_for_inference(; NameError: name 'optimize_for_inference_lib' is not defined; ```. The error is a bit misleading, as that import is valid, but in a try/except block [here](https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L37) which actually fails because openvino is not installed in the container. . In the deepvariant image, there is `/opt/intel/openvino/`, while there is nothing like that in the deeptrio image. Thanks,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/416:2121,Deployability,install,installed,2121," single command deeptrio under the PACBIO model; ` call_variants.py:355] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio).`. There are references to the pileup height defaulting to 0 and thus taking the value in **dv_constants.py**, but it appears in the **run_deeptrio.py** script [here](https://github.com/google/deepvariant/blob/5bee15713085316ae706c7f61274cd438b0006dc/scripts/run_deeptrio.py#L315), that the pileup is set to 60 and 40 **unless** being run in WES mode, then it is 100 each. I then reran the make_examples with explicit pileupheights for child and parent as 100, and the warning went away. The second issue is using openvino in deeptrio. When checking the **deeptrio_metric.md** doc, I didn't see any reference to openvino, so it may not be fully supported still, but it is a command line option. However, when using the same style of command that I got working in #404, I got the following error. ```; File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_fmlnwqjy/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph; graph_def = optimize_for_inference_lib.optimize_for_inference(; NameError: name 'optimize_for_inference_lib' is not defined; ```. The error is a bit misleading, as that import is valid, but in a try/except block [here](https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/openvino_estimator.py#L37) which actually fails because openvino is not installed in the container. . In the deepvariant image, there is `/opt/intel/openvino/`, while there is nothing like that in the deeptrio image. Thanks,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/416
https://github.com/google/deepvariant/issues/419:811,Availability,Error,Error,811,"**Describe the issue:**; Running DeepVariant v1.1.0 on viral amplicon PacBio HiFi data, aligned with pbmm2, using the run_deepvariant script. Core dump during the make_examples step with exit status 252. **Setup**; - Operating system: CentOS Linux release 7.4.1708 (Core), singularity version 3.5.3-1.el7; - DeepVariant version: 1.1.0; - Installation method (Docker, built from source, etc.): singularity image pulled from docker://google/deepvariant:1.1.0; - Type of data: PacBio HiFi amplicons. **Steps to reproduce:**; - Command:; ```bash; singularity exec --bind /scratch:/tmp,/usr/lib/locale/ \; docker://google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ref.fa \; --reads reads.bam \; --output_vcf ""deepvariant/output.vcf.gz"" \; --num_shards 24 -v 2; ```; - Error trace: (if applicable); ```bash; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 2. real 0m35.091s; user 0m1.452s; sys 0m1.237s; I0205 10:26:31.374659 47922265431040 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:2696,Availability,error,errors,2696,"bam \; --output_vcf ""deepvariant/output.vcf.gz"" \; --num_shards 24 -v 2; ```; - Error trace: (if applicable); ```bash; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 2. real 0m35.091s; user 0m1.452s; sys 0m1.237s; I0205 10:26:31.374659 47922265431040 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 252.; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; - I have limited access to docker in general, and no access to docker on this machine. I've been running v1.0.0 on singularity on different input types (human WGS) regularly without errors. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:248,Deployability,release,release,248,"**Describe the issue:**; Running DeepVariant v1.1.0 on viral amplicon PacBio HiFi data, aligned with pbmm2, using the run_deepvariant script. Core dump during the make_examples step with exit status 252. **Setup**; - Operating system: CentOS Linux release 7.4.1708 (Core), singularity version 3.5.3-1.el7; - DeepVariant version: 1.1.0; - Installation method (Docker, built from source, etc.): singularity image pulled from docker://google/deepvariant:1.1.0; - Type of data: PacBio HiFi amplicons. **Steps to reproduce:**; - Command:; ```bash; singularity exec --bind /scratch:/tmp,/usr/lib/locale/ \; docker://google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ref.fa \; --reads reads.bam \; --output_vcf ""deepvariant/output.vcf.gz"" \; --num_shards 24 -v 2; ```; - Error trace: (if applicable); ```bash; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 2. real 0m35.091s; user 0m1.452s; sys 0m1.237s; I0205 10:26:31.374659 47922265431040 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:338,Deployability,Install,Installation,338,"**Describe the issue:**; Running DeepVariant v1.1.0 on viral amplicon PacBio HiFi data, aligned with pbmm2, using the run_deepvariant script. Core dump during the make_examples step with exit status 252. **Setup**; - Operating system: CentOS Linux release 7.4.1708 (Core), singularity version 3.5.3-1.el7; - DeepVariant version: 1.1.0; - Installation method (Docker, built from source, etc.): singularity image pulled from docker://google/deepvariant:1.1.0; - Type of data: PacBio HiFi amplicons. **Steps to reproduce:**; - Command:; ```bash; singularity exec --bind /scratch:/tmp,/usr/lib/locale/ \; docker://google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ref.fa \; --reads reads.bam \; --output_vcf ""deepvariant/output.vcf.gz"" \; --num_shards 24 -v 2; ```; - Error trace: (if applicable); ```bash; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 2. real 0m35.091s; user 0m1.452s; sys 0m1.237s; I0205 10:26:31.374659 47922265431040 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:2531,Security,access,access,2531,"bam \; --output_vcf ""deepvariant/output.vcf.gz"" \; --num_shards 24 -v 2; ```; - Error trace: (if applicable); ```bash; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 2. real 0m35.091s; user 0m1.452s; sys 0m1.237s; I0205 10:26:31.374659 47922265431040 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 252.; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; - I have limited access to docker in general, and no access to docker on this machine. I've been running v1.0.0 on singularity on different input types (human WGS) regularly without errors. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:2567,Security,access,access,2567,"bam \; --output_vcf ""deepvariant/output.vcf.gz"" \; --num_shards 24 -v 2; ```; - Error trace: (if applicable); ```bash; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 2. real 0m35.091s; user 0m1.452s; sys 0m1.237s; I0205 10:26:31.374659 47922265431040 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 252.; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; - I have limited access to docker in general, and no access to docker on this machine. I've been running v1.0.0 on singularity on different input types (human WGS) regularly without errors. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:2318,Testability,test,test,2318,"bam \; --output_vcf ""deepvariant/output.vcf.gz"" \; --num_shards 24 -v 2; ```; - Error trace: (if applicable); ```bash; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 2. real 0m35.091s; user 0m1.452s; sys 0m1.237s; I0205 10:26:31.374659 47922265431040 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 252.; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; - I have limited access to docker in general, and no access to docker on this machine. I've been running v1.0.0 on singularity on different input types (human WGS) regularly without errors. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/419:2354,Testability,test,test,2354,"bam \; --output_vcf ""deepvariant/output.vcf.gz"" \; --num_shards 24 -v 2; ```; - Error trace: (if applicable); ```bash; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 2. real 0m35.091s; user 0m1.452s; sys 0m1.237s; I0205 10:26:31.374659 47922265431040 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 252.; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; - I have limited access to docker in general, and no access to docker on this machine. I've been running v1.0.0 on singularity on different input types (human WGS) regularly without errors. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/419
https://github.com/google/deepvariant/issues/420:487,Security,password,password,487,"I am running Docker image of your software. Pls help to solve the problem. Here are the codes for running it:. BIN_VERSION=""1.1.0""; export OUTPUT_DIR=/home/manager/deepvariant-run/input; sudo docker run -v ""${OUTPUT_DIR}"":/opt/deepvariant/output dajunluo/deepvariant python run_deepvariant.py \; --model_type=WGS \; --ref=Reference.fasta \; --reads=newtest.bam \; --output_vcf=../output/test_output.vcf.gz \; --output_gvcf=../test_output/output.g.vcf.gz; ls $OUTPUT_DIR. Result:. [sudo] password for manager: ; [E::hts_open_format] Failed to open file newtest.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open newtest.bam. real	0m6.581s; user	0m4.128s; sys	0m1.476s; Traceback (most recent call last):; File ""run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/420
https://github.com/google/deepvariant/issues/420:3077,Testability,Test,Test,3077,".reads) as sam_reader:; File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open newtest.bam. real	0m6.581s; user	0m4.128s; sys	0m1.476s; Traceback (most recent call last):; File ""run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Reference.fasta"" --reads ""newtest.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Reference.fasta"" --reads ""newtest.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}. data newtest.bam Reference Reference.fasta Test Test.bam. ------------------; (program exited with code: 0); Press return to continue. Thx for helping me",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/420
https://github.com/google/deepvariant/issues/420:3082,Testability,Test,Test,3082,".reads) as sam_reader:; File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open newtest.bam. real	0m6.581s; user	0m4.128s; sys	0m1.476s; Traceback (most recent call last):; File ""run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Reference.fasta"" --reads ""newtest.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Reference.fasta"" --reads ""newtest.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}. data newtest.bam Reference Reference.fasta Test Test.bam. ------------------; (program exited with code: 0); Press return to continue. Thx for helping me",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/420
https://github.com/google/deepvariant/issues/422:80,Availability,error,error,80,"Hello, when running DeepVariant on a machine with a GPU, we get ; [the attached error](https://github.com/google/deepvariant/files/5947987/DeepVariantError.txt); which seems to indicate that DeepVariant cannot find the samples in the working directory which is a solid state drive contained within the node. Oddly enough, when we rerun without removing the files in the /tmp directory, DeepVariant completes without error. Do you have any explanation for this? The submit command is below as system information. **Setup**; - Operating system: CentOS7, cuda/11.0; - DeepVariant version: v1.1.0; - Installation method: Singularity; - Type of data: PacBio HiFi from SQII with hg38. **Steps to reproduce:**; - Command: `singularity run --nv --bind $(readlink -f dv_wd):/wd /path/to/deepvariant/images/deepvariant_1.1.0-gpu.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/wd/hg38.ref.fasta --reads=/wd/${sample}.bam --output_vcf=/wd/${sample}.vcf --output_gvcf=/wd/${sample}.gvcf --novcf_stats_report --intermediate_results_dir=/tmp/deepvariant_tmp/$( whoami )_${sample}/ --num_shards=${threads}`; - Error trace: Included above; - We have also tried out a similar process running on another machine without a GPU, and we do not see this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/422
https://github.com/google/deepvariant/issues/422:416,Availability,error,error,416,"Hello, when running DeepVariant on a machine with a GPU, we get ; [the attached error](https://github.com/google/deepvariant/files/5947987/DeepVariantError.txt); which seems to indicate that DeepVariant cannot find the samples in the working directory which is a solid state drive contained within the node. Oddly enough, when we rerun without removing the files in the /tmp directory, DeepVariant completes without error. Do you have any explanation for this? The submit command is below as system information. **Setup**; - Operating system: CentOS7, cuda/11.0; - DeepVariant version: v1.1.0; - Installation method: Singularity; - Type of data: PacBio HiFi from SQII with hg38. **Steps to reproduce:**; - Command: `singularity run --nv --bind $(readlink -f dv_wd):/wd /path/to/deepvariant/images/deepvariant_1.1.0-gpu.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/wd/hg38.ref.fasta --reads=/wd/${sample}.bam --output_vcf=/wd/${sample}.vcf --output_gvcf=/wd/${sample}.gvcf --novcf_stats_report --intermediate_results_dir=/tmp/deepvariant_tmp/$( whoami )_${sample}/ --num_shards=${threads}`; - Error trace: Included above; - We have also tried out a similar process running on another machine without a GPU, and we do not see this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/422
https://github.com/google/deepvariant/issues/422:1115,Availability,Error,Error,1115,"Hello, when running DeepVariant on a machine with a GPU, we get ; [the attached error](https://github.com/google/deepvariant/files/5947987/DeepVariantError.txt); which seems to indicate that DeepVariant cannot find the samples in the working directory which is a solid state drive contained within the node. Oddly enough, when we rerun without removing the files in the /tmp directory, DeepVariant completes without error. Do you have any explanation for this? The submit command is below as system information. **Setup**; - Operating system: CentOS7, cuda/11.0; - DeepVariant version: v1.1.0; - Installation method: Singularity; - Type of data: PacBio HiFi from SQII with hg38. **Steps to reproduce:**; - Command: `singularity run --nv --bind $(readlink -f dv_wd):/wd /path/to/deepvariant/images/deepvariant_1.1.0-gpu.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/wd/hg38.ref.fasta --reads=/wd/${sample}.bam --output_vcf=/wd/${sample}.vcf --output_gvcf=/wd/${sample}.gvcf --novcf_stats_report --intermediate_results_dir=/tmp/deepvariant_tmp/$( whoami )_${sample}/ --num_shards=${threads}`; - Error trace: Included above; - We have also tried out a similar process running on another machine without a GPU, and we do not see this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/422
https://github.com/google/deepvariant/issues/422:596,Deployability,Install,Installation,596,"Hello, when running DeepVariant on a machine with a GPU, we get ; [the attached error](https://github.com/google/deepvariant/files/5947987/DeepVariantError.txt); which seems to indicate that DeepVariant cannot find the samples in the working directory which is a solid state drive contained within the node. Oddly enough, when we rerun without removing the files in the /tmp directory, DeepVariant completes without error. Do you have any explanation for this? The submit command is below as system information. **Setup**; - Operating system: CentOS7, cuda/11.0; - DeepVariant version: v1.1.0; - Installation method: Singularity; - Type of data: PacBio HiFi from SQII with hg38. **Steps to reproduce:**; - Command: `singularity run --nv --bind $(readlink -f dv_wd):/wd /path/to/deepvariant/images/deepvariant_1.1.0-gpu.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/wd/hg38.ref.fasta --reads=/wd/${sample}.bam --output_vcf=/wd/${sample}.vcf --output_gvcf=/wd/${sample}.gvcf --novcf_stats_report --intermediate_results_dir=/tmp/deepvariant_tmp/$( whoami )_${sample}/ --num_shards=${threads}`; - Error trace: Included above; - We have also tried out a similar process running on another machine without a GPU, and we do not see this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/422
https://github.com/google/deepvariant/issues/426:10,Testability,test,test,10,This is a test.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/426
https://github.com/google/deepvariant/issues/429:1684,Availability,Error,Error,1684,"nome from creating the BAM files. . **Steps to reproduce:** ; - Command:; ```; singularity run \; -B /usr/lib/locale/:/usr/lib/locale/,""${REF_PATH}/"":/ref_dir/,""${BAM_PATH}/"":/in_dir/,""${RESULTS_DIR}/"":/out_dir/ \; deepvariant_deeptrio-${BIN_VERSION_DT}.sif \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa \; --intermediate_results_dir=""/out_dir/${trioName}/"" \; --sample_name_child ""199713"" \; --sample_name_parent1 ""199710"" \; --sample_name_parent2 ""199718"" \; --reads_child=""/in_dir/199713.realigned.recalibrated.bam"" \; --reads_parent1=""/in_dir/199710.realigned.recalibrated.bam"" \; --reads_parent2=""/in_dir/199718.realigned.recalibrated.bam"" \; --output_vcf_child=""/out_dir/199713.output.vcf.gz"" \; --output_vcf_parent1=""/out_dir/199710.output.vcf.gz"" \; --output_vcf_parent2=""/out_dir/199718.output.vcf.gz"" \; --logging_dir=""/out_dir/${trioName}/"" \; --num_shards=$(nproc) \; --vcf_stats_report=true \; ```. - Error trace: (if applicable); ```; I0307 04:23:48.405982 46912496319168 call_variants.py:458] Processed 9497443 examples in 18550 batches [0.321 sec per 100]; I0307 04:23:48.406211 46912496319168 call_variants.py:461] Done calling variants from a total of 9497443 examples.; real	507m54.839s; user	17892m22.565s; sys	172m54.026s; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_child.tfrecord.gz"" --outfile ""/out_dir/199713.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_child.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_child.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent1.tfrecord.gz"" --outfile ""/out_dir/199710.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:3290,Availability,error,errors,3290,"ord_path ""/out_dir/199713-199710-199718/gvcf_child.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_child.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent1.tfrecord.gz"" --outfile ""/out_dir/199710.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent1.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent1.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent2.tfrecord.gz"" --outfile ""/out_dir/199718.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent2.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent2.log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.705964 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; real	0m3.173s; user	0m3.003s; sys	0m3.160s; real	0m3.194s; user	0m3.299s; sys	0m4.216s; real	0m3.254s; user	0m3.024s; sys	0m2.808s; post_process returns: [0, 0, 0]; real	2008m37.771s; user	78330m54.158s; sys	730m9.042s; ```. **Does the quick start test work on your system?** Yes.; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes, see below:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \;",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:3434,Availability,error,errors,3434,"** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent1.tfrecord.gz"" --outfile ""/out_dir/199710.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent1.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent1.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent2.tfrecord.gz"" --outfile ""/out_dir/199718.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent2.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent2.log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.705964 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; real	0m3.173s; user	0m3.003s; sys	0m3.160s; real	0m3.194s; user	0m3.299s; sys	0m4.216s; real	0m3.254s; user	0m3.024s; sys	0m2.808s; post_process returns: [0, 0, 0]; real	2008m37.771s; user	78330m54.158s; sys	730m9.042s; ```. **Does the quick start test work on your system?** Yes.; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes, see below:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=/input/GRCh38_no_alt_analysis_set.fasta \; --reads_child=/input/HG002.c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:3578,Availability,error,errors,3578,"3-199710-199718/call_variants_output_parent1.tfrecord.gz"" --outfile ""/out_dir/199710.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent1.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent1.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent2.tfrecord.gz"" --outfile ""/out_dir/199718.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent2.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent2.log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.705964 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; real	0m3.173s; user	0m3.003s; sys	0m3.160s; real	0m3.194s; user	0m3.299s; sys	0m4.216s; real	0m3.254s; user	0m3.024s; sys	0m2.808s; post_process returns: [0, 0, 0]; real	2008m37.771s; user	78330m54.158s; sys	730m9.042s; ```. **Does the quick start test work on your system?** Yes.; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes, see below:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=/input/GRCh38_no_alt_analysis_set.fasta \; --reads_child=/input/HG002.chr20.10_10p1mb.bam \; --reads_parent1=/input/HG003.chr20.10_10p1mb.bam \; --reads_parent2=/input/HG004.chr20.10_10p1mb.bam \; --output_vcf_child",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:5242,Availability,error,error,5242,"log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.705964 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; real	0m3.173s; user	0m3.003s; sys	0m3.160s; real	0m3.194s; user	0m3.299s; sys	0m4.216s; real	0m3.254s; user	0m3.024s; sys	0m2.808s; post_process returns: [0, 0, 0]; real	2008m37.771s; user	78330m54.158s; sys	730m9.042s; ```. **Does the quick start test work on your system?** Yes.; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes, see below:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=/input/GRCh38_no_alt_analysis_set.fasta \; --reads_child=/input/HG002.chr20.10_10p1mb.bam \; --reads_parent1=/input/HG003.chr20.10_10p1mb.bam \; --reads_parent2=/input/HG004.chr20.10_10p1mb.bam \; --output_vcf_child /output/HG002.output.vcf.gz \; --output_vcf_parent1 /output/HG003.output.vcf.gz \; --output_vcf_parent2 /output/HG004.output.vcf.gz \; --sample_name_child 'HG002' \; --sample_name_parent1 'HG003' \; --sample_name_parent2 'HG004' \; --num_shards $(nproc) \; --regions ""chr20:10,000,000-10,010,000"" \; --intermediate_results_dir /output/intermediate_results_dir \; ```. **Any additional context:**; DeepVariant's single command `run_deepvariant` works on my system via Singularity. DeepVariant `postprocess_variants` successfully produces a VCF file when omitting the `--output_gvcf` flag using the same data and singularity path bindings. Only DeepTrio seems to throw an error.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:406,Deployability,Install,Installation,406,"**Describe the issue:**; Are the `--output_gvcf` flags with DeepTrio required arguments? . In the `--helpfull` page of DeepTrio, they are not indicated as required; however, when omitting just these flags, DeepTrio fails to generate the expected VCF outputs. ; ```; --output_gvcf_child ; --output_gvcf_parent1; --output_gvcf_parent2; ```. **Setup**; - Operating system: Linux; - DeepTrio version: 1.1.0; - Installation method (Docker, built from source, etc.): Singularity ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) I am running DeepTrio using cattle genomes on a local machine with 56 CPUs and 500GB RAM using the same cattle reference genome from creating the BAM files. . **Steps to reproduce:** ; - Command:; ```; singularity run \; -B /usr/lib/locale/:/usr/lib/locale/,""${REF_PATH}/"":/ref_dir/,""${BAM_PATH}/"":/in_dir/,""${RESULTS_DIR}/"":/out_dir/ \; deepvariant_deeptrio-${BIN_VERSION_DT}.sif \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa \; --intermediate_results_dir=""/out_dir/${trioName}/"" \; --sample_name_child ""199713"" \; --sample_name_parent1 ""199710"" \; --sample_name_parent2 ""199718"" \; --reads_child=""/in_dir/199713.realigned.recalibrated.bam"" \; --reads_parent1=""/in_dir/199710.realigned.recalibrated.bam"" \; --reads_parent2=""/in_dir/199718.realigned.recalibrated.bam"" \; --output_vcf_child=""/out_dir/199713.output.vcf.gz"" \; --output_vcf_parent1=""/out_dir/199710.output.vcf.gz"" \; --output_vcf_parent2=""/out_dir/199718.output.vcf.gz"" \; --logging_dir=""/out_dir/${trioName}/"" \; --num_shards=$(nproc) \; --vcf_stats_report=true \; ```. - Error trace: (if applicable); ```; I0307 04:23:48.405982 46912496319168 call_variants.py:458] Processed 9497443 examples in 18550 batches [0.321 sec per 100]; I0307 04:23:48.406211 46912496319168 call_variants.py:461] Done calling variants from a total of 9497443 examples.; real	507m54.839s; user	17892m22.565s; sys	",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:2418,Testability,log,log,2418,"-output_vcf_parent1=""/out_dir/199710.output.vcf.gz"" \; --output_vcf_parent2=""/out_dir/199718.output.vcf.gz"" \; --logging_dir=""/out_dir/${trioName}/"" \; --num_shards=$(nproc) \; --vcf_stats_report=true \; ```. - Error trace: (if applicable); ```; I0307 04:23:48.405982 46912496319168 call_variants.py:458] Processed 9497443 examples in 18550 batches [0.321 sec per 100]; I0307 04:23:48.406211 46912496319168 call_variants.py:461] Done calling variants from a total of 9497443 examples.; real	507m54.839s; user	17892m22.565s; sys	172m54.026s; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_child.tfrecord.gz"" --outfile ""/out_dir/199713.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_child.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_child.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent1.tfrecord.gz"" --outfile ""/out_dir/199710.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent1.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent1.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent2.tfrecord.gz"" --outfile ""/out_dir/199718.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent2.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent2.log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires bot",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:2833,Testability,log,log,2833,"iants.py:461] Done calling variants from a total of 9497443 examples.; real	507m54.839s; user	17892m22.565s; sys	172m54.026s; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_child.tfrecord.gz"" --outfile ""/out_dir/199713.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_child.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_child.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent1.tfrecord.gz"" --outfile ""/out_dir/199710.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent1.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent1.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent2.tfrecord.gz"" --outfile ""/out_dir/199718.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent2.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent2.log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.705964 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; real	0m3.173s; user	0m3.003s; sys	0m3.160s; real	0m3.194s; user	0m3.299s; sys	0m4.216s; real	0m3.254s; user	0m3.024s; sys	0m2.808s; post_process returns: [0, 0, 0]; real	2008m37.771s; user	78330m54.158s; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:3248,Testability,log,log,3248," --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_child.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_child.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent1.tfrecord.gz"" --outfile ""/out_dir/199710.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent1.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent1.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent2.tfrecord.gz"" --outfile ""/out_dir/199718.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent2.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent2.log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.705964 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; real	0m3.173s; user	0m3.003s; sys	0m3.160s; real	0m3.194s; user	0m3.299s; sys	0m4.216s; real	0m3.254s; user	0m3.024s; sys	0m2.808s; post_process returns: [0, 0, 0]; real	2008m37.771s; user	78330m54.158s; sys	730m9.042s; ```. **Does the quick start test work on your system?** Yes.; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes, see below:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:dee",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:3933,Testability,test,test,3933,"S-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent2.tfrecord.gz"" --outfile ""/out_dir/199718.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent2.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent2.log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.705964 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; real	0m3.173s; user	0m3.003s; sys	0m3.160s; real	0m3.194s; user	0m3.299s; sys	0m4.216s; real	0m3.254s; user	0m3.024s; sys	0m2.808s; post_process returns: [0, 0, 0]; real	2008m37.771s; user	78330m54.158s; sys	730m9.042s; ```. **Does the quick start test work on your system?** Yes.; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes, see below:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=/input/GRCh38_no_alt_analysis_set.fasta \; --reads_child=/input/HG002.chr20.10_10p1mb.bam \; --reads_parent1=/input/HG003.chr20.10_10p1mb.bam \; --reads_parent2=/input/HG004.chr20.10_10p1mb.bam \; --output_vcf_child /output/HG002.output.vcf.gz \; --output_vcf_parent1 /output/HG003.output.vcf.gz \; --output_vcf_parent2 /output/HG004.output.vcf.gz \; --sample_name_child 'HG002' \; --sample_name_parent1 'HG003' \; --sample_name_parent2 'HG004' \; --num_shards $(nproc) \; --regions ""chr20:10,000,000-10,010,000"" \; --intermediate_results_dir /output/intermediate_results_dir \; ``",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/429:3974,Testability,test,test,3974,"9713-199710-199718/call_variants_output_parent2.tfrecord.gz"" --outfile ""/out_dir/199718.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent2.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent2.log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.705964 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; real	0m3.173s; user	0m3.003s; sys	0m3.160s; real	0m3.194s; user	0m3.299s; sys	0m4.216s; real	0m3.254s; user	0m3.024s; sys	0m2.808s; post_process returns: [0, 0, 0]; real	2008m37.771s; user	78330m54.158s; sys	730m9.042s; ```. **Does the quick start test work on your system?** Yes.; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes, see below:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=/input/GRCh38_no_alt_analysis_set.fasta \; --reads_child=/input/HG002.chr20.10_10p1mb.bam \; --reads_parent1=/input/HG003.chr20.10_10p1mb.bam \; --reads_parent2=/input/HG004.chr20.10_10p1mb.bam \; --output_vcf_child /output/HG002.output.vcf.gz \; --output_vcf_parent1 /output/HG003.output.vcf.gz \; --output_vcf_parent2 /output/HG004.output.vcf.gz \; --sample_name_child 'HG002' \; --sample_name_parent1 'HG003' \; --sample_name_parent2 'HG004' \; --num_shards $(nproc) \; --regions ""chr20:10,000,000-10,010,000"" \; --intermediate_results_dir /output/intermediate_results_dir \; ```. **Any additional context:**; DeepVariant's",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/429
https://github.com/google/deepvariant/issues/430:65,Availability,avail,available,65,"Is there a plan to support nVidia GPU Cuda 11 version in dockers available in the dockerhub? . Current version of `google/deepvariant:latest-gpu` uses Cuda 10, which does not work on newest nVidia GPUs.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/430
https://github.com/google/deepvariant/issues/431:127,Availability,error,error,127,"I ran deeptrio on a trio WGS data. I got the gvcf and vcf for parent 1 and 2 but I didn't get output from child. There were no error messages that I could find as to why. The output seems complete. **Setup**; - Operating system: Windows 10; - DeepVariant version: DeepTrio version 1.1.0; - Installation method: Docker; - Type of data: Illumina, GRCh38, trio WGS. **Steps to reproduce:**; - Command:; `/opt/deepvariant/bin/deeptrio/run_deeptrio . - --model_type=WGS ; - --ref=GRCh38_full_analysis_set_plus_decoy_hla.fa ; - --reads_child=20A0012672_P_GRCh38.bam ; - --reads_parent1=20A0012673_M_GRCh38.bam ; - --reads_parent2=NBVY8432_GRCh38.bam; - --output_vcf_child 20A0012672_P_GRCh38_deeptrio.vcf.gz ; - --output_vcf_parent1 20A0012673_M_GRCh38_deeptrio.vcf.gz ; - --output_vcf_parent2 NBVY8432_GRCh38_deeptrio.vcf.gz ; - --sample_name_child '20A0012672_P' ; - --sample_name_parent1 '20A0012673_M' ; - --sample_name_parent2 'NBVY8432' ; - --num_shards $(nproc) ; - --intermediate_results_dir ../home/tmp ; - --output_gvcf_child 20A0012672_P_GRCh38_deeptrio.gvcf.gz ; - --output_gvcf_parent1 20A0012673_M_GRCh38_deeptrio.gvcf.gz ; - --output_gvcf_parent2 NBVY8432_GRCh38_deeptrio.gvcf.gz`. - Error trace: NA",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:1193,Availability,Error,Error,1193,"I ran deeptrio on a trio WGS data. I got the gvcf and vcf for parent 1 and 2 but I didn't get output from child. There were no error messages that I could find as to why. The output seems complete. **Setup**; - Operating system: Windows 10; - DeepVariant version: DeepTrio version 1.1.0; - Installation method: Docker; - Type of data: Illumina, GRCh38, trio WGS. **Steps to reproduce:**; - Command:; `/opt/deepvariant/bin/deeptrio/run_deeptrio . - --model_type=WGS ; - --ref=GRCh38_full_analysis_set_plus_decoy_hla.fa ; - --reads_child=20A0012672_P_GRCh38.bam ; - --reads_parent1=20A0012673_M_GRCh38.bam ; - --reads_parent2=NBVY8432_GRCh38.bam; - --output_vcf_child 20A0012672_P_GRCh38_deeptrio.vcf.gz ; - --output_vcf_parent1 20A0012673_M_GRCh38_deeptrio.vcf.gz ; - --output_vcf_parent2 NBVY8432_GRCh38_deeptrio.vcf.gz ; - --sample_name_child '20A0012672_P' ; - --sample_name_parent1 '20A0012673_M' ; - --sample_name_parent2 'NBVY8432' ; - --num_shards $(nproc) ; - --intermediate_results_dir ../home/tmp ; - --output_gvcf_child 20A0012672_P_GRCh38_deeptrio.gvcf.gz ; - --output_gvcf_parent1 20A0012673_M_GRCh38_deeptrio.gvcf.gz ; - --output_gvcf_parent2 NBVY8432_GRCh38_deeptrio.gvcf.gz`. - Error trace: NA",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:290,Deployability,Install,Installation,290,"I ran deeptrio on a trio WGS data. I got the gvcf and vcf for parent 1 and 2 but I didn't get output from child. There were no error messages that I could find as to why. The output seems complete. **Setup**; - Operating system: Windows 10; - DeepVariant version: DeepTrio version 1.1.0; - Installation method: Docker; - Type of data: Illumina, GRCh38, trio WGS. **Steps to reproduce:**; - Command:; `/opt/deepvariant/bin/deeptrio/run_deeptrio . - --model_type=WGS ; - --ref=GRCh38_full_analysis_set_plus_decoy_hla.fa ; - --reads_child=20A0012672_P_GRCh38.bam ; - --reads_parent1=20A0012673_M_GRCh38.bam ; - --reads_parent2=NBVY8432_GRCh38.bam; - --output_vcf_child 20A0012672_P_GRCh38_deeptrio.vcf.gz ; - --output_vcf_parent1 20A0012673_M_GRCh38_deeptrio.vcf.gz ; - --output_vcf_parent2 NBVY8432_GRCh38_deeptrio.vcf.gz ; - --sample_name_child '20A0012672_P' ; - --sample_name_parent1 '20A0012673_M' ; - --sample_name_parent2 'NBVY8432' ; - --num_shards $(nproc) ; - --intermediate_results_dir ../home/tmp ; - --output_gvcf_child 20A0012672_P_GRCh38_deeptrio.gvcf.gz ; - --output_gvcf_parent1 20A0012673_M_GRCh38_deeptrio.gvcf.gz ; - --output_gvcf_parent2 NBVY8432_GRCh38_deeptrio.gvcf.gz`. - Error trace: NA",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/431:133,Integrability,message,messages,133,"I ran deeptrio on a trio WGS data. I got the gvcf and vcf for parent 1 and 2 but I didn't get output from child. There were no error messages that I could find as to why. The output seems complete. **Setup**; - Operating system: Windows 10; - DeepVariant version: DeepTrio version 1.1.0; - Installation method: Docker; - Type of data: Illumina, GRCh38, trio WGS. **Steps to reproduce:**; - Command:; `/opt/deepvariant/bin/deeptrio/run_deeptrio . - --model_type=WGS ; - --ref=GRCh38_full_analysis_set_plus_decoy_hla.fa ; - --reads_child=20A0012672_P_GRCh38.bam ; - --reads_parent1=20A0012673_M_GRCh38.bam ; - --reads_parent2=NBVY8432_GRCh38.bam; - --output_vcf_child 20A0012672_P_GRCh38_deeptrio.vcf.gz ; - --output_vcf_parent1 20A0012673_M_GRCh38_deeptrio.vcf.gz ; - --output_vcf_parent2 NBVY8432_GRCh38_deeptrio.vcf.gz ; - --sample_name_child '20A0012672_P' ; - --sample_name_parent1 '20A0012673_M' ; - --sample_name_parent2 'NBVY8432' ; - --num_shards $(nproc) ; - --intermediate_results_dir ../home/tmp ; - --output_gvcf_child 20A0012672_P_GRCh38_deeptrio.gvcf.gz ; - --output_gvcf_parent1 20A0012673_M_GRCh38_deeptrio.gvcf.gz ; - --output_gvcf_parent2 NBVY8432_GRCh38_deeptrio.gvcf.gz`. - Error trace: NA",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/431
https://github.com/google/deepvariant/issues/432:158,Availability,error,error,158,"I tried to build deepvariant through my own docker image. I installed deepvariant and openvino toolkit. But, when I run _call_variants.zip_ script, I get the error **_name 'optimize_for_inference_lib' is not defined_**. I was retracing steps and came to conclusion that `from openvino.inference_engine import StatusCode` part of the code is failing. StatusCode cannot be imported. Have you ever encountered the same problem ?. I installed DeepVariant1.1.0 version via Docker using Ubuntu 18.04. The command I run was ; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32 ` . and got this error:; ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:661,Availability,checkpoint,checkpoint,661,"I tried to build deepvariant through my own docker image. I installed deepvariant and openvino toolkit. But, when I run _call_variants.zip_ script, I get the error **_name 'optimize_for_inference_lib' is not defined_**. I was retracing steps and came to conclusion that `from openvino.inference_engine import StatusCode` part of the code is failing. StatusCode cannot be imported. Have you ever encountered the same problem ?. I installed DeepVariant1.1.0 version via Docker using Ubuntu 18.04. The command I run was ; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32 ` . and got this error:; ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:811,Availability,error,error,811,"I tried to build deepvariant through my own docker image. I installed deepvariant and openvino toolkit. But, when I run _call_variants.zip_ script, I get the error **_name 'optimize_for_inference_lib' is not defined_**. I was retracing steps and came to conclusion that `from openvino.inference_engine import StatusCode` part of the code is failing. StatusCode cannot be imported. Have you ever encountered the same problem ?. I installed DeepVariant1.1.0 version via Docker using Ubuntu 18.04. The command I run was ; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32 ` . and got this error:; ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:60,Deployability,install,installed,60,"I tried to build deepvariant through my own docker image. I installed deepvariant and openvino toolkit. But, when I run _call_variants.zip_ script, I get the error **_name 'optimize_for_inference_lib' is not defined_**. I was retracing steps and came to conclusion that `from openvino.inference_engine import StatusCode` part of the code is failing. StatusCode cannot be imported. Have you ever encountered the same problem ?. I installed DeepVariant1.1.0 version via Docker using Ubuntu 18.04. The command I run was ; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32 ` . and got this error:; ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/432:429,Deployability,install,installed,429,"I tried to build deepvariant through my own docker image. I installed deepvariant and openvino toolkit. But, when I run _call_variants.zip_ script, I get the error **_name 'optimize_for_inference_lib' is not defined_**. I was retracing steps and came to conclusion that `from openvino.inference_engine import StatusCode` part of the code is failing. StatusCode cannot be imported. Have you ever encountered the same problem ?. I installed DeepVariant1.1.0 version via Docker using Ubuntu 18.04. The command I run was ; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32 ` . and got this error:; ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 409, in call_variants; checkpoint_path, input_fn=tf_dataset, model=model); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 89, in __init__; freeze_graph(model, checkpoint_path, tensor_shape); File ""/tmp/Bazel.runfiles_xn6q5j3y/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/432
https://github.com/google/deepvariant/issues/434:245,Availability,error,error,245,"**Describe the issue:**; The prints that read base quality scores cannot be read, as result, no variants are reported. However, I can visualize these values in the reads in IGV. How is that these values cannot be read? This is the line with the error, which repeats one after. 2021-03-26 19:12:43.550815: W third_party/nucleus/io/sam_reader.cc:534] Could not read base quality scores m64036_210113_122249/147655225/ccs: Not found: Could not read base quality scores. **Setup**; - Operative system: Ubuntu 20.04; - DeepVariant version: 1.1.0 (latest); - Installation method: docker; - Type of data: PacBio HiFi. BAM files aligned to the reference with `minimap2 -ax map-pb`. **Steps to reproduce:**; - Command:; ```; docker run \; -v /home/user/working_directory:/input \; -v /home/user/working_directory:/output \; google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/reference.fa \; --reads=/input/file.bam \; --output_vcf=/output/file.vcf \; --call_variants_extra_args=""use_openvino=true"" \; --num_shards=4 \; --logging_dir=/output/logs; ```. **Does the quick start test work on your system?**; Yes. The test works without problem.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434
https://github.com/google/deepvariant/issues/434:553,Deployability,Install,Installation,553,"**Describe the issue:**; The prints that read base quality scores cannot be read, as result, no variants are reported. However, I can visualize these values in the reads in IGV. How is that these values cannot be read? This is the line with the error, which repeats one after. 2021-03-26 19:12:43.550815: W third_party/nucleus/io/sam_reader.cc:534] Could not read base quality scores m64036_210113_122249/147655225/ccs: Not found: Could not read base quality scores. **Setup**; - Operative system: Ubuntu 20.04; - DeepVariant version: 1.1.0 (latest); - Installation method: docker; - Type of data: PacBio HiFi. BAM files aligned to the reference with `minimap2 -ax map-pb`. **Steps to reproduce:**; - Command:; ```; docker run \; -v /home/user/working_directory:/input \; -v /home/user/working_directory:/output \; google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/reference.fa \; --reads=/input/file.bam \; --output_vcf=/output/file.vcf \; --call_variants_extra_args=""use_openvino=true"" \; --num_shards=4 \; --logging_dir=/output/logs; ```. **Does the quick start test work on your system?**; Yes. The test works without problem.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434
https://github.com/google/deepvariant/issues/434:1085,Testability,log,logs,1085,"**Describe the issue:**; The prints that read base quality scores cannot be read, as result, no variants are reported. However, I can visualize these values in the reads in IGV. How is that these values cannot be read? This is the line with the error, which repeats one after. 2021-03-26 19:12:43.550815: W third_party/nucleus/io/sam_reader.cc:534] Could not read base quality scores m64036_210113_122249/147655225/ccs: Not found: Could not read base quality scores. **Setup**; - Operative system: Ubuntu 20.04; - DeepVariant version: 1.1.0 (latest); - Installation method: docker; - Type of data: PacBio HiFi. BAM files aligned to the reference with `minimap2 -ax map-pb`. **Steps to reproduce:**; - Command:; ```; docker run \; -v /home/user/working_directory:/input \; -v /home/user/working_directory:/output \; google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/reference.fa \; --reads=/input/file.bam \; --output_vcf=/output/file.vcf \; --call_variants_extra_args=""use_openvino=true"" \; --num_shards=4 \; --logging_dir=/output/logs; ```. **Does the quick start test work on your system?**; Yes. The test works without problem.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434
https://github.com/google/deepvariant/issues/434:1119,Testability,test,test,1119,"**Describe the issue:**; The prints that read base quality scores cannot be read, as result, no variants are reported. However, I can visualize these values in the reads in IGV. How is that these values cannot be read? This is the line with the error, which repeats one after. 2021-03-26 19:12:43.550815: W third_party/nucleus/io/sam_reader.cc:534] Could not read base quality scores m64036_210113_122249/147655225/ccs: Not found: Could not read base quality scores. **Setup**; - Operative system: Ubuntu 20.04; - DeepVariant version: 1.1.0 (latest); - Installation method: docker; - Type of data: PacBio HiFi. BAM files aligned to the reference with `minimap2 -ax map-pb`. **Steps to reproduce:**; - Command:; ```; docker run \; -v /home/user/working_directory:/input \; -v /home/user/working_directory:/output \; google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/reference.fa \; --reads=/input/file.bam \; --output_vcf=/output/file.vcf \; --call_variants_extra_args=""use_openvino=true"" \; --num_shards=4 \; --logging_dir=/output/logs; ```. **Does the quick start test work on your system?**; Yes. The test works without problem.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434
https://github.com/google/deepvariant/issues/434:1157,Testability,test,test,1157,"**Describe the issue:**; The prints that read base quality scores cannot be read, as result, no variants are reported. However, I can visualize these values in the reads in IGV. How is that these values cannot be read? This is the line with the error, which repeats one after. 2021-03-26 19:12:43.550815: W third_party/nucleus/io/sam_reader.cc:534] Could not read base quality scores m64036_210113_122249/147655225/ccs: Not found: Could not read base quality scores. **Setup**; - Operative system: Ubuntu 20.04; - DeepVariant version: 1.1.0 (latest); - Installation method: docker; - Type of data: PacBio HiFi. BAM files aligned to the reference with `minimap2 -ax map-pb`. **Steps to reproduce:**; - Command:; ```; docker run \; -v /home/user/working_directory:/input \; -v /home/user/working_directory:/output \; google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/reference.fa \; --reads=/input/file.bam \; --output_vcf=/output/file.vcf \; --call_variants_extra_args=""use_openvino=true"" \; --num_shards=4 \; --logging_dir=/output/logs; ```. **Does the quick start test work on your system?**; Yes. The test works without problem.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/434
https://github.com/google/deepvariant/issues/435:778,Availability,checkpoint,checkpoint,778,"```(bash; BIN_VERSION=""1.1.0""; INPUT_DIR=""${PWD}/lecture8_data"". ls -1 ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/AO6-output""; mkdir -p ""${OUTPUT_DIR}"". docker run \; 	-v ""${INPUT_DIR}"":""/input"" \; 	-v ""${OUTPUT_DIR}"":""/output"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type=WGS \; 	--ref=/input/ref.fa \; 	--reads=/input/sample.bam \; 	--output_vcf=/output/OUTPUT_VCF.vfc \; 	--output_gvcf=/output/OUTPUT_GVCF.vfc \; 	--call_variants_extra_args=""use_openvino=true"" \; 	--num_shards=$(nproc) \; 	--logging_dir=/output/logs. ```. ```{bash}; ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpgv2oy1s_/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpgv2oy1s_/make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 323, in call_variants; first_example = tf_utils.get_one_example_from_examples_path(examples_filename); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/tf_utils.py"", line 205, in get_one_example_from_examples_path; 'Cannot find matching files with the pattern ""{}""'.format(source))",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435
https://github.com/google/deepvariant/issues/435:559,Testability,log,logs,559,"```(bash; BIN_VERSION=""1.1.0""; INPUT_DIR=""${PWD}/lecture8_data"". ls -1 ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/AO6-output""; mkdir -p ""${OUTPUT_DIR}"". docker run \; 	-v ""${INPUT_DIR}"":""/input"" \; 	-v ""${OUTPUT_DIR}"":""/output"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type=WGS \; 	--ref=/input/ref.fa \; 	--reads=/input/sample.bam \; 	--output_vcf=/output/OUTPUT_VCF.vfc \; 	--output_gvcf=/output/OUTPUT_GVCF.vfc \; 	--call_variants_extra_args=""use_openvino=true"" \; 	--num_shards=$(nproc) \; 	--logging_dir=/output/logs. ```. ```{bash}; ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpgv2oy1s_/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpgv2oy1s_/make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 323, in call_variants; first_example = tf_utils.get_one_example_from_examples_path(examples_filename); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/tf_utils.py"", line 205, in get_one_example_from_examples_path; 'Cannot find matching files with the pattern ""{}""'.format(source))",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435
https://github.com/google/deepvariant/issues/435:854,Testability,log,logs,854,"```(bash; BIN_VERSION=""1.1.0""; INPUT_DIR=""${PWD}/lecture8_data"". ls -1 ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/AO6-output""; mkdir -p ""${OUTPUT_DIR}"". docker run \; 	-v ""${INPUT_DIR}"":""/input"" \; 	-v ""${OUTPUT_DIR}"":""/output"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type=WGS \; 	--ref=/input/ref.fa \; 	--reads=/input/sample.bam \; 	--output_vcf=/output/OUTPUT_VCF.vfc \; 	--output_gvcf=/output/OUTPUT_GVCF.vfc \; 	--call_variants_extra_args=""use_openvino=true"" \; 	--num_shards=$(nproc) \; 	--logging_dir=/output/logs. ```. ```{bash}; ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpgv2oy1s_/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpgv2oy1s_/make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 323, in call_variants; first_example = tf_utils.get_one_example_from_examples_path(examples_filename); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/tf_utils.py"", line 205, in get_one_example_from_examples_path; 'Cannot find matching files with the pattern ""{}""'.format(source))",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435
https://github.com/google/deepvariant/issues/435:873,Testability,log,log,873,"```(bash; BIN_VERSION=""1.1.0""; INPUT_DIR=""${PWD}/lecture8_data"". ls -1 ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/AO6-output""; mkdir -p ""${OUTPUT_DIR}"". docker run \; 	-v ""${INPUT_DIR}"":""/input"" \; 	-v ""${OUTPUT_DIR}"":""/output"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type=WGS \; 	--ref=/input/ref.fa \; 	--reads=/input/sample.bam \; 	--output_vcf=/output/OUTPUT_VCF.vfc \; 	--output_gvcf=/output/OUTPUT_GVCF.vfc \; 	--call_variants_extra_args=""use_openvino=true"" \; 	--num_shards=$(nproc) \; 	--logging_dir=/output/logs. ```. ```{bash}; ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpgv2oy1s_/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpgv2oy1s_/make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 323, in call_variants; first_example = tf_utils.get_one_example_from_examples_path(examples_filename); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/tf_utils.py"", line 205, in get_one_example_from_examples_path; 'Cannot find matching files with the pattern ""{}""'.format(source))",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435
https://github.com/google/deepvariant/issues/435:2483,Testability,log,logs,2483,"nfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 323, in call_variants; first_example = tf_utils.get_one_example_from_examples_path(examples_filename); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/tf_utils.py"", line 205, in get_one_example_from_examples_path; 'Cannot find matching files with the pattern ""{}""'.format(source)); ValueError: Cannot find matching files with the pattern ""/tmp/tmpgv2oy1s_/make_examples.tfrecord@8.gz"". real	0m2.022s; user	0m2.036s; sys	0m0.413s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ref.fa"" --infile ""/tmp/tmpgv2oy1s_/call_variants_output.tfrecord.gz"" --outfile ""/output/OUTPUT_VCF.vfc"" --nonvariant_site_tfrecord_path ""/tmp/tmpgv2oy1s_/gvcf.tfrecord@8.gz"" --gvcf_outfile ""/output/OUTPUT_GVCF.vfc"" ) 2>&1 | tee /output/logs/postprocess_variants.log. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1184, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1107, in main; sample_name = get_sample_name(); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1053, in get_sample_name; _, record = get_cvo_paths_and_first_record(); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_googl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435
https://github.com/google/deepvariant/issues/435:2509,Testability,log,log,2509,"t/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 323, in call_variants; first_example = tf_utils.get_one_example_from_examples_path(examples_filename); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/tf_utils.py"", line 205, in get_one_example_from_examples_path; 'Cannot find matching files with the pattern ""{}""'.format(source)); ValueError: Cannot find matching files with the pattern ""/tmp/tmpgv2oy1s_/make_examples.tfrecord@8.gz"". real	0m2.022s; user	0m2.036s; sys	0m0.413s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ref.fa"" --infile ""/tmp/tmpgv2oy1s_/call_variants_output.tfrecord.gz"" --outfile ""/output/OUTPUT_VCF.vfc"" --nonvariant_site_tfrecord_path ""/tmp/tmpgv2oy1s_/gvcf.tfrecord@8.gz"" --gvcf_outfile ""/output/OUTPUT_GVCF.vfc"" ) 2>&1 | tee /output/logs/postprocess_variants.log. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1184, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1107, in main; sample_name = get_sample_name(); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1053, in get_sample_name; _, record = get_cvo_paths_and_first_record(); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/po",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/435
https://github.com/google/deepvariant/issues/436:68,Availability,avail,available,68,"Hi! The current bioconda recipe requires precompiled binaries to be available at `https://github.com/google/deepvariant/releases/download/v{{ version }}/deepvariant.zip`. I've updated the bioconda recipe to work with v1.0.0, but for a smooth update to v1.1.0, it would be excellent if you could provide zipped binaries with the release. The bioconda recipe: https://github.com/bioconda/bioconda-recipes/tree/master/recipes/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/436
https://github.com/google/deepvariant/issues/436:129,Availability,down,download,129,"Hi! The current bioconda recipe requires precompiled binaries to be available at `https://github.com/google/deepvariant/releases/download/v{{ version }}/deepvariant.zip`. I've updated the bioconda recipe to work with v1.0.0, but for a smooth update to v1.1.0, it would be excellent if you could provide zipped binaries with the release. The bioconda recipe: https://github.com/bioconda/bioconda-recipes/tree/master/recipes/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/436
https://github.com/google/deepvariant/issues/436:120,Deployability,release,releases,120,"Hi! The current bioconda recipe requires precompiled binaries to be available at `https://github.com/google/deepvariant/releases/download/v{{ version }}/deepvariant.zip`. I've updated the bioconda recipe to work with v1.0.0, but for a smooth update to v1.1.0, it would be excellent if you could provide zipped binaries with the release. The bioconda recipe: https://github.com/bioconda/bioconda-recipes/tree/master/recipes/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/436
https://github.com/google/deepvariant/issues/436:176,Deployability,update,updated,176,"Hi! The current bioconda recipe requires precompiled binaries to be available at `https://github.com/google/deepvariant/releases/download/v{{ version }}/deepvariant.zip`. I've updated the bioconda recipe to work with v1.0.0, but for a smooth update to v1.1.0, it would be excellent if you could provide zipped binaries with the release. The bioconda recipe: https://github.com/bioconda/bioconda-recipes/tree/master/recipes/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/436
https://github.com/google/deepvariant/issues/436:242,Deployability,update,update,242,"Hi! The current bioconda recipe requires precompiled binaries to be available at `https://github.com/google/deepvariant/releases/download/v{{ version }}/deepvariant.zip`. I've updated the bioconda recipe to work with v1.0.0, but for a smooth update to v1.1.0, it would be excellent if you could provide zipped binaries with the release. The bioconda recipe: https://github.com/bioconda/bioconda-recipes/tree/master/recipes/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/436
https://github.com/google/deepvariant/issues/436:328,Deployability,release,release,328,"Hi! The current bioconda recipe requires precompiled binaries to be available at `https://github.com/google/deepvariant/releases/download/v{{ version }}/deepvariant.zip`. I've updated the bioconda recipe to work with v1.0.0, but for a smooth update to v1.1.0, it would be excellent if you could provide zipped binaries with the release. The bioconda recipe: https://github.com/bioconda/bioconda-recipes/tree/master/recipes/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/436
https://github.com/google/deepvariant/issues/437:823,Usability,clear,clear,823,"Hi,; I have a question regarding the use of the _**realign_reads**_ parameter for make_examples.py script. I am interested in running DeepVariant for PACBIO long reads data, and I don't want to realign reads before variant calling. It is stated in the official description: _--[no]realign_reads: If True, locally realign reads before calling variants. Reads longer than 500 bp are never realigned.(default: 'true')_, but, in the description for running PACBIO data it is stated that **_Please note, that if you create your own script make_examples must be called with --norealign_reads flag for PacBio long reads._**. I am having trouble understanding the meaning of realign reads parameter, does the TRUE value realign or doesn't realign reads (basically is the parameter NOrealign _reads, or realign reads)? Just want to clear things up so I don't waste time on long runs. Thank you in advance! :)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/437
https://github.com/google/deepvariant/issues/440:854,Deployability,Install,Installation,854,"![DeepTrio_QUAL](https://user-images.githubusercontent.com/22089494/114759224-e5d49180-9d2b-11eb-9c5e-cb33c9979d2d.png); Hello, . I am running DeepTrio for a dataset with known true-positive SNPs and indels. I followed guidelines for DeepTrio but had to change --config DeepVariantWGS to DeepVariant_unfiltered at the glnexus_cli step as a default QUAL threshold of 10 removed a lot of my TP calls.; I have compared distributions of QUAL score in the TP subset and all calls found by DeepTrio. Please see an attached histogram of all DeepTrio calls vs TP calls. Could you please tell me if it is expected that QUAL of TP calls is between 0 and30, while there are calls with QUAL of up to 100? If not, what I could do wrong?; Thank you!. Best regards,; Maria. **Setup**; - Operating system: Linux; - DeepVariant version: deepvariant_deeptrio-1.1.0.sif; - Installation method (Docker, built from source, etc.): singularity/3.6.4; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) HiSeq X Ten, hg38, WGS",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440
https://github.com/google/deepvariant/issues/440:263,Modifiability,config,config,263,"![DeepTrio_QUAL](https://user-images.githubusercontent.com/22089494/114759224-e5d49180-9d2b-11eb-9c5e-cb33c9979d2d.png); Hello, . I am running DeepTrio for a dataset with known true-positive SNPs and indels. I followed guidelines for DeepTrio but had to change --config DeepVariantWGS to DeepVariant_unfiltered at the glnexus_cli step as a default QUAL threshold of 10 removed a lot of my TP calls.; I have compared distributions of QUAL score in the TP subset and all calls found by DeepTrio. Please see an attached histogram of all DeepTrio calls vs TP calls. Could you please tell me if it is expected that QUAL of TP calls is between 0 and30, while there are calls with QUAL of up to 100? If not, what I could do wrong?; Thank you!. Best regards,; Maria. **Setup**; - Operating system: Linux; - DeepVariant version: deepvariant_deeptrio-1.1.0.sif; - Installation method (Docker, built from source, etc.): singularity/3.6.4; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) HiSeq X Ten, hg38, WGS",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440
https://github.com/google/deepvariant/issues/440:219,Usability,guid,guidelines,219,"![DeepTrio_QUAL](https://user-images.githubusercontent.com/22089494/114759224-e5d49180-9d2b-11eb-9c5e-cb33c9979d2d.png); Hello, . I am running DeepTrio for a dataset with known true-positive SNPs and indels. I followed guidelines for DeepTrio but had to change --config DeepVariantWGS to DeepVariant_unfiltered at the glnexus_cli step as a default QUAL threshold of 10 removed a lot of my TP calls.; I have compared distributions of QUAL score in the TP subset and all calls found by DeepTrio. Please see an attached histogram of all DeepTrio calls vs TP calls. Could you please tell me if it is expected that QUAL of TP calls is between 0 and30, while there are calls with QUAL of up to 100? If not, what I could do wrong?; Thank you!. Best regards,; Maria. **Setup**; - Operating system: Linux; - DeepVariant version: deepvariant_deeptrio-1.1.0.sif; - Installation method (Docker, built from source, etc.): singularity/3.6.4; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) HiSeq X Ten, hg38, WGS",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/440
https://github.com/google/deepvariant/issues/441:342,Availability,error,error,342,"Hi, I was working on update the deepvariant source code from ubuntu 16.04 to 18.04 and python 3.6 to python 3.8. Now I met a problem in build_release_binaries.shell scripts. . bazel build -c opt \; --output_filter=DONT_MATCH_ANYTHING \; --noshow_loading_progress \; --show_result=0 \; ${DV_COPT_FLAGS} \; --build_python_zip \; :binaries. The error is below:; [1,442 / 1,802] Compiling third_party/nucleus/protos/struct.pb.cc; 1s local ... (128 actions, 48 running); (17:42:57) [1,544 / 1,802] Compiling external/org_tensorflow/tensorflow/core/util/test_log.pb.cc; 6s local ... (128 actions, 47 running); (17:43:03) ERROR: /opt/deepvariant/deepvariant/realigner/python/BUILD:54:1: C++ compilation of rule '//deepvariant/realigner/python:ssw_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python3.8 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; TF_CONFIGURE_IOS=0 \; TF_ENABLE_XLA=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.d '-frandom-seed=bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.o' -fPIC -DTF_USE_SNAPPY -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DHAVE_SYS_UIO_H -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/k8-opt/bin -iquote external/libssw -iquote bazel-out/k8-opt/bin/external/libssw -iquote external/org_tensorflow -iquote bazel-out/k8-opt/bin/external/org_tensorflow -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_g",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441
https://github.com/google/deepvariant/issues/441:615,Availability,ERROR,ERROR,615,"Hi, I was working on update the deepvariant source code from ubuntu 16.04 to 18.04 and python 3.6 to python 3.8. Now I met a problem in build_release_binaries.shell scripts. . bazel build -c opt \; --output_filter=DONT_MATCH_ANYTHING \; --noshow_loading_progress \; --show_result=0 \; ${DV_COPT_FLAGS} \; --build_python_zip \; :binaries. The error is below:; [1,442 / 1,802] Compiling third_party/nucleus/protos/struct.pb.cc; 1s local ... (128 actions, 48 running); (17:42:57) [1,544 / 1,802] Compiling external/org_tensorflow/tensorflow/core/util/test_log.pb.cc; 6s local ... (128 actions, 47 running); (17:43:03) ERROR: /opt/deepvariant/deepvariant/realigner/python/BUILD:54:1: C++ compilation of rule '//deepvariant/realigner/python:ssw_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python3.8 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; TF_CONFIGURE_IOS=0 \; TF_ENABLE_XLA=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.d '-frandom-seed=bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.o' -fPIC -DTF_USE_SNAPPY -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DHAVE_SYS_UIO_H -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/k8-opt/bin -iquote external/libssw -iquote bazel-out/k8-opt/bin/external/libssw -iquote external/org_tensorflow -iquote bazel-out/k8-opt/bin/external/org_tensorflow -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_g",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441
https://github.com/google/deepvariant/issues/441:776,Availability,error,error,776,"Hi, I was working on update the deepvariant source code from ubuntu 16.04 to 18.04 and python 3.6 to python 3.8. Now I met a problem in build_release_binaries.shell scripts. . bazel build -c opt \; --output_filter=DONT_MATCH_ANYTHING \; --noshow_loading_progress \; --show_result=0 \; ${DV_COPT_FLAGS} \; --build_python_zip \; :binaries. The error is below:; [1,442 / 1,802] Compiling third_party/nucleus/protos/struct.pb.cc; 1s local ... (128 actions, 48 running); (17:42:57) [1,544 / 1,802] Compiling external/org_tensorflow/tensorflow/core/util/test_log.pb.cc; 6s local ... (128 actions, 47 running); (17:43:03) ERROR: /opt/deepvariant/deepvariant/realigner/python/BUILD:54:1: C++ compilation of rule '//deepvariant/realigner/python:ssw_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python3.8 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; TF_CONFIGURE_IOS=0 \; TF_ENABLE_XLA=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.d '-frandom-seed=bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.o' -fPIC -DTF_USE_SNAPPY -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DHAVE_SYS_UIO_H -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/k8-opt/bin -iquote external/libssw -iquote bazel-out/k8-opt/bin/external/libssw -iquote external/org_tensorflow -iquote bazel-out/k8-opt/bin/external/org_tensorflow -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_g",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441
https://github.com/google/deepvariant/issues/441:4506,Availability,error,error,4506,"e bazel-out/k8-opt/bin/external/snappy -iquote external/clif -iquote bazel-out/k8-opt/bin/external/clif -iquote external/local_config_python -iquote bazel-out/k8-opt/bin/external/local_config_python -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/local_config_python/python_include -isystem bazel-out/k8-opt/bin/external/local_config_python/python_include -w -DAUTOLOAD_DYNAMIC_KERNELS -Wno-maybe-uninitialized -Wno-unused-function '-march=corei7' -Wno-sign-compare -Wno-write-strings '-std=c++14' '-std=c++11' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c bazel-out/k8-opt/bin/deepvariant/realigner/python/ssw.cc -o bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.o); Execution platform: @local_execution_config_platform//:platform; bazel-out/k8-opt/bin/deepvariant/realigner/python/ssw.cc:283:1: error: cannot convert 'std::nullptr_t' to 'Py_ssize_t {aka long int}' in initialization; };; ^; bazel-out/k8-opt/bin/deepvariant/realigner/python/ssw.cc:534:1: error: cannot convert 'std::nullptr_t' to 'Py_ssize_t {aka long int}' in initialization; };; ^; bazel-out/k8-opt/bin/deepvariant/realigner/python/ssw.cc:815:1: error: cannot convert 'std::nullptr_t' to 'Py_ssize_t {aka long int}' in initialization; };. I knew that ""nullptr"" might be change to ""NULL"" in pyth3.8. But how should I change the specific files?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441
https://github.com/google/deepvariant/issues/441:4666,Availability,error,error,4666,"e bazel-out/k8-opt/bin/external/snappy -iquote external/clif -iquote bazel-out/k8-opt/bin/external/clif -iquote external/local_config_python -iquote bazel-out/k8-opt/bin/external/local_config_python -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/local_config_python/python_include -isystem bazel-out/k8-opt/bin/external/local_config_python/python_include -w -DAUTOLOAD_DYNAMIC_KERNELS -Wno-maybe-uninitialized -Wno-unused-function '-march=corei7' -Wno-sign-compare -Wno-write-strings '-std=c++14' '-std=c++11' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c bazel-out/k8-opt/bin/deepvariant/realigner/python/ssw.cc -o bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.o); Execution platform: @local_execution_config_platform//:platform; bazel-out/k8-opt/bin/deepvariant/realigner/python/ssw.cc:283:1: error: cannot convert 'std::nullptr_t' to 'Py_ssize_t {aka long int}' in initialization; };; ^; bazel-out/k8-opt/bin/deepvariant/realigner/python/ssw.cc:534:1: error: cannot convert 'std::nullptr_t' to 'Py_ssize_t {aka long int}' in initialization; };; ^; bazel-out/k8-opt/bin/deepvariant/realigner/python/ssw.cc:815:1: error: cannot convert 'std::nullptr_t' to 'Py_ssize_t {aka long int}' in initialization; };. I knew that ""nullptr"" might be change to ""NULL"" in pyth3.8. But how should I change the specific files?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441
https://github.com/google/deepvariant/issues/441:4826,Availability,error,error,4826,"e bazel-out/k8-opt/bin/external/snappy -iquote external/clif -iquote bazel-out/k8-opt/bin/external/clif -iquote external/local_config_python -iquote bazel-out/k8-opt/bin/external/local_config_python -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/local_config_python/python_include -isystem bazel-out/k8-opt/bin/external/local_config_python/python_include -w -DAUTOLOAD_DYNAMIC_KERNELS -Wno-maybe-uninitialized -Wno-unused-function '-march=corei7' -Wno-sign-compare -Wno-write-strings '-std=c++14' '-std=c++11' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c bazel-out/k8-opt/bin/deepvariant/realigner/python/ssw.cc -o bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.o); Execution platform: @local_execution_config_platform//:platform; bazel-out/k8-opt/bin/deepvariant/realigner/python/ssw.cc:283:1: error: cannot convert 'std::nullptr_t' to 'Py_ssize_t {aka long int}' in initialization; };; ^; bazel-out/k8-opt/bin/deepvariant/realigner/python/ssw.cc:534:1: error: cannot convert 'std::nullptr_t' to 'Py_ssize_t {aka long int}' in initialization; };; ^; bazel-out/k8-opt/bin/deepvariant/realigner/python/ssw.cc:815:1: error: cannot convert 'std::nullptr_t' to 'Py_ssize_t {aka long int}' in initialization; };. I knew that ""nullptr"" might be change to ""NULL"" in pyth3.8. But how should I change the specific files?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441
https://github.com/google/deepvariant/issues/441:21,Deployability,update,update,21,"Hi, I was working on update the deepvariant source code from ubuntu 16.04 to 18.04 and python 3.6 to python 3.8. Now I met a problem in build_release_binaries.shell scripts. . bazel build -c opt \; --output_filter=DONT_MATCH_ANYTHING \; --noshow_loading_progress \; --show_result=0 \; ${DV_COPT_FLAGS} \; --build_python_zip \; :binaries. The error is below:; [1,442 / 1,802] Compiling third_party/nucleus/protos/struct.pb.cc; 1s local ... (128 actions, 48 running); (17:42:57) [1,544 / 1,802] Compiling external/org_tensorflow/tensorflow/core/util/test_log.pb.cc; 6s local ... (128 actions, 47 running); (17:43:03) ERROR: /opt/deepvariant/deepvariant/realigner/python/BUILD:54:1: C++ compilation of rule '//deepvariant/realigner/python:ssw_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python3.8 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; TF_CONFIGURE_IOS=0 \; TF_ENABLE_XLA=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.d '-frandom-seed=bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.o' -fPIC -DTF_USE_SNAPPY -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DHAVE_SYS_UIO_H -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/k8-opt/bin -iquote external/libssw -iquote bazel-out/k8-opt/bin/external/libssw -iquote external/org_tensorflow -iquote bazel-out/k8-opt/bin/external/org_tensorflow -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_g",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441
https://github.com/google/deepvariant/issues/441:813,Performance,cache,cache,813,"Hi, I was working on update the deepvariant source code from ubuntu 16.04 to 18.04 and python 3.6 to python 3.8. Now I met a problem in build_release_binaries.shell scripts. . bazel build -c opt \; --output_filter=DONT_MATCH_ANYTHING \; --noshow_loading_progress \; --show_result=0 \; ${DV_COPT_FLAGS} \; --build_python_zip \; :binaries. The error is below:; [1,442 / 1,802] Compiling third_party/nucleus/protos/struct.pb.cc; 1s local ... (128 actions, 48 running); (17:42:57) [1,544 / 1,802] Compiling external/org_tensorflow/tensorflow/core/util/test_log.pb.cc; 6s local ... (128 actions, 47 running); (17:43:03) ERROR: /opt/deepvariant/deepvariant/realigner/python/BUILD:54:1: C++ compilation of rule '//deepvariant/realigner/python:ssw_cclib' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python3.8 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; TF_CONFIGURE_IOS=0 \; TF_ENABLE_XLA=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.d '-frandom-seed=bazel-out/k8-opt/bin/deepvariant/realigner/python/_objs/ssw_cclib/ssw.pic.o' -fPIC -DTF_USE_SNAPPY -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DHAVE_SYS_UIO_H -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/k8-opt/bin -iquote external/libssw -iquote bazel-out/k8-opt/bin/external/libssw -iquote external/org_tensorflow -iquote bazel-out/k8-opt/bin/external/org_tensorflow -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_g",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/441
https://github.com/google/deepvariant/pull/442:2,Deployability,Install,Install,2,"* Install OpenVINO by pip; * Update OpenVINO to latest 2021.3 version; * Use `enum34==1.1.8` to fix ""AttributeError: module 'enum' has no attribute 'IntFlag'"" (https://github.com/python-poetry/poetry/issues/1122#issuecomment-628037127). test run: https://github.com/dkurt/deepvariant/actions/runs/755874669",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/442
https://github.com/google/deepvariant/pull/442:29,Deployability,Update,Update,29,"* Install OpenVINO by pip; * Update OpenVINO to latest 2021.3 version; * Use `enum34==1.1.8` to fix ""AttributeError: module 'enum' has no attribute 'IntFlag'"" (https://github.com/python-poetry/poetry/issues/1122#issuecomment-628037127). test run: https://github.com/dkurt/deepvariant/actions/runs/755874669",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/442
https://github.com/google/deepvariant/pull/442:237,Testability,test,test,237,"* Install OpenVINO by pip; * Update OpenVINO to latest 2021.3 version; * Use `enum34==1.1.8` to fix ""AttributeError: module 'enum' has no attribute 'IntFlag'"" (https://github.com/python-poetry/poetry/issues/1122#issuecomment-628037127). test run: https://github.com/dkurt/deepvariant/actions/runs/755874669",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/442
https://github.com/google/deepvariant/issues/443:186,Availability,error,error,186,"Hello,. I am trying to install DeepVariant from source on Ubuntu 1.18.04. The build-prereq.sh script finished well,; but build_and_test.sh has stopped unexpectedly do not displaying any error:. ```; (18:54:51) INFO: Found applicable config definition build:linux in file /data1/SOFT/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels; (18:54:51) INFO: Found applicable config definition build:dynamic_kernels in file /data1/SOFT/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (18:54:51) INFO: Current date is 2021-04-16; (18:54:51) INFO: Build option --build_python_zip has changed, discarding analysis cache.; (18:54:51) INFO: Analyzed target //:licenses_zip (0 packages loaded, 22 targets configured).; (18:54:51) INFO: Found 1 target...; (18:54:51) INFO: Elapsed time: 0.224s, Critical Path: 0.00s; (18:54:51) INFO: 0 processes.; + echo 'Expect a usage message:'; Expect a usage message:; + python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; + grep /call_variants.py:; /tmp/Bazel.runfiles_5qjtwbro/runfiles/com_google_deepvariant/deepvariant/call_variants.py:; + :; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443
https://github.com/google/deepvariant/issues/443:1020,Availability,echo,echo,1020,"Hello,. I am trying to install DeepVariant from source on Ubuntu 1.18.04. The build-prereq.sh script finished well,; but build_and_test.sh has stopped unexpectedly do not displaying any error:. ```; (18:54:51) INFO: Found applicable config definition build:linux in file /data1/SOFT/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels; (18:54:51) INFO: Found applicable config definition build:dynamic_kernels in file /data1/SOFT/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (18:54:51) INFO: Current date is 2021-04-16; (18:54:51) INFO: Build option --build_python_zip has changed, discarding analysis cache.; (18:54:51) INFO: Analyzed target //:licenses_zip (0 packages loaded, 22 targets configured).; (18:54:51) INFO: Found 1 target...; (18:54:51) INFO: Elapsed time: 0.224s, Critical Path: 0.00s; (18:54:51) INFO: 0 processes.; + echo 'Expect a usage message:'; Expect a usage message:; + python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; + grep /call_variants.py:; /tmp/Bazel.runfiles_5qjtwbro/runfiles/com_google_deepvariant/deepvariant/call_variants.py:; + :; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443
https://github.com/google/deepvariant/issues/443:23,Deployability,install,install,23,"Hello,. I am trying to install DeepVariant from source on Ubuntu 1.18.04. The build-prereq.sh script finished well,; but build_and_test.sh has stopped unexpectedly do not displaying any error:. ```; (18:54:51) INFO: Found applicable config definition build:linux in file /data1/SOFT/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels; (18:54:51) INFO: Found applicable config definition build:dynamic_kernels in file /data1/SOFT/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (18:54:51) INFO: Current date is 2021-04-16; (18:54:51) INFO: Build option --build_python_zip has changed, discarding analysis cache.; (18:54:51) INFO: Analyzed target //:licenses_zip (0 packages loaded, 22 targets configured).; (18:54:51) INFO: Found 1 target...; (18:54:51) INFO: Elapsed time: 0.224s, Critical Path: 0.00s; (18:54:51) INFO: 0 processes.; + echo 'Expect a usage message:'; Expect a usage message:; + python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; + grep /call_variants.py:; /tmp/Bazel.runfiles_5qjtwbro/runfiles/com_google_deepvariant/deepvariant/call_variants.py:; + :; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443
https://github.com/google/deepvariant/issues/443:1041,Integrability,message,message,1041,"Hello,. I am trying to install DeepVariant from source on Ubuntu 1.18.04. The build-prereq.sh script finished well,; but build_and_test.sh has stopped unexpectedly do not displaying any error:. ```; (18:54:51) INFO: Found applicable config definition build:linux in file /data1/SOFT/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels; (18:54:51) INFO: Found applicable config definition build:dynamic_kernels in file /data1/SOFT/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (18:54:51) INFO: Current date is 2021-04-16; (18:54:51) INFO: Build option --build_python_zip has changed, discarding analysis cache.; (18:54:51) INFO: Analyzed target //:licenses_zip (0 packages loaded, 22 targets configured).; (18:54:51) INFO: Found 1 target...; (18:54:51) INFO: Elapsed time: 0.224s, Critical Path: 0.00s; (18:54:51) INFO: 0 processes.; + echo 'Expect a usage message:'; Expect a usage message:; + python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; + grep /call_variants.py:; /tmp/Bazel.runfiles_5qjtwbro/runfiles/com_google_deepvariant/deepvariant/call_variants.py:; + :; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443
https://github.com/google/deepvariant/issues/443:1067,Integrability,message,message,1067,"Hello,. I am trying to install DeepVariant from source on Ubuntu 1.18.04. The build-prereq.sh script finished well,; but build_and_test.sh has stopped unexpectedly do not displaying any error:. ```; (18:54:51) INFO: Found applicable config definition build:linux in file /data1/SOFT/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels; (18:54:51) INFO: Found applicable config definition build:dynamic_kernels in file /data1/SOFT/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (18:54:51) INFO: Current date is 2021-04-16; (18:54:51) INFO: Build option --build_python_zip has changed, discarding analysis cache.; (18:54:51) INFO: Analyzed target //:licenses_zip (0 packages loaded, 22 targets configured).; (18:54:51) INFO: Found 1 target...; (18:54:51) INFO: Elapsed time: 0.224s, Critical Path: 0.00s; (18:54:51) INFO: 0 processes.; + echo 'Expect a usage message:'; Expect a usage message:; + python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; + grep /call_variants.py:; /tmp/Bazel.runfiles_5qjtwbro/runfiles/com_google_deepvariant/deepvariant/call_variants.py:; + :; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443
https://github.com/google/deepvariant/issues/443:233,Modifiability,config,config,233,"Hello,. I am trying to install DeepVariant from source on Ubuntu 1.18.04. The build-prereq.sh script finished well,; but build_and_test.sh has stopped unexpectedly do not displaying any error:. ```; (18:54:51) INFO: Found applicable config definition build:linux in file /data1/SOFT/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels; (18:54:51) INFO: Found applicable config definition build:dynamic_kernels in file /data1/SOFT/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (18:54:51) INFO: Current date is 2021-04-16; (18:54:51) INFO: Build option --build_python_zip has changed, discarding analysis cache.; (18:54:51) INFO: Analyzed target //:licenses_zip (0 packages loaded, 22 targets configured).; (18:54:51) INFO: Found 1 target...; (18:54:51) INFO: Elapsed time: 0.224s, Critical Path: 0.00s; (18:54:51) INFO: 0 processes.; + echo 'Expect a usage message:'; Expect a usage message:; + python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; + grep /call_variants.py:; /tmp/Bazel.runfiles_5qjtwbro/runfiles/com_google_deepvariant/deepvariant/call_variants.py:; + :; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443
https://github.com/google/deepvariant/issues/443:450,Modifiability,config,config,450,"Hello,. I am trying to install DeepVariant from source on Ubuntu 1.18.04. The build-prereq.sh script finished well,; but build_and_test.sh has stopped unexpectedly do not displaying any error:. ```; (18:54:51) INFO: Found applicable config definition build:linux in file /data1/SOFT/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels; (18:54:51) INFO: Found applicable config definition build:dynamic_kernels in file /data1/SOFT/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (18:54:51) INFO: Current date is 2021-04-16; (18:54:51) INFO: Build option --build_python_zip has changed, discarding analysis cache.; (18:54:51) INFO: Analyzed target //:licenses_zip (0 packages loaded, 22 targets configured).; (18:54:51) INFO: Found 1 target...; (18:54:51) INFO: Elapsed time: 0.224s, Critical Path: 0.00s; (18:54:51) INFO: 0 processes.; + echo 'Expect a usage message:'; Expect a usage message:; + python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; + grep /call_variants.py:; /tmp/Bazel.runfiles_5qjtwbro/runfiles/com_google_deepvariant/deepvariant/call_variants.py:; + :; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443
https://github.com/google/deepvariant/issues/443:508,Modifiability,config,config,508,"Hello,. I am trying to install DeepVariant from source on Ubuntu 1.18.04. The build-prereq.sh script finished well,; but build_and_test.sh has stopped unexpectedly do not displaying any error:. ```; (18:54:51) INFO: Found applicable config definition build:linux in file /data1/SOFT/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels; (18:54:51) INFO: Found applicable config definition build:dynamic_kernels in file /data1/SOFT/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (18:54:51) INFO: Current date is 2021-04-16; (18:54:51) INFO: Build option --build_python_zip has changed, discarding analysis cache.; (18:54:51) INFO: Analyzed target //:licenses_zip (0 packages loaded, 22 targets configured).; (18:54:51) INFO: Found 1 target...; (18:54:51) INFO: Elapsed time: 0.224s, Critical Path: 0.00s; (18:54:51) INFO: 0 processes.; + echo 'Expect a usage message:'; Expect a usage message:; + python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; + grep /call_variants.py:; /tmp/Bazel.runfiles_5qjtwbro/runfiles/com_google_deepvariant/deepvariant/call_variants.py:; + :; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443
https://github.com/google/deepvariant/issues/443:876,Modifiability,config,configured,876,"Hello,. I am trying to install DeepVariant from source on Ubuntu 1.18.04. The build-prereq.sh script finished well,; but build_and_test.sh has stopped unexpectedly do not displaying any error:. ```; (18:54:51) INFO: Found applicable config definition build:linux in file /data1/SOFT/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels; (18:54:51) INFO: Found applicable config definition build:dynamic_kernels in file /data1/SOFT/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (18:54:51) INFO: Current date is 2021-04-16; (18:54:51) INFO: Build option --build_python_zip has changed, discarding analysis cache.; (18:54:51) INFO: Analyzed target //:licenses_zip (0 packages loaded, 22 targets configured).; (18:54:51) INFO: Found 1 target...; (18:54:51) INFO: Elapsed time: 0.224s, Critical Path: 0.00s; (18:54:51) INFO: 0 processes.; + echo 'Expect a usage message:'; Expect a usage message:; + python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; + grep /call_variants.py:; /tmp/Bazel.runfiles_5qjtwbro/runfiles/com_google_deepvariant/deepvariant/call_variants.py:; + :; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443
https://github.com/google/deepvariant/issues/443:788,Performance,cache,cache,788,"Hello,. I am trying to install DeepVariant from source on Ubuntu 1.18.04. The build-prereq.sh script finished well,; but build_and_test.sh has stopped unexpectedly do not displaying any error:. ```; (18:54:51) INFO: Found applicable config definition build:linux in file /data1/SOFT/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels; (18:54:51) INFO: Found applicable config definition build:dynamic_kernels in file /data1/SOFT/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (18:54:51) INFO: Current date is 2021-04-16; (18:54:51) INFO: Build option --build_python_zip has changed, discarding analysis cache.; (18:54:51) INFO: Analyzed target //:licenses_zip (0 packages loaded, 22 targets configured).; (18:54:51) INFO: Found 1 target...; (18:54:51) INFO: Elapsed time: 0.224s, Critical Path: 0.00s; (18:54:51) INFO: 0 processes.; + echo 'Expect a usage message:'; Expect a usage message:; + python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; + grep /call_variants.py:; /tmp/Bazel.runfiles_5qjtwbro/runfiles/com_google_deepvariant/deepvariant/call_variants.py:; + :; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443
https://github.com/google/deepvariant/issues/443:857,Performance,load,loaded,857,"Hello,. I am trying to install DeepVariant from source on Ubuntu 1.18.04. The build-prereq.sh script finished well,; but build_and_test.sh has stopped unexpectedly do not displaying any error:. ```; (18:54:51) INFO: Found applicable config definition build:linux in file /data1/SOFT/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels; (18:54:51) INFO: Found applicable config definition build:dynamic_kernels in file /data1/SOFT/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (18:54:51) INFO: Current date is 2021-04-16; (18:54:51) INFO: Build option --build_python_zip has changed, discarding analysis cache.; (18:54:51) INFO: Analyzed target //:licenses_zip (0 packages loaded, 22 targets configured).; (18:54:51) INFO: Found 1 target...; (18:54:51) INFO: Elapsed time: 0.224s, Critical Path: 0.00s; (18:54:51) INFO: 0 processes.; + echo 'Expect a usage message:'; Expect a usage message:; + python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; + grep /call_variants.py:; /tmp/Bazel.runfiles_5qjtwbro/runfiles/com_google_deepvariant/deepvariant/call_variants.py:; + :; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/443
https://github.com/google/deepvariant/issues/444:69,Availability,error,error,69,"Hi there,. We ran deepvariant on test data, and we had the following error:. I0416 16:33:15.202579 46954465520640 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 3 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/tmp/tmpmxakjtgh/make_examples.tfrecord@4.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 2. We converted docker image to singularity sandbox. And our command is like this:; singularity exec -B /data -B /home -B /localhd/ \; ../deepvariant-cpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards ${nproc} \; --regions chr20. I am guessing we may have misconfigured some module. Any idea how to fix it?. Thanks. George",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/444
https://github.com/google/deepvariant/issues/444:1273,Modifiability,sandbox,sandbox,1273,"Hi there,. We ran deepvariant on test data, and we had the following error:. I0416 16:33:15.202579 46954465520640 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 3 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/tmp/tmpmxakjtgh/make_examples.tfrecord@4.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 2. We converted docker image to singularity sandbox. And our command is like this:; singularity exec -B /data -B /home -B /localhd/ \; ../deepvariant-cpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards ${nproc} \; --regions chr20. I am guessing we may have misconfigured some module. Any idea how to fix it?. Thanks. George",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/444
https://github.com/google/deepvariant/issues/444:33,Testability,test,test,33,"Hi there,. We ran deepvariant on test data, and we had the following error:. I0416 16:33:15.202579 46954465520640 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 3 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/tmp/tmpmxakjtgh/make_examples.tfrecord@4.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 2. We converted docker image to singularity sandbox. And our command is like this:; singularity exec -B /data -B /home -B /localhd/ \; ../deepvariant-cpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards ${nproc} \; --regions chr20. I am guessing we may have misconfigured some module. Any idea how to fix it?. Thanks. George",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/444
https://github.com/google/deepvariant/issues/444:1273,Testability,sandbox,sandbox,1273,"Hi there,. We ran deepvariant on test data, and we had the following error:. I0416 16:33:15.202579 46954465520640 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 3 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/tmp/tmpmxakjtgh/make_examples.tfrecord@4.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 2. We converted docker image to singularity sandbox. And our command is like this:; singularity exec -B /data -B /home -B /localhd/ \; ../deepvariant-cpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards ${nproc} \; --regions chr20. I am guessing we may have misconfigured some module. Any idea how to fix it?. Thanks. George",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/444
https://github.com/google/deepvariant/issues/445:168,Deployability,update,updated,168,"Hi, it's getting harder to build deepvariant, even using bioconda as everything it moving to python3.7 or higher.; Would it be possible to get the build and Dockerfile updated to 3.7? And/or could you provide some guidance on what is needed?. Using the docker container works perfectly. But I want to add bcftools and samtools (for example) to the container and also have it work on singularity.; thanks,; -B",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445
https://github.com/google/deepvariant/issues/445:214,Usability,guid,guidance,214,"Hi, it's getting harder to build deepvariant, even using bioconda as everything it moving to python3.7 or higher.; Would it be possible to get the build and Dockerfile updated to 3.7? And/or could you provide some guidance on what is needed?. Using the docker container works perfectly. But I want to add bcftools and samtools (for example) to the container and also have it work on singularity.; thanks,; -B",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/445
https://github.com/google/deepvariant/issues/446:1143,Availability,Error,Error,1143,"rra.bio/)`; - DeepVariant version:; `1.1.0`; - Installation method (Docker, built from source, etc.):; `google/deepvariant:1.1.0`; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); ```; Pacbio bam from GIAB ; https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/NA12878/PacBio_SequelII_CCS_11kb/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam; reference from Broad GCP; gs://gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta; Though this is a slight ref mismatch, B37 vs hs37. I don't think that should cause that problem? The make_examples step finished successfully.; ```; **Steps to reproduce:**; - Command:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=${REF_GENOME_FASTA} \; --reads=${input_read} \; --num_shards=${NUM_THREADS} \; --output_vcf=${basename}.vcf.gz; ```; - Error trace: (if applicable); ```parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /cromwell_root/gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta --reads /cromwell_root/fc-13e1404e-623c-489f-956c-b388fa9fb975/bams/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam --examples /cromwell_root/tmp.e4eeba80/tmpphthddeo/make_examples.tfrecord@20.gz --add_hp_channel --alt_aligned_pileup diff_channels --parse_sam_aux_fields --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 6; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /cromwell_root/gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta --reads /cromwell_root/fc-13e1404e-623c-489f-956c-b388fa9fb975/bams/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam --examples /cromwell_root/tmp.e4eeba80/tmpphthddeo/make_examples.tfrecord@20.gz --add_hp_channel --alt_aligned_pileup diff_channels --parse_sam_aux_fields --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --tas",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/446
https://github.com/google/deepvariant/issues/446:3983,Availability,error,error,3983,"realign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 7. real 133m6.986s; user 230m53.643s; sys 1m59.690s; I0425 04:55:14.754644 140234455082752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 19 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta"" --reads ""/cromwell_root/fc-13e1404e-623c-489f-956c-b388fa9fb975/bams/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam"" --examples ""/cromwell_root/tmp.e4eeba80/tmpphthddeo/make_examples.tfrecord@20.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --parse_sam_aux_fields --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 247.; 2021/04/25 04:55:23 Starting delocalization.; ```; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; `Yes. I tested the PacBio case study on chr20 on my system. It ran through without any problems`; **Any additional context:**; `The total error log is quite lengthy but mostly just logging make_examples.py. I can share with you the Terra workflow which you can reproduce. To do that, I probably need your email address. `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/446
https://github.com/google/deepvariant/issues/446:283,Deployability,Install,Installation,283,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; ```; Deepvariant failed on PACBIO data. ; ```. **Setup**; - Operating system:; `google cloud through [Terra](https://terra.bio/)`; - DeepVariant version:; `1.1.0`; - Installation method (Docker, built from source, etc.):; `google/deepvariant:1.1.0`; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); ```; Pacbio bam from GIAB ; https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/NA12878/PacBio_SequelII_CCS_11kb/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam; reference from Broad GCP; gs://gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta; Though this is a slight ref mismatch, B37 vs hs37. I don't think that should cause that problem? The make_examples step finished successfully.; ```; **Steps to reproduce:**; - Command:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=${REF_GENOME_FASTA} \; --reads=${input_read} \; --num_shards=${NUM_THREADS} \; --output_vcf=${basename}.vcf.gz; ```; - Error trace: (if applicable); ```parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /cromwell_root/gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta --reads /cromwell_root/fc-13e1404e-623c-489f-956c-b388fa9fb975/bams/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam --examples /cromwell_root/tmp.e4eeba80/tmpphthddeo/make_examples.tfrecord@20.gz --add_hp_channel --alt_aligned_pileup diff_channels --parse_sam_aux_fields --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 6; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /cromwell_root/gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta --reads /cromwell_root/fc-13e1404e-623c-489f-956c-b388fa9fb975/bams/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.tri",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/446
https://github.com/google/deepvariant/issues/446:3649,Testability,test,test,3649,"realign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 7. real 133m6.986s; user 230m53.643s; sys 1m59.690s; I0425 04:55:14.754644 140234455082752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 19 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta"" --reads ""/cromwell_root/fc-13e1404e-623c-489f-956c-b388fa9fb975/bams/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam"" --examples ""/cromwell_root/tmp.e4eeba80/tmpphthddeo/make_examples.tfrecord@20.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --parse_sam_aux_fields --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 247.; 2021/04/25 04:55:23 Starting delocalization.; ```; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; `Yes. I tested the PacBio case study on chr20 on my system. It ran through without any problems`; **Any additional context:**; `The total error log is quite lengthy but mostly just logging make_examples.py. I can share with you the Terra workflow which you can reproduce. To do that, I probably need your email address. `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/446
https://github.com/google/deepvariant/issues/446:3685,Testability,test,test,3685,"realign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 7. real 133m6.986s; user 230m53.643s; sys 1m59.690s; I0425 04:55:14.754644 140234455082752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 19 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta"" --reads ""/cromwell_root/fc-13e1404e-623c-489f-956c-b388fa9fb975/bams/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam"" --examples ""/cromwell_root/tmp.e4eeba80/tmpphthddeo/make_examples.tfrecord@20.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --parse_sam_aux_fields --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 247.; 2021/04/25 04:55:23 Starting delocalization.; ```; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; `Yes. I tested the PacBio case study on chr20 on my system. It ran through without any problems`; **Any additional context:**; `The total error log is quite lengthy but mostly just logging make_examples.py. I can share with you the Terra workflow which you can reproduce. To do that, I probably need your email address. `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/446
https://github.com/google/deepvariant/issues/446:3853,Testability,test,tested,3853,"realign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 7. real 133m6.986s; user 230m53.643s; sys 1m59.690s; I0425 04:55:14.754644 140234455082752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 19 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta"" --reads ""/cromwell_root/fc-13e1404e-623c-489f-956c-b388fa9fb975/bams/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam"" --examples ""/cromwell_root/tmp.e4eeba80/tmpphthddeo/make_examples.tfrecord@20.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --parse_sam_aux_fields --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 247.; 2021/04/25 04:55:23 Starting delocalization.; ```; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; `Yes. I tested the PacBio case study on chr20 on my system. It ran through without any problems`; **Any additional context:**; `The total error log is quite lengthy but mostly just logging make_examples.py. I can share with you the Terra workflow which you can reproduce. To do that, I probably need your email address. `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/446
https://github.com/google/deepvariant/issues/446:3989,Testability,log,log,3989,"realign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 7. real 133m6.986s; user 230m53.643s; sys 1m59.690s; I0425 04:55:14.754644 140234455082752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 19 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta"" --reads ""/cromwell_root/fc-13e1404e-623c-489f-956c-b388fa9fb975/bams/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam"" --examples ""/cromwell_root/tmp.e4eeba80/tmpphthddeo/make_examples.tfrecord@20.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --parse_sam_aux_fields --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 247.; 2021/04/25 04:55:23 Starting delocalization.; ```; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; `Yes. I tested the PacBio case study on chr20 on my system. It ran through without any problems`; **Any additional context:**; `The total error log is quite lengthy but mostly just logging make_examples.py. I can share with you the Terra workflow which you can reproduce. To do that, I probably need your email address. `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/446
https://github.com/google/deepvariant/issues/446:4026,Testability,log,logging,4026,"realign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 7. real 133m6.986s; user 230m53.643s; sys 1m59.690s; I0425 04:55:14.754644 140234455082752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 19 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta"" --reads ""/cromwell_root/fc-13e1404e-623c-489f-956c-b388fa9fb975/bams/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam"" --examples ""/cromwell_root/tmp.e4eeba80/tmpphthddeo/make_examples.tfrecord@20.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --parse_sam_aux_fields --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 247.; 2021/04/25 04:55:23 Starting delocalization.; ```; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; `Yes. I tested the PacBio case study on chr20 on my system. It ran through without any problems`; **Any additional context:**; `The total error log is quite lengthy but mostly just logging make_examples.py. I can share with you the Terra workflow which you can reproduce. To do that, I probably need your email address. `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/446
https://github.com/google/deepvariant/issues/448:193,Testability,test,test,193,Run a interactive mode of docker container (different than deepvariant container) and plan to build the deepvariant from source:. run the build-prereq.sh and run build_and_test.sh. Then how to test deepvariant function?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/448
https://github.com/google/deepvariant/issues/450:193,Availability,avail,available,193,"Well, this is not really a problem. It's rather a question. Someone asked about the de novo germline calling last year [#377](https://github.com/google/deepvariant/issues/377). Deeptrio is now available and I want to ask general question in regards with denovo variants in the child. . Background: the biggest issue with calling de novo variants (i.e. variants that are found in proband, generally as heterozygous, negative in parents) is there are ton's of false negative calls (Type II error) (i.e. variants not called in parents but are visually obvious in the alignment). . It seems to me that DeepTrio should address this issue particularly well but I am not sure if that is the motivation behind DeepTrio. How does DeepTrio handle type 2 errors for de novo germline variant calling? can this be added as an enhancement?. Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/450
https://github.com/google/deepvariant/issues/450:488,Availability,error,error,488,"Well, this is not really a problem. It's rather a question. Someone asked about the de novo germline calling last year [#377](https://github.com/google/deepvariant/issues/377). Deeptrio is now available and I want to ask general question in regards with denovo variants in the child. . Background: the biggest issue with calling de novo variants (i.e. variants that are found in proband, generally as heterozygous, negative in parents) is there are ton's of false negative calls (Type II error) (i.e. variants not called in parents but are visually obvious in the alignment). . It seems to me that DeepTrio should address this issue particularly well but I am not sure if that is the motivation behind DeepTrio. How does DeepTrio handle type 2 errors for de novo germline variant calling? can this be added as an enhancement?. Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/450
https://github.com/google/deepvariant/issues/450:744,Availability,error,errors,744,"Well, this is not really a problem. It's rather a question. Someone asked about the de novo germline calling last year [#377](https://github.com/google/deepvariant/issues/377). Deeptrio is now available and I want to ask general question in regards with denovo variants in the child. . Background: the biggest issue with calling de novo variants (i.e. variants that are found in proband, generally as heterozygous, negative in parents) is there are ton's of false negative calls (Type II error) (i.e. variants not called in parents but are visually obvious in the alignment). . It seems to me that DeepTrio should address this issue particularly well but I am not sure if that is the motivation behind DeepTrio. How does DeepTrio handle type 2 errors for de novo germline variant calling? can this be added as an enhancement?. Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/450
https://github.com/google/deepvariant/issues/450:813,Modifiability,enhance,enhancement,813,"Well, this is not really a problem. It's rather a question. Someone asked about the de novo germline calling last year [#377](https://github.com/google/deepvariant/issues/377). Deeptrio is now available and I want to ask general question in regards with denovo variants in the child. . Background: the biggest issue with calling de novo variants (i.e. variants that are found in proband, generally as heterozygous, negative in parents) is there are ton's of false negative calls (Type II error) (i.e. variants not called in parents but are visually obvious in the alignment). . It seems to me that DeepTrio should address this issue particularly well but I am not sure if that is the motivation behind DeepTrio. How does DeepTrio handle type 2 errors for de novo germline variant calling? can this be added as an enhancement?. Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/450
https://github.com/google/deepvariant/issues/452:25,Availability,error,error,25,"Hi,. I got the following error: . I'm using Docker version 1.1.0; gpu NVIDIA GeForce RTX 3090. Any suggestion or advice?. Thanks in advance.; Amin. `2021-05-06 16:56:50.765879: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1; I0506 16:56:52.008759 140393620989696 call_variants.py:338] Shape of input examples: [100, 221, 6]; 2021-05-06 16:56:52.013998: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-05-06 16:56:52.046181: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz; 2021-05-06 16:56:52.053674: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x47507d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-05-06 16:56:52.053727: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2021-05-06 16:56:52.058754: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1; 2021-05-06 16:56:52.188018: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x47b9240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:; 2021-05-06 16:56:52.188089: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6; 2021-05-06 16:56:52.191811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: ; pciBusID: 0000:20:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6; coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s; 2021-05-06 16:56:52.191885: I tensorflow/st",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452
https://github.com/google/deepvariant/issues/452:4468,Modifiability,config,config,4468,"/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0; 2021-05-06 16:56:52.216108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1; 2021-05-06 16:58:21.551842: E tensorflow/core/common_runtime/session.cc:91] Failed to create session: Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid; 2021-05-06 16:58:21.551943: E tensorflow/c/c_api.cc:2184] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 393, in call_variants; with tf.compat.v1.Session(config=config) as sess:; File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1586, in __init__; super(Session, self).__init__(target, graph, config=config); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 701, in __init__; self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts); tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid. real	1m31.942s; user	1m31.967s; sys	0m8.062s; `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452
https://github.com/google/deepvariant/issues/452:4475,Modifiability,config,config,4475,"/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0; 2021-05-06 16:56:52.216108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1; 2021-05-06 16:58:21.551842: E tensorflow/core/common_runtime/session.cc:91] Failed to create session: Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid; 2021-05-06 16:58:21.551943: E tensorflow/c/c_api.cc:2184] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 393, in call_variants; with tf.compat.v1.Session(config=config) as sess:; File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1586, in __init__; super(Session, self).__init__(target, graph, config=config); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 701, in __init__; self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts); tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid. real	1m31.942s; user	1m31.967s; sys	0m8.062s; `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452
https://github.com/google/deepvariant/issues/452:4645,Modifiability,config,config,4645,"/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0; 2021-05-06 16:56:52.216108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1; 2021-05-06 16:58:21.551842: E tensorflow/core/common_runtime/session.cc:91] Failed to create session: Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid; 2021-05-06 16:58:21.551943: E tensorflow/c/c_api.cc:2184] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 393, in call_variants; with tf.compat.v1.Session(config=config) as sess:; File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1586, in __init__; super(Session, self).__init__(target, graph, config=config); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 701, in __init__; self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts); tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid. real	1m31.942s; user	1m31.967s; sys	0m8.062s; `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452
https://github.com/google/deepvariant/issues/452:4652,Modifiability,config,config,4652,"/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0; 2021-05-06 16:56:52.216108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1; 2021-05-06 16:58:21.551842: E tensorflow/core/common_runtime/session.cc:91] Failed to create session: Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid; 2021-05-06 16:58:21.551943: E tensorflow/c/c_api.cc:2184] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_1q2x77gk/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 393, in call_variants; with tf.compat.v1.Session(config=config) as sess:; File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1586, in __init__; super(Session, self).__init__(target, graph, config=config); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 701, in __init__; self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts); tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid. real	1m31.942s; user	1m31.967s; sys	0m8.062s; `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452
https://github.com/google/deepvariant/issues/452:503,Performance,optimiz,optimized,503,"Hi,. I got the following error: . I'm using Docker version 1.1.0; gpu NVIDIA GeForce RTX 3090. Any suggestion or advice?. Thanks in advance.; Amin. `2021-05-06 16:56:50.765879: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1; I0506 16:56:52.008759 140393620989696 call_variants.py:338] Shape of input examples: [100, 221, 6]; 2021-05-06 16:56:52.013998: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-05-06 16:56:52.046181: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz; 2021-05-06 16:56:52.053674: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x47507d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-05-06 16:56:52.053727: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2021-05-06 16:56:52.058754: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1; 2021-05-06 16:56:52.188018: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x47b9240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:; 2021-05-06 16:56:52.188089: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6; 2021-05-06 16:56:52.191811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: ; pciBusID: 0000:20:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6; coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s; 2021-05-06 16:56:52.191885: I tensorflow/st",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452
https://github.com/google/deepvariant/issues/452:602,Performance,perform,performance-critical,602,"Hi,. I got the following error: . I'm using Docker version 1.1.0; gpu NVIDIA GeForce RTX 3090. Any suggestion or advice?. Thanks in advance.; Amin. `2021-05-06 16:56:50.765879: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1; I0506 16:56:52.008759 140393620989696 call_variants.py:338] Shape of input examples: [100, 221, 6]; 2021-05-06 16:56:52.013998: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-05-06 16:56:52.046181: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz; 2021-05-06 16:56:52.053674: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x47507d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-05-06 16:56:52.053727: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2021-05-06 16:56:52.058754: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1; 2021-05-06 16:56:52.188018: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x47b9240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:; 2021-05-06 16:56:52.188089: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6; 2021-05-06 16:56:52.191811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: ; pciBusID: 0000:20:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6; coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s; 2021-05-06 16:56:52.191885: I tensorflow/st",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/452
https://github.com/google/deepvariant/issues/453:1012,Availability,Error,Error,1012,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**. We found a back-to-back call of two SNPs that we cannot explain as the BAM file suggests a deletion. IGV screenshot: https://www.dropbox.com/s/c0wfelxc1cca14b/igv_snapshot.png?dl=0. Happy to provide a BAM file etc. But maybe this is easy enough to explain; I just cannot figure out why this comes out as:; TC and GC and not as TC and G/-. chr19 15174241 rs1044006 T C 66 . AC=1;AF=0.5;AN=2;AQ=66;DP=78 GT:AD:DP:GQ:PL:RNC 0/1:0,78:78:10:20,0,9:.; chr19 15174242 chr19_15174242_G_C G C 53 . AC=1;AF=0.5;AN=2;AQ=53;DP=177 GT:AD:DP:GQ:PL:RNC 0/1:77,100:177:50:53,0,52:. **Setup**; - Operating system: Centos 7; - DeepVariant version: 1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): Novaseq 6000, exomes, GRCh38. **Steps to reproduce:**; - Command: Does not apply.; - Error trace: (if applicable)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/453
https://github.com/google/deepvariant/issues/453:754,Deployability,Install,Installation,754,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**. We found a back-to-back call of two SNPs that we cannot explain as the BAM file suggests a deletion. IGV screenshot: https://www.dropbox.com/s/c0wfelxc1cca14b/igv_snapshot.png?dl=0. Happy to provide a BAM file etc. But maybe this is easy enough to explain; I just cannot figure out why this comes out as:; TC and GC and not as TC and G/-. chr19 15174241 rs1044006 T C 66 . AC=1;AF=0.5;AN=2;AQ=66;DP=78 GT:AD:DP:GQ:PL:RNC 0/1:0,78:78:10:20,0,9:.; chr19 15174242 chr19_15174242_G_C G C 53 . AC=1;AF=0.5;AN=2;AQ=53;DP=177 GT:AD:DP:GQ:PL:RNC 0/1:77,100:177:50:53,0,52:. **Setup**; - Operating system: Centos 7; - DeepVariant version: 1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?): Novaseq 6000, exomes, GRCh38. **Steps to reproduce:**; - Command: Does not apply.; - Error trace: (if applicable)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/453
https://github.com/google/deepvariant/issues/454:254,Availability,avail,available,254,"Hello, . I've been attempting to use the customized_classes_labeler to train a DeepVariant model. Specifically, I've been trying use the ""callsets"" field from the INFO field of a Genome In A Bottle VCF file. I've been working with NA12878, VCF/BED files available here: ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/latest/GRCh38/. At first, I could not make training examples using this as that field is an integer, but by making a copy of the VCF where I changed that field to be a string, I was able to make examples (using the `--labeler algorithm`, `--customized_classes_labeler_info_field_name`, and `--customized_classes_labeler_classes_list` options) and train the model. However, when I use the best model from training to predict variants, this class label information is not included in the VCF file. Am I misinterpreting how to use this customized class labeling? Any suggestions on how to incorporate this field into training and variant prediction? Thank you for your time!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/454
https://github.com/google/deepvariant/issues/454:312,Deployability,release,release,312,"Hello, . I've been attempting to use the customized_classes_labeler to train a DeepVariant model. Specifically, I've been trying use the ""callsets"" field from the INFO field of a Genome In A Bottle VCF file. I've been working with NA12878, VCF/BED files available here: ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/latest/GRCh38/. At first, I could not make training examples using this as that field is an integer, but by making a copy of the VCF where I changed that field to be a string, I was able to make examples (using the `--labeler algorithm`, `--customized_classes_labeler_info_field_name`, and `--customized_classes_labeler_classes_list` options) and train the model. However, when I use the best model from training to predict variants, this class label information is not included in the VCF file. Am I misinterpreting how to use this customized class labeling? Any suggestions on how to incorporate this field into training and variant prediction? Thank you for your time!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/454
https://github.com/google/deepvariant/issues/454:750,Safety,predict,predict,750,"Hello, . I've been attempting to use the customized_classes_labeler to train a DeepVariant model. Specifically, I've been trying use the ""callsets"" field from the INFO field of a Genome In A Bottle VCF file. I've been working with NA12878, VCF/BED files available here: ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/latest/GRCh38/. At first, I could not make training examples using this as that field is an integer, but by making a copy of the VCF where I changed that field to be a string, I was able to make examples (using the `--labeler algorithm`, `--customized_classes_labeler_info_field_name`, and `--customized_classes_labeler_classes_list` options) and train the model. However, when I use the best model from training to predict variants, this class label information is not included in the VCF file. Am I misinterpreting how to use this customized class labeling? Any suggestions on how to incorporate this field into training and variant prediction? Thank you for your time!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/454
https://github.com/google/deepvariant/issues/454:969,Safety,predict,prediction,969,"Hello, . I've been attempting to use the customized_classes_labeler to train a DeepVariant model. Specifically, I've been trying use the ""callsets"" field from the INFO field of a Genome In A Bottle VCF file. I've been working with NA12878, VCF/BED files available here: ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/latest/GRCh38/. At first, I could not make training examples using this as that field is an integer, but by making a copy of the VCF where I changed that field to be a string, I was able to make examples (using the `--labeler algorithm`, `--customized_classes_labeler_info_field_name`, and `--customized_classes_labeler_classes_list` options) and train the model. However, when I use the best model from training to predict variants, this class label information is not included in the VCF file. Am I misinterpreting how to use this customized class labeling? Any suggestions on how to incorporate this field into training and variant prediction? Thank you for your time!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/454
https://github.com/google/deepvariant/issues/455:716,Availability,Error,Error,716,"**Describe the issue:**; run demo inside Best practices for multi-sample variant calling with DeepVariant failed. **Setup**; - Operating system: centos 7,; - DeepVariant version:1.1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: docker run -v ""${DIR}"":""/data"" google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/data/hs37d5.fa"" --reads=""/data/HG002.bam"" --regions=""/data/agilent_sureselect_human_all_exon_v5_b37_targets.bed"" --output_vcf=""/data/HG002.vcf.gz"" --output_gvcf=""/data/HG002.gvcf.gz"" --num_shards=25; - Error trace: (if applicable): ; [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1529, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2126, in m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455
https://github.com/google/deepvariant/issues/455:796,Availability,error,error,796,"**Describe the issue:**; run demo inside Best practices for multi-sample variant calling with DeepVariant failed. **Setup**; - Operating system: centos 7,; - DeepVariant version:1.1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: docker run -v ""${DIR}"":""/data"" google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/data/hs37d5.fa"" --reads=""/data/HG002.bam"" --regions=""/data/agilent_sureselect_human_all_exon_v5_b37_targets.bed"" --output_vcf=""/data/HG002.vcf.gz"" --output_gvcf=""/data/HG002.gvcf.gz"" --num_shards=25; - Error trace: (if applicable): ; [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1529, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2126, in m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455
https://github.com/google/deepvariant/issues/455:2854,Availability,error,error,2854,"n9h1txbv/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2126, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2003, in make_examples_runner; candidates, examples, gvcfs = region_processor.process(region); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1472, in process; self.in_memory_sam_reader.replace_reads(self.region_reads(region)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1534, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5. **Does the quick start test work on your system?**; it work. **Any additional context:**; I also run the mistaken command directly inside docker container, . [gosadmin@node3 trio]$ docker run -it -v ""${DIR}"":""/data"" google/deepvariant:${VERSION} /bin/bash. root@d2911291b750:/data# ls -alh; total 42G; drwxrwxr-x 2 1000 1000 4.0K May 8 03:39 .; drwxr-xr-x 1 root root 29 May 8 08:21 ..; -rw-rw-r-- 1 1000 1000 6.9M Apr 30 08:19 HG002.bai; -rw-rw-r-- 1 1000 1000 9.7G Apr 30 08:19 HG002.bam; -rw-rw-r-- 1 1000 1000 9.6M Apr 30 09:01 HG002_truth.bed; -rw-rw-r-- 1 1000 1000 147M Apr 30 09:00 HG002_truth.vcf.gz; -rw-rw-r-- 1 1000 1000 1.5M Apr 30 09:00 HG002_truth.vcf.gz.tbi; -rw-rw-r-- 1 1000 1000 6.8M Apr 30 08:33 HG003.bai; -rw-rw-r-- 1 1000 1000 8.4G Apr 30 08:33 HG003.bam; -rw-rw-r-- 1 1000 1000 9.2M Apr 30 0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455
https://github.com/google/deepvariant/issues/455:7235,Availability,error,error,7235,"m_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; I0508 07:36:27.267024 140432686110464 genomics_reader.py:223] Reading /data/HG002.bam with NativeSamReader; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; I0508 07:36:27.306630 140432686110464 genomics_reader.py:223] Reading /data/HG002.bam with NativeSamReader; I0508 07:36:27.465119 140432686110464 make_examples.py:648] Task 8/25: Writing examples to /tmp/tmpd7h_gv7l/make_examples.tfrecord-00008-of-00025.gz; I0508 07:36:27.465305 140432686110464 make_examples.py:648] Task 8/25: Writing gvcf records to /tmp/tmpd7h_gv7l/gvcf.tfrecord-00008-of-00025.gz; I0508 07:36:27.465700 140432686110464 make_examples.py:648] Task 8/25: Overhead for preparing inputs: 74 seconds; I0508 07:36:27.529915 140432686110464 make_examples.py:648] Task 8/25: 0 candidates (0 examples) [0.06s elapsed]; [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y3fq",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455
https://github.com/google/deepvariant/issues/455:9270,Availability,error,error,9270," File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner; candidates, examples, gvcfs, runtimes = region_processor.process(region); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process; reads = self.region_reads(region); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; root@a8d04f73bc21:/opt/deepvariant/bin# exit. i also test version 1.0.0, it also return same exception",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455
https://github.com/google/deepvariant/issues/455:9447,Availability,error,error,9447," File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner; candidates, examples, gvcfs, runtimes = region_processor.process(region); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process; reads = self.region_reads(region); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; root@a8d04f73bc21:/opt/deepvariant/bin# exit. i also test version 1.0.0, it also return same exception",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455
https://github.com/google/deepvariant/issues/455:187,Deployability,Install,Installation,187,"**Describe the issue:**; run demo inside Best practices for multi-sample variant calling with DeepVariant failed. **Setup**; - Operating system: centos 7,; - DeepVariant version:1.1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: docker run -v ""${DIR}"":""/data"" google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/data/hs37d5.fa"" --reads=""/data/HG002.bam"" --regions=""/data/agilent_sureselect_human_all_exon_v5_b37_targets.bed"" --output_vcf=""/data/HG002.vcf.gz"" --output_gvcf=""/data/HG002.gvcf.gz"" --num_shards=25; - Error trace: (if applicable): ; [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1529, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2126, in m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455
https://github.com/google/deepvariant/issues/455:992,Modifiability,extend,extend,992,"*Describe the issue:**; run demo inside Best practices for multi-sample variant calling with DeepVariant failed. **Setup**; - Operating system: centos 7,; - DeepVariant version:1.1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: docker run -v ""${DIR}"":""/data"" google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/data/hs37d5.fa"" --reads=""/data/HG002.bam"" --regions=""/data/agilent_sureselect_human_all_exon_v5_b37_targets.bed"" --output_vcf=""/data/HG002.vcf.gz"" --output_gvcf=""/data/HG002.gvcf.gz"" --num_shards=25; - Error trace: (if applicable): ; [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1529, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2126, in m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455
https://github.com/google/deepvariant/issues/455:7431,Modifiability,extend,extend,7431,"ding /data/HG002.bam with NativeSamReader; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; I0508 07:36:27.306630 140432686110464 genomics_reader.py:223] Reading /data/HG002.bam with NativeSamReader; I0508 07:36:27.465119 140432686110464 make_examples.py:648] Task 8/25: Writing examples to /tmp/tmpd7h_gv7l/make_examples.tfrecord-00008-of-00025.gz; I0508 07:36:27.465305 140432686110464 make_examples.py:648] Task 8/25: Writing gvcf records to /tmp/tmpd7h_gv7l/gvcf.tfrecord-00008-of-00025.gz; I0508 07:36:27.465700 140432686110464 make_examples.py:648] Task 8/25: Overhead for preparing inputs: 74 seconds; I0508 07:36:27.529915 140432686110464 make_examples.py:648] Task 8/25: 0 candidates (0 examples) [0.06s elapsed]; [E::bgzf_read] Read block operation failed with error 2 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455
https://github.com/google/deepvariant/issues/455:3010,Testability,test,test,3010,"les_runner(options); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2003, in make_examples_runner; candidates, examples, gvcfs = region_processor.process(region); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1472, in process; self.in_memory_sam_reader.replace_reads(self.region_reads(region)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1534, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5. **Does the quick start test work on your system?**; it work. **Any additional context:**; I also run the mistaken command directly inside docker container, . [gosadmin@node3 trio]$ docker run -it -v ""${DIR}"":""/data"" google/deepvariant:${VERSION} /bin/bash. root@d2911291b750:/data# ls -alh; total 42G; drwxrwxr-x 2 1000 1000 4.0K May 8 03:39 .; drwxr-xr-x 1 root root 29 May 8 08:21 ..; -rw-rw-r-- 1 1000 1000 6.9M Apr 30 08:19 HG002.bai; -rw-rw-r-- 1 1000 1000 9.7G Apr 30 08:19 HG002.bam; -rw-rw-r-- 1 1000 1000 9.6M Apr 30 09:01 HG002_truth.bed; -rw-rw-r-- 1 1000 1000 147M Apr 30 09:00 HG002_truth.vcf.gz; -rw-rw-r-- 1 1000 1000 1.5M Apr 30 09:00 HG002_truth.vcf.gz.tbi; -rw-rw-r-- 1 1000 1000 6.8M Apr 30 08:33 HG003.bai; -rw-rw-r-- 1 1000 1000 8.4G Apr 30 08:33 HG003.bam; -rw-rw-r-- 1 1000 1000 9.2M Apr 30 09:03 HG003_truth.bed; -rw-rw-r-- 1 1000 1000 129M Apr 30 09:02 HG003_truth.vcf.gz; -rw-rw-r-- 1 1000 1000 1.5M Apr 30 09:02 HG003_truth.vcf.gz.tbi; -rw-rw-r-- 1 1000 1000 7.0M May 7 07:42 HG004.bai; -rw-rw-r-- 1 100",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455
https://github.com/google/deepvariant/issues/455:9583,Testability,test,test,9583," File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner; candidates, examples, gvcfs, runtimes = region_processor.process(region); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process; reads = self.region_reads(region); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; root@a8d04f73bc21:/opt/deepvariant/bin# exit. i also test version 1.0.0, it also return same exception",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/455
https://github.com/google/deepvariant/issues/456:112,Availability,avail,available,112,"Would like to train from basic tensorflow, without dockers and other API's. Is the TensorFlow code for training available?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/456
https://github.com/google/deepvariant/issues/457:115,Availability,error,error,115,"Dear developers! I am very curious to use DeepVariant on our in house data. In trying to do so, I stumbled upon an error I cannot seem to circumvent. . **Problem:**; I am trying to run my bamfile that originated from a pacbio LAA output, mapped with minimap2. I receive the error that it's unable to read any records. As I got the warGning (lol!) that --'add_hp_channel' is set but not 'parse_sam_aux_fields'. . **Initial command:**; sudo docker run -v ""2021-05-11_deepvariant_PB"":""/input/"" -v ""2021-05-11_deepvariant_PB/output_DV"":""/output/"" google/deepvariant:""1.1.0"" /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/ref.fasta --reads=/input/R9_Z-1707-003_cluster1_RC492.bam --output_vcf=/output/output.vcf.gz. **What I tried:**; I tried to rerun with the following extra argument: --make_examples_extra_args=""parse_sam_aux_fields=true"".; This gives me the ValueError from run_deepvariant.py that it is in conflict with the sort_by_haplotypes flag, eventhough I didn't use it. Then, I tried to add both arguments: --make_examples_extra_args=""sort_by_haplotypes=false,parse_sam_aux_fields=true"", but this gives the same ValueError. ; `ValueError: The extra_args ""parse_sam_aux_fields"" conflicts with other flags. Please fix and try again. Starting in v1.1.0, if you are running with PACBIO and want to use HP tags, please use the new --use_hp_information flag instead of using --make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`. I also tried to run the command with --sample_name=Z-1707-003_cluster1_RC492_phase0 (the RG for the bamfile), which does not give the warning anymore, but still leaves me with an empty vcf. **Tool stderr for the initial command:**; ```; I0511 12:24:29.658635 140614860437248 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpq5tvks3j. ***** Intermediate results will be written to /tmp/tmpq5tvks3j in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:274,Availability,error,error,274,"Dear developers! I am very curious to use DeepVariant on our in house data. In trying to do so, I stumbled upon an error I cannot seem to circumvent. . **Problem:**; I am trying to run my bamfile that originated from a pacbio LAA output, mapped with minimap2. I receive the error that it's unable to read any records. As I got the warGning (lol!) that --'add_hp_channel' is set but not 'parse_sam_aux_fields'. . **Initial command:**; sudo docker run -v ""2021-05-11_deepvariant_PB"":""/input/"" -v ""2021-05-11_deepvariant_PB/output_DV"":""/output/"" google/deepvariant:""1.1.0"" /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/ref.fasta --reads=/input/R9_Z-1707-003_cluster1_RC492.bam --output_vcf=/output/output.vcf.gz. **What I tried:**; I tried to rerun with the following extra argument: --make_examples_extra_args=""parse_sam_aux_fields=true"".; This gives me the ValueError from run_deepvariant.py that it is in conflict with the sort_by_haplotypes flag, eventhough I didn't use it. Then, I tried to add both arguments: --make_examples_extra_args=""sort_by_haplotypes=false,parse_sam_aux_fields=true"", but this gives the same ValueError. ; `ValueError: The extra_args ""parse_sam_aux_fields"" conflicts with other flags. Please fix and try again. Starting in v1.1.0, if you are running with PACBIO and want to use HP tags, please use the new --use_hp_information flag instead of using --make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`. I also tried to run the command with --sample_name=Z-1707-003_cluster1_RC492_phase0 (the RG for the bamfile), which does not give the warning anymore, but still leaves me with an empty vcf. **Tool stderr for the initial command:**; ```; I0511 12:24:29.658635 140614860437248 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpq5tvks3j. ***** Intermediate results will be written to /tmp/tmpq5tvks3j in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/457:4514,Availability,checkpoint,checkpoint,4514,"44992 genomics_reader.py:223] Reading /input/R9_Z-1707-003_cluster1_RC492.bam with NativeSamReader; I0511 12:24:32.453339 140409179444992 genomics_reader.py:223] Reading /input/R9_Z-1707-003_cluster1_RC492.bam with NativeSamReader; I0511 12:24:32.579413 140409179444992 make_examples.py:648] Writing examples to /tmp/tmpq5tvks3j/make_examples.tfrecord-00000-of-00001.gz; I0511 12:24:32.579596 140409179444992 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0511 12:24:32.587054 140409179444992 make_examples.py:648] 0 candidates (0 examples) [0.01s elapsed]; I0511 12:24:32.591045 140409179444992 make_examples.py:648] Found 0 candidate variants; I0511 12:24:32.591111 140409179444992 make_examples.py:648] Created 0 examples. real 0m3.165s; user 0m3.133s; sys 0m1.450s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpq5tvks3j/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpq5tvks3j/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"" ). W0511 12:24:34.935784 140411820246784 call_variants.py:327] Unable to read any records from /tmp/tmpq5tvks3j/make_examples.tfrecord@1.gz. Output will contain zero records. real 0m2.355s; user 0m2.789s; sys 0m1.594s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ref.fasta"" --infile ""/tmp/tmpq5tvks3j/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" ). I0511 12:24:37.234371 139970945300224 postprocess_variants.py:1083] Could not determine sample name and --sample_name is unset. Using the default sample name. Sample name: default; I0511 12:24:37.235468 139970945300224 postprocess_variants.py:1111] call_variants_output is empty. Writing out empty VCF.; I0511 12:24:37.235656 139970945300224 postprocess_variants.py:1139] Writing variants to VCF.; I0511 12:24:37.235709 139970945300224 postprocess_variants.py:723] Writing output to VCF file: /output/output.vcf.gz; I0511 12:24:37.236480 13997094530",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/457
https://github.com/google/deepvariant/issues/458:109,Availability,error,error,109,"Hi everyone,. I tested DeepVariant 1.1.0 on PACBIO data (HG002, chr20), using PACBIO model and got following error in call_variants.py script:; `I0519 14:15:32.843033 139847781275392 call_variants.py:338] Shape of input examples: [100, 221, 8]; Traceback (most recent call last):; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:1767,Availability,checkpoint,checkpoint,1767,"aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --alt_aligned_pileup diff_channels --reads /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/HG002.merged.bam --ref Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --norealign_reads --regions 20 --sample_name HG002 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --use_openvino --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:1800,Availability,checkpoint,checkpoint,1800,"aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --alt_aligned_pileup diff_channels --reads /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/HG002.merged.bam --ref Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --norealign_reads --regions 20 --sample_name HG002 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --use_openvino --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:2619,Availability,checkpoint,checkpoint,2619,"les_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --alt_aligned_pileup diff_channels --reads /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/HG002.merged.bam --ref Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --norealign_reads --regions 20 --sample_name HG002 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --use_openvino --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./HG002.vcf ) > ./postprocess_variants.log 2>&1`. When I try and run with HYBRID model, everything goes smoothly. Do you have some input on this?. Thanks,; Ajsa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:16,Testability,test,tested,16,"Hi everyone,. I tested DeepVariant 1.1.0 on PACBIO data (HG002, chr20), using PACBIO model and got following error in call_variants.py script:; `I0519 14:15:32.843033 139847781275392 call_variants.py:338] Shape of input examples: [100, 221, 8]; Traceback (most recent call last):; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:1928,Testability,LOG,LOGDIR,1928,"9-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --alt_aligned_pileup diff_channels --reads /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/HG002.merged.bam --ref Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --norealign_reads --regions 20 --sample_name HG002 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --use_openvino --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./HG002.vcf ) > ./postprocess_variants.log 2>&1`. When I try and run with HYBRID model, everything go",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:1952,Testability,log,logs,1952,"9-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --alt_aligned_pileup diff_channels --reads /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/HG002.merged.bam --ref Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --norealign_reads --regions 20 --sample_name HG002 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --use_openvino --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./HG002.vcf ) > ./postprocess_variants.log 2>&1`. When I try and run with HYBRID model, everything go",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:1973,Testability,LOG,LOGDIR,1973,"9-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --alt_aligned_pileup diff_channels --reads /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/HG002.merged.bam --ref Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --norealign_reads --regions 20 --sample_name HG002 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --use_openvino --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./HG002.vcf ) > ./postprocess_variants.log 2>&1`. When I try and run with HYBRID model, everything go",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:2061,Testability,LOG,LOGDIR,2061,"9-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --alt_aligned_pileup diff_channels --reads /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/HG002.merged.bam --ref Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --norealign_reads --regions 20 --sample_name HG002 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --use_openvino --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./HG002.vcf ) > ./postprocess_variants.log 2>&1`. When I try and run with HYBRID model, everything go",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:2069,Testability,log,log,2069,"9-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --alt_aligned_pileup diff_channels --reads /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/HG002.merged.bam --ref Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --norealign_reads --regions 20 --sample_name HG002 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --use_openvino --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./HG002.vcf ) > ./postprocess_variants.log 2>&1`. When I try and run with HYBRID model, everything go",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:2083,Testability,LOG,LOGDIR,2083,"9-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --alt_aligned_pileup diff_channels --reads /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/HG002.merged.bam --ref Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --norealign_reads --regions 20 --sample_name HG002 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --use_openvino --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./HG002.vcf ) > ./postprocess_variants.log 2>&1`. When I try and run with HYBRID model, everything go",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:2466,Testability,log,log,2466,"les_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --alt_aligned_pileup diff_channels --reads /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/HG002.merged.bam --ref Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --norealign_reads --regions 20 --sample_name HG002 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --use_openvino --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./HG002.vcf ) > ./postprocess_variants.log 2>&1`. When I try and run with HYBRID model, everything goes smoothly. Do you have some input on this?. Thanks,; Ajsa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:2712,Testability,log,log,2712,"les_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --alt_aligned_pileup diff_channels --reads /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/HG002.merged.bam --ref Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --norealign_reads --regions 20 --sample_name HG002 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --use_openvino --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./HG002.vcf ) > ./postprocess_variants.log 2>&1`. When I try and run with HYBRID model, everything goes smoothly. Do you have some input on this?. Thanks,; Ajsa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/458:2950,Testability,log,log,2950,"les_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --alt_aligned_pileup diff_channels --reads /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/HG002.merged.bam --ref Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --norealign_reads --regions 20 --sample_name HG002 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --use_openvino --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./HG002.vcf ) > ./postprocess_variants.log 2>&1`. When I try and run with HYBRID model, everything goes smoothly. Do you have some input on this?. Thanks,; Ajsa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/458
https://github.com/google/deepvariant/issues/459:437,Availability,Error,Error,437,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:232,Deployability,Install,Installation,232,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:490,Testability,test,test,490,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:526,Testability,test,test,526,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/459:120,Usability,clear,clear,120,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/459
https://github.com/google/deepvariant/issues/462:205,Availability,down,downloaded,205,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:; Yes. **Describe the issue:**; Launching an Ubuntu 20.04 server t2 micro EC2 on AWS, installed docker using snap, downloaded data from quickstart guide verbatim https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md. **Setup**; - Operating system: Ubuntu 20.04 server t2 micro EC2 on AWS; - DeepVariant version: BIN_VERSION=""1.1.0""; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Quick start data. **Steps to reproduce:**; - Command:; ```; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir; ```. - Error trace: (if applicable); ```; Unable to find image 'google/deepvariant:1.1.0' locally; 1.1.0: Pulling from google/deepvariant; be8ec4e48d7f: Pull complete ; 33b8b485aff0: Pull complete ; d887158cc58c: Pull complete ; 05895bb28c18: Pull complete ; 35be0878dcf6: Pull complete ; 03fb656082b2: Pull complete ; 1d3e393af6d8: Pull complete ; 9663085972fa: Pull complete ; 10ac03989960: Pull complete ; 401f11974a9b: Pull complete ; 67f12673f7e4: Pull complete ; 99116330e4f4: Pull complete ; 6fbbce8e3587: Pull complete ; c223e83ce2e3: Pull complete ; c02ebb3220a1: Pull complete ; 0c7a427ce17a: Pull complete ; ec9cd66333fe: Pull complete ; 9d57046ae5b9: Pull complete ; 0f54",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:1325,Availability,Error,Error,1325,": Ubuntu 20.04 server t2 micro EC2 on AWS; - DeepVariant version: BIN_VERSION=""1.1.0""; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Quick start data. **Steps to reproduce:**; - Command:; ```; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir; ```. - Error trace: (if applicable); ```; Unable to find image 'google/deepvariant:1.1.0' locally; 1.1.0: Pulling from google/deepvariant; be8ec4e48d7f: Pull complete ; 33b8b485aff0: Pull complete ; d887158cc58c: Pull complete ; 05895bb28c18: Pull complete ; 35be0878dcf6: Pull complete ; 03fb656082b2: Pull complete ; 1d3e393af6d8: Pull complete ; 9663085972fa: Pull complete ; 10ac03989960: Pull complete ; 401f11974a9b: Pull complete ; 67f12673f7e4: Pull complete ; 99116330e4f4: Pull complete ; 6fbbce8e3587: Pull complete ; c223e83ce2e3: Pull complete ; c02ebb3220a1: Pull complete ; 0c7a427ce17a: Pull complete ; ec9cd66333fe: Pull complete ; 9d57046ae5b9: Pull complete ; 0f5478ac499a: Pull complete ; b07098b67a6d: Pull complete ; 0accf0f55269: Pull complete ; ccc95462eb8f: Pull complete ; f1416983139e: Pull complete ; 2242c582e0cc: Pull complete ; 8f749be1be0b: Pull complete ; 03fdf02906f9: Pull complete ; ea2763a10d98: Pull complete ; fff529645086: Pull complete ; 42ad15be12fa: Pull complete ; 82830610edc8: Pull complete ; d1a85d",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:2776,Availability,Down,Downloaded,2776,gene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir; ```. - Error trace: (if applicable); ```; Unable to find image 'google/deepvariant:1.1.0' locally; 1.1.0: Pulling from google/deepvariant; be8ec4e48d7f: Pull complete ; 33b8b485aff0: Pull complete ; d887158cc58c: Pull complete ; 05895bb28c18: Pull complete ; 35be0878dcf6: Pull complete ; 03fb656082b2: Pull complete ; 1d3e393af6d8: Pull complete ; 9663085972fa: Pull complete ; 10ac03989960: Pull complete ; 401f11974a9b: Pull complete ; 67f12673f7e4: Pull complete ; 99116330e4f4: Pull complete ; 6fbbce8e3587: Pull complete ; c223e83ce2e3: Pull complete ; c02ebb3220a1: Pull complete ; 0c7a427ce17a: Pull complete ; ec9cd66333fe: Pull complete ; 9d57046ae5b9: Pull complete ; 0f5478ac499a: Pull complete ; b07098b67a6d: Pull complete ; 0accf0f55269: Pull complete ; ccc95462eb8f: Pull complete ; f1416983139e: Pull complete ; 2242c582e0cc: Pull complete ; 8f749be1be0b: Pull complete ; 03fdf02906f9: Pull complete ; ea2763a10d98: Pull complete ; fff529645086: Pull complete ; 42ad15be12fa: Pull complete ; 82830610edc8: Pull complete ; d1a85d710a45: Pull complete ; a7463a89d05f: Pull complete ; 966acfe9e7ff: Pull complete ; 987808dd3c93: Pull complete ; 079d9da9d9ee: Pull complete ; 875aa906c231: Pull complete ; 9463688d586c: Pull complete ; 2943d0ab0b4f: Pull complete ; 7a91c30c18a7: Pull complete ; 66996f762384: Pull complete ; 90237953ba0a: Pull complete ; Digest: sha256:b269b5b547bd8aa46f104f7bbdffaf6cdcf1df0143cfe85aa7f5a68767fa49bc; Status: Downloaded newer image for google/deepvariant:1.1.0; I0611 15:20:51.532788 140688071014144 run_deepvariant.py:317] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | p,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:176,Deployability,install,installed,176,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:; Yes. **Describe the issue:**; Launching an Ubuntu 20.04 server t2 micro EC2 on AWS, installed docker using snap, downloaded data from quickstart guide verbatim https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md. **Setup**; - Operating system: Ubuntu 20.04 server t2 micro EC2 on AWS; - DeepVariant version: BIN_VERSION=""1.1.0""; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Quick start data. **Steps to reproduce:**; - Command:; ```; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir; ```. - Error trace: (if applicable); ```; Unable to find image 'google/deepvariant:1.1.0' locally; 1.1.0: Pulling from google/deepvariant; be8ec4e48d7f: Pull complete ; 33b8b485aff0: Pull complete ; d887158cc58c: Pull complete ; 05895bb28c18: Pull complete ; 35be0878dcf6: Pull complete ; 03fb656082b2: Pull complete ; 1d3e393af6d8: Pull complete ; 9663085972fa: Pull complete ; 10ac03989960: Pull complete ; 401f11974a9b: Pull complete ; 67f12673f7e4: Pull complete ; 99116330e4f4: Pull complete ; 6fbbce8e3587: Pull complete ; c223e83ce2e3: Pull complete ; c02ebb3220a1: Pull complete ; 0c7a427ce17a: Pull complete ; ec9cd66333fe: Pull complete ; 9d57046ae5b9: Pull complete ; 0f54",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:451,Deployability,Install,Installation,451,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:; Yes. **Describe the issue:**; Launching an Ubuntu 20.04 server t2 micro EC2 on AWS, installed docker using snap, downloaded data from quickstart guide verbatim https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md. **Setup**; - Operating system: Ubuntu 20.04 server t2 micro EC2 on AWS; - DeepVariant version: BIN_VERSION=""1.1.0""; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Quick start data. **Steps to reproduce:**; - Command:; ```; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir; ```. - Error trace: (if applicable); ```; Unable to find image 'google/deepvariant:1.1.0' locally; 1.1.0: Pulling from google/deepvariant; be8ec4e48d7f: Pull complete ; 33b8b485aff0: Pull complete ; d887158cc58c: Pull complete ; 05895bb28c18: Pull complete ; 35be0878dcf6: Pull complete ; 03fb656082b2: Pull complete ; 1d3e393af6d8: Pull complete ; 9663085972fa: Pull complete ; 10ac03989960: Pull complete ; 401f11974a9b: Pull complete ; 67f12673f7e4: Pull complete ; 99116330e4f4: Pull complete ; 6fbbce8e3587: Pull complete ; c223e83ce2e3: Pull complete ; c02ebb3220a1: Pull complete ; 0c7a427ce17a: Pull complete ; ec9cd66333fe: Pull complete ; 9d57046ae5b9: Pull complete ; 0f54",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:6188,Testability,test,test,6188," using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-11 15:22:06.016750: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0. real	1m16.629s; user	1m9.338s; sys	0m1.008s; I0611 15:22:08.176606 140688071014144 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""/input/HG003.novaseq.wes_idt.100x.dedup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""/input/idt_capture_novogene.grch38.bed"" --task {} )' returned non-zero exit status 247.; ```. **Does the quick start test work on your system?** No; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:6227,Testability,test,test,6227," using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-11 15:22:06.016750: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0. real	1m16.629s; user	1m9.338s; sys	0m1.008s; I0611 15:22:08.176606 140688071014144 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""/input/HG003.novaseq.wes_idt.100x.dedup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""/input/idt_capture_novogene.grch38.bed"" --task {} )' returned non-zero exit status 247.; ```. **Does the quick start test work on your system?** No; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/462:237,Usability,guid,guide,237,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:; Yes. **Describe the issue:**; Launching an Ubuntu 20.04 server t2 micro EC2 on AWS, installed docker using snap, downloaded data from quickstart guide verbatim https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md. **Setup**; - Operating system: Ubuntu 20.04 server t2 micro EC2 on AWS; - DeepVariant version: BIN_VERSION=""1.1.0""; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Quick start data. **Steps to reproduce:**; - Command:; ```; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir; ```. - Error trace: (if applicable); ```; Unable to find image 'google/deepvariant:1.1.0' locally; 1.1.0: Pulling from google/deepvariant; be8ec4e48d7f: Pull complete ; 33b8b485aff0: Pull complete ; d887158cc58c: Pull complete ; 05895bb28c18: Pull complete ; 35be0878dcf6: Pull complete ; 03fb656082b2: Pull complete ; 1d3e393af6d8: Pull complete ; 9663085972fa: Pull complete ; 10ac03989960: Pull complete ; 401f11974a9b: Pull complete ; 67f12673f7e4: Pull complete ; 99116330e4f4: Pull complete ; 6fbbce8e3587: Pull complete ; c223e83ce2e3: Pull complete ; c02ebb3220a1: Pull complete ; 0c7a427ce17a: Pull complete ; ec9cd66333fe: Pull complete ; 9d57046ae5b9: Pull complete ; 0f54",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/462
https://github.com/google/deepvariant/issues/463:214,Energy Efficiency,allocate,allocate,214,I am using deepvariant on our cluster using singularity.; I could setup singularity and get it running. My problem now is that the process is pretty slow and it takes more than a day for just one sample. However I allocate more cpu core and memory does not help.; Increasing the number of shard does not resolve it and even make it worse.; Could someone please give me some clue on how to find it solution for this issue?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/463
https://github.com/google/deepvariant/issues/465:39,Availability,error,error,39,"Does anyone know what caused the below error?; I use deepvariant image on singularity and running it on a cluster but this error happens on many machines.; I don't know what causes this error. I0624 02:14:00.095050 47429297437696 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0624 02:14:01.826225 47429297437696 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0624 02:14:01.954994 47429297437696 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:123,Availability,error,error,123,"Does anyone know what caused the below error?; I use deepvariant image on singularity and running it on a cluster but this error happens on many machines.; I don't know what causes this error. I0624 02:14:00.095050 47429297437696 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0624 02:14:01.826225 47429297437696 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0624 02:14:01.954994 47429297437696 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:186,Availability,error,error,186,"Does anyone know what caused the below error?; I use deepvariant image on singularity and running it on a cluster but this error happens on many machines.; I don't know what causes this error. I0624 02:14:00.095050 47429297437696 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0624 02:14:01.826225 47429297437696 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0624 02:14:01.954994 47429297437696 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1900,Availability,checkpoint,checkpoint,1900,"les runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/S-001737188.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/S-001737188.g.vcf.gz"" ) 2>&1 | tee /output/logs/postprocess_variants.log. real	10m14.442s; user	0m1.472s; sys	0m1.164s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:423,Testability,log,logs,423,"Does anyone know what caused the below error?; I use deepvariant image on singularity and running it on a cluster but this error happens on many machines.; I don't know what causes this error. I0624 02:14:00.095050 47429297437696 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0624 02:14:01.826225 47429297437696 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0624 02:14:01.954994 47429297437696 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:439,Testability,log,logs,439,"Does anyone know what caused the below error?; I use deepvariant image on singularity and running it on a cluster but this error happens on many machines.; I don't know what causes this error. I0624 02:14:00.095050 47429297437696 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0624 02:14:01.826225 47429297437696 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0624 02:14:01.954994 47429297437696 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:570,Testability,log,logs,570,"Does anyone know what caused the below error?; I use deepvariant image on singularity and running it on a cluster but this error happens on many machines.; I don't know what causes this error. I0624 02:14:00.095050 47429297437696 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0624 02:14:01.826225 47429297437696 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0624 02:14:01.954994 47429297437696 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1039,Testability,log,logs,1039,"deepvariant image on singularity and running it on a cluster but this error happens on many machines.; I don't know what causes this error. I0624 02:14:00.095050 47429297437696 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0624 02:14:01.826225 47429297437696 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0624 02:14:01.954994 47429297437696 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1197,Testability,log,logs,1197,"or. I0624 02:14:00.095050 47429297437696 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0624 02:14:01.826225 47429297437696 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0624 02:14:01.954994 47429297437696 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --infi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1216,Testability,log,log,1216,"29297437696 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0624 02:14:01.826225 47429297437696 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0624 02:14:01.954994 47429297437696 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --infile ""/output/intermediate_resu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1492,Testability,log,logs,1492,"reating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/S-001737188.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/S-001737188.g.vcf.gz"" ) 2>&1 | tee /output/logs/postprocess_variants.log. real	10m14.442s; user	0m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1976,Testability,log,logs,1976,"les runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/S-001737188.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/S-001737188.g.vcf.gz"" ) 2>&1 | tee /output/logs/postprocess_variants.log. real	10m14.442s; user	0m1.472s; sys	0m1.164s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:1995,Testability,log,log,1995,"les runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/S-001737188.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/S-001737188.g.vcf.gz"" ) 2>&1 | tee /output/logs/postprocess_variants.log. real	10m14.442s; user	0m1.472s; sys	0m1.164s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:2452,Testability,log,logs,2452,"les runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/S-001737188.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/S-001737188.g.vcf.gz"" ) 2>&1 | tee /output/logs/postprocess_variants.log. real	10m14.442s; user	0m1.472s; sys	0m1.164s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/465:2478,Testability,log,log,2478,"les runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/S-001737188.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/S-001737188.g.vcf.gz"" ) 2>&1 | tee /output/logs/postprocess_variants.log. real	10m14.442s; user	0m1.472s; sys	0m1.164s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/465
https://github.com/google/deepvariant/issues/466:103,Safety,detect,detect,103,"Hi everyone,. I wanted to ask if I could use DeepVariant with **Tumor Illumina samples** to accurately detect **Germline** variants? Can it accurately distinguish them from the Somatic ones? I am interested in Germline variants only. Thank you for your time,; Konstantinos Kyriakidis",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/466
https://github.com/google/deepvariant/issues/469:71,Availability,error,error,71,"When using the provided docker image, google/deepvariant:1.1.0-gpu, an error is produced if --use_tpu is used with model_train. The command:. ```; docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:1.1.0-gpu \; /opt/deepvariant/bin/model_train \; --gcp_project="""" \; --tpu_name="""" \; --tpu_zone=""us-central1-a"" \; --use_tpu \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${OUTPUT_BUCKET}/training-tpu/"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=32 \; --learning_rate=0.0005 \; --start_from_checkpoint=""${INPUT_BUCKET}/wgs_model/model.ckpt"" ; ```. The error that prints is:. ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 303, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_461ld2s6/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_461ld2s6/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 291, in main; parse_and_run(); File ""/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 236, in parse_and_run; FLAGS.gcp_project) if FLAGS.use_tpu else ''; File ""/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_deepvariant/deepvariant/tf_utils.py"", line 391, in resolve_master; tpu=[tpu_name], zone=tpu_zone, project=gcp_project).get_master(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 257, in get_master; return self.master(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:675,Availability,error,error,675,"When using the provided docker image, google/deepvariant:1.1.0-gpu, an error is produced if --use_tpu is used with model_train. The command:. ```; docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:1.1.0-gpu \; /opt/deepvariant/bin/model_train \; --gcp_project="""" \; --tpu_name="""" \; --tpu_zone=""us-central1-a"" \; --use_tpu \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${OUTPUT_BUCKET}/training-tpu/"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=32 \; --learning_rate=0.0005 \; --start_from_checkpoint=""${INPUT_BUCKET}/wgs_model/model.ckpt"" ; ```. The error that prints is:. ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 303, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_461ld2s6/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_461ld2s6/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 291, in main; parse_and_run(); File ""/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_deepvariant/deepvariant/model_train.py"", line 236, in parse_and_run; FLAGS.gcp_project) if FLAGS.use_tpu else ''; File ""/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_deepvariant/deepvariant/tf_utils.py"", line 391, in resolve_master; tpu=[tpu_name], zone=tpu_zone, project=gcp_project).get_master(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 257, in get_master; return self.master(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:2865,Deployability,install,install,2865,""", line 257, in get_master; return self.master(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 241, in master; cluster_spec = self.cluster_spec(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 311, in cluster_spec; network_endpoints = self._cloud_tpu_client.network_endpoints(); File ""/usr/local/lib/python3.6/dist-packages/cloud_tpu_client/client.py"", line 320, in network_endpoints; response = self._fetch_cloud_tpu_metadata(); File ""/usr/local/lib/python3.6/dist-packages/cloud_tpu_client/client.py"", line 234, in _fetch_cloud_tpu_metadata; service = self._tpu_service(); File ""/usr/local/lib/python3.6/dist-packages/cloud_tpu_client/client.py"", line 209, in _tpu_service; raise RuntimeError('Missing runtime dependency on the Google API client. '; RuntimeError: Missing runtime dependency on the Google API client. Run `pip install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:3734,Deployability,patch,patching,3734,"ne 209, in _tpu_service; raise RuntimeError('Missing runtime dependency on the Google API client. '; RuntimeError: Missing runtime dependency on the Google API client. Run `pip install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:3790,Deployability,patch,patch,3790,"client. '; RuntimeError: Missing runtime dependency on the Google API client. Run `pip install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4692,Deployability,patch,patch,4692,"iousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/python3.6/dist-packages/googleapiclient/discovery.py discovery.patch. CMD [""/opt/deepvariant/bin/run_deepvariant"", ""--help""]; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4722,Deployability,patch,patch,4722,"iousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/python3.6/dist-packages/googleapiclient/discovery.py discovery.patch. CMD [""/opt/deepvariant/bin/run_deepvariant"", ""--help""]; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4843,Deployability,install,install,4843,"iousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/python3.6/dist-packages/googleapiclient/discovery.py discovery.patch. CMD [""/opt/deepvariant/bin/run_deepvariant"", ""--help""]; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4853,Deployability,upgrade,upgrade,4853,"iousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/python3.6/dist-packages/googleapiclient/discovery.py discovery.patch. CMD [""/opt/deepvariant/bin/run_deepvariant"", ""--help""]; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4887,Deployability,install,install,4887,"iousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/python3.6/dist-packages/googleapiclient/discovery.py discovery.patch. CMD [""/opt/deepvariant/bin/run_deepvariant"", ""--help""]; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4897,Deployability,upgrade,upgrade,4897,"iousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/python3.6/dist-packages/googleapiclient/discovery.py discovery.patch. CMD [""/opt/deepvariant/bin/run_deepvariant"", ""--help""]; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4982,Deployability,patch,patch,4982,"iousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/python3.6/dist-packages/googleapiclient/discovery.py discovery.patch. CMD [""/opt/deepvariant/bin/run_deepvariant"", ""--help""]; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:5011,Deployability,patch,patch,5011,"iousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/python3.6/dist-packages/googleapiclient/discovery.py discovery.patch. CMD [""/opt/deepvariant/bin/run_deepvariant"", ""--help""]; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:5095,Deployability,patch,patch,5095,"iousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/python3.6/dist-packages/googleapiclient/discovery.py discovery.patch. CMD [""/opt/deepvariant/bin/run_deepvariant"", ""--help""]; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:2749,Integrability,depend,dependency,2749,"p_project).get_master(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 257, in get_master; return self.master(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 241, in master; cluster_spec = self.cluster_spec(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 311, in cluster_spec; network_endpoints = self._cloud_tpu_client.network_endpoints(); File ""/usr/local/lib/python3.6/dist-packages/cloud_tpu_client/client.py"", line 320, in network_endpoints; response = self._fetch_cloud_tpu_metadata(); File ""/usr/local/lib/python3.6/dist-packages/cloud_tpu_client/client.py"", line 234, in _fetch_cloud_tpu_metadata; service = self._tpu_service(); File ""/usr/local/lib/python3.6/dist-packages/cloud_tpu_client/client.py"", line 209, in _tpu_service; raise RuntimeError('Missing runtime dependency on the Google API client. '; RuntimeError: Missing runtime dependency on the Google API client. Run `pip install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:2819,Integrability,depend,dependency,2819,"/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 257, in get_master; return self.master(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 241, in master; cluster_spec = self.cluster_spec(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py"", line 311, in cluster_spec; network_endpoints = self._cloud_tpu_client.network_endpoints(); File ""/usr/local/lib/python3.6/dist-packages/cloud_tpu_client/client.py"", line 320, in network_endpoints; response = self._fetch_cloud_tpu_metadata(); File ""/usr/local/lib/python3.6/dist-packages/cloud_tpu_client/client.py"", line 234, in _fetch_cloud_tpu_metadata; service = self._tpu_service(); File ""/usr/local/lib/python3.6/dist-packages/cloud_tpu_client/client.py"", line 209, in _tpu_service; raise RuntimeError('Missing runtime dependency on the Google API client. '; RuntimeError: Missing runtime dependency on the Google API client. Run `pip install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < impor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:3625,Modifiability,config,configuring,3625,"ne 209, in _tpu_service; raise RuntimeError('Missing runtime dependency on the Google API client. '; RuntimeError: Missing runtime dependency on the Google API client. Run `pip install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4403,Performance,load,loader,4403,"iousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/python3.6/dist-packages/googleapiclient/discovery.py discovery.patch. CMD [""/opt/deepvariant/bin/run_deepvariant"", ""--help""]; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:3646,Safety,avoid,avoid,3646,"ne 209, in _tpu_service; raise RuntimeError('Missing runtime dependency on the Google API client. '; RuntimeError: Missing runtime dependency on the Google API client. Run `pip install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:3880,Safety,avoid,avoid,3880," install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip;",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4023,Usability,Simpl,SimpleNamespace,4023,"orted from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/pytho",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/469:4068,Usability,Simpl,SimpleNamespace,4068,"3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/python3.6/dist-packages/googleapiclient/discovery.py discovery.patch. CMD [""/opt/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/469
https://github.com/google/deepvariant/issues/470:1547,Availability,avail,available,1547,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; I am debugging a set of false negative calls in a benchmarking set (NA12878, Agilent exome provided by a collaborator). . In the process, I came across a call that makes no sense to me and was wondering what a plausible explanation might be:. Final VCF:; `chr1 109161996 rs678238 A G 39 . AC=1;AF=0.5;AN=2;AQ=39;DP=218 GT:AD:DP:GQ:PL:RNC 0/1:0,218:218:15:39,0,14:.; `. And the gVCF:; `chr1 109161996 . A G,<*> 39.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:15:218:0,218,0:1,0:39,0,14,990,990,990; `. The true gtenotype at this position should be G|G. . However, note that the genotype is shown as 0|1 - even tho the ref allele as a depth of 0. This is supported by a manual inspection of the alignment. There really isn't an A there and it does not seem to be a ""problematic"" locus with long runs of A or G. The reads align perfectly without any gaps. . Screenshot: https://www.dropbox.com/s/sp2n2gfy3li2rjl/dv_locus_error.JPG?dl=0 , Allele frequency as per alignment: G: 100%. . So how come Deepvariant calls it like that? It really makes no sense to me :(. **Setup**; - Operating system: Centos 7, Docker container; - DeepVariant version: 1.1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Exome (Agilent V7, genome-in-a-bott reference). **Steps to reproduce:**; - Command: Not possible without the raw data...available upong request. ; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? . No. . **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:1576,Availability,Error,Error,1576,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; I am debugging a set of false negative calls in a benchmarking set (NA12878, Agilent exome provided by a collaborator). . In the process, I came across a call that makes no sense to me and was wondering what a plausible explanation might be:. Final VCF:; `chr1 109161996 rs678238 A G 39 . AC=1;AF=0.5;AN=2;AQ=39;DP=218 GT:AD:DP:GQ:PL:RNC 0/1:0,218:218:15:39,0,14:.; `. And the gVCF:; `chr1 109161996 . A G,<*> 39.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:15:218:0,218,0:1,0:39,0,14,990,990,990; `. The true gtenotype at this position should be G|G. . However, note that the genotype is shown as 0|1 - even tho the ref allele as a depth of 0. This is supported by a manual inspection of the alignment. There really isn't an A there and it does not seem to be a ""problematic"" locus with long runs of A or G. The reads align perfectly without any gaps. . Screenshot: https://www.dropbox.com/s/sp2n2gfy3li2rjl/dv_locus_error.JPG?dl=0 , Allele frequency as per alignment: G: 100%. . So how come Deepvariant calls it like that? It really makes no sense to me :(. **Setup**; - Operating system: Centos 7, Docker container; - DeepVariant version: 1.1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Exome (Agilent V7, genome-in-a-bott reference). **Steps to reproduce:**; - Command: Not possible without the raw data...available upong request. ; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? . No. . **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:1255,Deployability,Install,Installation,1255,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; I am debugging a set of false negative calls in a benchmarking set (NA12878, Agilent exome provided by a collaborator). . In the process, I came across a call that makes no sense to me and was wondering what a plausible explanation might be:. Final VCF:; `chr1 109161996 rs678238 A G 39 . AC=1;AF=0.5;AN=2;AQ=39;DP=218 GT:AD:DP:GQ:PL:RNC 0/1:0,218:218:15:39,0,14:.; `. And the gVCF:; `chr1 109161996 . A G,<*> 39.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:15:218:0,218,0:1,0:39,0,14,990,990,990; `. The true gtenotype at this position should be G|G. . However, note that the genotype is shown as 0|1 - even tho the ref allele as a depth of 0. This is supported by a manual inspection of the alignment. There really isn't an A there and it does not seem to be a ""problematic"" locus with long runs of A or G. The reads align perfectly without any gaps. . Screenshot: https://www.dropbox.com/s/sp2n2gfy3li2rjl/dv_locus_error.JPG?dl=0 , Allele frequency as per alignment: G: 100%. . So how come Deepvariant calls it like that? It really makes no sense to me :(. **Setup**; - Operating system: Centos 7, Docker container; - DeepVariant version: 1.1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Exome (Agilent V7, genome-in-a-bott reference). **Steps to reproduce:**; - Command: Not possible without the raw data...available upong request. ; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? . No. . **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:167,Testability,benchmark,benchmarking,167,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; I am debugging a set of false negative calls in a benchmarking set (NA12878, Agilent exome provided by a collaborator). . In the process, I came across a call that makes no sense to me and was wondering what a plausible explanation might be:. Final VCF:; `chr1 109161996 rs678238 A G 39 . AC=1;AF=0.5;AN=2;AQ=39;DP=218 GT:AD:DP:GQ:PL:RNC 0/1:0,218:218:15:39,0,14:.; `. And the gVCF:; `chr1 109161996 . A G,<*> 39.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:15:218:0,218,0:1,0:39,0,14,990,990,990; `. The true gtenotype at this position should be G|G. . However, note that the genotype is shown as 0|1 - even tho the ref allele as a depth of 0. This is supported by a manual inspection of the alignment. There really isn't an A there and it does not seem to be a ""problematic"" locus with long runs of A or G. The reads align perfectly without any gaps. . Screenshot: https://www.dropbox.com/s/sp2n2gfy3li2rjl/dv_locus_error.JPG?dl=0 , Allele frequency as per alignment: G: 100%. . So how come Deepvariant calls it like that? It really makes no sense to me :(. **Setup**; - Operating system: Centos 7, Docker container; - DeepVariant version: 1.1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Exome (Agilent V7, genome-in-a-bott reference). **Steps to reproduce:**; - Command: Not possible without the raw data...available upong request. ; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? . No. . **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:1629,Testability,test,test,1629,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; I am debugging a set of false negative calls in a benchmarking set (NA12878, Agilent exome provided by a collaborator). . In the process, I came across a call that makes no sense to me and was wondering what a plausible explanation might be:. Final VCF:; `chr1 109161996 rs678238 A G 39 . AC=1;AF=0.5;AN=2;AQ=39;DP=218 GT:AD:DP:GQ:PL:RNC 0/1:0,218:218:15:39,0,14:.; `. And the gVCF:; `chr1 109161996 . A G,<*> 39.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:15:218:0,218,0:1,0:39,0,14,990,990,990; `. The true gtenotype at this position should be G|G. . However, note that the genotype is shown as 0|1 - even tho the ref allele as a depth of 0. This is supported by a manual inspection of the alignment. There really isn't an A there and it does not seem to be a ""problematic"" locus with long runs of A or G. The reads align perfectly without any gaps. . Screenshot: https://www.dropbox.com/s/sp2n2gfy3li2rjl/dv_locus_error.JPG?dl=0 , Allele frequency as per alignment: G: 100%. . So how come Deepvariant calls it like that? It really makes no sense to me :(. **Setup**; - Operating system: Centos 7, Docker container; - DeepVariant version: 1.1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Exome (Agilent V7, genome-in-a-bott reference). **Steps to reproduce:**; - Command: Not possible without the raw data...available upong request. ; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? . No. . **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/470:1665,Testability,test,test,1665,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; I am debugging a set of false negative calls in a benchmarking set (NA12878, Agilent exome provided by a collaborator). . In the process, I came across a call that makes no sense to me and was wondering what a plausible explanation might be:. Final VCF:; `chr1 109161996 rs678238 A G 39 . AC=1;AF=0.5;AN=2;AQ=39;DP=218 GT:AD:DP:GQ:PL:RNC 0/1:0,218:218:15:39,0,14:.; `. And the gVCF:; `chr1 109161996 . A G,<*> 39.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:15:218:0,218,0:1,0:39,0,14,990,990,990; `. The true gtenotype at this position should be G|G. . However, note that the genotype is shown as 0|1 - even tho the ref allele as a depth of 0. This is supported by a manual inspection of the alignment. There really isn't an A there and it does not seem to be a ""problematic"" locus with long runs of A or G. The reads align perfectly without any gaps. . Screenshot: https://www.dropbox.com/s/sp2n2gfy3li2rjl/dv_locus_error.JPG?dl=0 , Allele frequency as per alignment: G: 100%. . So how come Deepvariant calls it like that? It really makes no sense to me :(. **Setup**; - Operating system: Centos 7, Docker container; - DeepVariant version: 1.1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Exome (Agilent V7, genome-in-a-bott reference). **Steps to reproduce:**; - Command: Not possible without the raw data...available upong request. ; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? . No. . **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/470
https://github.com/google/deepvariant/issues/471:76,Availability,error,error,76,"**Describe the issue:**; I follow the quick start guidelines, and meet this error. **Setup**; - Operating system: MacBook Air (M1, 2020); - DeepVariant version: 19.03.14; - Installation method (Docker, built from source, etc.): Docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) quick start data . **Steps to reproduce:**; - Command: sudo docker run --platform linux/amd64 google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/quickstart-output/output.vcf.gz --output_gvcf=/quickstart-output/output.g.vcf.gz --intermediate_results_dir /quickstart-output/intermediate_results_dir --num_shards=1; - Error trace: (if applicable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickst",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:855,Availability,Error,Error,855,"**Describe the issue:**; I follow the quick start guidelines, and meet this error. **Setup**; - Operating system: MacBook Air (M1, 2020); - DeepVariant version: 19.03.14; - Installation method (Docker, built from source, etc.): Docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) quick start data . **Steps to reproduce:**; - Command: sudo docker run --platform linux/amd64 google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/quickstart-output/output.vcf.gz --output_gvcf=/quickstart-output/output.g.vcf.gz --intermediate_results_dir /quickstart-output/intermediate_results_dir --num_shards=1; - Error trace: (if applicable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickst",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:173,Deployability,Install,Installation,173,"**Describe the issue:**; I follow the quick start guidelines, and meet this error. **Setup**; - Operating system: MacBook Air (M1, 2020); - DeepVariant version: 19.03.14; - Installation method (Docker, built from source, etc.): Docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) quick start data . **Steps to reproduce:**; - Command: sudo docker run --platform linux/amd64 google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/quickstart-output/output.vcf.gz --output_gvcf=/quickstart-output/output.g.vcf.gz --intermediate_results_dir /quickstart-output/intermediate_results_dir --num_shards=1; - Error trace: (if applicable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickst",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:1661,Energy Efficiency,monitor,monitoring,1661,",010,000"" --output_vcf=/quickstart-output/output.vcf.gz --output_gvcf=/quickstart-output/output.g.vcf.gz --intermediate_results_dir /quickstart-output/intermediate_results_dir --num_shards=1; - Error trace: (if applicable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sy",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:1825,Safety,Abort,Aborted,1825,"licable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in chec",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:531,Testability,test,testdata,531,"**Describe the issue:**; I follow the quick start guidelines, and meet this error. **Setup**; - Operating system: MacBook Air (M1, 2020); - DeepVariant version: 19.03.14; - Installation method (Docker, built from source, etc.): Docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) quick start data . **Steps to reproduce:**; - Command: sudo docker run --platform linux/amd64 google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/quickstart-output/output.vcf.gz --output_gvcf=/quickstart-output/output.g.vcf.gz --intermediate_results_dir /quickstart-output/intermediate_results_dir --num_shards=1; - Error trace: (if applicable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickst",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:591,Testability,test,testdata,591,"**Describe the issue:**; I follow the quick start guidelines, and meet this error. **Setup**; - Operating system: MacBook Air (M1, 2020); - DeepVariant version: 19.03.14; - Installation method (Docker, built from source, etc.): Docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) quick start data . **Steps to reproduce:**; - Command: sudo docker run --platform linux/amd64 google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/quickstart-output/output.vcf.gz --output_gvcf=/quickstart-output/output.g.vcf.gz --intermediate_results_dir /quickstart-output/intermediate_results_dir --num_shards=1; - Error trace: (if applicable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickst",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:1298,Testability,test,testdata,1298," Docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) quick start data . **Steps to reproduce:**; - Command: sudo docker run --platform linux/amd64 google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/quickstart-output/output.vcf.gz --output_gvcf=/quickstart-output/output.g.vcf.gz --intermediate_results_dir /quickstart-output/intermediate_results_dir --num_shards=1; - Error trace: (if applicable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:1360,Testability,test,testdata,1360,"t data . **Steps to reproduce:**; - Command: sudo docker run --platform linux/amd64 google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/quickstart-output/output.vcf.gz --output_gvcf=/quickstart-output/output.g.vcf.gz --intermediate_results_dir /quickstart-output/intermediate_results_dir --num_shards=1; - Error trace: (if applicable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; T",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:1945,Testability,test,testdata,1945,"licable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in chec",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:2005,Testability,test,testdata,2005,"t-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 -",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:3084,Testability,test,testdata,3084,"_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} )' returned non-zero exit status 250. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; no; **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:3146,Testability,test,testdata,3146,"_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} )' returned non-zero exit status 250. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; no; **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:3455,Testability,test,test,3455,"_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} )' returned non-zero exit status 250. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; no; **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:3491,Testability,test,test,3491,"_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} )' returned non-zero exit status 250. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; no; **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/471:50,Usability,guid,guidelines,50,"**Describe the issue:**; I follow the quick start guidelines, and meet this error. **Setup**; - Operating system: MacBook Air (M1, 2020); - DeepVariant version: 19.03.14; - Installation method (Docker, built from source, etc.): Docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) quick start data . **Steps to reproduce:**; - Command: sudo docker run --platform linux/amd64 google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/quickstart-output/output.vcf.gz --output_gvcf=/quickstart-output/output.g.vcf.gz --intermediate_results_dir /quickstart-output/intermediate_results_dir --num_shards=1; - Error trace: (if applicable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickst",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/471
https://github.com/google/deepvariant/issues/474:247,Energy Efficiency,schedul,scheduler,247,"Hi there,. I've been trying to figure out how to actually run deepvariant in a cluster environment but thus far, the instructions seems a little cryptic to me. ; Is there perhaps a step-by-step guide to running deepvariant on a cluster with a PBS scheduler for instance?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/474:194,Usability,guid,guide,194,"Hi there,. I've been trying to figure out how to actually run deepvariant in a cluster environment but thus far, the instructions seems a little cryptic to me. ; Is there perhaps a step-by-step guide to running deepvariant on a cluster with a PBS scheduler for instance?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/474
https://github.com/google/deepvariant/issues/475:101,Deployability,pipeline,pipeline,101,"Hello,. We have found that a known de novo variant was missed when using DeepTrio and GLnexus in our pipeline (WGS, hg38). I know that a similar issue has already been raised and appreciate the interesting discussion on this (i.e. https://github.com/google/deepvariant/issues/440), but to recap for others this was the result of two contributing factors:. 1. DeepTrio being less confident in the de novo call for the proband than when DeepVariant is run in singleton mode on the proband. In our case, comparing the output VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepT",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:755,Deployability,configurat,configuration,755,"Hello,. We have found that a known de novo variant was missed when using DeepTrio and GLnexus in our pipeline (WGS, hg38). I know that a similar issue has already been raised and appreciate the interesting discussion on this (i.e. https://github.com/google/deepvariant/issues/440), but to recap for others this was the result of two contributing factors:. 1. DeepTrio being less confident in the de novo call for the proband than when DeepVariant is run in singleton mode on the proband. In our case, comparing the output VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepT",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:950,Deployability,configurat,configuration,950,"Hello,. We have found that a known de novo variant was missed when using DeepTrio and GLnexus in our pipeline (WGS, hg38). I know that a similar issue has already been raised and appreciate the interesting discussion on this (i.e. https://github.com/google/deepvariant/issues/440), but to recap for others this was the result of two contributing factors:. 1. DeepTrio being less confident in the de novo call for the proband than when DeepVariant is run in singleton mode on the proband. In our case, comparing the output VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepT",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:1855,Deployability,release,released,1855,"tput VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepTrio? Is there a reason why the HG001/NA12891/NA12892 trios were used as training for the child model but not the parent model?. <br>. Many thanks,; Macabe. <br>. ![image](https://user-images.githubusercontent.com/37773554/128098808-740a1ab0-a6af-452f-8bed-d1f4ba0ceb80.png); Current DeepTrio training info (likely typo for Ashkenazim trio, cf. HG002/HG00**3**/HG004). [1] https://academic.oup.com/bioinformatics/article/36/24/5582/6064144 ; [2] https://github.com/google/deepvariant/tree/r1.2/deeptrio/testdata/input",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:755,Modifiability,config,configuration,755,"Hello,. We have found that a known de novo variant was missed when using DeepTrio and GLnexus in our pipeline (WGS, hg38). I know that a similar issue has already been raised and appreciate the interesting discussion on this (i.e. https://github.com/google/deepvariant/issues/440), but to recap for others this was the result of two contributing factors:. 1. DeepTrio being less confident in the de novo call for the proband than when DeepVariant is run in singleton mode on the proband. In our case, comparing the output VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepT",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:950,Modifiability,config,configuration,950,"Hello,. We have found that a known de novo variant was missed when using DeepTrio and GLnexus in our pipeline (WGS, hg38). I know that a similar issue has already been raised and appreciate the interesting discussion on this (i.e. https://github.com/google/deepvariant/issues/440), but to recap for others this was the result of two contributing factors:. 1. DeepTrio being less confident in the de novo call for the proband than when DeepVariant is run in singleton mode on the proband. In our case, comparing the output VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepT",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:1363,Testability,benchmark,benchmark,1363,"ent in the de novo call for the proband than when DeepVariant is run in singleton mode on the proband. In our case, comparing the output VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepTrio? Is there a reason why the HG001/NA12891/NA12892 trios were used as training for the child model but not the parent model?. <br>. Many thanks,; Macabe. <br>. ![image](https://user-images.githubusercontent.com/37773554/128098808-740a1ab0-a6af-452f-8bed-d1f4ba0ceb80.png); Current DeepTrio training info (likely typo for Ashkenazim trio, cf. HG002/HG00**3**/HG004). [1] https://acade",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:1603,Testability,test,test,1603,"tput VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepTrio? Is there a reason why the HG001/NA12891/NA12892 trios were used as training for the child model but not the parent model?. <br>. Many thanks,; Macabe. <br>. ![image](https://user-images.githubusercontent.com/37773554/128098808-740a1ab0-a6af-452f-8bed-d1f4ba0ceb80.png); Current DeepTrio training info (likely typo for Ashkenazim trio, cf. HG002/HG00**3**/HG004). [1] https://academic.oup.com/bioinformatics/article/36/24/5582/6064144 ; [2] https://github.com/google/deepvariant/tree/r1.2/deeptrio/testdata/input",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:1747,Testability,test,testing,1747,"tput VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepTrio? Is there a reason why the HG001/NA12891/NA12892 trios were used as training for the child model but not the parent model?. <br>. Many thanks,; Macabe. <br>. ![image](https://user-images.githubusercontent.com/37773554/128098808-740a1ab0-a6af-452f-8bed-d1f4ba0ceb80.png); Current DeepTrio training info (likely typo for Ashkenazim trio, cf. HG002/HG00**3**/HG004). [1] https://academic.oup.com/bioinformatics/article/36/24/5582/6064144 ; [2] https://github.com/google/deepvariant/tree/r1.2/deeptrio/testdata/input",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/475:2503,Testability,test,testdata,2503,"tput VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepTrio? Is there a reason why the HG001/NA12891/NA12892 trios were used as training for the child model but not the parent model?. <br>. Many thanks,; Macabe. <br>. ![image](https://user-images.githubusercontent.com/37773554/128098808-740a1ab0-a6af-452f-8bed-d1f4ba0ceb80.png); Current DeepTrio training info (likely typo for Ashkenazim trio, cf. HG002/HG00**3**/HG004). [1] https://academic.oup.com/bioinformatics/article/36/24/5582/6064144 ; [2] https://github.com/google/deepvariant/tree/r1.2/deeptrio/testdata/input",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/475
https://github.com/google/deepvariant/issues/476:283,Availability,error,error,283,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: yes. **Describe the issue:**; I am building the deep variant Dockerfile (v1.2) off the Databricks Runtime base image (Ubuntu 18.04).; Run into issues at Stage 'Install CLIF binary', I get the error,. `ModuleNotFoundError: No module named 'apt_pkg'`. I see in the build-prereq.sh script this comment,. `Build clif binary from scratch. Might not be ideal because it installs a; bunch of dependencies, but this works fine when we used this in a Dockerfile; because we don't do build-prereq.sh in the final image.`. Please advise how to get around this when building your own Docker Image. Cheers,. William. **Setup**; - Operating system: Ununtu 18.04; - DeepVariant version: 1.2; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:982,Availability,Error,Error,982,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: yes. **Describe the issue:**; I am building the deep variant Dockerfile (v1.2) off the Databricks Runtime base image (Ubuntu 18.04).; Run into issues at Stage 'Install CLIF binary', I get the error,. `ModuleNotFoundError: No module named 'apt_pkg'`. I see in the build-prereq.sh script this comment,. `Build clif binary from scratch. Might not be ideal because it installs a; bunch of dependencies, but this works fine when we used this in a Dockerfile; because we don't do build-prereq.sh in the final image.`. Please advise how to get around this when building your own Docker Image. Cheers,. William. **Setup**; - Operating system: Ununtu 18.04; - DeepVariant version: 1.2; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:251,Deployability,Install,Install,251,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: yes. **Describe the issue:**; I am building the deep variant Dockerfile (v1.2) off the Databricks Runtime base image (Ubuntu 18.04).; Run into issues at Stage 'Install CLIF binary', I get the error,. `ModuleNotFoundError: No module named 'apt_pkg'`. I see in the build-prereq.sh script this comment,. `Build clif binary from scratch. Might not be ideal because it installs a; bunch of dependencies, but this works fine when we used this in a Dockerfile; because we don't do build-prereq.sh in the final image.`. Please advise how to get around this when building your own Docker Image. Cheers,. William. **Setup**; - Operating system: Ununtu 18.04; - DeepVariant version: 1.2; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:455,Deployability,install,installs,455,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: yes. **Describe the issue:**; I am building the deep variant Dockerfile (v1.2) off the Databricks Runtime base image (Ubuntu 18.04).; Run into issues at Stage 'Install CLIF binary', I get the error,. `ModuleNotFoundError: No module named 'apt_pkg'`. I see in the build-prereq.sh script this comment,. `Build clif binary from scratch. Might not be ideal because it installs a; bunch of dependencies, but this works fine when we used this in a Dockerfile; because we don't do build-prereq.sh in the final image.`. Please advise how to get around this when building your own Docker Image. Cheers,. William. **Setup**; - Operating system: Ununtu 18.04; - DeepVariant version: 1.2; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:770,Deployability,Install,Installation,770,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: yes. **Describe the issue:**; I am building the deep variant Dockerfile (v1.2) off the Databricks Runtime base image (Ubuntu 18.04).; Run into issues at Stage 'Install CLIF binary', I get the error,. `ModuleNotFoundError: No module named 'apt_pkg'`. I see in the build-prereq.sh script this comment,. `Build clif binary from scratch. Might not be ideal because it installs a; bunch of dependencies, but this works fine when we used this in a Dockerfile; because we don't do build-prereq.sh in the final image.`. Please advise how to get around this when building your own Docker Image. Cheers,. William. **Setup**; - Operating system: Ununtu 18.04; - DeepVariant version: 1.2; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:476,Integrability,depend,dependencies,476,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: yes. **Describe the issue:**; I am building the deep variant Dockerfile (v1.2) off the Databricks Runtime base image (Ubuntu 18.04).; Run into issues at Stage 'Install CLIF binary', I get the error,. `ModuleNotFoundError: No module named 'apt_pkg'`. I see in the build-prereq.sh script this comment,. `Build clif binary from scratch. Might not be ideal because it installs a; bunch of dependencies, but this works fine when we used this in a Dockerfile; because we don't do build-prereq.sh in the final image.`. Please advise how to get around this when building your own Docker Image. Cheers,. William. **Setup**; - Operating system: Ununtu 18.04; - DeepVariant version: 1.2; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:1035,Testability,test,test,1035,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: yes. **Describe the issue:**; I am building the deep variant Dockerfile (v1.2) off the Databricks Runtime base image (Ubuntu 18.04).; Run into issues at Stage 'Install CLIF binary', I get the error,. `ModuleNotFoundError: No module named 'apt_pkg'`. I see in the build-prereq.sh script this comment,. `Build clif binary from scratch. Might not be ideal because it installs a; bunch of dependencies, but this works fine when we used this in a Dockerfile; because we don't do build-prereq.sh in the final image.`. Please advise how to get around this when building your own Docker Image. Cheers,. William. **Setup**; - Operating system: Ununtu 18.04; - DeepVariant version: 1.2; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/476:1071,Testability,test,test,1071,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: yes. **Describe the issue:**; I am building the deep variant Dockerfile (v1.2) off the Databricks Runtime base image (Ubuntu 18.04).; Run into issues at Stage 'Install CLIF binary', I get the error,. `ModuleNotFoundError: No module named 'apt_pkg'`. I see in the build-prereq.sh script this comment,. `Build clif binary from scratch. Might not be ideal because it installs a; bunch of dependencies, but this works fine when we used this in a Dockerfile; because we don't do build-prereq.sh in the final image.`. Please advise how to get around this when building your own Docker Image. Cheers,. William. **Setup**; - Operating system: Ununtu 18.04; - DeepVariant version: 1.2; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/476
https://github.com/google/deepvariant/issues/477:53,Availability,checkpoint,checkpoint,53,"What is the version of tensorflow for generating the checkpoint files (`index`, `meta`, `data`)?; And is there any way that I can load these checkpoints into a standalone tensorflow program and then dump it as a `.onnx` file?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/477
https://github.com/google/deepvariant/issues/477:141,Availability,checkpoint,checkpoints,141,"What is the version of tensorflow for generating the checkpoint files (`index`, `meta`, `data`)?; And is there any way that I can load these checkpoints into a standalone tensorflow program and then dump it as a `.onnx` file?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/477
https://github.com/google/deepvariant/issues/477:130,Performance,load,load,130,"What is the version of tensorflow for generating the checkpoint files (`index`, `meta`, `data`)?; And is there any way that I can load these checkpoints into a standalone tensorflow program and then dump it as a `.onnx` file?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/477
https://github.com/google/deepvariant/issues/478:134,Deployability,install,installed,134,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:; yes. **Describe the issue:**; Version 1.2 installed via docker on a linux server (over SSH login), running the quickstart test run:; - Expected behavior: when running without sudo, process uses current user's name privilege.; - What happened: file access denied if folder permission is 744. The run successfully returns if manually setting the relevant folders to permission 777, but output (vcf files and report) files were owned by nobody/nobody. . My understanding is that nobody is a special handle meant for OS housekeeping works. Is this an expected behavior? Is it docker?. **Setup**; - Operating system: CentOS 7 (`cat /etc/os-release`); - DeepVariant version: 1.2; - Installation method: docker; - Type of data: The test data and command described in [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). **Steps to reproduce:**; - Command: identical to those of [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). Environment variable setup lines were directly pasted into the shell, the 'run everything' command was pasted into a file `cmd.sh` which was then was ran with `. cmd.sh`. **Does the quick start test work on your system?**; Yes. Outputs are fine. **Any additional context:**; Except having to add `mkdir` and `chmod` lines to the script, I found the run successful. I can read/write to the files owned by nobody and the ownership will transfer automatically upon writing.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/478
https://github.com/google/deepvariant/issues/478:727,Deployability,release,release,727,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:; yes. **Describe the issue:**; Version 1.2 installed via docker on a linux server (over SSH login), running the quickstart test run:; - Expected behavior: when running without sudo, process uses current user's name privilege.; - What happened: file access denied if folder permission is 744. The run successfully returns if manually setting the relevant folders to permission 777, but output (vcf files and report) files were owned by nobody/nobody. . My understanding is that nobody is a special handle meant for OS housekeeping works. Is this an expected behavior? Is it docker?. **Setup**; - Operating system: CentOS 7 (`cat /etc/os-release`); - DeepVariant version: 1.2; - Installation method: docker; - Type of data: The test data and command described in [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). **Steps to reproduce:**; - Command: identical to those of [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). Environment variable setup lines were directly pasted into the shell, the 'run everything' command was pasted into a file `cmd.sh` which was then was ran with `. cmd.sh`. **Does the quick start test work on your system?**; Yes. Outputs are fine. **Any additional context:**; Except having to add `mkdir` and `chmod` lines to the script, I found the run successful. I can read/write to the files owned by nobody and the ownership will transfer automatically upon writing.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/478
https://github.com/google/deepvariant/issues/478:768,Deployability,Install,Installation,768,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:; yes. **Describe the issue:**; Version 1.2 installed via docker on a linux server (over SSH login), running the quickstart test run:; - Expected behavior: when running without sudo, process uses current user's name privilege.; - What happened: file access denied if folder permission is 744. The run successfully returns if manually setting the relevant folders to permission 777, but output (vcf files and report) files were owned by nobody/nobody. . My understanding is that nobody is a special handle meant for OS housekeeping works. Is this an expected behavior? Is it docker?. **Setup**; - Operating system: CentOS 7 (`cat /etc/os-release`); - DeepVariant version: 1.2; - Installation method: docker; - Type of data: The test data and command described in [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). **Steps to reproduce:**; - Command: identical to those of [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). Environment variable setup lines were directly pasted into the shell, the 'run everything' command was pasted into a file `cmd.sh` which was then was ran with `. cmd.sh`. **Does the quick start test work on your system?**; Yes. Outputs are fine. **Any additional context:**; Except having to add `mkdir` and `chmod` lines to the script, I found the run successful. I can read/write to the files owned by nobody and the ownership will transfer automatically upon writing.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/478
https://github.com/google/deepvariant/issues/478:1116,Modifiability,variab,variable,1116,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:; yes. **Describe the issue:**; Version 1.2 installed via docker on a linux server (over SSH login), running the quickstart test run:; - Expected behavior: when running without sudo, process uses current user's name privilege.; - What happened: file access denied if folder permission is 744. The run successfully returns if manually setting the relevant folders to permission 777, but output (vcf files and report) files were owned by nobody/nobody. . My understanding is that nobody is a special handle meant for OS housekeeping works. Is this an expected behavior? Is it docker?. **Setup**; - Operating system: CentOS 7 (`cat /etc/os-release`); - DeepVariant version: 1.2; - Installation method: docker; - Type of data: The test data and command described in [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). **Steps to reproduce:**; - Command: identical to those of [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). Environment variable setup lines were directly pasted into the shell, the 'run everything' command was pasted into a file `cmd.sh` which was then was ran with `. cmd.sh`. **Does the quick start test work on your system?**; Yes. Outputs are fine. **Any additional context:**; Except having to add `mkdir` and `chmod` lines to the script, I found the run successful. I can read/write to the files owned by nobody and the ownership will transfer automatically upon writing.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/478
https://github.com/google/deepvariant/issues/478:340,Security,access,access,340,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:; yes. **Describe the issue:**; Version 1.2 installed via docker on a linux server (over SSH login), running the quickstart test run:; - Expected behavior: when running without sudo, process uses current user's name privilege.; - What happened: file access denied if folder permission is 744. The run successfully returns if manually setting the relevant folders to permission 777, but output (vcf files and report) files were owned by nobody/nobody. . My understanding is that nobody is a special handle meant for OS housekeeping works. Is this an expected behavior? Is it docker?. **Setup**; - Operating system: CentOS 7 (`cat /etc/os-release`); - DeepVariant version: 1.2; - Installation method: docker; - Type of data: The test data and command described in [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). **Steps to reproduce:**; - Command: identical to those of [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). Environment variable setup lines were directly pasted into the shell, the 'run everything' command was pasted into a file `cmd.sh` which was then was ran with `. cmd.sh`. **Does the quick start test work on your system?**; Yes. Outputs are fine. **Any additional context:**; Except having to add `mkdir` and `chmod` lines to the script, I found the run successful. I can read/write to the files owned by nobody and the ownership will transfer automatically upon writing.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/478
https://github.com/google/deepvariant/issues/478:183,Testability,log,login,183,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:; yes. **Describe the issue:**; Version 1.2 installed via docker on a linux server (over SSH login), running the quickstart test run:; - Expected behavior: when running without sudo, process uses current user's name privilege.; - What happened: file access denied if folder permission is 744. The run successfully returns if manually setting the relevant folders to permission 777, but output (vcf files and report) files were owned by nobody/nobody. . My understanding is that nobody is a special handle meant for OS housekeeping works. Is this an expected behavior? Is it docker?. **Setup**; - Operating system: CentOS 7 (`cat /etc/os-release`); - DeepVariant version: 1.2; - Installation method: docker; - Type of data: The test data and command described in [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). **Steps to reproduce:**; - Command: identical to those of [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). Environment variable setup lines were directly pasted into the shell, the 'run everything' command was pasted into a file `cmd.sh` which was then was ran with `. cmd.sh`. **Does the quick start test work on your system?**; Yes. Outputs are fine. **Any additional context:**; Except having to add `mkdir` and `chmod` lines to the script, I found the run successful. I can read/write to the files owned by nobody and the ownership will transfer automatically upon writing.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/478
https://github.com/google/deepvariant/issues/478:214,Testability,test,test,214,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:; yes. **Describe the issue:**; Version 1.2 installed via docker on a linux server (over SSH login), running the quickstart test run:; - Expected behavior: when running without sudo, process uses current user's name privilege.; - What happened: file access denied if folder permission is 744. The run successfully returns if manually setting the relevant folders to permission 777, but output (vcf files and report) files were owned by nobody/nobody. . My understanding is that nobody is a special handle meant for OS housekeeping works. Is this an expected behavior? Is it docker?. **Setup**; - Operating system: CentOS 7 (`cat /etc/os-release`); - DeepVariant version: 1.2; - Installation method: docker; - Type of data: The test data and command described in [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). **Steps to reproduce:**; - Command: identical to those of [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). Environment variable setup lines were directly pasted into the shell, the 'run everything' command was pasted into a file `cmd.sh` which was then was ran with `. cmd.sh`. **Does the quick start test work on your system?**; Yes. Outputs are fine. **Any additional context:**; Except having to add `mkdir` and `chmod` lines to the script, I found the run successful. I can read/write to the files owned by nobody and the ownership will transfer automatically upon writing.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/478
https://github.com/google/deepvariant/issues/478:817,Testability,test,test,817,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:; yes. **Describe the issue:**; Version 1.2 installed via docker on a linux server (over SSH login), running the quickstart test run:; - Expected behavior: when running without sudo, process uses current user's name privilege.; - What happened: file access denied if folder permission is 744. The run successfully returns if manually setting the relevant folders to permission 777, but output (vcf files and report) files were owned by nobody/nobody. . My understanding is that nobody is a special handle meant for OS housekeeping works. Is this an expected behavior? Is it docker?. **Setup**; - Operating system: CentOS 7 (`cat /etc/os-release`); - DeepVariant version: 1.2; - Installation method: docker; - Type of data: The test data and command described in [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). **Steps to reproduce:**; - Command: identical to those of [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). Environment variable setup lines were directly pasted into the shell, the 'run everything' command was pasted into a file `cmd.sh` which was then was ran with `. cmd.sh`. **Does the quick start test work on your system?**; Yes. Outputs are fine. **Any additional context:**; Except having to add `mkdir` and `chmod` lines to the script, I found the run successful. I can read/write to the files owned by nobody and the ownership will transfer automatically upon writing.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/478
https://github.com/google/deepvariant/issues/478:1298,Testability,test,test,1298,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:; yes. **Describe the issue:**; Version 1.2 installed via docker on a linux server (over SSH login), running the quickstart test run:; - Expected behavior: when running without sudo, process uses current user's name privilege.; - What happened: file access denied if folder permission is 744. The run successfully returns if manually setting the relevant folders to permission 777, but output (vcf files and report) files were owned by nobody/nobody. . My understanding is that nobody is a special handle meant for OS housekeeping works. Is this an expected behavior? Is it docker?. **Setup**; - Operating system: CentOS 7 (`cat /etc/os-release`); - DeepVariant version: 1.2; - Installation method: docker; - Type of data: The test data and command described in [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). **Steps to reproduce:**; - Command: identical to those of [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). Environment variable setup lines were directly pasted into the shell, the 'run everything' command was pasted into a file `cmd.sh` which was then was ran with `. cmd.sh`. **Does the quick start test work on your system?**; Yes. Outputs are fine. **Any additional context:**; Except having to add `mkdir` and `chmod` lines to the script, I found the run successful. I can read/write to the files owned by nobody and the ownership will transfer automatically upon writing.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/478
https://github.com/google/deepvariant/issues/479:150,Energy Efficiency,efficient,efficient,150,"Hello! I've found a performance issue in deepvariant/data_providers.py: `batch()` should be called before `map()`, which could make your program more efficient. Here is [the tensorflow document](https://tensorflow.google.cn/guide/data_performance?hl=zh_cn#vectorized_mapping) to support it. Detailed description is listed below:. - deepvariant/data_providers.py: `dataset.batch(batch_size=batch_size, drop_remainder=True)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L316) should be called before `dataset.map(map_func=self.parse_tfexample, num_parallel_calls=tf.data.AUTOTUNE)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L314).; - deepvariant/data_providers.py: `dataset.batch(batch_size=batch_size)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L364) should be called before `dataset.map(map_func=self.parse_tfexample, num_parallel_calls=tf.data.AUTOTUNE)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L362). Besides, you need to check the function called in `map()`(e.g., `self.parse_tfexample` called in `dataset.map()`) whether to be affected or not to make the changed code work properly. For example, if `self.parse_tfexample` needs data with shape (x, y, z) as its input before fix, it would require data with shape (batch_size, x, y, z). Looking forward to your reply. Btw, I am very glad to create a PR to fix it if you are too busy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:20,Performance,perform,performance,20,"Hello! I've found a performance issue in deepvariant/data_providers.py: `batch()` should be called before `map()`, which could make your program more efficient. Here is [the tensorflow document](https://tensorflow.google.cn/guide/data_performance?hl=zh_cn#vectorized_mapping) to support it. Detailed description is listed below:. - deepvariant/data_providers.py: `dataset.batch(batch_size=batch_size, drop_remainder=True)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L316) should be called before `dataset.map(map_func=self.parse_tfexample, num_parallel_calls=tf.data.AUTOTUNE)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L314).; - deepvariant/data_providers.py: `dataset.batch(batch_size=batch_size)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L364) should be called before `dataset.map(map_func=self.parse_tfexample, num_parallel_calls=tf.data.AUTOTUNE)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L362). Besides, you need to check the function called in `map()`(e.g., `self.parse_tfexample` called in `dataset.map()`) whether to be affected or not to make the changed code work properly. For example, if `self.parse_tfexample` needs data with shape (x, y, z) as its input before fix, it would require data with shape (batch_size, x, y, z). Looking forward to your reply. Btw, I am very glad to create a PR to fix it if you are too busy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/479:224,Usability,guid,guide,224,"Hello! I've found a performance issue in deepvariant/data_providers.py: `batch()` should be called before `map()`, which could make your program more efficient. Here is [the tensorflow document](https://tensorflow.google.cn/guide/data_performance?hl=zh_cn#vectorized_mapping) to support it. Detailed description is listed below:. - deepvariant/data_providers.py: `dataset.batch(batch_size=batch_size, drop_remainder=True)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L316) should be called before `dataset.map(map_func=self.parse_tfexample, num_parallel_calls=tf.data.AUTOTUNE)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L314).; - deepvariant/data_providers.py: `dataset.batch(batch_size=batch_size)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L364) should be called before `dataset.map(map_func=self.parse_tfexample, num_parallel_calls=tf.data.AUTOTUNE)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L362). Besides, you need to check the function called in `map()`(e.g., `self.parse_tfexample` called in `dataset.map()`) whether to be affected or not to make the changed code work properly. For example, if `self.parse_tfexample` needs data with shape (x, y, z) as its input before fix, it would require data with shape (batch_size, x, y, z). Looking forward to your reply. Btw, I am very glad to create a PR to fix it if you are too busy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/479
https://github.com/google/deepvariant/issues/480:437,Availability,Error,Error,437,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; can deepvariant detect multiallelic positions, for example, Ref is A, and Alt is C, G. And the GT is denoted as 1/2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/480
https://github.com/google/deepvariant/issues/480:232,Deployability,Install,Installation,232,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; can deepvariant detect multiallelic positions, for example, Ref is A, and Alt is C, G. And the GT is denoted as 1/2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/480
https://github.com/google/deepvariant/issues/480:731,Safety,detect,detect,731,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; can deepvariant detect multiallelic positions, for example, Ref is A, and Alt is C, G. And the GT is denoted as 1/2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/480
https://github.com/google/deepvariant/issues/480:490,Testability,test,test,490,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; can deepvariant detect multiallelic positions, for example, Ref is A, and Alt is C, G. And the GT is denoted as 1/2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/480
https://github.com/google/deepvariant/issues/480:526,Testability,test,test,526,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; can deepvariant detect multiallelic positions, for example, Ref is A, and Alt is C, G. And the GT is denoted as 1/2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/480
https://github.com/google/deepvariant/issues/480:120,Usability,clear,clear,120,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; can deepvariant detect multiallelic positions, for example, Ref is A, and Alt is C, G. And the GT is denoted as 1/2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/480
https://github.com/google/deepvariant/issues/482:8,Availability,error,error,8,"Getting error while running deepvariant: ERROR[12893] error waiting for container: unexpected EOF. Command: ; docker run -v ""/home/input"":""/input"" -v ""/home/output/"":""/output"" gcr.io/deepvariant-docker/deepvariant:1.2.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/hg19.fa --reads=/input/sorted_printReads_bwa.bam --output_vcf=/output/ERR194147_output.vcf.gz --num_shards=2 --intermediate_results_dir=/output/ --output_gvcf=/output/ERR194147_output.g.vcf.gz",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:41,Availability,ERROR,ERROR,41,"Getting error while running deepvariant: ERROR[12893] error waiting for container: unexpected EOF. Command: ; docker run -v ""/home/input"":""/input"" -v ""/home/output/"":""/output"" gcr.io/deepvariant-docker/deepvariant:1.2.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/hg19.fa --reads=/input/sorted_printReads_bwa.bam --output_vcf=/output/ERR194147_output.vcf.gz --num_shards=2 --intermediate_results_dir=/output/ --output_gvcf=/output/ERR194147_output.g.vcf.gz",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/482:54,Availability,error,error,54,"Getting error while running deepvariant: ERROR[12893] error waiting for container: unexpected EOF. Command: ; docker run -v ""/home/input"":""/input"" -v ""/home/output/"":""/output"" gcr.io/deepvariant-docker/deepvariant:1.2.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/hg19.fa --reads=/input/sorted_printReads_bwa.bam --output_vcf=/output/ERR194147_output.vcf.gz --num_shards=2 --intermediate_results_dir=/output/ --output_gvcf=/output/ERR194147_output.g.vcf.gz",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/482
https://github.com/google/deepvariant/issues/483:219,Availability,error,error,219,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: yes. **Describe the issue:**; I have tried to run in my personal computer the WES deepvariant case. However I get the following error:; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@16.gz --gvcf /output/inter_res/gvcf.tfrecord@16.gz --regions /input/wes2_38_3col.sorted.bed --task 2. I have ran the following command with a successful docker installation:; 	BIN_VERSION=""1.2.0"". 	sudo docker run \; 	-v ""${PWD}/input"":""/input"" \; 	-v ""${PWD}/output"":""/output"" \; 	-v ""${PWD}/reference"":""/reference"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type WES \; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta \; 	--reads /input/wes_deepvarfast_38.sorted.bam \; 	--regions /input/wes2_38_3col.sorted.bed \; 	--output_vcf /output/output_38.vcf.gz \; 	--output_gvcf /output/output_38.g.vcf.gz \; 	--num_shards=8 \; 	--intermediate_results_dir /output/intermediate_results_dir; with bam and bed files I've created of my own sample (paired end sequencing result of a human genome). The alignment of the bam file was successful (used bwa and samtools) and created the bed file out of the bam file by bedtools. . I've further checked FAQ and tried to run the following command, to better understand what is the error or where it fails:; 	BIN_VERSION=""1.2.0"". 	sudo docker run; 	-v ""${PWD}/input"":""/input""; 	-v ""${PWD}/output"":""/output""; 	-v ""${PWD}/reference"":""/reference""; 	google/deepvariant:""${BIN_VERSION}""; 	/opt/deepvariant/bin/make_examples; 	--mode calling; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta; 	--reads /input/wes_deepvarfast_38.sorted.bam; 	--examples ""/output/make_examples.tfrecord@1.gz""; 	--gvcf ""/output/gvcf.tfrecord@1.gz""; 	--regions ""/input/wes2_38_3col.sorted",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:1520,Availability,error,error,1520,"ecord@16.gz --regions /input/wes2_38_3col.sorted.bed --task 2. I have ran the following command with a successful docker installation:; 	BIN_VERSION=""1.2.0"". 	sudo docker run \; 	-v ""${PWD}/input"":""/input"" \; 	-v ""${PWD}/output"":""/output"" \; 	-v ""${PWD}/reference"":""/reference"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type WES \; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta \; 	--reads /input/wes_deepvarfast_38.sorted.bam \; 	--regions /input/wes2_38_3col.sorted.bed \; 	--output_vcf /output/output_38.vcf.gz \; 	--output_gvcf /output/output_38.g.vcf.gz \; 	--num_shards=8 \; 	--intermediate_results_dir /output/intermediate_results_dir; with bam and bed files I've created of my own sample (paired end sequencing result of a human genome). The alignment of the bam file was successful (used bwa and samtools) and created the bed file out of the bam file by bedtools. . I've further checked FAQ and tried to run the following command, to better understand what is the error or where it fails:; 	BIN_VERSION=""1.2.0"". 	sudo docker run; 	-v ""${PWD}/input"":""/input""; 	-v ""${PWD}/output"":""/output""; 	-v ""${PWD}/reference"":""/reference""; 	google/deepvariant:""${BIN_VERSION}""; 	/opt/deepvariant/bin/make_examples; 	--mode calling; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta; 	--reads /input/wes_deepvarfast_38.sorted.bam; 	--examples ""/output/make_examples.tfrecord@1.gz""; 	--gvcf ""/output/gvcf.tfrecord@1.gz""; 	--regions ""/input/wes2_38_3col.sorted.bed"" \. However I get no error message, some lines of this kind are printed: ""Adding interval chr1:1523790-1523940 to intervaltree"" and than it finishes without creating any files. Any Idea of what happens and how can I make deepvariant work on my sample and create a vcf file?. (**Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2027,Availability,error,error,2027,"/input/wes2_38_3col.sorted.bed --task 2. I have ran the following command with a successful docker installation:; 	BIN_VERSION=""1.2.0"". 	sudo docker run \; 	-v ""${PWD}/input"":""/input"" \; 	-v ""${PWD}/output"":""/output"" \; 	-v ""${PWD}/reference"":""/reference"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type WES \; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta \; 	--reads /input/wes_deepvarfast_38.sorted.bam \; 	--regions /input/wes2_38_3col.sorted.bed \; 	--output_vcf /output/output_38.vcf.gz \; 	--output_gvcf /output/output_38.g.vcf.gz \; 	--num_shards=8 \; 	--intermediate_results_dir /output/intermediate_results_dir; with bam and bed files I've created of my own sample (paired end sequencing result of a human genome). The alignment of the bam file was successful (used bwa and samtools) and created the bed file out of the bam file by bedtools. . I've further checked FAQ and tried to run the following command, to better understand what is the error or where it fails:; 	BIN_VERSION=""1.2.0"". 	sudo docker run; 	-v ""${PWD}/input"":""/input""; 	-v ""${PWD}/output"":""/output""; 	-v ""${PWD}/reference"":""/reference""; 	google/deepvariant:""${BIN_VERSION}""; 	/opt/deepvariant/bin/make_examples; 	--mode calling; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta; 	--reads /input/wes_deepvarfast_38.sorted.bam; 	--examples ""/output/make_examples.tfrecord@1.gz""; 	--gvcf ""/output/gvcf.tfrecord@1.gz""; 	--regions ""/input/wes2_38_3col.sorted.bed"" \. However I get no error message, some lines of this kind are printed: ""Adding interval chr1:1523790-1523940 to intervaltree"" and than it finishes without creating any files. Any Idea of what happens and how can I make deepvariant work on my sample and create a vcf file?. (**Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes it works)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:612,Deployability,install,installation,612,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: yes. **Describe the issue:**; I have tried to run in my personal computer the WES deepvariant case. However I get the following error:; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@16.gz --gvcf /output/inter_res/gvcf.tfrecord@16.gz --regions /input/wes2_38_3col.sorted.bed --task 2. I have ran the following command with a successful docker installation:; 	BIN_VERSION=""1.2.0"". 	sudo docker run \; 	-v ""${PWD}/input"":""/input"" \; 	-v ""${PWD}/output"":""/output"" \; 	-v ""${PWD}/reference"":""/reference"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type WES \; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta \; 	--reads /input/wes_deepvarfast_38.sorted.bam \; 	--regions /input/wes2_38_3col.sorted.bed \; 	--output_vcf /output/output_38.vcf.gz \; 	--output_gvcf /output/output_38.g.vcf.gz \; 	--num_shards=8 \; 	--intermediate_results_dir /output/intermediate_results_dir; with bam and bed files I've created of my own sample (paired end sequencing result of a human genome). The alignment of the bam file was successful (used bwa and samtools) and created the bed file out of the bam file by bedtools. . I've further checked FAQ and tried to run the following command, to better understand what is the error or where it fails:; 	BIN_VERSION=""1.2.0"". 	sudo docker run; 	-v ""${PWD}/input"":""/input""; 	-v ""${PWD}/output"":""/output""; 	-v ""${PWD}/reference"":""/reference""; 	google/deepvariant:""${BIN_VERSION}""; 	/opt/deepvariant/bin/make_examples; 	--mode calling; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta; 	--reads /input/wes_deepvarfast_38.sorted.bam; 	--examples ""/output/make_examples.tfrecord@1.gz""; 	--gvcf ""/output/gvcf.tfrecord@1.gz""; 	--regions ""/input/wes2_38_3col.sorted",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2033,Integrability,message,message,2033,"/input/wes2_38_3col.sorted.bed --task 2. I have ran the following command with a successful docker installation:; 	BIN_VERSION=""1.2.0"". 	sudo docker run \; 	-v ""${PWD}/input"":""/input"" \; 	-v ""${PWD}/output"":""/output"" \; 	-v ""${PWD}/reference"":""/reference"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type WES \; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta \; 	--reads /input/wes_deepvarfast_38.sorted.bam \; 	--regions /input/wes2_38_3col.sorted.bed \; 	--output_vcf /output/output_38.vcf.gz \; 	--output_gvcf /output/output_38.g.vcf.gz \; 	--num_shards=8 \; 	--intermediate_results_dir /output/intermediate_results_dir; with bam and bed files I've created of my own sample (paired end sequencing result of a human genome). The alignment of the bam file was successful (used bwa and samtools) and created the bed file out of the bam file by bedtools. . I've further checked FAQ and tried to run the following command, to better understand what is the error or where it fails:; 	BIN_VERSION=""1.2.0"". 	sudo docker run; 	-v ""${PWD}/input"":""/input""; 	-v ""${PWD}/output"":""/output""; 	-v ""${PWD}/reference"":""/reference""; 	google/deepvariant:""${BIN_VERSION}""; 	/opt/deepvariant/bin/make_examples; 	--mode calling; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta; 	--reads /input/wes_deepvarfast_38.sorted.bam; 	--examples ""/output/make_examples.tfrecord@1.gz""; 	--gvcf ""/output/gvcf.tfrecord@1.gz""; 	--regions ""/input/wes2_38_3col.sorted.bed"" \. However I get no error message, some lines of this kind are printed: ""Adding interval chr1:1523790-1523940 to intervaltree"" and than it finishes without creating any files. Any Idea of what happens and how can I make deepvariant work on my sample and create a vcf file?. (**Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes it works)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2305,Testability,test,test,2305,"/input/wes2_38_3col.sorted.bed --task 2. I have ran the following command with a successful docker installation:; 	BIN_VERSION=""1.2.0"". 	sudo docker run \; 	-v ""${PWD}/input"":""/input"" \; 	-v ""${PWD}/output"":""/output"" \; 	-v ""${PWD}/reference"":""/reference"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type WES \; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta \; 	--reads /input/wes_deepvarfast_38.sorted.bam \; 	--regions /input/wes2_38_3col.sorted.bed \; 	--output_vcf /output/output_38.vcf.gz \; 	--output_gvcf /output/output_38.g.vcf.gz \; 	--num_shards=8 \; 	--intermediate_results_dir /output/intermediate_results_dir; with bam and bed files I've created of my own sample (paired end sequencing result of a human genome). The alignment of the bam file was successful (used bwa and samtools) and created the bed file out of the bam file by bedtools. . I've further checked FAQ and tried to run the following command, to better understand what is the error or where it fails:; 	BIN_VERSION=""1.2.0"". 	sudo docker run; 	-v ""${PWD}/input"":""/input""; 	-v ""${PWD}/output"":""/output""; 	-v ""${PWD}/reference"":""/reference""; 	google/deepvariant:""${BIN_VERSION}""; 	/opt/deepvariant/bin/make_examples; 	--mode calling; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta; 	--reads /input/wes_deepvarfast_38.sorted.bam; 	--examples ""/output/make_examples.tfrecord@1.gz""; 	--gvcf ""/output/gvcf.tfrecord@1.gz""; 	--regions ""/input/wes2_38_3col.sorted.bed"" \. However I get no error message, some lines of this kind are printed: ""Adding interval chr1:1523790-1523940 to intervaltree"" and than it finishes without creating any files. Any Idea of what happens and how can I make deepvariant work on my sample and create a vcf file?. (**Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes it works)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/483:2341,Testability,test,test,2341,"/input/wes2_38_3col.sorted.bed --task 2. I have ran the following command with a successful docker installation:; 	BIN_VERSION=""1.2.0"". 	sudo docker run \; 	-v ""${PWD}/input"":""/input"" \; 	-v ""${PWD}/output"":""/output"" \; 	-v ""${PWD}/reference"":""/reference"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type WES \; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta \; 	--reads /input/wes_deepvarfast_38.sorted.bam \; 	--regions /input/wes2_38_3col.sorted.bed \; 	--output_vcf /output/output_38.vcf.gz \; 	--output_gvcf /output/output_38.g.vcf.gz \; 	--num_shards=8 \; 	--intermediate_results_dir /output/intermediate_results_dir; with bam and bed files I've created of my own sample (paired end sequencing result of a human genome). The alignment of the bam file was successful (used bwa and samtools) and created the bed file out of the bam file by bedtools. . I've further checked FAQ and tried to run the following command, to better understand what is the error or where it fails:; 	BIN_VERSION=""1.2.0"". 	sudo docker run; 	-v ""${PWD}/input"":""/input""; 	-v ""${PWD}/output"":""/output""; 	-v ""${PWD}/reference"":""/reference""; 	google/deepvariant:""${BIN_VERSION}""; 	/opt/deepvariant/bin/make_examples; 	--mode calling; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta; 	--reads /input/wes_deepvarfast_38.sorted.bam; 	--examples ""/output/make_examples.tfrecord@1.gz""; 	--gvcf ""/output/gvcf.tfrecord@1.gz""; 	--regions ""/input/wes2_38_3col.sorted.bed"" \. However I get no error message, some lines of this kind are printed: ""Adding interval chr1:1523790-1523940 to intervaltree"" and than it finishes without creating any files. Any Idea of what happens and how can I make deepvariant work on my sample and create a vcf file?. (**Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes it works)",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/483
https://github.com/google/deepvariant/issues/484:153,Deployability,pipeline,pipeline,153,"Hi!. Is there a way to emit all sites in given regions (using a bed file) in the final VCF even if they are the same as reference? I want to use it in a pipeline in which any position not in the input VCF is assumed to be a ""no call"". Missing positions will not be interpreted as reference.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/484
https://github.com/google/deepvariant/issues/485:1062,Availability,Error,Error,1062,"epvariant/blob/r1.2/docs/FAQ.md**: **YES**. **Describe the issue:**. Manually selected regions (a single region is formed by a locus extending 500 bp to both sides) were used in my project to make examples, and it was also succeed in calling variants. However, when I running postprocess_variants, something went wrong. I check the log, and I guess it was related to the wrong ""call_variant_outputs"". So I printed one ""call_variant_outputs"" out of the whole tfrecord, and found out there are several repeated variant in one call. Where did I go wrong?. **The log file is attached.**; [postprocess_variants.log](https://github.com/google/deepvariant/files/7149887/postprocess_variants.log). **Setup**; - Operating system: ubuntu **16**; - DeepVariant version: **0.7.0**; - Installation method (Docker, built from source, etc.): **built from source**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **NO**. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). > W0912 23:51:01.891268 140429229119232 postprocess_variants.py:331] Alt allele indices found from call_variants_outputs for variant reference_bases: ""C""; alternate_bases: ""A""; calls {; info {; key: ""AD""; value {; values {; int_value: 17; }; values {; int_value: 4; }; }; }; info {; key: ""DP""; value {; values {; int_value: 21; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 0.190476190476; }; }; }; genotype: -1; genotype: -1; call_set_name: ""XY406-1""; }; end: 10147; reference_name: ""1""; start: 10146; is [[0], [0], [0]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 874, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:828,Deployability,Install,Installation,828,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: **YES**. **Describe the issue:**. Manually selected regions (a single region is formed by a locus extending 500 bp to both sides) were used in my project to make examples, and it was also succeed in calling variants. However, when I running postprocess_variants, something went wrong. I check the log, and I guess it was related to the wrong ""call_variant_outputs"". So I printed one ""call_variant_outputs"" out of the whole tfrecord, and found out there are several repeated variant in one call. Where did I go wrong?. **The log file is attached.**; [postprocess_variants.log](https://github.com/google/deepvariant/files/7149887/postprocess_variants.log). **Setup**; - Operating system: ubuntu **16**; - DeepVariant version: **0.7.0**; - Installation method (Docker, built from source, etc.): **built from source**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **NO**. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). > W0912 23:51:01.891268 140429229119232 postprocess_variants.py:331] Alt allele indices found from call_variants_outputs for variant reference_bases: ""C""; alternate_bases: ""A""; calls {; info {; key: ""AD""; value {; values {; int_value: 17; }; values {; int_value: 4; }; }; }; info {; key: ""DP""; value {; values {; int_value: 21; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 0.190476190476; }; }; }; genotype: -1; genotype: -1; call_set_name: ""XY406-1""; }; end: 10147; reference_name: ""1""; start: 10146; is [[0], [0], [0]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 874, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:189,Modifiability,extend,extending,189,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: **YES**. **Describe the issue:**. Manually selected regions (a single region is formed by a locus extending 500 bp to both sides) were used in my project to make examples, and it was also succeed in calling variants. However, when I running postprocess_variants, something went wrong. I check the log, and I guess it was related to the wrong ""call_variant_outputs"". So I printed one ""call_variant_outputs"" out of the whole tfrecord, and found out there are several repeated variant in one call. Where did I go wrong?. **The log file is attached.**; [postprocess_variants.log](https://github.com/google/deepvariant/files/7149887/postprocess_variants.log). **Setup**; - Operating system: ubuntu **16**; - DeepVariant version: **0.7.0**; - Installation method (Docker, built from source, etc.): **built from source**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **NO**. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). > W0912 23:51:01.891268 140429229119232 postprocess_variants.py:331] Alt allele indices found from call_variants_outputs for variant reference_bases: ""C""; alternate_bases: ""A""; calls {; info {; key: ""AD""; value {; values {; int_value: 17; }; values {; int_value: 4; }; }; }; info {; key: ""DP""; value {; values {; int_value: 21; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 0.190476190476; }; }; }; genotype: -1; genotype: -1; call_set_name: ""XY406-1""; }; end: 10147; reference_name: ""1""; start: 10146; is [[0], [0], [0]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 874, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:3048,Safety,sanity check,sanity check,3048,"nd from call_variants_outputs for variant reference_bases: ""C""; alternate_bases: ""A""; calls {; info {; key: ""AD""; value {; values {; int_value: 17; }; values {; int_value: 4; }; }; }; info {; key: ""DP""; value {; values {; int_value: 21; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 0.190476190476; }; }; }; genotype: -1; genotype: -1; call_set_name: ""XY406-1""; }; end: 10147; reference_name: ""1""; start: 10146; is [[0], [0], [0]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 874, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 851, in main; header=header); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 595, in write_variants_to_vcf; for variant in variant_generator:; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 110, in _group_overlapping_variants; for variant in sorted_variants:; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 631, in _transform_call_variants_output_to_variants; outputs, multi_allelic_qual_filter); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 559, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. **Does the quick start test work on your system?**; YES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:3114,Safety,sanity check,sanity check,3114,"nd from call_variants_outputs for variant reference_bases: ""C""; alternate_bases: ""A""; calls {; info {; key: ""AD""; value {; values {; int_value: 17; }; values {; int_value: 4; }; }; }; info {; key: ""DP""; value {; values {; int_value: 21; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 0.190476190476; }; }; }; genotype: -1; genotype: -1; call_set_name: ""XY406-1""; }; end: 10147; reference_name: ""1""; start: 10146; is [[0], [0], [0]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 874, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 851, in main; header=header); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 595, in write_variants_to_vcf; for variant in variant_generator:; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 110, in _group_overlapping_variants; for variant in sorted_variants:; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 631, in _transform_call_variants_output_to_variants; outputs, multi_allelic_qual_filter); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 559, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. **Does the quick start test work on your system?**; YES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:388,Testability,log,log,388,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: **YES**. **Describe the issue:**. Manually selected regions (a single region is formed by a locus extending 500 bp to both sides) were used in my project to make examples, and it was also succeed in calling variants. However, when I running postprocess_variants, something went wrong. I check the log, and I guess it was related to the wrong ""call_variant_outputs"". So I printed one ""call_variant_outputs"" out of the whole tfrecord, and found out there are several repeated variant in one call. Where did I go wrong?. **The log file is attached.**; [postprocess_variants.log](https://github.com/google/deepvariant/files/7149887/postprocess_variants.log). **Setup**; - Operating system: ubuntu **16**; - DeepVariant version: **0.7.0**; - Installation method (Docker, built from source, etc.): **built from source**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **NO**. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). > W0912 23:51:01.891268 140429229119232 postprocess_variants.py:331] Alt allele indices found from call_variants_outputs for variant reference_bases: ""C""; alternate_bases: ""A""; calls {; info {; key: ""AD""; value {; values {; int_value: 17; }; values {; int_value: 4; }; }; }; info {; key: ""DP""; value {; values {; int_value: 21; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 0.190476190476; }; }; }; genotype: -1; genotype: -1; call_set_name: ""XY406-1""; }; end: 10147; reference_name: ""1""; start: 10146; is [[0], [0], [0]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 874, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:615,Testability,log,log,615,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: **YES**. **Describe the issue:**. Manually selected regions (a single region is formed by a locus extending 500 bp to both sides) were used in my project to make examples, and it was also succeed in calling variants. However, when I running postprocess_variants, something went wrong. I check the log, and I guess it was related to the wrong ""call_variant_outputs"". So I printed one ""call_variant_outputs"" out of the whole tfrecord, and found out there are several repeated variant in one call. Where did I go wrong?. **The log file is attached.**; [postprocess_variants.log](https://github.com/google/deepvariant/files/7149887/postprocess_variants.log). **Setup**; - Operating system: ubuntu **16**; - DeepVariant version: **0.7.0**; - Installation method (Docker, built from source, etc.): **built from source**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **NO**. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). > W0912 23:51:01.891268 140429229119232 postprocess_variants.py:331] Alt allele indices found from call_variants_outputs for variant reference_bases: ""C""; alternate_bases: ""A""; calls {; info {; key: ""AD""; value {; values {; int_value: 17; }; values {; int_value: 4; }; }; }; info {; key: ""DP""; value {; values {; int_value: 21; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 0.190476190476; }; }; }; genotype: -1; genotype: -1; call_set_name: ""XY406-1""; }; end: 10147; reference_name: ""1""; start: 10146; is [[0], [0], [0]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 874, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:662,Testability,log,log,662,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: **YES**. **Describe the issue:**. Manually selected regions (a single region is formed by a locus extending 500 bp to both sides) were used in my project to make examples, and it was also succeed in calling variants. However, when I running postprocess_variants, something went wrong. I check the log, and I guess it was related to the wrong ""call_variant_outputs"". So I printed one ""call_variant_outputs"" out of the whole tfrecord, and found out there are several repeated variant in one call. Where did I go wrong?. **The log file is attached.**; [postprocess_variants.log](https://github.com/google/deepvariant/files/7149887/postprocess_variants.log). **Setup**; - Operating system: ubuntu **16**; - DeepVariant version: **0.7.0**; - Installation method (Docker, built from source, etc.): **built from source**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **NO**. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). > W0912 23:51:01.891268 140429229119232 postprocess_variants.py:331] Alt allele indices found from call_variants_outputs for variant reference_bases: ""C""; alternate_bases: ""A""; calls {; info {; key: ""AD""; value {; values {; int_value: 17; }; values {; int_value: 4; }; }; }; info {; key: ""DP""; value {; values {; int_value: 21; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 0.190476190476; }; }; }; genotype: -1; genotype: -1; call_set_name: ""XY406-1""; }; end: 10147; reference_name: ""1""; start: 10146; is [[0], [0], [0]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 874, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:740,Testability,log,log,740,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: **YES**. **Describe the issue:**. Manually selected regions (a single region is formed by a locus extending 500 bp to both sides) were used in my project to make examples, and it was also succeed in calling variants. However, when I running postprocess_variants, something went wrong. I check the log, and I guess it was related to the wrong ""call_variant_outputs"". So I printed one ""call_variant_outputs"" out of the whole tfrecord, and found out there are several repeated variant in one call. Where did I go wrong?. **The log file is attached.**; [postprocess_variants.log](https://github.com/google/deepvariant/files/7149887/postprocess_variants.log). **Setup**; - Operating system: ubuntu **16**; - DeepVariant version: **0.7.0**; - Installation method (Docker, built from source, etc.): **built from source**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **NO**. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). > W0912 23:51:01.891268 140429229119232 postprocess_variants.py:331] Alt allele indices found from call_variants_outputs for variant reference_bases: ""C""; alternate_bases: ""A""; calls {; info {; key: ""AD""; value {; values {; int_value: 17; }; values {; int_value: 4; }; }; }; info {; key: ""DP""; value {; values {; int_value: 21; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 0.190476190476; }; }; }; genotype: -1; genotype: -1; call_set_name: ""XY406-1""; }; end: 10147; reference_name: ""1""; start: 10146; is [[0], [0], [0]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 874, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/485:3151,Testability,test,test,3151,"nd from call_variants_outputs for variant reference_bases: ""C""; alternate_bases: ""A""; calls {; info {; key: ""AD""; value {; values {; int_value: 17; }; values {; int_value: 4; }; }; }; info {; key: ""DP""; value {; values {; int_value: 21; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 0.190476190476; }; }; }; genotype: -1; genotype: -1; call_set_name: ""XY406-1""; }; end: 10147; reference_name: ""1""; start: 10146; is [[0], [0], [0]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 874, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 851, in main; header=header); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 595, in write_variants_to_vcf; for variant in variant_generator:; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 110, in _group_overlapping_variants; for variant in sorted_variants:; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 631, in _transform_call_variants_output_to_variants; outputs, multi_allelic_qual_filter); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 559, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. **Does the quick start test work on your system?**; YES",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/485
https://github.com/google/deepvariant/issues/486:186,Availability,down,downstream,186,"Hello,. We noticed that adjacent variants of the same haplotype (i.e. MNVs) are being called as separate variants in the DeepVariant and DeepTrio outputs with VCF and gVCF files. During downstream processing these MNVs are then treated as two individual SNVs at two different loci, leading to faulty assessments. . For example two variants for a site of interest (reference TCG -> Serine) were separated between two lines in the DeepVariant/DeepTrio output VCF and then categorised as containing a nonsynonymous (T**G**G -> Tryptophan) and synonymous mutation (TC**A** -> Serine). Whereas the correct and desired way to handle this, at least for us but I imagine others too, would seem to be to recognise both mutations on a single line in the VCF as a combined substitution, which could then be identified as resulting in a stopgain (T**GA** -> Nonsense mutation). Are there plans to support these MNV calls in the DeepVariant/DeepTrio outputs? Or alternatively are there any current post-processing approaches that you may be using and can recommend to handle these cases? Can understand these may be challenging to manage in some aspects but could be important to flag given some recent literature around this topic. For reference this was using hg38 with WGS. We initially identified this using the original DeepTrio release (docker image deeptrio:1.0.1rc), but then also using the most recent DeepVariant release (via docker, v1.2). We also tested this to see whether the change to the unfiltered GLnexus config could be contributing to this for processing of DeepTrio gVCFs due to the joint genotyping parameter, but reverting back to the WGS config did not result in a merged MNV in this instance. . Many thanks,; Macabe.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:293,Availability,fault,faulty,293,"Hello,. We noticed that adjacent variants of the same haplotype (i.e. MNVs) are being called as separate variants in the DeepVariant and DeepTrio outputs with VCF and gVCF files. During downstream processing these MNVs are then treated as two individual SNVs at two different loci, leading to faulty assessments. . For example two variants for a site of interest (reference TCG -> Serine) were separated between two lines in the DeepVariant/DeepTrio output VCF and then categorised as containing a nonsynonymous (T**G**G -> Tryptophan) and synonymous mutation (TC**A** -> Serine). Whereas the correct and desired way to handle this, at least for us but I imagine others too, would seem to be to recognise both mutations on a single line in the VCF as a combined substitution, which could then be identified as resulting in a stopgain (T**GA** -> Nonsense mutation). Are there plans to support these MNV calls in the DeepVariant/DeepTrio outputs? Or alternatively are there any current post-processing approaches that you may be using and can recommend to handle these cases? Can understand these may be challenging to manage in some aspects but could be important to flag given some recent literature around this topic. For reference this was using hg38 with WGS. We initially identified this using the original DeepTrio release (docker image deeptrio:1.0.1rc), but then also using the most recent DeepVariant release (via docker, v1.2). We also tested this to see whether the change to the unfiltered GLnexus config could be contributing to this for processing of DeepTrio gVCFs due to the joint genotyping parameter, but reverting back to the WGS config did not result in a merged MNV in this instance. . Many thanks,; Macabe.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:1321,Deployability,release,release,1321,"Hello,. We noticed that adjacent variants of the same haplotype (i.e. MNVs) are being called as separate variants in the DeepVariant and DeepTrio outputs with VCF and gVCF files. During downstream processing these MNVs are then treated as two individual SNVs at two different loci, leading to faulty assessments. . For example two variants for a site of interest (reference TCG -> Serine) were separated between two lines in the DeepVariant/DeepTrio output VCF and then categorised as containing a nonsynonymous (T**G**G -> Tryptophan) and synonymous mutation (TC**A** -> Serine). Whereas the correct and desired way to handle this, at least for us but I imagine others too, would seem to be to recognise both mutations on a single line in the VCF as a combined substitution, which could then be identified as resulting in a stopgain (T**GA** -> Nonsense mutation). Are there plans to support these MNV calls in the DeepVariant/DeepTrio outputs? Or alternatively are there any current post-processing approaches that you may be using and can recommend to handle these cases? Can understand these may be challenging to manage in some aspects but could be important to flag given some recent literature around this topic. For reference this was using hg38 with WGS. We initially identified this using the original DeepTrio release (docker image deeptrio:1.0.1rc), but then also using the most recent DeepVariant release (via docker, v1.2). We also tested this to see whether the change to the unfiltered GLnexus config could be contributing to this for processing of DeepTrio gVCFs due to the joint genotyping parameter, but reverting back to the WGS config did not result in a merged MNV in this instance. . Many thanks,; Macabe.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:1410,Deployability,release,release,1410,"Hello,. We noticed that adjacent variants of the same haplotype (i.e. MNVs) are being called as separate variants in the DeepVariant and DeepTrio outputs with VCF and gVCF files. During downstream processing these MNVs are then treated as two individual SNVs at two different loci, leading to faulty assessments. . For example two variants for a site of interest (reference TCG -> Serine) were separated between two lines in the DeepVariant/DeepTrio output VCF and then categorised as containing a nonsynonymous (T**G**G -> Tryptophan) and synonymous mutation (TC**A** -> Serine). Whereas the correct and desired way to handle this, at least for us but I imagine others too, would seem to be to recognise both mutations on a single line in the VCF as a combined substitution, which could then be identified as resulting in a stopgain (T**GA** -> Nonsense mutation). Are there plans to support these MNV calls in the DeepVariant/DeepTrio outputs? Or alternatively are there any current post-processing approaches that you may be using and can recommend to handle these cases? Can understand these may be challenging to manage in some aspects but could be important to flag given some recent literature around this topic. For reference this was using hg38 with WGS. We initially identified this using the original DeepTrio release (docker image deeptrio:1.0.1rc), but then also using the most recent DeepVariant release (via docker, v1.2). We also tested this to see whether the change to the unfiltered GLnexus config could be contributing to this for processing of DeepTrio gVCFs due to the joint genotyping parameter, but reverting back to the WGS config did not result in a merged MNV in this instance. . Many thanks,; Macabe.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:1510,Modifiability,config,config,1510,"Hello,. We noticed that adjacent variants of the same haplotype (i.e. MNVs) are being called as separate variants in the DeepVariant and DeepTrio outputs with VCF and gVCF files. During downstream processing these MNVs are then treated as two individual SNVs at two different loci, leading to faulty assessments. . For example two variants for a site of interest (reference TCG -> Serine) were separated between two lines in the DeepVariant/DeepTrio output VCF and then categorised as containing a nonsynonymous (T**G**G -> Tryptophan) and synonymous mutation (TC**A** -> Serine). Whereas the correct and desired way to handle this, at least for us but I imagine others too, would seem to be to recognise both mutations on a single line in the VCF as a combined substitution, which could then be identified as resulting in a stopgain (T**GA** -> Nonsense mutation). Are there plans to support these MNV calls in the DeepVariant/DeepTrio outputs? Or alternatively are there any current post-processing approaches that you may be using and can recommend to handle these cases? Can understand these may be challenging to manage in some aspects but could be important to flag given some recent literature around this topic. For reference this was using hg38 with WGS. We initially identified this using the original DeepTrio release (docker image deeptrio:1.0.1rc), but then also using the most recent DeepVariant release (via docker, v1.2). We also tested this to see whether the change to the unfiltered GLnexus config could be contributing to this for processing of DeepTrio gVCFs due to the joint genotyping parameter, but reverting back to the WGS config did not result in a merged MNV in this instance. . Many thanks,; Macabe.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:1649,Modifiability,config,config,1649,"Hello,. We noticed that adjacent variants of the same haplotype (i.e. MNVs) are being called as separate variants in the DeepVariant and DeepTrio outputs with VCF and gVCF files. During downstream processing these MNVs are then treated as two individual SNVs at two different loci, leading to faulty assessments. . For example two variants for a site of interest (reference TCG -> Serine) were separated between two lines in the DeepVariant/DeepTrio output VCF and then categorised as containing a nonsynonymous (T**G**G -> Tryptophan) and synonymous mutation (TC**A** -> Serine). Whereas the correct and desired way to handle this, at least for us but I imagine others too, would seem to be to recognise both mutations on a single line in the VCF as a combined substitution, which could then be identified as resulting in a stopgain (T**GA** -> Nonsense mutation). Are there plans to support these MNV calls in the DeepVariant/DeepTrio outputs? Or alternatively are there any current post-processing approaches that you may be using and can recommend to handle these cases? Can understand these may be challenging to manage in some aspects but could be important to flag given some recent literature around this topic. For reference this was using hg38 with WGS. We initially identified this using the original DeepTrio release (docker image deeptrio:1.0.1rc), but then also using the most recent DeepVariant release (via docker, v1.2). We also tested this to see whether the change to the unfiltered GLnexus config could be contributing to this for processing of DeepTrio gVCFs due to the joint genotyping parameter, but reverting back to the WGS config did not result in a merged MNV in this instance. . Many thanks,; Macabe.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/486:1446,Testability,test,tested,1446,"Hello,. We noticed that adjacent variants of the same haplotype (i.e. MNVs) are being called as separate variants in the DeepVariant and DeepTrio outputs with VCF and gVCF files. During downstream processing these MNVs are then treated as two individual SNVs at two different loci, leading to faulty assessments. . For example two variants for a site of interest (reference TCG -> Serine) were separated between two lines in the DeepVariant/DeepTrio output VCF and then categorised as containing a nonsynonymous (T**G**G -> Tryptophan) and synonymous mutation (TC**A** -> Serine). Whereas the correct and desired way to handle this, at least for us but I imagine others too, would seem to be to recognise both mutations on a single line in the VCF as a combined substitution, which could then be identified as resulting in a stopgain (T**GA** -> Nonsense mutation). Are there plans to support these MNV calls in the DeepVariant/DeepTrio outputs? Or alternatively are there any current post-processing approaches that you may be using and can recommend to handle these cases? Can understand these may be challenging to manage in some aspects but could be important to flag given some recent literature around this topic. For reference this was using hg38 with WGS. We initially identified this using the original DeepTrio release (docker image deeptrio:1.0.1rc), but then also using the most recent DeepVariant release (via docker, v1.2). We also tested this to see whether the change to the unfiltered GLnexus config could be contributing to this for processing of DeepTrio gVCFs due to the joint genotyping parameter, but reverting back to the WGS config did not result in a merged MNV in this instance. . Many thanks,; Macabe.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/486
https://github.com/google/deepvariant/issues/487:580,Availability,down,downstream,580,"Hello,. I want to suggest here to left-aligne indel in the DeepVariant output VCF / gVCF to avoid the issue described below. I'm running DeepVariant v1.1.0 on a set of samples sequenced with Illumina 2x150 paired-end reads. My workflow right now includes calling variant using DV and then merge individual gVCFs using GLnexus as described in your best-practices for multi-sample VCF. Inspecting the resulting cohort VCF I've noticed that the representation of indels in repetitive / homopolymer regions is not normalized to the leftmost position and this generates odd situations downstream. Essentially, the multi-sample VCF, would contain 2 different variants that, when left-aligned downstream using for example bcftools norm, become the same locus generating duplicated vars with different genotypes. I didn't notice this issue with recent versions of GATK so I suppose they left-align indels in the output VCF. See the example below:. These are 2 indel variants in my multi-sample VCF:; ```; chr3 105259621 chr3_105259621_T_TTA T TTA; chr3 105259623 chr3_105259623_A_ATA A ATA; ```; As you can see in the screenshot, the actual locus is a repetitive region with TA repeats, so the exact location of a TA insertion in the stretch can not be known.; ![image](https://user-images.githubusercontent.com/51458073/134807481-94191333-32bc-4249-ac11-b265711e435e.png). When I apply bcftools norm, it changes the second one to the leftmost position, making it identical to the first one (which is the expected behavior). So in the end I have 2 duplicated vars in my VCF, each with different genotypes: ; ```; chr3 105259621 chr3_105259621_T_TTA T TTA; chr3 105259621 chr3_105259623_A_ATA T TTA; ```; This situation creates troubles for downstream analysis and segregation, even if probably most of these variants can be discarded since they are likely artifacts.; The problem does not affect many single allele variants (just 51 out of 24054518 in my dataset), but it affects lot of the multi-allelic ones",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/487
https://github.com/google/deepvariant/issues/487:686,Availability,down,downstream,686,"Hello,. I want to suggest here to left-aligne indel in the DeepVariant output VCF / gVCF to avoid the issue described below. I'm running DeepVariant v1.1.0 on a set of samples sequenced with Illumina 2x150 paired-end reads. My workflow right now includes calling variant using DV and then merge individual gVCFs using GLnexus as described in your best-practices for multi-sample VCF. Inspecting the resulting cohort VCF I've noticed that the representation of indels in repetitive / homopolymer regions is not normalized to the leftmost position and this generates odd situations downstream. Essentially, the multi-sample VCF, would contain 2 different variants that, when left-aligned downstream using for example bcftools norm, become the same locus generating duplicated vars with different genotypes. I didn't notice this issue with recent versions of GATK so I suppose they left-align indels in the output VCF. See the example below:. These are 2 indel variants in my multi-sample VCF:; ```; chr3 105259621 chr3_105259621_T_TTA T TTA; chr3 105259623 chr3_105259623_A_ATA A ATA; ```; As you can see in the screenshot, the actual locus is a repetitive region with TA repeats, so the exact location of a TA insertion in the stretch can not be known.; ![image](https://user-images.githubusercontent.com/51458073/134807481-94191333-32bc-4249-ac11-b265711e435e.png). When I apply bcftools norm, it changes the second one to the leftmost position, making it identical to the first one (which is the expected behavior). So in the end I have 2 duplicated vars in my VCF, each with different genotypes: ; ```; chr3 105259621 chr3_105259621_T_TTA T TTA; chr3 105259621 chr3_105259623_A_ATA T TTA; ```; This situation creates troubles for downstream analysis and segregation, even if probably most of these variants can be discarded since they are likely artifacts.; The problem does not affect many single allele variants (just 51 out of 24054518 in my dataset), but it affects lot of the multi-allelic ones",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/487
https://github.com/google/deepvariant/issues/487:1732,Availability,down,downstream,1732,"quenced with Illumina 2x150 paired-end reads. My workflow right now includes calling variant using DV and then merge individual gVCFs using GLnexus as described in your best-practices for multi-sample VCF. Inspecting the resulting cohort VCF I've noticed that the representation of indels in repetitive / homopolymer regions is not normalized to the leftmost position and this generates odd situations downstream. Essentially, the multi-sample VCF, would contain 2 different variants that, when left-aligned downstream using for example bcftools norm, become the same locus generating duplicated vars with different genotypes. I didn't notice this issue with recent versions of GATK so I suppose they left-align indels in the output VCF. See the example below:. These are 2 indel variants in my multi-sample VCF:; ```; chr3 105259621 chr3_105259621_T_TTA T TTA; chr3 105259623 chr3_105259623_A_ATA A ATA; ```; As you can see in the screenshot, the actual locus is a repetitive region with TA repeats, so the exact location of a TA insertion in the stretch can not be known.; ![image](https://user-images.githubusercontent.com/51458073/134807481-94191333-32bc-4249-ac11-b265711e435e.png). When I apply bcftools norm, it changes the second one to the leftmost position, making it identical to the first one (which is the expected behavior). So in the end I have 2 duplicated vars in my VCF, each with different genotypes: ; ```; chr3 105259621 chr3_105259621_T_TTA T TTA; chr3 105259621 chr3_105259623_A_ATA T TTA; ```; This situation creates troubles for downstream analysis and segregation, even if probably most of these variants can be discarded since they are likely artifacts.; The problem does not affect many single allele variants (just 51 out of 24054518 in my dataset), but it affects lot of the multi-allelic ones. If indels were leftaligned before output, this would solve the issue I think and likely many multi-allelic will become single-allele. Any plan for this in the future?. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/487
https://github.com/google/deepvariant/issues/487:92,Safety,avoid,avoid,92,"Hello,. I want to suggest here to left-aligne indel in the DeepVariant output VCF / gVCF to avoid the issue described below. I'm running DeepVariant v1.1.0 on a set of samples sequenced with Illumina 2x150 paired-end reads. My workflow right now includes calling variant using DV and then merge individual gVCFs using GLnexus as described in your best-practices for multi-sample VCF. Inspecting the resulting cohort VCF I've noticed that the representation of indels in repetitive / homopolymer regions is not normalized to the leftmost position and this generates odd situations downstream. Essentially, the multi-sample VCF, would contain 2 different variants that, when left-aligned downstream using for example bcftools norm, become the same locus generating duplicated vars with different genotypes. I didn't notice this issue with recent versions of GATK so I suppose they left-align indels in the output VCF. See the example below:. These are 2 indel variants in my multi-sample VCF:; ```; chr3 105259621 chr3_105259621_T_TTA T TTA; chr3 105259623 chr3_105259623_A_ATA A ATA; ```; As you can see in the screenshot, the actual locus is a repetitive region with TA repeats, so the exact location of a TA insertion in the stretch can not be known.; ![image](https://user-images.githubusercontent.com/51458073/134807481-94191333-32bc-4249-ac11-b265711e435e.png). When I apply bcftools norm, it changes the second one to the leftmost position, making it identical to the first one (which is the expected behavior). So in the end I have 2 duplicated vars in my VCF, each with different genotypes: ; ```; chr3 105259621 chr3_105259621_T_TTA T TTA; chr3 105259621 chr3_105259623_A_ATA T TTA; ```; This situation creates troubles for downstream analysis and segregation, even if probably most of these variants can be discarded since they are likely artifacts.; The problem does not affect many single allele variants (just 51 out of 24054518 in my dataset), but it affects lot of the multi-allelic ones",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/487
https://github.com/google/deepvariant/issues/488:840,Availability,error,error,840,"I ran DeepVariant twice based on ""https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md"". ; deepvariant1- whatshap phase- whatshap haplotag-deepvariant2; Now I also want to use DeepTrio.I used the haplotagged.bam(generated from whatshap haplotag) as input bam.When I ran DeepTrio,the output.vcf.gz was generated normally.However,the log file showed the following warning message:; ------------------------; I0926 14:26:35.659228 47028170803008 call_variants.py:336] Shape of input examples: [140, 221, 9]; W0926 14:26:35.665323 47028170803008 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An **error** will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; 2021-09-26 14:26:35.668419: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-09-26 14:26:35.669638: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2021-09-26 14:26:35.671197: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2600000000 Hz; WARNING:tensorflow:Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; W0926 14:26:35.690017 47028170803008 estimator.py:1846] Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; ------------------------; The version I used:; DeepVariant 1.1.0; glnexus v1.3.1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:409,Integrability,message,message,409,"I ran DeepVariant twice based on ""https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md"". ; deepvariant1- whatshap phase- whatshap haplotag-deepvariant2; Now I also want to use DeepTrio.I used the haplotagged.bam(generated from whatshap haplotag) as input bam.When I ran DeepTrio,the output.vcf.gz was generated normally.However,the log file showed the following warning message:; ------------------------; I0926 14:26:35.659228 47028170803008 call_variants.py:336] Shape of input examples: [140, 221, 9]; W0926 14:26:35.665323 47028170803008 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An **error** will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; 2021-09-26 14:26:35.668419: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-09-26 14:26:35.669638: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2021-09-26 14:26:35.671197: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2600000000 Hz; WARNING:tensorflow:Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; W0926 14:26:35.690017 47028170803008 estimator.py:1846] Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; ------------------------; The version I used:; DeepVariant 1.1.0; glnexus v1.3.1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:2050,Integrability,message,message,2050,"hap phase- whatshap haplotag-deepvariant2; Now I also want to use DeepTrio.I used the haplotagged.bam(generated from whatshap haplotag) as input bam.When I ran DeepTrio,the output.vcf.gz was generated normally.However,the log file showed the following warning message:; ------------------------; I0926 14:26:35.659228 47028170803008 call_variants.py:336] Shape of input examples: [140, 221, 9]; W0926 14:26:35.665323 47028170803008 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An **error** will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; 2021-09-26 14:26:35.668419: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-09-26 14:26:35.669638: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2021-09-26 14:26:35.671197: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2600000000 Hz; WARNING:tensorflow:Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; W0926 14:26:35.690017 47028170803008 estimator.py:1846] Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; ------------------------; The version I used:; DeepVariant 1.1.0; glnexus v1.3.1; whatshap 1.0; DeepTrio 1.2.0. Does the warning message affect the results or can be ignored?; Should I use a higher version of DeepVariant(1.2.0)?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1151,Performance,optimiz,optimized,1151,"hap phase- whatshap haplotag-deepvariant2; Now I also want to use DeepTrio.I used the haplotagged.bam(generated from whatshap haplotag) as input bam.When I ran DeepTrio,the output.vcf.gz was generated normally.However,the log file showed the following warning message:; ------------------------; I0926 14:26:35.659228 47028170803008 call_variants.py:336] Shape of input examples: [140, 221, 9]; W0926 14:26:35.665323 47028170803008 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An **error** will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; 2021-09-26 14:26:35.668419: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-09-26 14:26:35.669638: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2021-09-26 14:26:35.671197: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2600000000 Hz; WARNING:tensorflow:Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; W0926 14:26:35.690017 47028170803008 estimator.py:1846] Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; ------------------------; The version I used:; DeepVariant 1.1.0; glnexus v1.3.1; whatshap 1.0; DeepTrio 1.2.0. Does the warning message affect the results or can be ignored?; Should I use a higher version of DeepVariant(1.2.0)?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1251,Performance,perform,performance-critical,1251,"hap phase- whatshap haplotag-deepvariant2; Now I also want to use DeepTrio.I used the haplotagged.bam(generated from whatshap haplotag) as input bam.When I ran DeepTrio,the output.vcf.gz was generated normally.However,the log file showed the following warning message:; ------------------------; I0926 14:26:35.659228 47028170803008 call_variants.py:336] Shape of input examples: [140, 221, 9]; W0926 14:26:35.665323 47028170803008 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An **error** will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; 2021-09-26 14:26:35.668419: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-09-26 14:26:35.669638: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2021-09-26 14:26:35.671197: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2600000000 Hz; WARNING:tensorflow:Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; W0926 14:26:35.690017 47028170803008 estimator.py:1846] Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; ------------------------; The version I used:; DeepVariant 1.1.0; glnexus v1.3.1; whatshap 1.0; DeepTrio 1.2.0. Does the warning message affect the results or can be ignored?; Should I use a higher version of DeepVariant(1.2.0)?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1536,Performance,Tune,Tune,1536,"hap phase- whatshap haplotag-deepvariant2; Now I also want to use DeepTrio.I used the haplotagged.bam(generated from whatshap haplotag) as input bam.When I ran DeepTrio,the output.vcf.gz was generated normally.However,the log file showed the following warning message:; ------------------------; I0926 14:26:35.659228 47028170803008 call_variants.py:336] Shape of input examples: [140, 221, 9]; W0926 14:26:35.665323 47028170803008 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An **error** will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; 2021-09-26 14:26:35.668419: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-09-26 14:26:35.669638: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2021-09-26 14:26:35.671197: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2600000000 Hz; WARNING:tensorflow:Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; W0926 14:26:35.690017 47028170803008 estimator.py:1846] Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; ------------------------; The version I used:; DeepVariant 1.1.0; glnexus v1.3.1; whatshap 1.0; DeepTrio 1.2.0. Does the warning message affect the results or can be ignored?; Should I use a higher version of DeepVariant(1.2.0)?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:1585,Performance,perform,performance,1585,"hap phase- whatshap haplotag-deepvariant2; Now I also want to use DeepTrio.I used the haplotagged.bam(generated from whatshap haplotag) as input bam.When I ran DeepTrio,the output.vcf.gz was generated normally.However,the log file showed the following warning message:; ------------------------; I0926 14:26:35.659228 47028170803008 call_variants.py:336] Shape of input examples: [140, 221, 9]; W0926 14:26:35.665323 47028170803008 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An **error** will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; 2021-09-26 14:26:35.668419: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-09-26 14:26:35.669638: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2021-09-26 14:26:35.671197: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2600000000 Hz; WARNING:tensorflow:Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; W0926 14:26:35.690017 47028170803008 estimator.py:1846] Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; ------------------------; The version I used:; DeepVariant 1.1.0; glnexus v1.3.1; whatshap 1.0; DeepTrio 1.2.0. Does the warning message affect the results or can be ignored?; Should I use a higher version of DeepVariant(1.2.0)?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/488:371,Testability,log,log,371,"I ran DeepVariant twice based on ""https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md"". ; deepvariant1- whatshap phase- whatshap haplotag-deepvariant2; Now I also want to use DeepTrio.I used the haplotagged.bam(generated from whatshap haplotag) as input bam.When I ran DeepTrio,the output.vcf.gz was generated normally.However,the log file showed the following warning message:; ------------------------; I0926 14:26:35.659228 47028170803008 call_variants.py:336] Shape of input examples: [140, 221, 9]; W0926 14:26:35.665323 47028170803008 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An **error** will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; 2021-09-26 14:26:35.668419: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-09-26 14:26:35.669638: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2021-09-26 14:26:35.671197: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2600000000 Hz; WARNING:tensorflow:Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; W0926 14:26:35.690017 47028170803008 estimator.py:1846] Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; ------------------------; The version I used:; DeepVariant 1.1.0; glnexus v1.3.1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/488
https://github.com/google/deepvariant/issues/489:26,Availability,error,error,26,"Hello.; We encountered an error when rebuild our docker image, and it didn't build despite no changes. ```bash; [91m+ apt-get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:2333,Availability,error,error,2333," libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. - 100%[=============================================================================================================================================================================================",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:2406,Availability,error,error,2406," libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. - 100%[=============================================================================================================================================================================================",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:7415,Availability,error,error,7415,"llowing information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal/ llvm-toolchain-focal-11 main'; Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease; Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease; Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease; Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:9248,Availability,avail,available,9248,"untu.com/ubuntu focal-updates InRelease; Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease; Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 not available; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; # 12; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; # 13; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; Focal (20.04) LTS - Last update : Sun, 10 Oct 2021 23:59:52 UTC / Revision: 20211010053033+67964fc4b241; # i386 not available; deb http://apt.llvm.org/focal/ llvm-toolchain-focal main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal main; # 12; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; # 13; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main. ```; `llvm-toolchain-bionic-11` was changed ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:9771,Availability,avail,available,9771,"buntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 not available; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; # 12; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; # 13; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; Focal (20.04) LTS - Last update : Sun, 10 Oct 2021 23:59:52 UTC / Revision: 20211010053033+67964fc4b241; # i386 not available; deb http://apt.llvm.org/focal/ llvm-toolchain-focal main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal main; # 12; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; # 13; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main. ```; `llvm-toolchain-bionic-11` was changed today.; ![image](https://user-images.githubusercontent.com/41360525/136817825-71faa887-08bb-49e7-9126-036e6412d90d.png). Any help?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:127,Deployability,install,install,127,"Hello.; We encountered an error when rebuild our docker image, and it didn't build despite no changes. ```bash; [91m+ apt-get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:425,Deployability,install,installed,425,"Hello.; We encountered an error when rebuild our docker image, and it didn't build despite no changes. ```bash; [91m+ apt-get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:849,Deployability,install,installed,849,"Hello.; We encountered an error when rebuild our docker image, and it didn't build despite no changes. ```bash; [91m+ apt-get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:902,Deployability,install,installable,902,"Hello.; We encountered an error when rebuild our docker image, and it didn't build despite no changes. ```bash; [91m+ apt-get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:977,Deployability,install,installed,977,"Hello.; We encountered an error when rebuild our docker image, and it didn't build despite no changes. ```bash; [91m+ apt-get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:1113,Deployability,install,installed,1113,get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; De,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:1247,Deployability,install,installed,1247,package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:1373,Deployability,install,installed,1373,"buntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:1517,Deployability,install,installed,1517,"ble; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http:/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:1653,Deployability,install,installed,1653," help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:1718,Deployability,install,installable,1718,"ndencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - &",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:1793,Deployability,install,installed,1793,"ec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-too",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:1860,Deployability,install,installable,1860,"ibgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:1983,Deployability,install,installed,1983,"epends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::56",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:2110,Deployability,install,installed,2110,"alled; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK;",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:2238,Deployability,install,installed,2238,"nds: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. - 100%[====================================================================",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:2353,Deployability,install,install,2353," libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. - 100%[=============================================================================================================================================================================================",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:2610,Deployability,update,update,2610,".4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. - 100%[====================================================================================================================================================================================================>] 3.07K --.-KB/s in 0s. 2021-10-11 18:34:23 (51.5 MB/s) - written to stdout [3145/3145]. OK; Get:2 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease [20.8 kB]; Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [24",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:2624,Deployability,install,install,2624,".4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. - 100%[====================================================================================================================================================================================================>] 3.07K --.-KB/s in 0s. 2021-10-11 18:34:23 (51.5 MB/s) - written to stdout [3145/3145]. OK; Get:2 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease [20.8 kB]; Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [24",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:3753,Deployability,update,updates,3753,"ory ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. - 100%[====================================================================================================================================================================================================>] 3.07K --.-KB/s in 0s. 2021-10-11 18:34:23 (51.5 MB/s) - written to stdout [3145/3145]. OK; Get:2 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease [20.8 kB]; Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]; Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]; Get:6 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/u",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:4321,Deployability,update,updates,4321,===========================================================>] 3.07K --.-KB/s in 0s. 2021-10-11 18:34:23 (51.5 MB/s) - written to stdout [3145/3145]. OK; Get:2 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease [20.8 kB]; Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]; Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]; Get:6 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 P,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:4413,Deployability,update,updates,4413,1.5 MB/s) - written to stdout [3145/3145]. OK; Get:2 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease [20.8 kB]; Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]; Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]; Get:6 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#;,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:4504,Deployability,update,updates,4504,a/ubuntu bionic InRelease [20.8 kB]; Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]; Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]; Get:6 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:4594,Deployability,update,updates,4594,ease [242 kB]; Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]; Get:6 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archi,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:5463,Deployability,update,update,5463,8 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the s,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:5644,Deployability,update,updates,5644,ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:5978,Deployability,upgrade,upgraded,5978,erse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; D,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:6055,Deployability,install,install,6055, amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:6192,Deployability,install,installed,6192,Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~202110,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:6616,Deployability,install,installed,6616,"http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:6669,Deployability,install,installable,6669,"lease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; C",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:6749,Deployability,install,installed,6749,"lease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; C",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:6822,Deployability,install,installed,6822,"hain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:6958,Deployability,install,installed,6958,"es can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:7092,Deployability,install,installed,7092,"Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:7218,Deployability,install,installed,7218,"f you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal/ llvm-toolchain-focal-11 main'; Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease; Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease; Hit:4 http://archiv",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:7279,Deployability,install,installed,7279,"f you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal/ llvm-toolchain-focal-11 main'; Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease; Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease; Hit:4 http://archiv",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:8232,Deployability,update,updates,8232,"vm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal/ llvm-toolchain-focal-11 main'; Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease; Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease; Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease; Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:8618,Deployability,update,update,8618,"onnecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal/ llvm-toolchain-focal-11 main'; Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease; Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease; Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease; Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 not available; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; # 12; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; # 13; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; Focal (20.04) LTS - Last update : Sun, 10 Oct 2021 23:59:52 UTC / Re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:8643,Deployability,install,install,8643,"onnecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal/ llvm-toolchain-focal-11 main'; Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease; Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease; Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease; Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 not available; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; # 12; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; # 13; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; Focal (20.04) LTS - Last update : Sun, 10 Oct 2021 23:59:52 UTC / Re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:9157,Deployability,update,update,9157,"untu.com/ubuntu focal-updates InRelease; Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease; Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 not available; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; # 12; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; # 13; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; Focal (20.04) LTS - Last update : Sun, 10 Oct 2021 23:59:52 UTC / Revision: 20211010053033+67964fc4b241; # i386 not available; deb http://apt.llvm.org/focal/ llvm-toolchain-focal main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal main; # 12; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; # 13; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main. ```; `llvm-toolchain-bionic-11` was changed ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:9680,Deployability,update,update,9680,"buntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 not available; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; # 12; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-12 main; # 13; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main; Focal (20.04) LTS - Last update : Sun, 10 Oct 2021 23:59:52 UTC / Revision: 20211010053033+67964fc4b241; # i386 not available; deb http://apt.llvm.org/focal/ llvm-toolchain-focal main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal main; # 12; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-12 main; # 13; deb http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main; deb-src http://apt.llvm.org/focal/ llvm-toolchain-focal-13 main. ```; `llvm-toolchain-bionic-11` was changed today.; ![image](https://user-images.githubusercontent.com/41360525/136817825-71faa887-08bb-49e7-9126-036e6412d90d.png). Any help?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:280,Integrability,depend,dependency,280,"Hello.; We encountered an error when rebuild our docker image, and it didn't build despite no changes. ```bash; [91m+ apt-get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:729,Integrability,depend,dependencies,729,"Hello.; We encountered an error when rebuild our docker image, and it didn't build despite no changes. ```bash; [91m+ apt-get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:755,Integrability,Depend,Depends,755,"Hello.; We encountered an error when rebuild our docker image, and it didn't build despite no changes. ```bash; [91m+ apt-get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:860,Integrability,Depend,Depends,860,"Hello.; We encountered an error when rebuild our docker image, and it didn't build despite no changes. ```bash; [91m+ apt-get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:915,Integrability,Depend,Depends,915,"Hello.; We encountered an error when rebuild our docker image, and it didn't build despite no changes. ```bash; [91m+ apt-get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:988,Integrability,Depend,Depends,988,"Hello.; We encountered an error when rebuild our docker image, and it didn't build despite no changes. ```bash; [91m+ apt-get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:1124,Integrability,Depend,Depends,1124,get install -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev zlib1g-dev; [0mReading package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; De,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:1258,Integrability,Depend,Depends,1258,package lists...; Building dependency tree...; Reading state information...; zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:1402,Integrability,Depend,Depends,1402,"buntu2).; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:1528,Integrability,Depend,Depends,1528,"ble; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http:/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:1676,Integrability,Depend,Depends,1676," help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:1731,Integrability,Depend,Depends,1731,"ndencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - &",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:1818,Integrability,Depend,Depends,1818,"ec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-too",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:1873,Integrability,Depend,Depends,1873,"ibgcc-s1 (>= 3.0) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:1994,Integrability,Depend,Depends,1994,"epends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::56",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:2121,Integrability,Depend,Depends,2121,"alled; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libclang-11-dev : Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; libllvm11 : Depends: libgcc-s1 (>= 3.3) but it is not installable; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; llvm-11-dev : Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: llvm-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang-cpp11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; [91mE: Unable to correct problems, you have held broken packages.; ```. After that error I've tried to install `clang-11` on fresh `ubuntu-18` but got same error:; ```bash; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". apt update && apt install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK;",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:5907,Integrability,depend,dependency,5907,security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~2021101,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:6113,Integrability,depend,dependency,6113,tu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (=,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:6496,Integrability,depend,dependencies,6496,"u bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:6522,Integrability,Depend,Depends,6522,"u bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:6627,Integrability,Depend,Depends,6627,"http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:6682,Integrability,Depend,Depends,6682,"lease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; C",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:6760,Integrability,Depend,Depends,6760,"lease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; C",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:6833,Integrability,Depend,Depends,6833,"hain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:6969,Integrability,Depend,Depends,6969,"es can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:7103,Integrability,Depend,Depends,7103,"Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11) but 8.4.0-1ubuntu1~18.04 is to be installed; Depends: libclang-common-11-dev (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: llvm-11-linker-tools (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Depends: libclang1-11 (= 1:11.1.0~++20211010011718+1fdec59bffc1-1~exp1~20211010132151.4) but it is not going to be installed; Recommends: llvm-11-dev but it is not going to be installed; E: Unable to correct problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:3643,Security,secur,security,3643,"t install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. - 100%[====================================================================================================================================================================================================>] 3.07K --.-KB/s in 0s. 2021-10-11 18:34:23 (51.5 MB/s) - written to stdout [3145/3145]. OK; Get:2 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease [20.8 kB]; Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]; Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]; Get:6 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:3677,Security,secur,security,3677,"//apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. - 100%[====================================================================================================================================================================================================>] 3.07K --.-KB/s in 0s. 2021-10-11 18:34:23 (51.5 MB/s) - written to stdout [3145/3145]. OK; Get:2 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease [20.8 kB]; Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]; Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]; Get:6 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:4827,Security,secur,security,4827,r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.o,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:4861,Security,secur,security,4861,rchive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lis,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:4920,Security,secur,security,4920,rchive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lis,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:4954,Security,secur,security,4954,chive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 pac,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:5014,Security,secur,security,5014,ckages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgr,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:5048,Security,secur,security,5048,c/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:5102,Security,secur,security,5102,c/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:5136,Security,secur,security,5136,verse amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information...,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:5743,Security,secur,security,5743,Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~),MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:5777,Security,secur,security,5777,amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11),MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:8330,Security,secur,security,8330,"t problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal/ llvm-toolchain-focal-11 main'; Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease; Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease; Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease; Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 not available; deb http://apt.llvm.org/bionic/ llvm-toolchain-bio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/489:8363,Security,secur,security,8363,"e tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal/ llvm-toolchain-focal-11 main'; Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease; Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease; Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease; Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 not available; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bion",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/489
https://github.com/google/deepvariant/issues/490:165,Deployability,pipeline,pipeline,165,"Hi,. I am trying to run DeepVariant 1.2.0 on a few human samples PacBio HiFi data (about 30x coverage per sample). I first ran my samples through the [PEPPER-Margin pipeline r0.4](https://github.com/kishwarshafin/pepper) to get a haplotagged BAM file. Then I ran DeepVariant as follows:; ```; singularity exec -B ${SOME_PATHS} deepvariant_1.2.0.sif bash /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref ${PATH_TO_REF} --reads MARGIN_PHASED.PEPPER_SNP_MARGIN.happlotagged.bam --output_vcf sample.vcf.gz --output_gvcf sample.g.vcf.gz --num_shards 24 --make_examples_extra_args=""realign_reads=false,min_mapping_quality=5"" --sample_name MYSAMPLE --use-hp-information;; ```. I have two problems:; 1. Right from the beginning (`CALL VARIANT MODULE SELECTED`), for each interval processed. I get thousands of `READ TAG: n_elements is zero` messages in the console. What does it mean and is it a problem or just a warning?; 2. I allocate 200GB of RAM for per job and they all seem to systematically fail on memory. I do not recall DeepVariant using that much memory in the past but I might be wrong. Is 200GB too light for a human genome PacBio Hifi 30x coverage dataset?. Thank you for your help,; Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:935,Energy Efficiency,allocate,allocate,935,"Hi,. I am trying to run DeepVariant 1.2.0 on a few human samples PacBio HiFi data (about 30x coverage per sample). I first ran my samples through the [PEPPER-Margin pipeline r0.4](https://github.com/kishwarshafin/pepper) to get a haplotagged BAM file. Then I ran DeepVariant as follows:; ```; singularity exec -B ${SOME_PATHS} deepvariant_1.2.0.sif bash /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref ${PATH_TO_REF} --reads MARGIN_PHASED.PEPPER_SNP_MARGIN.happlotagged.bam --output_vcf sample.vcf.gz --output_gvcf sample.g.vcf.gz --num_shards 24 --make_examples_extra_args=""realign_reads=false,min_mapping_quality=5"" --sample_name MYSAMPLE --use-hp-information;; ```. I have two problems:; 1. Right from the beginning (`CALL VARIANT MODULE SELECTED`), for each interval processed. I get thousands of `READ TAG: n_elements is zero` messages in the console. What does it mean and is it a problem or just a warning?; 2. I allocate 200GB of RAM for per job and they all seem to systematically fail on memory. I do not recall DeepVariant using that much memory in the past but I might be wrong. Is 200GB too light for a human genome PacBio Hifi 30x coverage dataset?. Thank you for your help,; Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/490:847,Integrability,message,messages,847,"Hi,. I am trying to run DeepVariant 1.2.0 on a few human samples PacBio HiFi data (about 30x coverage per sample). I first ran my samples through the [PEPPER-Margin pipeline r0.4](https://github.com/kishwarshafin/pepper) to get a haplotagged BAM file. Then I ran DeepVariant as follows:; ```; singularity exec -B ${SOME_PATHS} deepvariant_1.2.0.sif bash /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref ${PATH_TO_REF} --reads MARGIN_PHASED.PEPPER_SNP_MARGIN.happlotagged.bam --output_vcf sample.vcf.gz --output_gvcf sample.g.vcf.gz --num_shards 24 --make_examples_extra_args=""realign_reads=false,min_mapping_quality=5"" --sample_name MYSAMPLE --use-hp-information;; ```. I have two problems:; 1. Right from the beginning (`CALL VARIANT MODULE SELECTED`), for each interval processed. I get thousands of `READ TAG: n_elements is zero` messages in the console. What does it mean and is it a problem or just a warning?; 2. I allocate 200GB of RAM for per job and they all seem to systematically fail on memory. I do not recall DeepVariant using that much memory in the past but I might be wrong. Is 200GB too light for a human genome PacBio Hifi 30x coverage dataset?. Thank you for your help,; Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/490
https://github.com/google/deepvariant/issues/491:178,Availability,error,error,178,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:; Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**; - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`; - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`; - Installation method (Docker, built from source, etc.): Docker; - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```; # This is the command from Pepper, but judged from the log, the command failed during the DV stage.; run_pepper_margin_deepvariant \; call_variant \; -b ~{bam} \; -f ~{ref_fasta} \; -t ""${num_core}"" \; -s ""${SM}"" \; -o ""~{output_root}"" \; -p ""~{prefix}"" \; --gvcf \; --phased_output \; --ont; ```; Relevant part of the log file (which is over 200MB):. ```; run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont; [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED; [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND; -------; mkdir -p /cromwell_root/pepper_output; ; mkdir -p /cromwell_root/pepper_output/logs; ; mkdir -p /cromwell_root/pepper_output/intermediate_files;; -------; [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND; -------; time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-8",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:5650,Availability,echo,echo,5650," ; [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec; [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES; [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/; [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10; [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013.; [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14; [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092.; [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s; user	579m29.953s; sys	11m32.825s; [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND; -------; mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; ; bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; ; tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; ; rm -rf /cromwell_root/pepper_output/pepper_snp/; ; echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; ; zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq; -------; CONTIGS FOUND IN PEPPER SNP VCF:; chr10; chr14; [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND; -------; time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;; mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PH",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:21509,Availability,echo,echo,21509,"epper_hp/; [11-03-2021 14:32:23] INFO: PROCESSING HAPLOTAG: 0; [11-03-2021 14:32:24] INFO: PROCESSING CONTIG: chr10; [11-03-2021 14:39:30] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 378085 TOTAL TIME SPENT: 7 Min 6 Sec; [11-03-2021 14:39:37] INFO: PROCESSING CONTIG: chr14; [11-03-2021 14:39:48] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 2550 TOTAL TIME SPENT: 0 Min 11 Sec; [11-03-2021 14:39:49] TOTAL ELAPSED TIME FOR VARIANT CALLING: 26 Min 43 Sec. real	26m44.555s; user	1220m53.668s; sys	15m35.409s; [11-03-2021 14:39:49] INFO: [6/9] RUNNING THE FOLLOWING COMMAND; -------; mv /cromwell_root/pepper_output/pepper_hp/*.vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; ; bgzip /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; ; tabix -p vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz; ; rm -rf /cromwell_root/pepper_output/pepper_hp/; -------; [11-03-2021 14:39:53] INFO: [7/9] RUNNING THE FOLLOWING COMMAND; -------; mkdir -p /cromwell_root/pepper_output/dv_intermediate_outputs/; ; echo ""STARTING DEEPVARIANT""; ; time /opt/deepvariant/bin/run_deepvariant --model_type=WGS --customized_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:36017,Availability,error,error,36017,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]; I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]; I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]; I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]; I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]; I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]; I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]; I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]; I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]; I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]; I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]; # the program died here; ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks!; Steve",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:440,Deployability,Install,Installation,440,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:; Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**; - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`; - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`; - Installation method (Docker, built from source, etc.): Docker; - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```; # This is the command from Pepper, but judged from the log, the command failed during the DV stage.; run_pepper_margin_deepvariant \; call_variant \; -b ~{bam} \; -f ~{ref_fasta} \; -t ""${num_core}"" \; -s ""${SM}"" \; -o ""~{output_root}"" \; -p ""~{prefix}"" \; --gvcf \; --phased_output \; --ont; ```; Relevant part of the log file (which is over 200MB):. ```; run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont; [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED; [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND; -------; mkdir -p /cromwell_root/pepper_output; ; mkdir -p /cromwell_root/pepper_output/logs; ; mkdir -p /cromwell_root/pepper_output/intermediate_files;; -------; [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND; -------; time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-8",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:35861,Energy Efficiency,allocate,allocated,35861,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]; I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]; I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]; I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]; I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]; I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]; I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]; I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]; I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]; I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]; I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]; # the program died here; ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks!; Steve",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:1235,Performance,cache,cacheCopy,1235,"*Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**; - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`; - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`; - Installation method (Docker, built from source, etc.): Docker; - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```; # This is the command from Pepper, but judged from the log, the command failed during the DV stage.; run_pepper_margin_deepvariant \; call_variant \; -b ~{bam} \; -f ~{ref_fasta} \; -t ""${num_core}"" \; -s ""${SM}"" \; -o ""~{output_root}"" \; -p ""~{prefix}"" \; --gvcf \; --phased_output \; --ont; ```; Relevant part of the log file (which is over 200MB):. ```; run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont; [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED; [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND; -------; mkdir -p /cromwell_root/pepper_output; ; mkdir -p /cromwell_root/pepper_output/logs; ; mkdir -p /cromwell_root/pepper_output/intermediate_files;; -------; [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND; -------; time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:2173,Performance,cache,cacheCopy,2173,\; --gvcf \; --phased_output \; --ont; ```; Relevant part of the log file (which is over 200MB):. ```; run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont; [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED; [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND; -------; mkdir -p /cromwell_root/pepper_output; ; mkdir -p /cromwell_root/pepper_output/logs; ; mkdir -p /cromwell_root/pepper_output/intermediate_files;; -------; [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND; -------; time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log; -------; [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED.; [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING.; [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041; [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/; [11-03-2021,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:4047,Performance,LOAD,LOADING,4047,"11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']; [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895; [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS; [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]; ...; [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]; [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]; [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY.; [11-03-2021 13:44:25] FINISHED IMAGE GENERATION; [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec; [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE; [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/; [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP.; [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64; [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1; [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX; [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35.; [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35.; [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35.; [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35.; [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35.; [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35.; [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35.; [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY.; [11-03-2021 13:50:44] INFO: FINISHED PREDICTION; [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec; [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. ; [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec; [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES; [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/; [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10; [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:6179,Performance,cache,cacheCopy,6179,"PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013.; [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14; [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092.; [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s; user	579m29.953s; sys	11m32.825s; [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND; -------; mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; ; bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; ; tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; ; rm -rf /cromwell_root/pepper_output/pepper_snp/; ; echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; ; zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq; -------; CONTIGS FOUND IN PEPPER SNP VCF:; chr10; chr14; [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND; -------; time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;; mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; ; samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; -------; Running OpenMP with 64 threads.; > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json; > Parsed 346237 HET VCF entrie",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
https://github.com/google/deepvariant/issues/491:15446,Performance,LOAD,LOADING,15446,[THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]; [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]; [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]; [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]; [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]; [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]; [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY.; [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION; [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec; [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE; [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/; [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP.; [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64; [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1; [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX; [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66.; [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66.; [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66.; [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66.; [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66.; [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66.; [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66.; [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66.; [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66.; [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66.; [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66.; [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 60/66.; [11-03-2021 14:31:57] INFO: BATCHES PROCESSED 65/66.; [11-03-2021 14:31:05] INFO: THREAD 46 FINISHED SUCCESSFULLY.; [11-03-2021 14:31:07] INFO: THREAD 19 FINISHED SUCCESSFULLY.; [11-03-2021 14:31:09] INFO: THREAD 38 FINISHED SUCCESSFULLY.; [11-03-2021 14:31:19] INFO: THREAD 3 FINISHED SUCCESSFULLY.; [1,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/491
