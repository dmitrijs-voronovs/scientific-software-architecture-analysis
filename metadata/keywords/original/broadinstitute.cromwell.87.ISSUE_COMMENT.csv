id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103145934:183,Usability,clear,clear,183,"@mcovarr @scottfrazer For point 2 I know you all disagree with me but personally I don't see the need to _force_ parallelism just to prove it works at this juncture. IMO it should be clear if it is working as intended in short order so as long as there's no obvious block to it happening I don't see the worth of it. . I'd like to think that we can trust ""I'm making asynchronous calls"" to mean ""they might be running in parallel""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103145934
https://github.com/broadinstitute/cromwell/pull/20#issuecomment-104351643:177,Integrability,message,message,177,"This should incorporate PR feedback delta:; 1. Scott's suggestion for input expression storage and evaluation.; 2. Jeff's suggestions for eliminating the `CheckExecutionStatus` message and handler case.; 3. My suggestion for putting mutable workflow state behind an actor. I'll try to address 2 and 3 in the time remaining in the sprint, 1 will have to wait for motivating user stories.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-104351643
https://github.com/broadinstitute/cromwell/pull/20#issuecomment-104351643:27,Usability,feedback,feedback,27,"This should incorporate PR feedback delta:; 1. Scott's suggestion for input expression storage and evaluation.; 2. Jeff's suggestions for eliminating the `CheckExecutionStatus` message and handler case.; 3. My suggestion for putting mutable workflow state behind an actor. I'll try to address 2 and 3 in the time remaining in the sprint, 1 will have to wait for motivating user stories.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-104351643
https://github.com/broadinstitute/cromwell/pull/44#issuecomment-113509547:45,Usability,clear,clear,45,"The work just started, it's not particularly clear at the moment what changes we'll have to make in the first place, much less to sync w/ this. This will need to wait until that work is complete.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/44#issuecomment-113509547
https://github.com/broadinstitute/cromwell/pull/50#issuecomment-112552200:205,Security,access,access,205,"This isn't due to the IDE, it's a readability thing for some definition of readability. These functions are effectively methods on `AstNode` but we can't write them as methods because we don't have (easy) access to the underlying class. In this case the implicit class is merely a vector to insert those methods so we aren't calling them as functions. This way it's clear that `foo` is acting on `a` when you say `a.foo(x)` instead of `foo(a, x)`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/50#issuecomment-112552200
https://github.com/broadinstitute/cromwell/pull/50#issuecomment-112552200:366,Usability,clear,clear,366,"This isn't due to the IDE, it's a readability thing for some definition of readability. These functions are effectively methods on `AstNode` but we can't write them as methods because we don't have (easy) access to the underlying class. In this case the implicit class is merely a vector to insert those methods so we aren't calling them as functions. This way it's clear that `foo` is acting on `a` when you say `a.foo(x)` instead of `foo(a, x)`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/50#issuecomment-112552200
https://github.com/broadinstitute/cromwell/pull/78#issuecomment-118239642:57,Usability,feedback,feedback,57,Coveralls seems happy with the latest changes. Ready for feedback.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/78#issuecomment-118239642
https://github.com/broadinstitute/cromwell/pull/78#issuecomment-118435565:32,Usability,clear,clear,32,"Updated everything that I had a clear fix for. Open to other suggestions, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/78#issuecomment-118435565
https://github.com/broadinstitute/cromwell/pull/88#issuecomment-121002228:86,Usability,simpl,simply,86,"@mcovarr Most of your comments have been referencing original code/comments which has simply been moved around. While I agree w/ them the question is if those should be fixed up here or not. I think so unless they end up larger, which I haven't seen - thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/88#issuecomment-121002228
https://github.com/broadinstitute/cromwell/pull/114#issuecomment-126404896:361,Usability,undo,undocumented,361,"Updated per @kshakir's AC above. :smile: . The ""produces"" content type is not 100% right in the Swagger annotations here and probably in other APIs. This API is documented as producing application/json which it will for successful requests, but for failed requests text/plain is produced instead. Given the way the ""Try it out!""s are built, they will fail with undocumented 406s if the underlying request fails with a documented status.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/114#issuecomment-126404896
https://github.com/broadinstitute/cromwell/pull/135#issuecomment-132193021:837,Usability,guid,guide,837,"@scottfrazer I've told you before that I've been barking about this since day one, I've just become increasingly vocal about it over the last month+. I change them when I see them, and for old ones if they're close enough to a diffed line in a PR I ask someone to change it, but they're mostly new instances being pointed out. As for changing existing code, sure. I could do that. But every PR you submit about 50% of your calls are using the bad style, so do I then make a new PR every sprint to go back and fix them? Or do I point out that they should be changed as I did on this one and you get upset again? . TBH what really catches my eye and leads me to closer scrutiny in the first place is that you consistently bounce between multiple styles of calling these things, often within the same larger block of code. Unlike the style guide, I don't have a preference between the two primary modes people use (admittedly it's because I prefer the one they don't, although I begrudgingly use their favorite) as long as they're generally self-consistent (and/or there's a reason to deviate for a particular one, it happens). Similarly little things like is it `case (k, v)` or `case (k,v)`. I don't care, but I'd rather not see it bouncing back and forth all over the place over 50 lines of code. Pick one and be consistent. Is it `foo: Type`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/135#issuecomment-132193021
https://github.com/broadinstitute/cromwell/pull/135#issuecomment-132202963:297,Usability,guid,guide,297,"@scottfrazer actually people do care about these things and I know for a fact we've lost a would be user over general code quality (read: not functionality) reasons. Like it or not people look at the code when they're going to assess if it's worth using. I don't think ""try to follow the official guide and be self consistent"" is a high bar",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/135#issuecomment-132202963
https://github.com/broadinstitute/cromwell/pull/135#issuecomment-132206454:44,Usability,guid,guide,44,"> I don't think ""try to follow the official guide and be self consistent"" is a high bar. But you know what IS a high bar? Having to change all your coding style, which previously was perfectly acceptable by the whole team (regards to `.map{`, whitespace semantics), on a whim because Jeff's attitude about something suddenly shifted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/135#issuecomment-132206454
https://github.com/broadinstitute/cromwell/pull/140#issuecomment-132375040:208,Usability,simpl,simple,208,"@mcovarr I thought you were captain multi-line HOF? . re: reviewers I was just going to wait until the end of the sprint in order to make sure it doesn't cause any rebase, even though initially it was such a simple change that it'd seem unlikely. Call yourself a reviewer and then designate one tomorrow or thursday :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/140#issuecomment-132375040
https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134728173:122,Usability,guid,guide,122,@scottfrazer In order to save people's diffs I'll make a general comment to please try to conform to the snazzy new style guide,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134728173
https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134736397:368,Deployability,release,release,368,"Unsurprisingly I had a handful of stylistic requests (although 2 I noted as being of the ""I'd appreciate you trying"" variety) as I realize they were largely c&p and need refactoring anyways). And there was the one comment I made about the general adherence to the guide - whitespace, HOF syntax, etc. I like the general structure of this though. . In terms of path to release my main concerns were addressed in separate line items, but were a) To what extent has this been tested on JES and b) What is necessary from this branch for s/g (i.e. beyond ""it'll suck to rebase after it"")",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134736397
https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134736397:170,Modifiability,refactor,refactoring,170,"Unsurprisingly I had a handful of stylistic requests (although 2 I noted as being of the ""I'd appreciate you trying"" variety) as I realize they were largely c&p and need refactoring anyways). And there was the one comment I made about the general adherence to the guide - whitespace, HOF syntax, etc. I like the general structure of this though. . In terms of path to release my main concerns were addressed in separate line items, but were a) To what extent has this been tested on JES and b) What is necessary from this branch for s/g (i.e. beyond ""it'll suck to rebase after it"")",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134736397
https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134736397:473,Testability,test,tested,473,"Unsurprisingly I had a handful of stylistic requests (although 2 I noted as being of the ""I'd appreciate you trying"" variety) as I realize they were largely c&p and need refactoring anyways). And there was the one comment I made about the general adherence to the guide - whitespace, HOF syntax, etc. I like the general structure of this though. . In terms of path to release my main concerns were addressed in separate line items, but were a) To what extent has this been tested on JES and b) What is necessary from this branch for s/g (i.e. beyond ""it'll suck to rebase after it"")",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134736397
https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134736397:264,Usability,guid,guide,264,"Unsurprisingly I had a handful of stylistic requests (although 2 I noted as being of the ""I'd appreciate you trying"" variety) as I realize they were largely c&p and need refactoring anyways). And there was the one comment I made about the general adherence to the guide - whitespace, HOF syntax, etc. I like the general structure of this though. . In terms of path to release my main concerns were addressed in separate line items, but were a) To what extent has this been tested on JES and b) What is necessary from this branch for s/g (i.e. beyond ""it'll suck to rebase after it"")",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134736397
https://github.com/broadinstitute/cromwell/pull/164#issuecomment-136129605:558,Usability,clear,clear,558,"While a `Set` with that implementation of `equals` should provide equivalent behavior and wouldn't obviously break anything internal to Cromwell, as a client of this API I wouldn't expect that the compiler-written `equals` of `case class WorkflowInput` had been manually overridden that way. I think most of the problem here was a violation of principle of least surprise: the Rawls consumers of this API reasonably assumed that since the Cromwell API returned workflow inputs in an ordered collection, that order was important. Both `Set` and `Map` make it clear that order is actually not important, but `Map` makes it unmistakable that the String keys must be unique without the consumer of the API having to dig into the implementation of `WorkflowInput#equals`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/164#issuecomment-136129605
https://github.com/broadinstitute/cromwell/pull/173#issuecomment-141495429:160,Usability,simpl,simpler,160,https://github.com/scottfrazer/hermes/pull/52 was spawned from discussion Miguel and I had about allowing the parser to specify imports so the grammar file was simpler and could utilize library code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/173#issuecomment-141495429
https://github.com/broadinstitute/cromwell/pull/192#issuecomment-141792771:44,Usability,learn,learning-scalaz,44,Or maybe `UnionTypes`?. http://eed3si9n.com/learning-scalaz/Coproducts.html,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/192#issuecomment-141792771
https://github.com/broadinstitute/cromwell/pull/195#issuecomment-142314130:9,Usability,simpl,simplest,9,"Yes, the simplest way I can think of would be having . ```; object DataAccessObject {; lazy val dataAccess = DataAccess(); }; ```. And then just do . ```; import DataAccessObject._; ```. whenever needed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/195#issuecomment-142314130
https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495:771,Availability,failure,failures,771,"Sorry all. I was WAY off base about that ""scalability"" thing. It turns out, if one shutdowns one's database pool, the database doesn't allow you to open any new connections. :blush:. Perhaps someday, someone will run `ab` against cromwell and see where it really does fall over, but today wasn't that day. Based on the exceptions I saw, I mistakenly thought it was an internal pool being starved, but when I actually attached a debugger, found out it was because the pool wasn't really _there_ anymore. So I closed the #199 with more extensive refactoring. Currently, even with simpler refactoring to remove calling `DataAccess.instance.shutdown()`, there seems to be something else weird in `data_access_singleton` that I need to figure out. I'm getting repeatable test failures on `WorkflowManagerActorSpec`'s ""should Try to restart workflows when there are workflows in restartable states"". Still debugging…",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495
https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495:544,Modifiability,refactor,refactoring,544,"Sorry all. I was WAY off base about that ""scalability"" thing. It turns out, if one shutdowns one's database pool, the database doesn't allow you to open any new connections. :blush:. Perhaps someday, someone will run `ab` against cromwell and see where it really does fall over, but today wasn't that day. Based on the exceptions I saw, I mistakenly thought it was an internal pool being starved, but when I actually attached a debugger, found out it was because the pool wasn't really _there_ anymore. So I closed the #199 with more extensive refactoring. Currently, even with simpler refactoring to remove calling `DataAccess.instance.shutdown()`, there seems to be something else weird in `data_access_singleton` that I need to figure out. I'm getting repeatable test failures on `WorkflowManagerActorSpec`'s ""should Try to restart workflows when there are workflows in restartable states"". Still debugging…",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495
https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495:586,Modifiability,refactor,refactoring,586,"Sorry all. I was WAY off base about that ""scalability"" thing. It turns out, if one shutdowns one's database pool, the database doesn't allow you to open any new connections. :blush:. Perhaps someday, someone will run `ab` against cromwell and see where it really does fall over, but today wasn't that day. Based on the exceptions I saw, I mistakenly thought it was an internal pool being starved, but when I actually attached a debugger, found out it was because the pool wasn't really _there_ anymore. So I closed the #199 with more extensive refactoring. Currently, even with simpler refactoring to remove calling `DataAccess.instance.shutdown()`, there seems to be something else weird in `data_access_singleton` that I need to figure out. I'm getting repeatable test failures on `WorkflowManagerActorSpec`'s ""should Try to restart workflows when there are workflows in restartable states"". Still debugging…",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495
https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495:42,Performance,scalab,scalability,42,"Sorry all. I was WAY off base about that ""scalability"" thing. It turns out, if one shutdowns one's database pool, the database doesn't allow you to open any new connections. :blush:. Perhaps someday, someone will run `ab` against cromwell and see where it really does fall over, but today wasn't that day. Based on the exceptions I saw, I mistakenly thought it was an internal pool being starved, but when I actually attached a debugger, found out it was because the pool wasn't really _there_ anymore. So I closed the #199 with more extensive refactoring. Currently, even with simpler refactoring to remove calling `DataAccess.instance.shutdown()`, there seems to be something else weird in `data_access_singleton` that I need to figure out. I'm getting repeatable test failures on `WorkflowManagerActorSpec`'s ""should Try to restart workflows when there are workflows in restartable states"". Still debugging…",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495
https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495:766,Testability,test,test,766,"Sorry all. I was WAY off base about that ""scalability"" thing. It turns out, if one shutdowns one's database pool, the database doesn't allow you to open any new connections. :blush:. Perhaps someday, someone will run `ab` against cromwell and see where it really does fall over, but today wasn't that day. Based on the exceptions I saw, I mistakenly thought it was an internal pool being starved, but when I actually attached a debugger, found out it was because the pool wasn't really _there_ anymore. So I closed the #199 with more extensive refactoring. Currently, even with simpler refactoring to remove calling `DataAccess.instance.shutdown()`, there seems to be something else weird in `data_access_singleton` that I need to figure out. I'm getting repeatable test failures on `WorkflowManagerActorSpec`'s ""should Try to restart workflows when there are workflows in restartable states"". Still debugging…",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495
https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495:578,Usability,simpl,simpler,578,"Sorry all. I was WAY off base about that ""scalability"" thing. It turns out, if one shutdowns one's database pool, the database doesn't allow you to open any new connections. :blush:. Perhaps someday, someone will run `ab` against cromwell and see where it really does fall over, but today wasn't that day. Based on the exceptions I saw, I mistakenly thought it was an internal pool being starved, but when I actually attached a debugger, found out it was because the pool wasn't really _there_ anymore. So I closed the #199 with more extensive refactoring. Currently, even with simpler refactoring to remove calling `DataAccess.instance.shutdown()`, there seems to be something else weird in `data_access_singleton` that I need to figure out. I'm getting repeatable test failures on `WorkflowManagerActorSpec`'s ""should Try to restart workflows when there are workflows in restartable states"". Still debugging…",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495
https://github.com/broadinstitute/cromwell/pull/233#issuecomment-146951243:77,Testability,test,tests,77,Please don't merge this PR yet. I believe the changes I've made are okay but tests are failing pretty consistently for reasons that aren't clear yet.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/233#issuecomment-146951243
https://github.com/broadinstitute/cromwell/pull/233#issuecomment-146951243:139,Usability,clear,clear,139,Please don't merge this PR yet. I believe the changes I've made are okay but tests are failing pretty consistently for reasons that aren't clear yet.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/233#issuecomment-146951243
https://github.com/broadinstitute/cromwell/pull/265#issuecomment-153032210:83,Usability,feedback,feedback,83,Don't mind waiting to press the merge button at all. Would love to have folks give feedback ahead of time though.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/265#issuecomment-153032210
https://github.com/broadinstitute/cromwell/pull/265#issuecomment-153723485:89,Usability,feedback,feedback,89,"Thanks for the suggestion. `Main.run` is now much smaller. Please let me know additional feedback, and when time to rebase/squash/merge.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/265#issuecomment-153723485
https://github.com/broadinstitute/cromwell/pull/266#issuecomment-153122691:131,Usability,simpl,simple,131,@geoffjentry I don't see any harm in having a 2.7K file hosted by the cromwell server for the convenience it brings as an easy and simple way of getting timing information from cromwell. Are you saying it should _also_ be part of cromtool?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/266#issuecomment-153122691
https://github.com/broadinstitute/cromwell/pull/283#issuecomment-155473668:59,Testability,test,test,59,"They just restarted. Never mind! . If you can get a Tyburn test for Array[Array[File]] added and working, that'd be awesome. But I suspect it's more work than a simple coercion addition.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/283#issuecomment-155473668
https://github.com/broadinstitute/cromwell/pull/283#issuecomment-155473668:161,Usability,simpl,simple,161,"They just restarted. Never mind! . If you can get a Tyburn test for Array[Array[File]] added and working, that'd be awesome. But I suspect it's more work than a simple coercion addition.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/283#issuecomment-155473668
https://github.com/broadinstitute/cromwell/pull/305#issuecomment-162635666:231,Usability,simpl,simply,231,Is there a particular commit related to the bugfix that you think needs review? Bug squashing is not usually a contentious issue for me. :stuck_out_tongue: . I don't know the answer to your template question but I hope it works as simply as this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-162635666
https://github.com/broadinstitute/cromwell/pull/307#issuecomment-160984769:23,Testability,mock,mock-mock,23,@kshakir Do you mean a mock-mock or a simple stub? I like the latter but find the former to be meh.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-160984769
https://github.com/broadinstitute/cromwell/pull/307#issuecomment-160984769:45,Testability,stub,stub,45,@kshakir Do you mean a mock-mock or a simple stub? I like the latter but find the former to be meh.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-160984769
https://github.com/broadinstitute/cromwell/pull/307#issuecomment-160984769:38,Usability,simpl,simple,38,@kshakir Do you mean a mock-mock or a simple stub? I like the latter but find the former to be meh.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/307#issuecomment-160984769
https://github.com/broadinstitute/cromwell/pull/314#issuecomment-162990401:38,Safety,timeout,timeout,38,"Commented on at least simplifying the timeout code, possibly removing it altogether. Back to @mcovarr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/314#issuecomment-162990401
https://github.com/broadinstitute/cromwell/pull/314#issuecomment-162990401:22,Usability,simpl,simplifying,22,"Commented on at least simplifying the timeout code, possibly removing it altogether. Back to @mcovarr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/314#issuecomment-162990401
https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:360,Availability,down,down,360,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702
https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:1359,Availability,error,errors,1359,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702
https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:1413,Availability,error,error,1413,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702
https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:772,Modifiability,refactor,refactoring,772,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702
https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:1308,Performance,load,load,1308,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702
https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:670,Safety,timeout,timeout,670,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702
https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:48,Security,hash,hashing,48,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702
https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:628,Security,hash,hash,628,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702
https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:857,Security,hash,hashFuture,857,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702
https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:872,Security,hash,hash,872,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702
https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:1075,Security,hash,hash,1075,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702
https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:138,Testability,test,tests,138,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702
https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:445,Testability,log,logged,445,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702
https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:579,Testability,test,testing,579,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702
https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:654,Testability,test,tests,654,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702
https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:1313,Testability,test,testing,1313,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702
https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:1337,Testability,log,log,1337,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702
https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:67,Usability,clear,clear,67,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702
https://github.com/broadinstitute/cromwell/pull/329#issuecomment-165169137:87,Modifiability,refactor,refactor,87,"There is already a relatively heavy conflict between the job_avoidance branch and the ""refactor Engine Fns"" branch that I'm currently rebasing right now. This PR seems simple enough to add to the rebased version though so it shouldn't be too bad. Just please let me know if you modify `GoogleCrendential` on job_avoidance branch @kshakir",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/329#issuecomment-165169137
https://github.com/broadinstitute/cromwell/pull/329#issuecomment-165169137:168,Usability,simpl,simple,168,"There is already a relatively heavy conflict between the job_avoidance branch and the ""refactor Engine Fns"" branch that I'm currently rebasing right now. This PR seems simple enough to add to the rebased version though so it shouldn't be too bad. Just please let me know if you modify `GoogleCrendential` on job_avoidance branch @kshakir",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/329#issuecomment-165169137
https://github.com/broadinstitute/cromwell/pull/329#issuecomment-165170061:301,Modifiability,refactor,refactor,301,"This is not super urgent, but did seem quite simple and I would like to get; it checked in if possible. Let me know what you guys think. On Wed, Dec 16, 2015 at 11:40 AM, Thib notifications@github.com wrote:. > There is already a relatively heavy conflict between the job_avoidance; > branch and the ""refactor Engine Fns"" branch that I'm currently rebasing; > right now. This PR seems simple enough to add to the rebased version though; > so it shouldn't be too bad. Just please let me know if you modify; > GoogleCrendential on job_avoidance branch @kshakir; > https://github.com/kshakir; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/pull/329#issuecomment-165169137; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/329#issuecomment-165170061
https://github.com/broadinstitute/cromwell/pull/329#issuecomment-165170061:45,Usability,simpl,simple,45,"This is not super urgent, but did seem quite simple and I would like to get; it checked in if possible. Let me know what you guys think. On Wed, Dec 16, 2015 at 11:40 AM, Thib notifications@github.com wrote:. > There is already a relatively heavy conflict between the job_avoidance; > branch and the ""refactor Engine Fns"" branch that I'm currently rebasing; > right now. This PR seems simple enough to add to the rebased version though; > so it shouldn't be too bad. Just please let me know if you modify; > GoogleCrendential on job_avoidance branch @kshakir; > https://github.com/kshakir; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/pull/329#issuecomment-165169137; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/329#issuecomment-165170061
https://github.com/broadinstitute/cromwell/pull/329#issuecomment-165170061:385,Usability,simpl,simple,385,"This is not super urgent, but did seem quite simple and I would like to get; it checked in if possible. Let me know what you guys think. On Wed, Dec 16, 2015 at 11:40 AM, Thib notifications@github.com wrote:. > There is already a relatively heavy conflict between the job_avoidance; > branch and the ""refactor Engine Fns"" branch that I'm currently rebasing; > right now. This PR seems simple enough to add to the rebased version though; > so it shouldn't be too bad. Just please let me know if you modify; > GoogleCrendential on job_avoidance branch @kshakir; > https://github.com/kshakir; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/pull/329#issuecomment-165169137; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/329#issuecomment-165170061
https://github.com/broadinstitute/cromwell/pull/332#issuecomment-165261506:5,Usability,clear,clear,5,[not clear](https://books.google.com/ngrams/graph?content=preemptible%2C+preemptable&year_start=1800&year_end=2000&corpus=15&smoothing=3&share=&direct_url=t1%3B%2Cpreemptible%3B%2Cc0%3B.t1%3B%2Cpreemptable%3B%2Cc0),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/332#issuecomment-165261506
https://github.com/broadinstitute/cromwell/pull/343#issuecomment-166692810:20,Usability,undo,undo,20,"Note that this does undo PR #340 - that one ended up fixing a symptom, not the underlying issue",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/343#issuecomment-166692810
https://github.com/broadinstitute/cromwell/pull/349#issuecomment-169074667:125,Deployability,configurat,configuration,125,What about simply a unit test that verifies that it does (resp. doesn't) parse a correct (resp. wrong) conf file (wrt Google configuration) ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/349#issuecomment-169074667
https://github.com/broadinstitute/cromwell/pull/349#issuecomment-169074667:125,Modifiability,config,configuration,125,What about simply a unit test that verifies that it does (resp. doesn't) parse a correct (resp. wrong) conf file (wrt Google configuration) ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/349#issuecomment-169074667
https://github.com/broadinstitute/cromwell/pull/349#issuecomment-169074667:25,Testability,test,test,25,What about simply a unit test that verifies that it does (resp. doesn't) parse a correct (resp. wrong) conf file (wrt Google configuration) ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/349#issuecomment-169074667
https://github.com/broadinstitute/cromwell/pull/349#issuecomment-169074667:11,Usability,simpl,simply,11,What about simply a unit test that verifies that it does (resp. doesn't) parse a correct (resp. wrong) conf file (wrt Google configuration) ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/349#issuecomment-169074667
https://github.com/broadinstitute/cromwell/pull/378#issuecomment-171695991:189,Usability,clear,clear,189,"OK, I was assuming all the copying would be intra-workflow and not inter-workflow, but that might not be a correct assumption. The ticket does say the name shouldn't be a UUID but it's not clear what the name _should_ be. Perhaps we should ask ops exactly how they'd like these names to look.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/378#issuecomment-171695991
https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175780493:16,Usability,clear,cleared,16,"OK, this is now cleared to merge pending squash and rebase.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175780493
https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174086368:342,Integrability,interface,interface,342,"@geoffjentry I'm not sure if it's a 5 min change, but surely it's not a thing for 5 days. We are still working on some of the caching behaviors (we are still not clear on that yet).. and that's why a face-to-face with you guys will help us in understanding the implications of some of the refactoring. The change to implement the new backend interface in it's own will not be much, but the some functionalities like caching may still be lacking as of right now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174086368
https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174086368:289,Modifiability,refactor,refactoring,289,"@geoffjentry I'm not sure if it's a 5 min change, but surely it's not a thing for 5 days. We are still working on some of the caching behaviors (we are still not clear on that yet).. and that's why a face-to-face with you guys will help us in understanding the implications of some of the refactoring. The change to implement the new backend interface in it's own will not be much, but the some functionalities like caching may still be lacking as of right now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174086368
https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174086368:162,Usability,clear,clear,162,"@geoffjentry I'm not sure if it's a 5 min change, but surely it's not a thing for 5 days. We are still working on some of the caching behaviors (we are still not clear on that yet).. and that's why a face-to-face with you guys will help us in understanding the implications of some of the refactoring. The change to implement the new backend interface in it's own will not be much, but the some functionalities like caching may still be lacking as of right now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174086368
https://github.com/broadinstitute/cromwell/pull/424#issuecomment-180509918:190,Security,access,access,190,"It's entirely possible that this resolves all of the big bang problems, including the spray responsiveness. From what I saw yesterday the pain and suffering was primarily coming from the DB access.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/424#issuecomment-180509918
https://github.com/broadinstitute/cromwell/pull/424#issuecomment-180509918:92,Usability,responsiv,responsiveness,92,"It's entirely possible that this resolves all of the big bang problems, including the spray responsiveness. From what I saw yesterday the pain and suffering was primarily coming from the DB access.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/424#issuecomment-180509918
https://github.com/broadinstitute/cromwell/pull/427#issuecomment-181493008:107,Usability,feedback,feedback,107,"I give a tentative :+1:. Like Chris said, I'd like to see it with the changes you got from the code review feedback from yourself :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-181493008
https://github.com/broadinstitute/cromwell/issues/437#issuecomment-182384900:92,Usability,resume,resumed,92,If the server is turned off and brought back up any previously running workflows need to be resumed as they would have previous to this PR,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/437#issuecomment-182384900
https://github.com/broadinstitute/cromwell/issues/439#issuecomment-186006370:137,Usability,simpl,simple,137,It's just a single example. . I find it entirely plausible that a backend might want to act upon something at a workflow level and not a simple task level.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/439#issuecomment-186006370
https://github.com/broadinstitute/cromwell/issues/443#issuecomment-182587558:130,Usability,simpl,simplify,130,@cjllanwarne @geoffjentry are we open to the idea of moving the execution of final calls to the backend? It's possible that could simplify this ticket.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/443#issuecomment-182587558
https://github.com/broadinstitute/cromwell/issues/472#issuecomment-252058323:155,Modifiability,config,configureable,155,"I'm also running into this. Would be nice to get some guidance on a fix from the cromwell team. Ideally, maybe both the user and volume mount location are configureable? Not sure yet what the best fix is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/472#issuecomment-252058323
https://github.com/broadinstitute/cromwell/issues/472#issuecomment-252058323:54,Usability,guid,guidance,54,"I'm also running into this. Would be nice to get some guidance on a fix from the cromwell team. Ideally, maybe both the user and volume mount location are configureable? Not sure yet what the best fix is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/472#issuecomment-252058323
https://github.com/broadinstitute/cromwell/issues/472#issuecomment-273559606:12,Usability,simpl,simple,12,"@mcovarr: A simple print of the help text from any of our Docker images would fit the bill:; https://hub.docker.com/r/broadinstitute/viral-ngs/tags/. A relevant command would be: `taxon_filter.py --help` (should exit 0):. ```; task dockerTest {; command {; taxon_filter.py --help; }. runtime {; memory: ""8GB""; docker: ""broadinstitute/viral-ngs""; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/472#issuecomment-273559606
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:62,Availability,failure,failure,62,"tl;dr I'd like to squash / rebase / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:104,Availability,failure,failure,104,"tl;dr I'd like to squash / rebase / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:265,Availability,failure,failure,265,"tl;dr I'd like to squash / rebase / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:504,Integrability,message,message,504,"tl;dr I'd like to squash / rebase / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:589,Integrability,message,message,589,"tl;dr I'd like to squash / rebase / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:814,Integrability,message,messages,814,"tl;dr I'd like to squash / rebase / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:856,Integrability,message,message,856,"tl;dr I'd like to squash / rebase / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:2046,Performance,concurren,concurrent,2046,"ystem)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ```. This quickly falls afoul of a uniqueness constraint:. ```; [ERROR] [03/03/2016 23:43:32.236] [ForkJoinPool-3-worker-1] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: Could not persist runtime attributes; java.sql.SQLIntegrityConstraintVi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:899,Safety,timeout,timeout,899,"tl;dr I'd like to squash / rebase / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:3039,Security,integrity,integrity,3039,"-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ```. This quickly falls afoul of a uniqueness constraint:. ```; [ERROR] [03/03/2016 23:43:32.236] [ForkJoinPool-3-worker-1] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: Could not persist runtime attributes; java.sql.SQLIntegrityConstraintViolationException: integrity constraint violation: unique constraint or index violation; UK_RUNTIME_ATTRIBUTE table: RUNTIME_ATTRIBUTES; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source); ```. From that point on it's basically a trainwreck of tests cross-talking.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:57,Testability,test,test,57,"tl;dr I'd like to squash / rebase / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:286,Testability,test,test,286,"tl;dr I'd like to squash / rebase / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:710,Testability,test,test,710,"tl;dr I'd like to squash / rebase / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:810,Testability,log,log,810,"tl;dr I'd like to squash / rebase / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:852,Testability,log,log,852,"tl;dr I'd like to squash / rebase / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:948,Testability,test,test-system-akka,948,"tl;dr I'd like to squash / rebase / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpret",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:1015,Testability,test,test-system,1015," / merge this despite a test failure during one run since I think that failure was due to unrelated Docker pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart,",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:1132,Testability,test,test-system-akka,1132,"er pull issues. So one build for this branch failed:. https://travis-ci.org/broadinstitute/cromwell/builds/113532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:1185,Testability,test,test-system,1185,"532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:1266,Testability,test,test-system,1266,"532462. The first failure was a docker test, and looking at this more closely something seems to have gone awry pulling the Docker image. Our build scripts should pre-pull `ubuntu:latest` and normally this takes about 10 seconds and produces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:1479,Testability,test,test-system-akka,1479,"uces a nice success message. In this run the Docker image pull took more than 43 seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] In",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:1547,Testability,test,test-system,1547," seconds and the success message appears to be cut off:. ```; Pulling repository docker.io/library/ubuntu; age for ubuntu:latest; ```. The Docker test looks like it's going fine until it's time to actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:1731,Testability,test,test-system,1731," actually run a call, at which point there are no log messages for 16 seconds, and when the log message does arrive it seems to indicate a timeout:. ```; [INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ``",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:1885,Testability,test,test,1885,"INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ```. This quickly falls afoul of a uniqueness constraint:. ```; [ERROR] [03/03/2016 23:43:32.236] [ForkJoinPool-3-worker-1] [WorkflowActor(akka://test-system)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:2149,Testability,test,test-system,2149,"spatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ```. This quickly falls afoul of a uniqueness constraint:. ```; [ERROR] [03/03/2016 23:43:32.236] [ForkJoinPool-3-worker-1] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: Could not persist runtime attributes; java.sql.SQLIntegrityConstraintViolationException: integrity constraint violation: unique constraint or index violation; UK_RUNTIME_ATTRIBUTE table: RUNTIME_ATTRIBUTES; at org.h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:2289,Testability,test,test-system,2289,"ray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ```. This quickly falls afoul of a uniqueness constraint:. ```; [ERROR] [03/03/2016 23:43:32.236] [ForkJoinPool-3-worker-1] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: Could not persist runtime attributes; java.sql.SQLIntegrityConstraintViolationException: integrity constraint violation: unique constraint or index violation; UK_RUNTIME_ATTRIBUTE table: RUNTIME_ATTRIBUTES; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown So",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:2461,Testability,test,test-system,2461,"tem-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ```. This quickly falls afoul of a uniqueness constraint:. ```; [ERROR] [03/03/2016 23:43:32.236] [ForkJoinPool-3-worker-1] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: Could not persist runtime attributes; java.sql.SQLIntegrityConstraintViolationException: integrity constraint violation: unique constraint or index violation; UK_RUNTIME_ATTRIBUTE table: RUNTIME_ATTRIBUTES; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source); ```. From that point on it's basically a trainwreck of tests cross-talkin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:2592,Testability,test,test-system,2592,"-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ```. This quickly falls afoul of a uniqueness constraint:. ```; [ERROR] [03/03/2016 23:43:32.236] [ForkJoinPool-3-worker-1] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: Could not persist runtime attributes; java.sql.SQLIntegrityConstraintViolationException: integrity constraint violation: unique constraint or index violation; UK_RUNTIME_ATTRIBUTE table: RUNTIME_ATTRIBUTES; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source); ```. From that point on it's basically a trainwreck of tests cross-talking.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:2904,Testability,test,test-system,2904,"-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ```. This quickly falls afoul of a uniqueness constraint:. ```; [ERROR] [03/03/2016 23:43:32.236] [ForkJoinPool-3-worker-1] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: Could not persist runtime attributes; java.sql.SQLIntegrityConstraintViolationException: integrity constraint violation: unique constraint or index violation; UK_RUNTIME_ATTRIBUTE table: RUNTIME_ATTRIBUTES; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source); ```. From that point on it's basically a trainwreck of tests cross-talking.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:3470,Testability,test,tests,3470,"-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ```. This quickly falls afoul of a uniqueness constraint:. ```; [ERROR] [03/03/2016 23:43:32.236] [ForkJoinPool-3-worker-1] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: Could not persist runtime attributes; java.sql.SQLIntegrityConstraintViolationException: integrity constraint violation: unique constraint or index violation; UK_RUNTIME_ATTRIBUTE table: RUNTIME_ATTRIBUTES; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source); at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source); ```. From that point on it's basically a trainwreck of tests cross-talking.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:1952,Usability,pause,paused,1952,"INFO] [03/03/2016 23:43:02.128] [test-system-akka.actor.default-dispatcher-2] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ```. This quickly falls afoul of a uniqueness constraint:. ```; [ERROR] [03/03/2016 23:43:32.236] [ForkJoinPool-3-worker-1] [WorkflowActor(akka://test-system)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344:1973,Usability,pause,paused,1973,"ystem)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Starting.; [WARN] [03/03/2016 23:43:18.664] [test-system-akka.actor.default-dispatcher-4] [akka://test-system/system/IO-TCP/selectors/$a/1] received dead letter from Actor[akka://test-system/user/IO-HTTP/group-0/1#-1001288108]: Write(ByteString(),spray.io.SslTlsSupport$WriteChunkAck$@22a4ed01); ```. There's another 13 second hang shortly thereafter:. ```; [INFO] [03/03/2016 23:43:19.002] [test-system-akka.actor.default-dispatcher-10] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: persisting status of a to Running.; [INFO] [03/03/2016 23:43:32.134] [pool-7-thread-13-ScalaTest-running-CallCachingWorkflowSpec] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = None, effective id = c21e652b-b5f0-4435-a390-b1d61d1c9b4a; ```. Next it looks like a test is started up while pointing to the same in-memory db as this paused workflow. The paused workflow is interpreted as a workflow needing restart, so another concurrent copy is launched. . ```; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Found 1 workflow to restart.; [INFO] [03/03/2016 23:43:32.139] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor Restarting workflow ID: 299b2fc4-6a26-462f-96e3-1281f172d197; [INFO] [03/03/2016 23:43:32.152] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] Invoking restartableWorkflow on 299b2fc4; [INFO] [03/03/2016 23:43:32.153] [ForkJoinPool-3-worker-3] [akka://test-system/user/$$h] WorkflowManagerActor submitWorkflow input id = Some(299b2fc4-6a26-462f-96e3-1281f172d197), effective id = 299b2fc4-6a26-462f-96e3-1281f172d197; ```. This quickly falls afoul of a uniqueness constraint:. ```; [ERROR] [03/03/2016 23:43:32.236] [ForkJoinPool-3-worker-1] [WorkflowActor(akka://test-system)] WorkflowActor [UUID(299b2fc4)]: Could not persist runtime attributes; java.sql.SQLIntegrityConstraintVi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-192361344
https://github.com/broadinstitute/cromwell/pull/505#issuecomment-194339115:109,Energy Efficiency,schedul,schedule,109,@cjllanwarne I asked @Horneth if we could use this during one of the upcoming lunch & learns which i need to schedule,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/505#issuecomment-194339115
https://github.com/broadinstitute/cromwell/pull/505#issuecomment-194339115:86,Usability,learn,learns,86,@cjllanwarne I asked @Horneth if we could use this during one of the upcoming lunch & learns which i need to schedule,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/505#issuecomment-194339115
https://github.com/broadinstitute/cromwell/pull/528#issuecomment-194307168:37,Usability,simpl,simplicity,37,WHEEL: @geoffjentry @mcovarr ... for simplicity can you also review the WDL4s and Tyburn PRs? They're short!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/528#issuecomment-194307168
https://github.com/broadinstitute/cromwell/issues/564#issuecomment-197299959:463,Integrability,wrap,wrap,463,:+1: a bunch. It didn't take long to make me question why we tried to resolve a problem which involved doing too much stuff up front by adding even more stuff up front. . I was talking about something a lot like this to one of our two main internal customers (I believe FC) and they were good with the idea - however I'd say that we should make sure to have official buy in from our internal policy makers just to be sure. The dept itself seems to be learning to wrap its collective head around models where not everything is immediate & 100% consistent fashion so it should be an easy sell now.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/564#issuecomment-197299959
https://github.com/broadinstitute/cromwell/issues/564#issuecomment-197299959:451,Usability,learn,learning,451,:+1: a bunch. It didn't take long to make me question why we tried to resolve a problem which involved doing too much stuff up front by adding even more stuff up front. . I was talking about something a lot like this to one of our two main internal customers (I believe FC) and they were good with the idea - however I'd say that we should make sure to have official buy in from our internal policy makers just to be sure. The dept itself seems to be learning to wrap its collective head around models where not everything is immediate & 100% consistent fashion so it should be an easy sell now.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/564#issuecomment-197299959
https://github.com/broadinstitute/cromwell/issues/569#issuecomment-197505173:385,Security,validat,validation,385,"I'd agree with that. I've always felt that the VA should only return a yes/no, although perhaps my issue is being overly pedantic with the name of the actor. The argument for this though was that because people (including yourself, IIRC) didn't want to receive the simple yes/no and thus replicate the parsing of the input files the VA was handing back the parsed components. Once the validation is taken out of WorkflowDescriptor's apply() method it becomes a simple case class constructed by those same values so the effect is the same.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/569#issuecomment-197505173
https://github.com/broadinstitute/cromwell/issues/569#issuecomment-197505173:265,Usability,simpl,simple,265,"I'd agree with that. I've always felt that the VA should only return a yes/no, although perhaps my issue is being overly pedantic with the name of the actor. The argument for this though was that because people (including yourself, IIRC) didn't want to receive the simple yes/no and thus replicate the parsing of the input files the VA was handing back the parsed components. Once the validation is taken out of WorkflowDescriptor's apply() method it becomes a simple case class constructed by those same values so the effect is the same.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/569#issuecomment-197505173
https://github.com/broadinstitute/cromwell/issues/569#issuecomment-197505173:461,Usability,simpl,simple,461,"I'd agree with that. I've always felt that the VA should only return a yes/no, although perhaps my issue is being overly pedantic with the name of the actor. The argument for this though was that because people (including yourself, IIRC) didn't want to receive the simple yes/no and thus replicate the parsing of the input files the VA was handing back the parsed components. Once the validation is taken out of WorkflowDescriptor's apply() method it becomes a simple case class constructed by those same values so the effect is the same.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/569#issuecomment-197505173
https://github.com/broadinstitute/cromwell/issues/581#issuecomment-197643112:1007,Integrability,message,message,1007,"Full sketch of the idea as it applied to the pluggable_backends branch below. This particular ticket is concerned only with moving the DB code and having the engine be able to work with backends to determine which calls are resumable. Those might really be two separate pieces that two people could work in parallel. Cromwell would need to wake up and scan its database for Running workflows with Running calls. Something in Cromwell would then need to classify calls into resumable or not resumable. e.g. for JES, figure out if Cromwell has a JES Run ID that could be used to resume polling an already-launched JES run. Only the JES backend would know how to make this determination, but backends don’t have access to the database. So Cromwell would need to gather up representations of possibly resumable executions, examine on which backend type the executions had been running, create CallActors for each execution using a specified backend type (not a currently supported use case), and send a Restart message parameterized by the representation of the execution. The CallActor would need to create a backend of the specified type and then ask that backend if the execution is resumable. Resumable executions would result in a Resumed message making its way back to the WorkflowActor, otherwise WorkflowActor would get a NotResumable message. For NotResumable executions the WorkflowActor should be free to choose whatever backend it pleases to restart the call and shouldn’t necessarily be bound by the backend type that was chosen for the previous attempt.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/581#issuecomment-197643112
https://github.com/broadinstitute/cromwell/issues/581#issuecomment-197643112:1240,Integrability,message,message,1240,"Full sketch of the idea as it applied to the pluggable_backends branch below. This particular ticket is concerned only with moving the DB code and having the engine be able to work with backends to determine which calls are resumable. Those might really be two separate pieces that two people could work in parallel. Cromwell would need to wake up and scan its database for Running workflows with Running calls. Something in Cromwell would then need to classify calls into resumable or not resumable. e.g. for JES, figure out if Cromwell has a JES Run ID that could be used to resume polling an already-launched JES run. Only the JES backend would know how to make this determination, but backends don’t have access to the database. So Cromwell would need to gather up representations of possibly resumable executions, examine on which backend type the executions had been running, create CallActors for each execution using a specified backend type (not a currently supported use case), and send a Restart message parameterized by the representation of the execution. The CallActor would need to create a backend of the specified type and then ask that backend if the execution is resumable. Resumable executions would result in a Resumed message making its way back to the WorkflowActor, otherwise WorkflowActor would get a NotResumable message. For NotResumable executions the WorkflowActor should be free to choose whatever backend it pleases to restart the call and shouldn’t necessarily be bound by the backend type that was chosen for the previous attempt.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/581#issuecomment-197643112
https://github.com/broadinstitute/cromwell/issues/581#issuecomment-197643112:1339,Integrability,message,message,1339,"Full sketch of the idea as it applied to the pluggable_backends branch below. This particular ticket is concerned only with moving the DB code and having the engine be able to work with backends to determine which calls are resumable. Those might really be two separate pieces that two people could work in parallel. Cromwell would need to wake up and scan its database for Running workflows with Running calls. Something in Cromwell would then need to classify calls into resumable or not resumable. e.g. for JES, figure out if Cromwell has a JES Run ID that could be used to resume polling an already-launched JES run. Only the JES backend would know how to make this determination, but backends don’t have access to the database. So Cromwell would need to gather up representations of possibly resumable executions, examine on which backend type the executions had been running, create CallActors for each execution using a specified backend type (not a currently supported use case), and send a Restart message parameterized by the representation of the execution. The CallActor would need to create a backend of the specified type and then ask that backend if the execution is resumable. Resumable executions would result in a Resumed message making its way back to the WorkflowActor, otherwise WorkflowActor would get a NotResumable message. For NotResumable executions the WorkflowActor should be free to choose whatever backend it pleases to restart the call and shouldn’t necessarily be bound by the backend type that was chosen for the previous attempt.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/581#issuecomment-197643112
https://github.com/broadinstitute/cromwell/issues/581#issuecomment-197643112:1015,Modifiability,parameteriz,parameterized,1015,"Full sketch of the idea as it applied to the pluggable_backends branch below. This particular ticket is concerned only with moving the DB code and having the engine be able to work with backends to determine which calls are resumable. Those might really be two separate pieces that two people could work in parallel. Cromwell would need to wake up and scan its database for Running workflows with Running calls. Something in Cromwell would then need to classify calls into resumable or not resumable. e.g. for JES, figure out if Cromwell has a JES Run ID that could be used to resume polling an already-launched JES run. Only the JES backend would know how to make this determination, but backends don’t have access to the database. So Cromwell would need to gather up representations of possibly resumable executions, examine on which backend type the executions had been running, create CallActors for each execution using a specified backend type (not a currently supported use case), and send a Restart message parameterized by the representation of the execution. The CallActor would need to create a backend of the specified type and then ask that backend if the execution is resumable. Resumable executions would result in a Resumed message making its way back to the WorkflowActor, otherwise WorkflowActor would get a NotResumable message. For NotResumable executions the WorkflowActor should be free to choose whatever backend it pleases to restart the call and shouldn’t necessarily be bound by the backend type that was chosen for the previous attempt.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/581#issuecomment-197643112
https://github.com/broadinstitute/cromwell/issues/581#issuecomment-197643112:709,Security,access,access,709,"Full sketch of the idea as it applied to the pluggable_backends branch below. This particular ticket is concerned only with moving the DB code and having the engine be able to work with backends to determine which calls are resumable. Those might really be two separate pieces that two people could work in parallel. Cromwell would need to wake up and scan its database for Running workflows with Running calls. Something in Cromwell would then need to classify calls into resumable or not resumable. e.g. for JES, figure out if Cromwell has a JES Run ID that could be used to resume polling an already-launched JES run. Only the JES backend would know how to make this determination, but backends don’t have access to the database. So Cromwell would need to gather up representations of possibly resumable executions, examine on which backend type the executions had been running, create CallActors for each execution using a specified backend type (not a currently supported use case), and send a Restart message parameterized by the representation of the execution. The CallActor would need to create a backend of the specified type and then ask that backend if the execution is resumable. Resumable executions would result in a Resumed message making its way back to the WorkflowActor, otherwise WorkflowActor would get a NotResumable message. For NotResumable executions the WorkflowActor should be free to choose whatever backend it pleases to restart the call and shouldn’t necessarily be bound by the backend type that was chosen for the previous attempt.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/581#issuecomment-197643112
https://github.com/broadinstitute/cromwell/issues/581#issuecomment-197643112:577,Usability,resume,resume,577,"Full sketch of the idea as it applied to the pluggable_backends branch below. This particular ticket is concerned only with moving the DB code and having the engine be able to work with backends to determine which calls are resumable. Those might really be two separate pieces that two people could work in parallel. Cromwell would need to wake up and scan its database for Running workflows with Running calls. Something in Cromwell would then need to classify calls into resumable or not resumable. e.g. for JES, figure out if Cromwell has a JES Run ID that could be used to resume polling an already-launched JES run. Only the JES backend would know how to make this determination, but backends don’t have access to the database. So Cromwell would need to gather up representations of possibly resumable executions, examine on which backend type the executions had been running, create CallActors for each execution using a specified backend type (not a currently supported use case), and send a Restart message parameterized by the representation of the execution. The CallActor would need to create a backend of the specified type and then ask that backend if the execution is resumable. Resumable executions would result in a Resumed message making its way back to the WorkflowActor, otherwise WorkflowActor would get a NotResumable message. For NotResumable executions the WorkflowActor should be free to choose whatever backend it pleases to restart the call and shouldn’t necessarily be bound by the backend type that was chosen for the previous attempt.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/581#issuecomment-197643112
https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198424257:45,Safety,abort,abort,45,"To be clear, fail-fast should not attempt to abort currently running calls even if those calls might otherwise continue to run for a long time?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198424257
https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198424257:6,Usability,clear,clear,6,"To be clear, fail-fast should not attempt to abort currently running calls even if those calls might otherwise continue to run for a long time?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198424257
https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198425156:249,Safety,abort,abort,249,"Yes that is correct. A future feature could be ""fail immediate"" which would terminate running jobs, but that isn't this one. > On Mar 18, 2016, at 11:55 AM, mcovarr notifications@github.com wrote:; > ; > To be clear, fail-fast should not attempt to abort currently running calls even if those calls might otherwise continue to run for a long time?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly or view it on GitHub",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198425156
https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198425156:210,Usability,clear,clear,210,"Yes that is correct. A future feature could be ""fail immediate"" which would terminate running jobs, but that isn't this one. > On Mar 18, 2016, at 11:55 AM, mcovarr notifications@github.com wrote:; > ; > To be clear, fail-fast should not attempt to abort currently running calls even if those calls might otherwise continue to run for a long time?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly or view it on GitHub",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587#issuecomment-198425156
https://github.com/broadinstitute/cromwell/issues/601#issuecomment-254317040:72,Availability,avail,available,72,"@kcibul @ruchim Could you opine (since ""Job Avoidance"" is certainly now available) what kind of behaviour we want if we ""clear up"" a call-cached task?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/601#issuecomment-254317040
https://github.com/broadinstitute/cromwell/issues/601#issuecomment-254317040:138,Performance,cache,cached,138,"@kcibul @ruchim Could you opine (since ""Job Avoidance"" is certainly now available) what kind of behaviour we want if we ""clear up"" a call-cached task?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/601#issuecomment-254317040
https://github.com/broadinstitute/cromwell/issues/601#issuecomment-254317040:121,Usability,clear,clear,121,"@kcibul @ruchim Could you opine (since ""Job Avoidance"" is certainly now available) what kind of behaviour we want if we ""clear up"" a call-cached task?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/601#issuecomment-254317040
https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203469472:136,Modifiability,config,config,136,"@Horneth @cjllanwarne sure we can make the choice of auths explicit in `genomics` and `filesystems`. I did want to keep it clear in the config which auth was for Cromwell and which was for the user so we didn't make it impossible to implement call log copying in FireCloud (in case someday we want to use that feature there). How about something like the following for FireCloud:. ``` hocon. // Same as the preceding FireCloud sample conf; google {; application-name = ""cromwell"". // There may be instances like the final call that copies call logs which will need to be able to generate both; // Cromwell and user authentication, so making these explicit.; cromwellAuthentication {; scheme = ""application_default""; }. // Used for engine functions involving the filesystem.; userAuthentication {; scheme = ""refresh""; client-id = ""secret_id""; client-secret = ""secret_secret""; }; }. // genomics with explicitly selected conf; genomics {; ...; auth = ""cromwell""; ...; }. ...; filesystems = [; // gcs filesystem with explicitly selected conf; gcs {; auth = ""user""; }; ]; ... ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203469472
https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203469472:615,Security,authenticat,authentication,615,"@Horneth @cjllanwarne sure we can make the choice of auths explicit in `genomics` and `filesystems`. I did want to keep it clear in the config which auth was for Cromwell and which was for the user so we didn't make it impossible to implement call log copying in FireCloud (in case someday we want to use that feature there). How about something like the following for FireCloud:. ``` hocon. // Same as the preceding FireCloud sample conf; google {; application-name = ""cromwell"". // There may be instances like the final call that copies call logs which will need to be able to generate both; // Cromwell and user authentication, so making these explicit.; cromwellAuthentication {; scheme = ""application_default""; }. // Used for engine functions involving the filesystem.; userAuthentication {; scheme = ""refresh""; client-id = ""secret_id""; client-secret = ""secret_secret""; }; }. // genomics with explicitly selected conf; genomics {; ...; auth = ""cromwell""; ...; }. ...; filesystems = [; // gcs filesystem with explicitly selected conf; gcs {; auth = ""user""; }; ]; ... ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203469472
https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203469472:248,Testability,log,log,248,"@Horneth @cjllanwarne sure we can make the choice of auths explicit in `genomics` and `filesystems`. I did want to keep it clear in the config which auth was for Cromwell and which was for the user so we didn't make it impossible to implement call log copying in FireCloud (in case someday we want to use that feature there). How about something like the following for FireCloud:. ``` hocon. // Same as the preceding FireCloud sample conf; google {; application-name = ""cromwell"". // There may be instances like the final call that copies call logs which will need to be able to generate both; // Cromwell and user authentication, so making these explicit.; cromwellAuthentication {; scheme = ""application_default""; }. // Used for engine functions involving the filesystem.; userAuthentication {; scheme = ""refresh""; client-id = ""secret_id""; client-secret = ""secret_secret""; }; }. // genomics with explicitly selected conf; genomics {; ...; auth = ""cromwell""; ...; }. ...; filesystems = [; // gcs filesystem with explicitly selected conf; gcs {; auth = ""user""; }; ]; ... ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203469472
https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203469472:544,Testability,log,logs,544,"@Horneth @cjllanwarne sure we can make the choice of auths explicit in `genomics` and `filesystems`. I did want to keep it clear in the config which auth was for Cromwell and which was for the user so we didn't make it impossible to implement call log copying in FireCloud (in case someday we want to use that feature there). How about something like the following for FireCloud:. ``` hocon. // Same as the preceding FireCloud sample conf; google {; application-name = ""cromwell"". // There may be instances like the final call that copies call logs which will need to be able to generate both; // Cromwell and user authentication, so making these explicit.; cromwellAuthentication {; scheme = ""application_default""; }. // Used for engine functions involving the filesystem.; userAuthentication {; scheme = ""refresh""; client-id = ""secret_id""; client-secret = ""secret_secret""; }; }. // genomics with explicitly selected conf; genomics {; ...; auth = ""cromwell""; ...; }. ...; filesystems = [; // gcs filesystem with explicitly selected conf; gcs {; auth = ""user""; }; ]; ... ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203469472
https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203469472:123,Usability,clear,clear,123,"@Horneth @cjllanwarne sure we can make the choice of auths explicit in `genomics` and `filesystems`. I did want to keep it clear in the config which auth was for Cromwell and which was for the user so we didn't make it impossible to implement call log copying in FireCloud (in case someday we want to use that feature there). How about something like the following for FireCloud:. ``` hocon. // Same as the preceding FireCloud sample conf; google {; application-name = ""cromwell"". // There may be instances like the final call that copies call logs which will need to be able to generate both; // Cromwell and user authentication, so making these explicit.; cromwellAuthentication {; scheme = ""application_default""; }. // Used for engine functions involving the filesystem.; userAuthentication {; scheme = ""refresh""; client-id = ""secret_id""; client-secret = ""secret_secret""; }; }. // genomics with explicitly selected conf; genomics {; ...; auth = ""cromwell""; ...; }. ...; filesystems = [; // gcs filesystem with explicitly selected conf; gcs {; auth = ""user""; }; ]; ... ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203469472
https://github.com/broadinstitute/cromwell/issues/649#issuecomment-209963451:56,Usability,clear,clear,56,@francares Now that this is back in ready column can we clear the assignment?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/649#issuecomment-209963451
https://github.com/broadinstitute/cromwell/issues/649#issuecomment-209990438:176,Usability,clear,clear,176,"Yes, we can.; On Apr 14, 2016 7:14 AM, ""Jeff Gentry"" notifications@github.com wrote:. > @francares https://github.com/francares Now that this is back in ready; > column can we clear the assignment?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/649#issuecomment-209963451",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/649#issuecomment-209990438
https://github.com/broadinstitute/cromwell/issues/652#issuecomment-212055939:116,Usability,simpl,simplistically,116,"Alright. So I'll base this off of #717, and this PR will target to run (hopefully) n number of calls in a workflow (simplistically, a three step workflow should be able to go through completion)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/652#issuecomment-212055939
https://github.com/broadinstitute/cromwell/pull/686#issuecomment-207555857:4,Usability,simpl,simplicity,4,"For simplicity, I'm making my two changes into a single PR - they're more entwined than I realised. Please see #688 instead!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/686#issuecomment-207555857
https://github.com/broadinstitute/cromwell/pull/688#issuecomment-208437456:46,Testability,test,tests,46,Is there any way to get even just some simple tests for `ShadowWorkflowActor`? I feel like there are some things in `ShadowWorkflowActor` that could be restructured for better testing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-208437456
https://github.com/broadinstitute/cromwell/pull/688#issuecomment-208437456:176,Testability,test,testing,176,Is there any way to get even just some simple tests for `ShadowWorkflowActor`? I feel like there are some things in `ShadowWorkflowActor` that could be restructured for better testing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-208437456
https://github.com/broadinstitute/cromwell/pull/688#issuecomment-208437456:39,Usability,simpl,simple,39,Is there any way to get even just some simple tests for `ShadowWorkflowActor`? I feel like there are some things in `ShadowWorkflowActor` that could be restructured for better testing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-208437456
https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209002741:63,Usability,clear,clear,63,:+1: for this iteration. There are some topics that are not so clear at this moment (such as sharing information between beforeAll->execute->afterAll) but I think we can move forward with this. It might suffer changes in the future but it's a starting point. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/688/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/688#issuecomment-209002741
https://github.com/broadinstitute/cromwell/pull/690#issuecomment-208375082:31,Modifiability,refactor,refactoring,31,"No current opinions on further refactoring DataAccess, as those discussions are outside of the parent PR, though happy participate in a tech talk if desired. I will admit I ""started it"" by embellishing beyond simply inverting the futures within `runTransactionWithRollbackRetry`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/690#issuecomment-208375082
https://github.com/broadinstitute/cromwell/pull/690#issuecomment-208375082:209,Usability,simpl,simply,209,"No current opinions on further refactoring DataAccess, as those discussions are outside of the parent PR, though happy participate in a tech talk if desired. I will admit I ""started it"" by embellishing beyond simply inverting the futures within `runTransactionWithRollbackRetry`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/690#issuecomment-208375082
https://github.com/broadinstitute/cromwell/pull/694#issuecomment-210003861:252,Deployability,patch,patches,252,"Let's chat IRL later ~~today~~. To be clear, I'm totally fine mothballing this PR. In the context of PBE, conceivably this ticket was introduced so one could implement other final calls that run on a backend?. EDIT: No rush on this ticket. Making some patches, but will chat about next steps some other time when we can.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/694#issuecomment-210003861
https://github.com/broadinstitute/cromwell/pull/694#issuecomment-210003861:38,Usability,clear,clear,38,"Let's chat IRL later ~~today~~. To be clear, I'm totally fine mothballing this PR. In the context of PBE, conceivably this ticket was introduced so one could implement other final calls that run on a backend?. EDIT: No rush on this ticket. Making some patches, but will chat about next steps some other time when we can.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/694#issuecomment-210003861
https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210606785:51,Testability,test,test,51,"Seems that it's not (easily) possible to correctly test restarts the way the code is structured now. . I need to be able to pass in a modified `LocalBackend` which overrides `isResumable`, and `isRestartable`, and `resume`. The problem is that `WorkflowDescriptor` has a reference to the `Backend` but also the `MaterializedWorkflowDescriptorActor` reaches into the global map of backends too... I had a very hard time constructing a custom `LocalBackend` that could be used for a workflow submission. So I ended up adding a happy-path test but I'll wait until the shadow world emerges before adding other tests I guess...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210606785
https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210606785:536,Testability,test,test,536,"Seems that it's not (easily) possible to correctly test restarts the way the code is structured now. . I need to be able to pass in a modified `LocalBackend` which overrides `isResumable`, and `isRestartable`, and `resume`. The problem is that `WorkflowDescriptor` has a reference to the `Backend` but also the `MaterializedWorkflowDescriptorActor` reaches into the global map of backends too... I had a very hard time constructing a custom `LocalBackend` that could be used for a workflow submission. So I ended up adding a happy-path test but I'll wait until the shadow world emerges before adding other tests I guess...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210606785
https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210606785:606,Testability,test,tests,606,"Seems that it's not (easily) possible to correctly test restarts the way the code is structured now. . I need to be able to pass in a modified `LocalBackend` which overrides `isResumable`, and `isRestartable`, and `resume`. The problem is that `WorkflowDescriptor` has a reference to the `Backend` but also the `MaterializedWorkflowDescriptorActor` reaches into the global map of backends too... I had a very hard time constructing a custom `LocalBackend` that could be used for a workflow submission. So I ended up adding a happy-path test but I'll wait until the shadow world emerges before adding other tests I guess...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210606785
https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210606785:215,Usability,resume,resume,215,"Seems that it's not (easily) possible to correctly test restarts the way the code is structured now. . I need to be able to pass in a modified `LocalBackend` which overrides `isResumable`, and `isRestartable`, and `resume`. The problem is that `WorkflowDescriptor` has a reference to the `Backend` but also the `MaterializedWorkflowDescriptorActor` reaches into the global map of backends too... I had a very hard time constructing a custom `LocalBackend` that could be used for a workflow submission. So I ended up adding a happy-path test but I'll wait until the shadow world emerges before adding other tests I guess...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210606785
https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214527720:316,Energy Efficiency,adapt,adapter,316,"I think technically these could be collapsed into one class, but from your description maybe it's preferable to keep this structure with the roles of the classes more clearly articulated? The outer actor has the responsibility of implementing the `BackendJobExecutionActor` trait, but can be implemented as a simple adapter to any backend-specific means of executing jobs, which here just happens to be an FSM. Not sure what the best nomenclature would be for this distinction.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214527720
https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214527720:316,Integrability,adapter,adapter,316,"I think technically these could be collapsed into one class, but from your description maybe it's preferable to keep this structure with the roles of the classes more clearly articulated? The outer actor has the responsibility of implementing the `BackendJobExecutionActor` trait, but can be implemented as a simple adapter to any backend-specific means of executing jobs, which here just happens to be an FSM. Not sure what the best nomenclature would be for this distinction.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214527720
https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214527720:316,Modifiability,adapt,adapter,316,"I think technically these could be collapsed into one class, but from your description maybe it's preferable to keep this structure with the roles of the classes more clearly articulated? The outer actor has the responsibility of implementing the `BackendJobExecutionActor` trait, but can be implemented as a simple adapter to any backend-specific means of executing jobs, which here just happens to be an FSM. Not sure what the best nomenclature would be for this distinction.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214527720
https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214527720:167,Usability,clear,clearly,167,"I think technically these could be collapsed into one class, but from your description maybe it's preferable to keep this structure with the roles of the classes more clearly articulated? The outer actor has the responsibility of implementing the `BackendJobExecutionActor` trait, but can be implemented as a simple adapter to any backend-specific means of executing jobs, which here just happens to be an FSM. Not sure what the best nomenclature would be for this distinction.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214527720
https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214527720:309,Usability,simpl,simple,309,"I think technically these could be collapsed into one class, but from your description maybe it's preferable to keep this structure with the roles of the classes more clearly articulated? The outer actor has the responsibility of implementing the `BackendJobExecutionActor` trait, but can be implemented as a simple adapter to any backend-specific means of executing jobs, which here just happens to be an FSM. Not sure what the best nomenclature would be for this distinction.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214527720
https://github.com/broadinstitute/cromwell/pull/743#issuecomment-215708494:347,Usability,simpl,simple,347,"@gauravs90 I'm :-1: on the actual graph implementation. If we were to stick with this scheme I'd want something much different. In terms of the scheme, I'm ok with the general concept (it's not unlike what I wanted to do originally as it turns out) but I question the utility of making this change. The old findRunnableCalls scheme was relatively simple and did the job - changing that seems out of scope for this ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/743#issuecomment-215708494
https://github.com/broadinstitute/cromwell/issues/757#issuecomment-215479245:54,Availability,error,error,54,"According to the naming rules in the [link within the error message](https://cloud.google.com/compute/docs/reference/latest/disks#name) ""the first character must be a lowercase letter,"" which your disk name does not comply with. I'm not an expert with JES, but perhaps the fix is to simply change the disk name to follow that rule?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757#issuecomment-215479245
https://github.com/broadinstitute/cromwell/issues/757#issuecomment-215479245:60,Integrability,message,message,60,"According to the naming rules in the [link within the error message](https://cloud.google.com/compute/docs/reference/latest/disks#name) ""the first character must be a lowercase letter,"" which your disk name does not comply with. I'm not an expert with JES, but perhaps the fix is to simply change the disk name to follow that rule?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757#issuecomment-215479245
https://github.com/broadinstitute/cromwell/issues/757#issuecomment-215479245:283,Usability,simpl,simply,283,"According to the naming rules in the [link within the error message](https://cloud.google.com/compute/docs/reference/latest/disks#name) ""the first character must be a lowercase letter,"" which your disk name does not comply with. I'm not an expert with JES, but perhaps the fix is to simply change the disk name to follow that rule?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757#issuecomment-215479245
https://github.com/broadinstitute/cromwell/issues/775#issuecomment-254301199:118,Performance,scalab,scalability,118,Closing this one. We use the status endpoint in our scaling tests so we can now regression test the responsiveness vs scalability too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-254301199
https://github.com/broadinstitute/cromwell/issues/775#issuecomment-254301199:60,Testability,test,tests,60,Closing this one. We use the status endpoint in our scaling tests so we can now regression test the responsiveness vs scalability too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-254301199
https://github.com/broadinstitute/cromwell/issues/775#issuecomment-254301199:91,Testability,test,test,91,Closing this one. We use the status endpoint in our scaling tests so we can now regression test the responsiveness vs scalability too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-254301199
https://github.com/broadinstitute/cromwell/issues/775#issuecomment-254301199:100,Usability,responsiv,responsiveness,100,Closing this one. We use the status endpoint in our scaling tests so we can now regression test the responsiveness vs scalability too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-254301199
https://github.com/broadinstitute/cromwell/pull/797#issuecomment-217940619:23,Usability,learn,learnt,23,@mcovarr those lessons learnt look pretty valuable too! Is there any reason for not ticketing them too?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/797#issuecomment-217940619
https://github.com/broadinstitute/cromwell/issues/810#issuecomment-217979491:243,Deployability,hotfix,hotfix,243,Yeah currently there are 3 non-exponentially-backed-off retries with 500 ms pauses in GcsFileSystemProvider#crc32cHash. Given the comments in the vicinity I don't think there would be strong objections to increasing the number of retries in a hotfix. This should be implemented differently in PBEs so it's not Thread.sleeping.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/810#issuecomment-217979491
https://github.com/broadinstitute/cromwell/issues/810#issuecomment-217979491:76,Usability,pause,pauses,76,Yeah currently there are 3 non-exponentially-backed-off retries with 500 ms pauses in GcsFileSystemProvider#crc32cHash. Given the comments in the vicinity I don't think there would be strong objections to increasing the number of retries in a hotfix. This should be implemented differently in PBEs so it's not Thread.sleeping.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/810#issuecomment-217979491
https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678:267,Deployability,rolling,rolling,267,"@horneth - so I see 2 Futures in here. One the little bit changing the state (around the Props) and the other was resolveAndEvaluate. As background for my statement I'll say the following:; - We've already seen firsthand the havoc which can erupt from having Futures rolling around inside an Actor. They break the Actor Model's abstraction that the internals of an actor are single threaded, meaning you now have to reason about shared mutable state, etc. We _can_ do that, but there are easier paths than actors to deal with that. We've been better about this recently but my concern is that it's too easy for stuff like that to sneak into what were previously pure Futures. Mixing Futures & Actors is not really a great idea.; - There are two async operations in the actor, which means that it is certainly doing two different things (I'll admit that the creation of an actor is a fairly lame 'thing'), disrupting Akka's mantra that actors should do one thing only. What I was suggesting was that the work being performed by these Futures be themselves pushed to their own actors. When they complete they can message back to this one, and those messages could be use to manage state transitions and such. (and to be clear, this is _not_ our little 'tol' code phrase - it's something I think we need to be much better about as we refactor cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678
https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678:1111,Integrability,message,message,1111,"@horneth - so I see 2 Futures in here. One the little bit changing the state (around the Props) and the other was resolveAndEvaluate. As background for my statement I'll say the following:; - We've already seen firsthand the havoc which can erupt from having Futures rolling around inside an Actor. They break the Actor Model's abstraction that the internals of an actor are single threaded, meaning you now have to reason about shared mutable state, etc. We _can_ do that, but there are easier paths than actors to deal with that. We've been better about this recently but my concern is that it's too easy for stuff like that to sneak into what were previously pure Futures. Mixing Futures & Actors is not really a great idea.; - There are two async operations in the actor, which means that it is certainly doing two different things (I'll admit that the creation of an actor is a fairly lame 'thing'), disrupting Akka's mantra that actors should do one thing only. What I was suggesting was that the work being performed by these Futures be themselves pushed to their own actors. When they complete they can message back to this one, and those messages could be use to manage state transitions and such. (and to be clear, this is _not_ our little 'tol' code phrase - it's something I think we need to be much better about as we refactor cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678
https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678:1147,Integrability,message,messages,1147,"@horneth - so I see 2 Futures in here. One the little bit changing the state (around the Props) and the other was resolveAndEvaluate. As background for my statement I'll say the following:; - We've already seen firsthand the havoc which can erupt from having Futures rolling around inside an Actor. They break the Actor Model's abstraction that the internals of an actor are single threaded, meaning you now have to reason about shared mutable state, etc. We _can_ do that, but there are easier paths than actors to deal with that. We've been better about this recently but my concern is that it's too easy for stuff like that to sneak into what were previously pure Futures. Mixing Futures & Actors is not really a great idea.; - There are two async operations in the actor, which means that it is certainly doing two different things (I'll admit that the creation of an actor is a fairly lame 'thing'), disrupting Akka's mantra that actors should do one thing only. What I was suggesting was that the work being performed by these Futures be themselves pushed to their own actors. When they complete they can message back to this one, and those messages could be use to manage state transitions and such. (and to be clear, this is _not_ our little 'tol' code phrase - it's something I think we need to be much better about as we refactor cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678
https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678:1331,Modifiability,refactor,refactor,1331,"@horneth - so I see 2 Futures in here. One the little bit changing the state (around the Props) and the other was resolveAndEvaluate. As background for my statement I'll say the following:; - We've already seen firsthand the havoc which can erupt from having Futures rolling around inside an Actor. They break the Actor Model's abstraction that the internals of an actor are single threaded, meaning you now have to reason about shared mutable state, etc. We _can_ do that, but there are easier paths than actors to deal with that. We've been better about this recently but my concern is that it's too easy for stuff like that to sneak into what were previously pure Futures. Mixing Futures & Actors is not really a great idea.; - There are two async operations in the actor, which means that it is certainly doing two different things (I'll admit that the creation of an actor is a fairly lame 'thing'), disrupting Akka's mantra that actors should do one thing only. What I was suggesting was that the work being performed by these Futures be themselves pushed to their own actors. When they complete they can message back to this one, and those messages could be use to manage state transitions and such. (and to be clear, this is _not_ our little 'tol' code phrase - it's something I think we need to be much better about as we refactor cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678
https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678:1014,Performance,perform,performed,1014,"@horneth - so I see 2 Futures in here. One the little bit changing the state (around the Props) and the other was resolveAndEvaluate. As background for my statement I'll say the following:; - We've already seen firsthand the havoc which can erupt from having Futures rolling around inside an Actor. They break the Actor Model's abstraction that the internals of an actor are single threaded, meaning you now have to reason about shared mutable state, etc. We _can_ do that, but there are easier paths than actors to deal with that. We've been better about this recently but my concern is that it's too easy for stuff like that to sneak into what were previously pure Futures. Mixing Futures & Actors is not really a great idea.; - There are two async operations in the actor, which means that it is certainly doing two different things (I'll admit that the creation of an actor is a fairly lame 'thing'), disrupting Akka's mantra that actors should do one thing only. What I was suggesting was that the work being performed by these Futures be themselves pushed to their own actors. When they complete they can message back to this one, and those messages could be use to manage state transitions and such. (and to be clear, this is _not_ our little 'tol' code phrase - it's something I think we need to be much better about as we refactor cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678
https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678:1218,Usability,clear,clear,1218,"@horneth - so I see 2 Futures in here. One the little bit changing the state (around the Props) and the other was resolveAndEvaluate. As background for my statement I'll say the following:; - We've already seen firsthand the havoc which can erupt from having Futures rolling around inside an Actor. They break the Actor Model's abstraction that the internals of an actor are single threaded, meaning you now have to reason about shared mutable state, etc. We _can_ do that, but there are easier paths than actors to deal with that. We've been better about this recently but my concern is that it's too easy for stuff like that to sneak into what were previously pure Futures. Mixing Futures & Actors is not really a great idea.; - There are two async operations in the actor, which means that it is certainly doing two different things (I'll admit that the creation of an actor is a fairly lame 'thing'), disrupting Akka's mantra that actors should do one thing only. What I was suggesting was that the work being performed by these Futures be themselves pushed to their own actors. When they complete they can message back to this one, and those messages could be use to manage state transitions and such. (and to be clear, this is _not_ our little 'tol' code phrase - it's something I think we need to be much better about as we refactor cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678
https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218610073:381,Availability,failure,failure,381,"@mcovarr And originally @Horneth had one fewer Future, listen to your own (well, my own) advice ;). Ok - so now that I have a chance to look at this more closely, it's unclear why there are any futures at all going on here. If I'm reading things right (as always, a big if) only one thing ever messages it (once) and then waits for a response. That response is either a success or failure. Why not just do the stuff it needs to do in the event loop (as nothing should be messaging it anyways)? The one argument I can come up with is that this would tie tie up one of the actorsystem dispatcher's threads but that's just as easily handled by giving this actor class its own dispatcher - that gets you the same effect as putting the Future in the global EC without all of the state changing and such. If you all want to make the claim that reasoning about the actor with futures kicking around inside is easier to reason about than multiple actors (a claim I vehemently disagree with), I'd put forth that the futures/states are themselves far more difficult to reason about than just simply doing the work straight up considering how simple this is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218610073
https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218610073:294,Integrability,message,messages,294,"@mcovarr And originally @Horneth had one fewer Future, listen to your own (well, my own) advice ;). Ok - so now that I have a chance to look at this more closely, it's unclear why there are any futures at all going on here. If I'm reading things right (as always, a big if) only one thing ever messages it (once) and then waits for a response. That response is either a success or failure. Why not just do the stuff it needs to do in the event loop (as nothing should be messaging it anyways)? The one argument I can come up with is that this would tie tie up one of the actorsystem dispatcher's threads but that's just as easily handled by giving this actor class its own dispatcher - that gets you the same effect as putting the Future in the global EC without all of the state changing and such. If you all want to make the claim that reasoning about the actor with futures kicking around inside is easier to reason about than multiple actors (a claim I vehemently disagree with), I'd put forth that the futures/states are themselves far more difficult to reason about than just simply doing the work straight up considering how simple this is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218610073
https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218610073:1082,Usability,simpl,simply,1082,"@mcovarr And originally @Horneth had one fewer Future, listen to your own (well, my own) advice ;). Ok - so now that I have a chance to look at this more closely, it's unclear why there are any futures at all going on here. If I'm reading things right (as always, a big if) only one thing ever messages it (once) and then waits for a response. That response is either a success or failure. Why not just do the stuff it needs to do in the event loop (as nothing should be messaging it anyways)? The one argument I can come up with is that this would tie tie up one of the actorsystem dispatcher's threads but that's just as easily handled by giving this actor class its own dispatcher - that gets you the same effect as putting the Future in the global EC without all of the state changing and such. If you all want to make the claim that reasoning about the actor with futures kicking around inside is easier to reason about than multiple actors (a claim I vehemently disagree with), I'd put forth that the futures/states are themselves far more difficult to reason about than just simply doing the work straight up considering how simple this is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218610073
https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218610073:1132,Usability,simpl,simple,1132,"@mcovarr And originally @Horneth had one fewer Future, listen to your own (well, my own) advice ;). Ok - so now that I have a chance to look at this more closely, it's unclear why there are any futures at all going on here. If I'm reading things right (as always, a big if) only one thing ever messages it (once) and then waits for a response. That response is either a success or failure. Why not just do the stuff it needs to do in the event loop (as nothing should be messaging it anyways)? The one argument I can come up with is that this would tie tie up one of the actorsystem dispatcher's threads but that's just as easily handled by giving this actor class its own dispatcher - that gets you the same effect as putting the Future in the global EC without all of the state changing and such. If you all want to make the claim that reasoning about the actor with futures kicking around inside is easier to reason about than multiple actors (a claim I vehemently disagree with), I'd put forth that the futures/states are themselves far more difficult to reason about than just simply doing the work straight up considering how simple this is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218610073
https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218612697:84,Usability,simpl,simplest,84,"Hmm yeah if we bulkhead the thread pool for this actor class, blocking might be the simplest thing to do.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218612697
https://github.com/broadinstitute/cromwell/pull/822#issuecomment-218866972:189,Integrability,inject,injection,189,"@gauravs90 I've generally seen the akka folks recommend directly passing references to actors which need to be used. That has multiple benefits (e.g. makes it easy to switch out and/or dep injection, etc). My off the cuff reaction is that that seems simpler to just pass the required reference around, although I'll admit I'm basing that purely on your description and not having looked at the changes yet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/822#issuecomment-218866972
https://github.com/broadinstitute/cromwell/pull/822#issuecomment-218866972:189,Security,inject,injection,189,"@gauravs90 I've generally seen the akka folks recommend directly passing references to actors which need to be used. That has multiple benefits (e.g. makes it easy to switch out and/or dep injection, etc). My off the cuff reaction is that that seems simpler to just pass the required reference around, although I'll admit I'm basing that purely on your description and not having looked at the changes yet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/822#issuecomment-218866972
https://github.com/broadinstitute/cromwell/pull/822#issuecomment-218866972:250,Usability,simpl,simpler,250,"@gauravs90 I've generally seen the akka folks recommend directly passing references to actors which need to be used. That has multiple benefits (e.g. makes it easy to switch out and/or dep injection, etc). My off the cuff reaction is that that seems simpler to just pass the required reference around, although I'll admit I'm basing that purely on your description and not having looked at the changes yet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/822#issuecomment-218866972
https://github.com/broadinstitute/cromwell/pull/823#issuecomment-218770232:31,Deployability,update,update,31,"@ruchim . I think it's fine to update the README even though it's not usable - the develop branch is known to be unstable at the moment, and isn't the default branch on github. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/823/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/823#issuecomment-218770232
https://github.com/broadinstitute/cromwell/pull/823#issuecomment-218770232:70,Usability,usab,usable,70,"@ruchim . I think it's fine to update the README even though it's not usable - the develop branch is known to be unstable at the moment, and isn't the default branch on github. 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/823/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/823#issuecomment-218770232
https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218895676:257,Integrability,inject,injection,257,"Quoting @geoffjentry from earlier comment so that it's not lost:. > @gauravs90 I've generally seen the akka folks recommend directly passing references to actors which need to be used. That has multiple benefits (e.g. makes it easy to switch out and/or dep injection, etc). My off the cuff reaction is that that seems simpler to just pass the required reference around, although I'll admit I'm basing that purely on your description and not having looked at the changes yet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218895676
https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218895676:257,Security,inject,injection,257,"Quoting @geoffjentry from earlier comment so that it's not lost:. > @gauravs90 I've generally seen the akka folks recommend directly passing references to actors which need to be used. That has multiple benefits (e.g. makes it easy to switch out and/or dep injection, etc). My off the cuff reaction is that that seems simpler to just pass the required reference around, although I'll admit I'm basing that purely on your description and not having looked at the changes yet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218895676
https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218895676:318,Usability,simpl,simpler,318,"Quoting @geoffjentry from earlier comment so that it's not lost:. > @gauravs90 I've generally seen the akka folks recommend directly passing references to actors which need to be used. That has multiple benefits (e.g. makes it easy to switch out and/or dep injection, etc). My off the cuff reaction is that that seems simpler to just pass the required reference around, although I'll admit I'm basing that purely on your description and not having looked at the changes yet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218895676
https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218898687:382,Availability,down,down,382,"@gauravs90 I don't doubt that passing the actorref around everywhere looked ugly. But in terms of moving parts and other such things my bet is that it's worth it. Your description of this scheme sounds a lot more complex (publishing, assuming everything going to metadata will always track w/ state change, having to shoehorn the data side in, etc) than simply passing the metadata down as is typically suggested.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218898687
https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218898687:354,Usability,simpl,simply,354,"@gauravs90 I don't doubt that passing the actorref around everywhere looked ugly. But in terms of moving parts and other such things my bet is that it's worth it. Your description of this scheme sounds a lot more complex (publishing, assuming everything going to metadata will always track w/ state change, having to shoehorn the data side in, etc) than simply passing the metadata down as is typically suggested.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218898687
https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121:119,Safety,avoid,avoidable,119,"@geoffjentry Verbosity is just a small part of that. Making different parts of the code aware of the each other, where avoidable, doesn't seem like a very good idea. If I'm not mistaken, what you're saying (and what I initially implemented) is something like the erstwhile `DataAccess` (which would be the `ServiceRegistoryActor` in the current world). With this, each step (actor) of the workflow had a reference to it and pushed to it independently. Any change to the dataAccess might have required changes to all the classes which were accessing it. Alternatively, if there was just one entity which handled the responsibility of collecting the metadata, by sniffing around the actors without their knowledge, and then pushed to the database, we need only change this entity for any modifications if there were to happen to the data access stuff. I'll try to explain with a very simple (and silly) analogy: (Honestly, couldn't come up with anything better.); Consider a ginormous Octopus (= `ServiceRegistry`) with a black ink on the tips of it's tentacles, with each of it's legs touching upon different rooms (= classes) in a house. If someday we decide to replace that octopus with something else, we'll be needing to wipe that ink from all the rooms upon which it was standing. On the other hand, if it were to sit and cuddle up just in a single room, there's simply less and comparatively easy work to do to wipe that up. It's simply the same idea here. The Metadata producing entities in the engine can just go about minding their own business, while a third party (those classes with some weird names currently [CromwellProfilerFsm and WorkflowProfilerActor]) handle what they are meant to do: Profile a given workflow execution. (all the while without explicitly telling those execution engine entities that it's reading it's state and data information). If the intentions are still not clear, let's talk about it tomorrow in the meeting to make progress with this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121
https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121:539,Security,access,accessing,539,"@geoffjentry Verbosity is just a small part of that. Making different parts of the code aware of the each other, where avoidable, doesn't seem like a very good idea. If I'm not mistaken, what you're saying (and what I initially implemented) is something like the erstwhile `DataAccess` (which would be the `ServiceRegistoryActor` in the current world). With this, each step (actor) of the workflow had a reference to it and pushed to it independently. Any change to the dataAccess might have required changes to all the classes which were accessing it. Alternatively, if there was just one entity which handled the responsibility of collecting the metadata, by sniffing around the actors without their knowledge, and then pushed to the database, we need only change this entity for any modifications if there were to happen to the data access stuff. I'll try to explain with a very simple (and silly) analogy: (Honestly, couldn't come up with anything better.); Consider a ginormous Octopus (= `ServiceRegistry`) with a black ink on the tips of it's tentacles, with each of it's legs touching upon different rooms (= classes) in a house. If someday we decide to replace that octopus with something else, we'll be needing to wipe that ink from all the rooms upon which it was standing. On the other hand, if it were to sit and cuddle up just in a single room, there's simply less and comparatively easy work to do to wipe that up. It's simply the same idea here. The Metadata producing entities in the engine can just go about minding their own business, while a third party (those classes with some weird names currently [CromwellProfilerFsm and WorkflowProfilerActor]) handle what they are meant to do: Profile a given workflow execution. (all the while without explicitly telling those execution engine entities that it's reading it's state and data information). If the intentions are still not clear, let's talk about it tomorrow in the meeting to make progress with this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121
https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121:836,Security,access,access,836,"@geoffjentry Verbosity is just a small part of that. Making different parts of the code aware of the each other, where avoidable, doesn't seem like a very good idea. If I'm not mistaken, what you're saying (and what I initially implemented) is something like the erstwhile `DataAccess` (which would be the `ServiceRegistoryActor` in the current world). With this, each step (actor) of the workflow had a reference to it and pushed to it independently. Any change to the dataAccess might have required changes to all the classes which were accessing it. Alternatively, if there was just one entity which handled the responsibility of collecting the metadata, by sniffing around the actors without their knowledge, and then pushed to the database, we need only change this entity for any modifications if there were to happen to the data access stuff. I'll try to explain with a very simple (and silly) analogy: (Honestly, couldn't come up with anything better.); Consider a ginormous Octopus (= `ServiceRegistry`) with a black ink on the tips of it's tentacles, with each of it's legs touching upon different rooms (= classes) in a house. If someday we decide to replace that octopus with something else, we'll be needing to wipe that ink from all the rooms upon which it was standing. On the other hand, if it were to sit and cuddle up just in a single room, there's simply less and comparatively easy work to do to wipe that up. It's simply the same idea here. The Metadata producing entities in the engine can just go about minding their own business, while a third party (those classes with some weird names currently [CromwellProfilerFsm and WorkflowProfilerActor]) handle what they are meant to do: Profile a given workflow execution. (all the while without explicitly telling those execution engine entities that it's reading it's state and data information). If the intentions are still not clear, let's talk about it tomorrow in the meeting to make progress with this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121
https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121:882,Usability,simpl,simple,882,"@geoffjentry Verbosity is just a small part of that. Making different parts of the code aware of the each other, where avoidable, doesn't seem like a very good idea. If I'm not mistaken, what you're saying (and what I initially implemented) is something like the erstwhile `DataAccess` (which would be the `ServiceRegistoryActor` in the current world). With this, each step (actor) of the workflow had a reference to it and pushed to it independently. Any change to the dataAccess might have required changes to all the classes which were accessing it. Alternatively, if there was just one entity which handled the responsibility of collecting the metadata, by sniffing around the actors without their knowledge, and then pushed to the database, we need only change this entity for any modifications if there were to happen to the data access stuff. I'll try to explain with a very simple (and silly) analogy: (Honestly, couldn't come up with anything better.); Consider a ginormous Octopus (= `ServiceRegistry`) with a black ink on the tips of it's tentacles, with each of it's legs touching upon different rooms (= classes) in a house. If someday we decide to replace that octopus with something else, we'll be needing to wipe that ink from all the rooms upon which it was standing. On the other hand, if it were to sit and cuddle up just in a single room, there's simply less and comparatively easy work to do to wipe that up. It's simply the same idea here. The Metadata producing entities in the engine can just go about minding their own business, while a third party (those classes with some weird names currently [CromwellProfilerFsm and WorkflowProfilerActor]) handle what they are meant to do: Profile a given workflow execution. (all the while without explicitly telling those execution engine entities that it's reading it's state and data information). If the intentions are still not clear, let's talk about it tomorrow in the meeting to make progress with this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121
https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121:1367,Usability,simpl,simply,1367,"@geoffjentry Verbosity is just a small part of that. Making different parts of the code aware of the each other, where avoidable, doesn't seem like a very good idea. If I'm not mistaken, what you're saying (and what I initially implemented) is something like the erstwhile `DataAccess` (which would be the `ServiceRegistoryActor` in the current world). With this, each step (actor) of the workflow had a reference to it and pushed to it independently. Any change to the dataAccess might have required changes to all the classes which were accessing it. Alternatively, if there was just one entity which handled the responsibility of collecting the metadata, by sniffing around the actors without their knowledge, and then pushed to the database, we need only change this entity for any modifications if there were to happen to the data access stuff. I'll try to explain with a very simple (and silly) analogy: (Honestly, couldn't come up with anything better.); Consider a ginormous Octopus (= `ServiceRegistry`) with a black ink on the tips of it's tentacles, with each of it's legs touching upon different rooms (= classes) in a house. If someday we decide to replace that octopus with something else, we'll be needing to wipe that ink from all the rooms upon which it was standing. On the other hand, if it were to sit and cuddle up just in a single room, there's simply less and comparatively easy work to do to wipe that up. It's simply the same idea here. The Metadata producing entities in the engine can just go about minding their own business, while a third party (those classes with some weird names currently [CromwellProfilerFsm and WorkflowProfilerActor]) handle what they are meant to do: Profile a given workflow execution. (all the while without explicitly telling those execution engine entities that it's reading it's state and data information). If the intentions are still not clear, let's talk about it tomorrow in the meeting to make progress with this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121
https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121:1435,Usability,simpl,simply,1435,"@geoffjentry Verbosity is just a small part of that. Making different parts of the code aware of the each other, where avoidable, doesn't seem like a very good idea. If I'm not mistaken, what you're saying (and what I initially implemented) is something like the erstwhile `DataAccess` (which would be the `ServiceRegistoryActor` in the current world). With this, each step (actor) of the workflow had a reference to it and pushed to it independently. Any change to the dataAccess might have required changes to all the classes which were accessing it. Alternatively, if there was just one entity which handled the responsibility of collecting the metadata, by sniffing around the actors without their knowledge, and then pushed to the database, we need only change this entity for any modifications if there were to happen to the data access stuff. I'll try to explain with a very simple (and silly) analogy: (Honestly, couldn't come up with anything better.); Consider a ginormous Octopus (= `ServiceRegistry`) with a black ink on the tips of it's tentacles, with each of it's legs touching upon different rooms (= classes) in a house. If someday we decide to replace that octopus with something else, we'll be needing to wipe that ink from all the rooms upon which it was standing. On the other hand, if it were to sit and cuddle up just in a single room, there's simply less and comparatively easy work to do to wipe that up. It's simply the same idea here. The Metadata producing entities in the engine can just go about minding their own business, while a third party (those classes with some weird names currently [CromwellProfilerFsm and WorkflowProfilerActor]) handle what they are meant to do: Profile a given workflow execution. (all the while without explicitly telling those execution engine entities that it's reading it's state and data information). If the intentions are still not clear, let's talk about it tomorrow in the meeting to make progress with this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121
https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121:1898,Usability,clear,clear,1898,"@geoffjentry Verbosity is just a small part of that. Making different parts of the code aware of the each other, where avoidable, doesn't seem like a very good idea. If I'm not mistaken, what you're saying (and what I initially implemented) is something like the erstwhile `DataAccess` (which would be the `ServiceRegistoryActor` in the current world). With this, each step (actor) of the workflow had a reference to it and pushed to it independently. Any change to the dataAccess might have required changes to all the classes which were accessing it. Alternatively, if there was just one entity which handled the responsibility of collecting the metadata, by sniffing around the actors without their knowledge, and then pushed to the database, we need only change this entity for any modifications if there were to happen to the data access stuff. I'll try to explain with a very simple (and silly) analogy: (Honestly, couldn't come up with anything better.); Consider a ginormous Octopus (= `ServiceRegistry`) with a black ink on the tips of it's tentacles, with each of it's legs touching upon different rooms (= classes) in a house. If someday we decide to replace that octopus with something else, we'll be needing to wipe that ink from all the rooms upon which it was standing. On the other hand, if it were to sit and cuddle up just in a single room, there's simply less and comparatively easy work to do to wipe that up. It's simply the same idea here. The Metadata producing entities in the engine can just go about minding their own business, while a third party (those classes with some weird names currently [CromwellProfilerFsm and WorkflowProfilerActor]) handle what they are meant to do: Profile a given workflow execution. (all the while without explicitly telling those execution engine entities that it's reading it's state and data information). If the intentions are still not clear, let's talk about it tomorrow in the meeting to make progress with this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-218970121
https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219041395:17,Usability,clear,clear,17,"@gauravs90 To be clear my primary objection is the assumption will always be tied to state changes and/or all actors involved will be FSM. The analogy to DataAccess isn't the same - the issue with DataAccess was that you were handing the keys to shared mutable state (the DB) to async processes. In this case the metadata service was designed to not work like that. . In my mind there already _is_ an abstraction for metadata, the metadata service. While I'm not opposed to putting something between the engine and it, I still fall back to the point that when the akka people talk about setting up your system so that one actor can share info with another they tend to recommend passing the reference directly and it's only particular situations where things start listening on eventstreams and such. So even if the proposal was to have an intermediary I'd still say to pass the actorref around ... and I still don't see the value in that intermediary if one were to do away with the notion that metadata always equaled state changes. Also I think it'll be far easier to transition _to_ the direction you're talking about than _from_, if things get more complicated we can always do osmething like you're proposing. It's fully in the engine, it's trivial to change.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219041395
https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219112569:124,Usability,usab,usable,124,Did you mean for this PR to be against develop? We're currently using this branch for pluggable backend work which won't be usable for GOTC for quite a while.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/830#issuecomment-219112569
https://github.com/broadinstitute/cromwell/issues/838#issuecomment-248064995:416,Availability,error,error,416,"## I want to. ## (i) scatter scatters as requested by @skazakoff above (aka ""nested scatter"") and also. ## (ii) re-array and scatter again (what I'm calling criss-cross scatter). As I show in this diagram: . ![img_4460](https://cloud.githubusercontent.com/assets/11543866/18210288/d489f7b4-7105-11e6-9737-59f75d4a6949.JPG). My attempt to ask WDL/Cromwell to do the first scatter-of-scatter above gives the following error:. ```; [2016-09-02 16:30:43,699] [error] WorkflowActor [e1b18d33]: Call failed to initialize: Failed to start call: Nested Scatters are not supported (yet).; ```. It was unclear how I would go about criss-cross scattering. My current solution around this is to run two separate WDL workflows. Between the two workflows, I use a simple python script to organize the files for the second scatter. The first WDL workflow is per sample, while the second WDL workflow acts on all the outputs of the multiple runs of the first workflow (multiple samples). ---. ## Example of workflows that I want to run within a single WDL workflow. These are scripts that run on the cloud that I would like to run within a single WDL script using the nest-scatter and criss-cross-scatter features. Currently, I run the first script per sample (1) for multiple samples, then run a helper script to organize all the outputs (1.5), and finally run the second WDL script across the multi-sample outputs per genomic interval (2). . I would like to be able to scatter the samples, scatter per interval, then run a differently organized (criss-cross) scatter across all the samples per interval. ### (1) First WDL workflow. #### UltimateScatterHaplotypeCaller_cloud_quicktest.wdl. ```; # ScatterHaplotypeCaller.wdl #############################################################; # Must use GATK v3.6, especially with hg38 where contig names have colons,; # as a bug in prior versions of GATK strips this off.; # Each BAM file represents a single different sample.; # Option to include an additional file to H",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/838#issuecomment-248064995
https://github.com/broadinstitute/cromwell/issues/838#issuecomment-248064995:456,Availability,error,error,456,"## I want to. ## (i) scatter scatters as requested by @skazakoff above (aka ""nested scatter"") and also. ## (ii) re-array and scatter again (what I'm calling criss-cross scatter). As I show in this diagram: . ![img_4460](https://cloud.githubusercontent.com/assets/11543866/18210288/d489f7b4-7105-11e6-9737-59f75d4a6949.JPG). My attempt to ask WDL/Cromwell to do the first scatter-of-scatter above gives the following error:. ```; [2016-09-02 16:30:43,699] [error] WorkflowActor [e1b18d33]: Call failed to initialize: Failed to start call: Nested Scatters are not supported (yet).; ```. It was unclear how I would go about criss-cross scattering. My current solution around this is to run two separate WDL workflows. Between the two workflows, I use a simple python script to organize the files for the second scatter. The first WDL workflow is per sample, while the second WDL workflow acts on all the outputs of the multiple runs of the first workflow (multiple samples). ---. ## Example of workflows that I want to run within a single WDL workflow. These are scripts that run on the cloud that I would like to run within a single WDL script using the nest-scatter and criss-cross-scatter features. Currently, I run the first script per sample (1) for multiple samples, then run a helper script to organize all the outputs (1.5), and finally run the second WDL script across the multi-sample outputs per genomic interval (2). . I would like to be able to scatter the samples, scatter per interval, then run a differently organized (criss-cross) scatter across all the samples per interval. ### (1) First WDL workflow. #### UltimateScatterHaplotypeCaller_cloud_quicktest.wdl. ```; # ScatterHaplotypeCaller.wdl #############################################################; # Must use GATK v3.6, especially with hg38 where contig names have colons,; # as a bug in prior versions of GATK strips this off.; # Each BAM file represents a single different sample.; # Option to include an additional file to H",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/838#issuecomment-248064995
https://github.com/broadinstitute/cromwell/issues/838#issuecomment-248064995:750,Usability,simpl,simple,750,"## I want to. ## (i) scatter scatters as requested by @skazakoff above (aka ""nested scatter"") and also. ## (ii) re-array and scatter again (what I'm calling criss-cross scatter). As I show in this diagram: . ![img_4460](https://cloud.githubusercontent.com/assets/11543866/18210288/d489f7b4-7105-11e6-9737-59f75d4a6949.JPG). My attempt to ask WDL/Cromwell to do the first scatter-of-scatter above gives the following error:. ```; [2016-09-02 16:30:43,699] [error] WorkflowActor [e1b18d33]: Call failed to initialize: Failed to start call: Nested Scatters are not supported (yet).; ```. It was unclear how I would go about criss-cross scattering. My current solution around this is to run two separate WDL workflows. Between the two workflows, I use a simple python script to organize the files for the second scatter. The first WDL workflow is per sample, while the second WDL workflow acts on all the outputs of the multiple runs of the first workflow (multiple samples). ---. ## Example of workflows that I want to run within a single WDL workflow. These are scripts that run on the cloud that I would like to run within a single WDL script using the nest-scatter and criss-cross-scatter features. Currently, I run the first script per sample (1) for multiple samples, then run a helper script to organize all the outputs (1.5), and finally run the second WDL script across the multi-sample outputs per genomic interval (2). . I would like to be able to scatter the samples, scatter per interval, then run a differently organized (criss-cross) scatter across all the samples per interval. ### (1) First WDL workflow. #### UltimateScatterHaplotypeCaller_cloud_quicktest.wdl. ```; # ScatterHaplotypeCaller.wdl #############################################################; # Must use GATK v3.6, especially with hg38 where contig names have colons,; # as a bug in prior versions of GATK strips this off.; # Each BAM file represents a single different sample.; # Option to include an additional file to H",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/838#issuecomment-248064995
https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850:406,Availability,error,errors,406,"we had emailed Dion about this a few weeks back, he said, . > ""Yep, we do run more tasks in prod, but even with those tasks we can not guarantee 100% of RPCs succeeding. Internally we do retry any backend dependencies silently (may manifest in slightly higher response times), but it's not unexpected to have a few sneak through. For these situations it's advisable to have a backoff / retry for 5xx level errors that are clearly a problem on our end.; > I've checked back on the time range on those two operations, there doesn't seem to be any wide spread issues during that time on our end. We do have monitoring on the unexpected error rates, would you say your error rates are higher than 0.1 or 0.01% ?  (per RPC call vs per operation, as I think you poll each operation a significant number of times?)."". In conversation, Miguel said:. > ""We have a retry on this call, but it does not back off very aggressively. I'll make a note of it with the Cromwell devs."". Almost all of these failures happened on 5/25. It seems like JES is mostly available, but when unavailable this error causes almost everything running to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850
https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850:633,Availability,error,error,633,"we had emailed Dion about this a few weeks back, he said, . > ""Yep, we do run more tasks in prod, but even with those tasks we can not guarantee 100% of RPCs succeeding. Internally we do retry any backend dependencies silently (may manifest in slightly higher response times), but it's not unexpected to have a few sneak through. For these situations it's advisable to have a backoff / retry for 5xx level errors that are clearly a problem on our end.; > I've checked back on the time range on those two operations, there doesn't seem to be any wide spread issues during that time on our end. We do have monitoring on the unexpected error rates, would you say your error rates are higher than 0.1 or 0.01% ?  (per RPC call vs per operation, as I think you poll each operation a significant number of times?)."". In conversation, Miguel said:. > ""We have a retry on this call, but it does not back off very aggressively. I'll make a note of it with the Cromwell devs."". Almost all of these failures happened on 5/25. It seems like JES is mostly available, but when unavailable this error causes almost everything running to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850
https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850:665,Availability,error,error,665,"we had emailed Dion about this a few weeks back, he said, . > ""Yep, we do run more tasks in prod, but even with those tasks we can not guarantee 100% of RPCs succeeding. Internally we do retry any backend dependencies silently (may manifest in slightly higher response times), but it's not unexpected to have a few sneak through. For these situations it's advisable to have a backoff / retry for 5xx level errors that are clearly a problem on our end.; > I've checked back on the time range on those two operations, there doesn't seem to be any wide spread issues during that time on our end. We do have monitoring on the unexpected error rates, would you say your error rates are higher than 0.1 or 0.01% ?  (per RPC call vs per operation, as I think you poll each operation a significant number of times?)."". In conversation, Miguel said:. > ""We have a retry on this call, but it does not back off very aggressively. I'll make a note of it with the Cromwell devs."". Almost all of these failures happened on 5/25. It seems like JES is mostly available, but when unavailable this error causes almost everything running to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850
https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850:988,Availability,failure,failures,988,"we had emailed Dion about this a few weeks back, he said, . > ""Yep, we do run more tasks in prod, but even with those tasks we can not guarantee 100% of RPCs succeeding. Internally we do retry any backend dependencies silently (may manifest in slightly higher response times), but it's not unexpected to have a few sneak through. For these situations it's advisable to have a backoff / retry for 5xx level errors that are clearly a problem on our end.; > I've checked back on the time range on those two operations, there doesn't seem to be any wide spread issues during that time on our end. We do have monitoring on the unexpected error rates, would you say your error rates are higher than 0.1 or 0.01% ?  (per RPC call vs per operation, as I think you poll each operation a significant number of times?)."". In conversation, Miguel said:. > ""We have a retry on this call, but it does not back off very aggressively. I'll make a note of it with the Cromwell devs."". Almost all of these failures happened on 5/25. It seems like JES is mostly available, but when unavailable this error causes almost everything running to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850
https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850:1043,Availability,avail,available,1043,"we had emailed Dion about this a few weeks back, he said, . > ""Yep, we do run more tasks in prod, but even with those tasks we can not guarantee 100% of RPCs succeeding. Internally we do retry any backend dependencies silently (may manifest in slightly higher response times), but it's not unexpected to have a few sneak through. For these situations it's advisable to have a backoff / retry for 5xx level errors that are clearly a problem on our end.; > I've checked back on the time range on those two operations, there doesn't seem to be any wide spread issues during that time on our end. We do have monitoring on the unexpected error rates, would you say your error rates are higher than 0.1 or 0.01% ?  (per RPC call vs per operation, as I think you poll each operation a significant number of times?)."". In conversation, Miguel said:. > ""We have a retry on this call, but it does not back off very aggressively. I'll make a note of it with the Cromwell devs."". Almost all of these failures happened on 5/25. It seems like JES is mostly available, but when unavailable this error causes almost everything running to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850
https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850:1080,Availability,error,error,1080,"we had emailed Dion about this a few weeks back, he said, . > ""Yep, we do run more tasks in prod, but even with those tasks we can not guarantee 100% of RPCs succeeding. Internally we do retry any backend dependencies silently (may manifest in slightly higher response times), but it's not unexpected to have a few sneak through. For these situations it's advisable to have a backoff / retry for 5xx level errors that are clearly a problem on our end.; > I've checked back on the time range on those two operations, there doesn't seem to be any wide spread issues during that time on our end. We do have monitoring on the unexpected error rates, would you say your error rates are higher than 0.1 or 0.01% ?  (per RPC call vs per operation, as I think you poll each operation a significant number of times?)."". In conversation, Miguel said:. > ""We have a retry on this call, but it does not back off very aggressively. I'll make a note of it with the Cromwell devs."". Almost all of these failures happened on 5/25. It seems like JES is mostly available, but when unavailable this error causes almost everything running to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850
https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850:604,Energy Efficiency,monitor,monitoring,604,"we had emailed Dion about this a few weeks back, he said, . > ""Yep, we do run more tasks in prod, but even with those tasks we can not guarantee 100% of RPCs succeeding. Internally we do retry any backend dependencies silently (may manifest in slightly higher response times), but it's not unexpected to have a few sneak through. For these situations it's advisable to have a backoff / retry for 5xx level errors that are clearly a problem on our end.; > I've checked back on the time range on those two operations, there doesn't seem to be any wide spread issues during that time on our end. We do have monitoring on the unexpected error rates, would you say your error rates are higher than 0.1 or 0.01% ?  (per RPC call vs per operation, as I think you poll each operation a significant number of times?)."". In conversation, Miguel said:. > ""We have a retry on this call, but it does not back off very aggressively. I'll make a note of it with the Cromwell devs."". Almost all of these failures happened on 5/25. It seems like JES is mostly available, but when unavailable this error causes almost everything running to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850
https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850:205,Integrability,depend,dependencies,205,"we had emailed Dion about this a few weeks back, he said, . > ""Yep, we do run more tasks in prod, but even with those tasks we can not guarantee 100% of RPCs succeeding. Internally we do retry any backend dependencies silently (may manifest in slightly higher response times), but it's not unexpected to have a few sneak through. For these situations it's advisable to have a backoff / retry for 5xx level errors that are clearly a problem on our end.; > I've checked back on the time range on those two operations, there doesn't seem to be any wide spread issues during that time on our end. We do have monitoring on the unexpected error rates, would you say your error rates are higher than 0.1 or 0.01% ?  (per RPC call vs per operation, as I think you poll each operation a significant number of times?)."". In conversation, Miguel said:. > ""We have a retry on this call, but it does not back off very aggressively. I'll make a note of it with the Cromwell devs."". Almost all of these failures happened on 5/25. It seems like JES is mostly available, but when unavailable this error causes almost everything running to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850
https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850:260,Performance,response time,response times,260,"we had emailed Dion about this a few weeks back, he said, . > ""Yep, we do run more tasks in prod, but even with those tasks we can not guarantee 100% of RPCs succeeding. Internally we do retry any backend dependencies silently (may manifest in slightly higher response times), but it's not unexpected to have a few sneak through. For these situations it's advisable to have a backoff / retry for 5xx level errors that are clearly a problem on our end.; > I've checked back on the time range on those two operations, there doesn't seem to be any wide spread issues during that time on our end. We do have monitoring on the unexpected error rates, would you say your error rates are higher than 0.1 or 0.01% ?  (per RPC call vs per operation, as I think you poll each operation a significant number of times?)."". In conversation, Miguel said:. > ""We have a retry on this call, but it does not back off very aggressively. I'll make a note of it with the Cromwell devs."". Almost all of these failures happened on 5/25. It seems like JES is mostly available, but when unavailable this error causes almost everything running to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850
https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850:422,Usability,clear,clearly,422,"we had emailed Dion about this a few weeks back, he said, . > ""Yep, we do run more tasks in prod, but even with those tasks we can not guarantee 100% of RPCs succeeding. Internally we do retry any backend dependencies silently (may manifest in slightly higher response times), but it's not unexpected to have a few sneak through. For these situations it's advisable to have a backoff / retry for 5xx level errors that are clearly a problem on our end.; > I've checked back on the time range on those two operations, there doesn't seem to be any wide spread issues during that time on our end. We do have monitoring on the unexpected error rates, would you say your error rates are higher than 0.1 or 0.01% ?  (per RPC call vs per operation, as I think you poll each operation a significant number of times?)."". In conversation, Miguel said:. > ""We have a retry on this call, but it does not back off very aggressively. I'll make a note of it with the Cromwell devs."". Almost all of these failures happened on 5/25. It seems like JES is mostly available, but when unavailable this error causes almost everything running to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850
https://github.com/broadinstitute/cromwell/pull/935#issuecomment-223613634:55,Usability,feedback,feedback,55,@kshakir this needs a rebase as I incorporated some PR feedback and a rebase on develop,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/935#issuecomment-223613634
https://github.com/broadinstitute/cromwell/pull/968#issuecomment-224391153:49,Usability,simpl,simple,49,@cjllanwarne So is this like python's range? The simple form at least. If so I wonder if that'd be a more obvious name for it in terms of the primary wdl userbase,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/968#issuecomment-224391153
https://github.com/broadinstitute/cromwell/pull/980#issuecomment-226338098:45,Usability,feedback,feedback,45,@sleongmgi Can you take a look at the review feedback and then we can merge this in. Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/980#issuecomment-226338098
https://github.com/broadinstitute/cromwell/pull/980#issuecomment-226547078:32,Usability,feedback,feedbacks,32,Thank you very much for all the feedbacks. I will go through them and make necessary changes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/980#issuecomment-226547078
https://github.com/broadinstitute/cromwell/pull/1014#issuecomment-226841977:16,Usability,clear,clear,16,"It's not really clear from reading #972 what the scope of this ticket should be, but ""exclude"" seems like it would be useful and hopefully not difficult to add?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1014#issuecomment-226841977
https://github.com/broadinstitute/cromwell/issues/1021#issuecomment-227486473:58,Usability,feedback,feedback,58,"We discussed this at standup today and wanted to get some feedback from @dvoet before deciding how to proceed. Currently the query API in PBE Cromwell may lag the engine/raw metadata journal by some amount of time (currently the refresh interval is 2 seconds, plus the time to do the refresh work). So it's possible that a query soon after a workflow submission will not immediately turn up any results for legitimate workflow IDs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1021#issuecomment-227486473
https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605:487,Deployability,deploy,deployment-manager,487,"Hi Kristian,. I understand, but what you're asking is very possible - see my previous discussion here about creating 1 billion simultaneous connections, and anything that is not accessible can be pre-cached via buckets during idle periods (i.e. nightly):. https://github.com/googlegenomics/utils-java/issues/62#issuecomment-220444203. So you should be able to create your own Pipeline implementation very easily via `gloud create`, [VM metadata startup scripts](https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts) and/or Dataflow Pipelines, and mimic JES:. https://cloud.google.com/sdk/gcloud/reference/compute/instances/create. https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts. https://cloud.google.com/dataflow/pipelines/constructing-your-pipeline#applying-transforms-to-process-pipeline-data. If you look at the JES API, you'll notice most of it mirrors the `gcloud` commands and parameters:. https://www.googleapis.com/discovery/v1/apis/genomics/v1alpha2/rest. Again the concepts to speed up searches on dynamically streaming (processed) analysis results has a foundation via inverted indices, which search engines use all the time - I posted a couple of these here:. https://github.com/ga4gh/schemas/pull/253#issuecomment-97525342. https://github.com/ga4gh/schemas/issues/142#issuecomment-55518571. This way your searches are always fresh and would operate without any delay. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605
https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605:703,Deployability,deploy,deployment-manager,703,"Hi Kristian,. I understand, but what you're asking is very possible - see my previous discussion here about creating 1 billion simultaneous connections, and anything that is not accessible can be pre-cached via buckets during idle periods (i.e. nightly):. https://github.com/googlegenomics/utils-java/issues/62#issuecomment-220444203. So you should be able to create your own Pipeline implementation very easily via `gloud create`, [VM metadata startup scripts](https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts) and/or Dataflow Pipelines, and mimic JES:. https://cloud.google.com/sdk/gcloud/reference/compute/instances/create. https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts. https://cloud.google.com/dataflow/pipelines/constructing-your-pipeline#applying-transforms-to-process-pipeline-data. If you look at the JES API, you'll notice most of it mirrors the `gcloud` commands and parameters:. https://www.googleapis.com/discovery/v1/apis/genomics/v1alpha2/rest. Again the concepts to speed up searches on dynamically streaming (processed) analysis results has a foundation via inverted indices, which search engines use all the time - I posted a couple of these here:. https://github.com/ga4gh/schemas/pull/253#issuecomment-97525342. https://github.com/ga4gh/schemas/issues/142#issuecomment-55518571. This way your searches are always fresh and would operate without any delay. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605
https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605:813,Deployability,pipeline,pipelines,813,"Hi Kristian,. I understand, but what you're asking is very possible - see my previous discussion here about creating 1 billion simultaneous connections, and anything that is not accessible can be pre-cached via buckets during idle periods (i.e. nightly):. https://github.com/googlegenomics/utils-java/issues/62#issuecomment-220444203. So you should be able to create your own Pipeline implementation very easily via `gloud create`, [VM metadata startup scripts](https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts) and/or Dataflow Pipelines, and mimic JES:. https://cloud.google.com/sdk/gcloud/reference/compute/instances/create. https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts. https://cloud.google.com/dataflow/pipelines/constructing-your-pipeline#applying-transforms-to-process-pipeline-data. If you look at the JES API, you'll notice most of it mirrors the `gcloud` commands and parameters:. https://www.googleapis.com/discovery/v1/apis/genomics/v1alpha2/rest. Again the concepts to speed up searches on dynamically streaming (processed) analysis results has a foundation via inverted indices, which search engines use all the time - I posted a couple of these here:. https://github.com/ga4gh/schemas/pull/253#issuecomment-97525342. https://github.com/ga4gh/schemas/issues/142#issuecomment-55518571. This way your searches are always fresh and would operate without any delay. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605
https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605:841,Deployability,pipeline,pipeline,841,"Hi Kristian,. I understand, but what you're asking is very possible - see my previous discussion here about creating 1 billion simultaneous connections, and anything that is not accessible can be pre-cached via buckets during idle periods (i.e. nightly):. https://github.com/googlegenomics/utils-java/issues/62#issuecomment-220444203. So you should be able to create your own Pipeline implementation very easily via `gloud create`, [VM metadata startup scripts](https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts) and/or Dataflow Pipelines, and mimic JES:. https://cloud.google.com/sdk/gcloud/reference/compute/instances/create. https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts. https://cloud.google.com/dataflow/pipelines/constructing-your-pipeline#applying-transforms-to-process-pipeline-data. If you look at the JES API, you'll notice most of it mirrors the `gcloud` commands and parameters:. https://www.googleapis.com/discovery/v1/apis/genomics/v1alpha2/rest. Again the concepts to speed up searches on dynamically streaming (processed) analysis results has a foundation via inverted indices, which search engines use all the time - I posted a couple of these here:. https://github.com/ga4gh/schemas/pull/253#issuecomment-97525342. https://github.com/ga4gh/schemas/issues/142#issuecomment-55518571. This way your searches are always fresh and would operate without any delay. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605
https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605:881,Deployability,pipeline,pipeline-data,881,"Hi Kristian,. I understand, but what you're asking is very possible - see my previous discussion here about creating 1 billion simultaneous connections, and anything that is not accessible can be pre-cached via buckets during idle periods (i.e. nightly):. https://github.com/googlegenomics/utils-java/issues/62#issuecomment-220444203. So you should be able to create your own Pipeline implementation very easily via `gloud create`, [VM metadata startup scripts](https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts) and/or Dataflow Pipelines, and mimic JES:. https://cloud.google.com/sdk/gcloud/reference/compute/instances/create. https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts. https://cloud.google.com/dataflow/pipelines/constructing-your-pipeline#applying-transforms-to-process-pipeline-data. If you look at the JES API, you'll notice most of it mirrors the `gcloud` commands and parameters:. https://www.googleapis.com/discovery/v1/apis/genomics/v1alpha2/rest. Again the concepts to speed up searches on dynamically streaming (processed) analysis results has a foundation via inverted indices, which search engines use all the time - I posted a couple of these here:. https://github.com/ga4gh/schemas/pull/253#issuecomment-97525342. https://github.com/ga4gh/schemas/issues/142#issuecomment-55518571. This way your searches are always fresh and would operate without any delay. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605
https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605:200,Performance,cache,cached,200,"Hi Kristian,. I understand, but what you're asking is very possible - see my previous discussion here about creating 1 billion simultaneous connections, and anything that is not accessible can be pre-cached via buckets during idle periods (i.e. nightly):. https://github.com/googlegenomics/utils-java/issues/62#issuecomment-220444203. So you should be able to create your own Pipeline implementation very easily via `gloud create`, [VM metadata startup scripts](https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts) and/or Dataflow Pipelines, and mimic JES:. https://cloud.google.com/sdk/gcloud/reference/compute/instances/create. https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts. https://cloud.google.com/dataflow/pipelines/constructing-your-pipeline#applying-transforms-to-process-pipeline-data. If you look at the JES API, you'll notice most of it mirrors the `gcloud` commands and parameters:. https://www.googleapis.com/discovery/v1/apis/genomics/v1alpha2/rest. Again the concepts to speed up searches on dynamically streaming (processed) analysis results has a foundation via inverted indices, which search engines use all the time - I posted a couple of these here:. https://github.com/ga4gh/schemas/pull/253#issuecomment-97525342. https://github.com/ga4gh/schemas/issues/142#issuecomment-55518571. This way your searches are always fresh and would operate without any delay. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605
https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605:178,Security,access,accessible,178,"Hi Kristian,. I understand, but what you're asking is very possible - see my previous discussion here about creating 1 billion simultaneous connections, and anything that is not accessible can be pre-cached via buckets during idle periods (i.e. nightly):. https://github.com/googlegenomics/utils-java/issues/62#issuecomment-220444203. So you should be able to create your own Pipeline implementation very easily via `gloud create`, [VM metadata startup scripts](https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts) and/or Dataflow Pipelines, and mimic JES:. https://cloud.google.com/sdk/gcloud/reference/compute/instances/create. https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts. https://cloud.google.com/dataflow/pipelines/constructing-your-pipeline#applying-transforms-to-process-pipeline-data. If you look at the JES API, you'll notice most of it mirrors the `gcloud` commands and parameters:. https://www.googleapis.com/discovery/v1/apis/genomics/v1alpha2/rest. Again the concepts to speed up searches on dynamically streaming (processed) analysis results has a foundation via inverted indices, which search engines use all the time - I posted a couple of these here:. https://github.com/ga4gh/schemas/pull/253#issuecomment-97525342. https://github.com/ga4gh/schemas/issues/142#issuecomment-55518571. This way your searches are always fresh and would operate without any delay. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605
https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605:519,Usability,guid,guide,519,"Hi Kristian,. I understand, but what you're asking is very possible - see my previous discussion here about creating 1 billion simultaneous connections, and anything that is not accessible can be pre-cached via buckets during idle periods (i.e. nightly):. https://github.com/googlegenomics/utils-java/issues/62#issuecomment-220444203. So you should be able to create your own Pipeline implementation very easily via `gloud create`, [VM metadata startup scripts](https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts) and/or Dataflow Pipelines, and mimic JES:. https://cloud.google.com/sdk/gcloud/reference/compute/instances/create. https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts. https://cloud.google.com/dataflow/pipelines/constructing-your-pipeline#applying-transforms-to-process-pipeline-data. If you look at the JES API, you'll notice most of it mirrors the `gcloud` commands and parameters:. https://www.googleapis.com/discovery/v1/apis/genomics/v1alpha2/rest. Again the concepts to speed up searches on dynamically streaming (processed) analysis results has a foundation via inverted indices, which search engines use all the time - I posted a couple of these here:. https://github.com/ga4gh/schemas/pull/253#issuecomment-97525342. https://github.com/ga4gh/schemas/issues/142#issuecomment-55518571. This way your searches are always fresh and would operate without any delay. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605
https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605:735,Usability,guid,guide,735,"Hi Kristian,. I understand, but what you're asking is very possible - see my previous discussion here about creating 1 billion simultaneous connections, and anything that is not accessible can be pre-cached via buckets during idle periods (i.e. nightly):. https://github.com/googlegenomics/utils-java/issues/62#issuecomment-220444203. So you should be able to create your own Pipeline implementation very easily via `gloud create`, [VM metadata startup scripts](https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts) and/or Dataflow Pipelines, and mimic JES:. https://cloud.google.com/sdk/gcloud/reference/compute/instances/create. https://cloud.google.com/deployment-manager/step-by-step-guide/setting-metadata-and-startup-scripts. https://cloud.google.com/dataflow/pipelines/constructing-your-pipeline#applying-transforms-to-process-pipeline-data. If you look at the JES API, you'll notice most of it mirrors the `gcloud` commands and parameters:. https://www.googleapis.com/discovery/v1/apis/genomics/v1alpha2/rest. Again the concepts to speed up searches on dynamically streaming (processed) analysis results has a foundation via inverted indices, which search engines use all the time - I posted a couple of these here:. https://github.com/ga4gh/schemas/pull/253#issuecomment-97525342. https://github.com/ga4gh/schemas/issues/142#issuecomment-55518571. This way your searches are always fresh and would operate without any delay. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058#issuecomment-228175605
https://github.com/broadinstitute/cromwell/pull/1060#issuecomment-228143719:167,Safety,timeout,timeout,167,"To be clear I'm not 💩 💩 ing the multiple ServiceRegistryActor idea, just pointing out one potential gotcha I can see. I have reservations about my hacky ""crank up the timeout"" ""solution"" in the #1057 PR and would definitely like to look at other options like this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1060#issuecomment-228143719
https://github.com/broadinstitute/cromwell/pull/1060#issuecomment-228143719:6,Usability,clear,clear,6,"To be clear I'm not 💩 💩 ing the multiple ServiceRegistryActor idea, just pointing out one potential gotcha I can see. I have reservations about my hacky ""crank up the timeout"" ""solution"" in the #1057 PR and would definitely like to look at other options like this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1060#issuecomment-228143719
https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772:602,Availability,down,down,602,"Had a brief chat w/ @dshiga - I was wondering if all slowness was directly tied to reading from metadata or general slowness and it was the latter (not that it couldn't be _caused_ by reading from MD, but it's across the board slowness). Three thoughts:; - We were talking yesterday about how someone not named me should try setting up typesafe monitor and use it to debug/profile/analyze. No time like the present!; - I can't find the ticket (@kcibul - do you know the right string to search for?) but we should look into the thread pools/dispatchers. This could be a case where something is bringing down the whole system but bulkheading would keep everything else responsive; - A while back we talked about streamlining submission such that WF submissions get a ""Submitted"" status but aren't necessarily immediately launched, and the system would pull them - allowing us to tune the rate at which we pull. Perhaps time to resurrect this one?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772
https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772:345,Energy Efficiency,monitor,monitor,345,"Had a brief chat w/ @dshiga - I was wondering if all slowness was directly tied to reading from metadata or general slowness and it was the latter (not that it couldn't be _caused_ by reading from MD, but it's across the board slowness). Three thoughts:; - We were talking yesterday about how someone not named me should try setting up typesafe monitor and use it to debug/profile/analyze. No time like the present!; - I can't find the ticket (@kcibul - do you know the right string to search for?) but we should look into the thread pools/dispatchers. This could be a case where something is bringing down the whole system but bulkheading would keep everything else responsive; - A while back we talked about streamlining submission such that WF submissions get a ""Submitted"" status but aren't necessarily immediately launched, and the system would pull them - allowing us to tune the rate at which we pull. Perhaps time to resurrect this one?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772
https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772:877,Performance,tune,tune,877,"Had a brief chat w/ @dshiga - I was wondering if all slowness was directly tied to reading from metadata or general slowness and it was the latter (not that it couldn't be _caused_ by reading from MD, but it's across the board slowness). Three thoughts:; - We were talking yesterday about how someone not named me should try setting up typesafe monitor and use it to debug/profile/analyze. No time like the present!; - I can't find the ticket (@kcibul - do you know the right string to search for?) but we should look into the thread pools/dispatchers. This could be a case where something is bringing down the whole system but bulkheading would keep everything else responsive; - A while back we talked about streamlining submission such that WF submissions get a ""Submitted"" status but aren't necessarily immediately launched, and the system would pull them - allowing us to tune the rate at which we pull. Perhaps time to resurrect this one?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772
https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772:667,Usability,responsiv,responsive,667,"Had a brief chat w/ @dshiga - I was wondering if all slowness was directly tied to reading from metadata or general slowness and it was the latter (not that it couldn't be _caused_ by reading from MD, but it's across the board slowness). Three thoughts:; - We were talking yesterday about how someone not named me should try setting up typesafe monitor and use it to debug/profile/analyze. No time like the present!; - I can't find the ticket (@kcibul - do you know the right string to search for?) but we should look into the thread pools/dispatchers. This could be a case where something is bringing down the whole system but bulkheading would keep everything else responsive; - A while back we talked about streamlining submission such that WF submissions get a ""Submitted"" status but aren't necessarily immediately launched, and the system would pull them - allowing us to tune the rate at which we pull. Perhaps time to resurrect this one?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772
https://github.com/broadinstitute/cromwell/issues/1070#issuecomment-228541825:324,Availability,avail,available,324,"Hi - is this in 0.19 proper or 0.19_hotfix? I believe this was fixed already in the latter. If you're using the former try the latter, if you're using the latter then clearly I'm wrong :). I'll also point a finger at develop which is radically different (many problems fixed, will have its own new problems) which should be available as 0.20 this week",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070#issuecomment-228541825
https://github.com/broadinstitute/cromwell/issues/1070#issuecomment-228541825:167,Usability,clear,clearly,167,"Hi - is this in 0.19 proper or 0.19_hotfix? I believe this was fixed already in the latter. If you're using the former try the latter, if you're using the latter then clearly I'm wrong :). I'll also point a finger at develop which is radically different (many problems fixed, will have its own new problems) which should be available as 0.20 this week",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070#issuecomment-228541825
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891:495,Availability,down,down,495,"Currently default runtime attributes are typechecked in the backend-specific runtime attribute classes (e.g. `JesRuntimeAttributes`, `LocalRuntimeAttributes`, etc) using attribute name to type mappings that are backend-specific. With our current static backend selection scheme, MWDA knows the backend to which a task will be sent at validation time. So while it's currently possible to refactor to expose backend-specific default runtime attribute typechecking to MWDA, that system would break down with a dynamic backend selection scheme. . It's also not clear how MWDA-composed runtime attributes would be handed to the backend-specific runtime attribute classes for the more substantive ""beyond typechecking"" round of validation and execution. It's possible we could copy the `NamespaceWithWorkflow` and write the relevant attributes into the tasks, but I'm not sure if we'd get into trouble later with bindings that no longer agree with the AST.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891:387,Modifiability,refactor,refactor,387,"Currently default runtime attributes are typechecked in the backend-specific runtime attribute classes (e.g. `JesRuntimeAttributes`, `LocalRuntimeAttributes`, etc) using attribute name to type mappings that are backend-specific. With our current static backend selection scheme, MWDA knows the backend to which a task will be sent at validation time. So while it's currently possible to refactor to expose backend-specific default runtime attribute typechecking to MWDA, that system would break down with a dynamic backend selection scheme. . It's also not clear how MWDA-composed runtime attributes would be handed to the backend-specific runtime attribute classes for the more substantive ""beyond typechecking"" round of validation and execution. It's possible we could copy the `NamespaceWithWorkflow` and write the relevant attributes into the tasks, but I'm not sure if we'd get into trouble later with bindings that no longer agree with the AST.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891:334,Security,validat,validation,334,"Currently default runtime attributes are typechecked in the backend-specific runtime attribute classes (e.g. `JesRuntimeAttributes`, `LocalRuntimeAttributes`, etc) using attribute name to type mappings that are backend-specific. With our current static backend selection scheme, MWDA knows the backend to which a task will be sent at validation time. So while it's currently possible to refactor to expose backend-specific default runtime attribute typechecking to MWDA, that system would break down with a dynamic backend selection scheme. . It's also not clear how MWDA-composed runtime attributes would be handed to the backend-specific runtime attribute classes for the more substantive ""beyond typechecking"" round of validation and execution. It's possible we could copy the `NamespaceWithWorkflow` and write the relevant attributes into the tasks, but I'm not sure if we'd get into trouble later with bindings that no longer agree with the AST.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891:399,Security,expose,expose,399,"Currently default runtime attributes are typechecked in the backend-specific runtime attribute classes (e.g. `JesRuntimeAttributes`, `LocalRuntimeAttributes`, etc) using attribute name to type mappings that are backend-specific. With our current static backend selection scheme, MWDA knows the backend to which a task will be sent at validation time. So while it's currently possible to refactor to expose backend-specific default runtime attribute typechecking to MWDA, that system would break down with a dynamic backend selection scheme. . It's also not clear how MWDA-composed runtime attributes would be handed to the backend-specific runtime attribute classes for the more substantive ""beyond typechecking"" round of validation and execution. It's possible we could copy the `NamespaceWithWorkflow` and write the relevant attributes into the tasks, but I'm not sure if we'd get into trouble later with bindings that no longer agree with the AST.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891:722,Security,validat,validation,722,"Currently default runtime attributes are typechecked in the backend-specific runtime attribute classes (e.g. `JesRuntimeAttributes`, `LocalRuntimeAttributes`, etc) using attribute name to type mappings that are backend-specific. With our current static backend selection scheme, MWDA knows the backend to which a task will be sent at validation time. So while it's currently possible to refactor to expose backend-specific default runtime attribute typechecking to MWDA, that system would break down with a dynamic backend selection scheme. . It's also not clear how MWDA-composed runtime attributes would be handed to the backend-specific runtime attribute classes for the more substantive ""beyond typechecking"" round of validation and execution. It's possible we could copy the `NamespaceWithWorkflow` and write the relevant attributes into the tasks, but I'm not sure if we'd get into trouble later with bindings that no longer agree with the AST.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891
https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891:557,Usability,clear,clear,557,"Currently default runtime attributes are typechecked in the backend-specific runtime attribute classes (e.g. `JesRuntimeAttributes`, `LocalRuntimeAttributes`, etc) using attribute name to type mappings that are backend-specific. With our current static backend selection scheme, MWDA knows the backend to which a task will be sent at validation time. So while it's currently possible to refactor to expose backend-specific default runtime attribute typechecking to MWDA, that system would break down with a dynamic backend selection scheme. . It's also not clear how MWDA-composed runtime attributes would be handed to the backend-specific runtime attribute classes for the more substantive ""beyond typechecking"" round of validation and execution. It's possible we could copy the `NamespaceWithWorkflow` and write the relevant attributes into the tasks, but I'm not sure if we'd get into trouble later with bindings that no longer agree with the AST.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-231157891
https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-229573997:296,Energy Efficiency,efficient,efficient,296,"+1; we like to compose WDL workflows using import of many atomic tasks, some of these will be very simple like generating a UUID or calling mkdir, some will be long running computations. Being able to specify the short tasks to run locally and handing the longer ones off to the cluster would be efficient.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-229573997
https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-229573997:99,Usability,simpl,simple,99,"+1; we like to compose WDL workflows using import of many atomic tasks, some of these will be very simple like generating a UUID or calling mkdir, some will be long running computations. Being able to specify the short tasks to run locally and handing the longer ones off to the cluster would be efficient.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-229573997
https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152:128,Integrability,message,message,128,"The simplest way this can be done is to use [Google Cloud Pub/Sub](https://cloud.google.com/pubsub/overview), and then have the message pushed to all subscribed VMs since one can get a list of Operations ([globally](https://cloud.google.com/compute/docs/reference/latest/globalOperations/list), [by region](https://cloud.google.com/compute/docs/reference/latest/regionOperations/list) or [zone](https://cloud.google.com/compute/docs/reference/latest/zoneOperations/list)). Then based on the message each individual VM can query itself, and provide the Operation resource status to a central location (Web service, Google Storage, etc). For more information, here are a link:. https://cloud.google.com/pubsub/overview. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152
https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152:491,Integrability,message,message,491,"The simplest way this can be done is to use [Google Cloud Pub/Sub](https://cloud.google.com/pubsub/overview), and then have the message pushed to all subscribed VMs since one can get a list of Operations ([globally](https://cloud.google.com/compute/docs/reference/latest/globalOperations/list), [by region](https://cloud.google.com/compute/docs/reference/latest/regionOperations/list) or [zone](https://cloud.google.com/compute/docs/reference/latest/zoneOperations/list)). Then based on the message each individual VM can query itself, and provide the Operation resource status to a central location (Web service, Google Storage, etc). For more information, here are a link:. https://cloud.google.com/pubsub/overview. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152
https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152:4,Usability,simpl,simplest,4,"The simplest way this can be done is to use [Google Cloud Pub/Sub](https://cloud.google.com/pubsub/overview), and then have the message pushed to all subscribed VMs since one can get a list of Operations ([globally](https://cloud.google.com/compute/docs/reference/latest/globalOperations/list), [by region](https://cloud.google.com/compute/docs/reference/latest/regionOperations/list) or [zone](https://cloud.google.com/compute/docs/reference/latest/zoneOperations/list)). Then based on the message each individual VM can query itself, and provide the Operation resource status to a central location (Web service, Google Storage, etc). For more information, here are a link:. https://cloud.google.com/pubsub/overview. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152
https://github.com/broadinstitute/cromwell/pull/1099#issuecomment-229972340:202,Usability,clear,clearly,202,"I think in April we chose Option 1 with a sprinkling of Option 3 in https://docs.google.com/a/broadinstitute.com/document/d/1feRDusWXQQ2pJ03sNHTNmrrnnwL3y-vtyF1fv_RdogU/edit?usp=sharing whereas this is clearly Option 2... (I'm not saying it's wrong, this seems to address all of the ""cons"" as I saw them with gusto... just pointing it out :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1099#issuecomment-229972340
https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231173340:223,Modifiability,variab,variable,223,"hey @jainh -- Building on @geoffjentry 's question, and after reading through the code here... I'm not sure what about this PR is actually Spark'ified. It seems like a copy and paste of the HtCondor backend (including some variable names!) and then using that to run a command inside a docker to connect to a pre-existing spark cluster with preloaded data. All this could already be done using any one of the docker-based back ends (e.g. Local or even Google). Am I missing something here? If not, it would be great to see more of a roadmap about where you see this going because it's not clear from the code here what direction it could take. The concept of running/orchestrating Spark jobs via Cromwell is an important one and could take many, many forms so clarifying which approach this is taking (and making that clear in naming/location/etc) is important. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231173340
https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231173340:589,Usability,clear,clear,589,"hey @jainh -- Building on @geoffjentry 's question, and after reading through the code here... I'm not sure what about this PR is actually Spark'ified. It seems like a copy and paste of the HtCondor backend (including some variable names!) and then using that to run a command inside a docker to connect to a pre-existing spark cluster with preloaded data. All this could already be done using any one of the docker-based back ends (e.g. Local or even Google). Am I missing something here? If not, it would be great to see more of a roadmap about where you see this going because it's not clear from the code here what direction it could take. The concept of running/orchestrating Spark jobs via Cromwell is an important one and could take many, many forms so clarifying which approach this is taking (and making that clear in naming/location/etc) is important. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231173340
https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231173340:818,Usability,clear,clear,818,"hey @jainh -- Building on @geoffjentry 's question, and after reading through the code here... I'm not sure what about this PR is actually Spark'ified. It seems like a copy and paste of the HtCondor backend (including some variable names!) and then using that to run a command inside a docker to connect to a pre-existing spark cluster with preloaded data. All this could already be done using any one of the docker-based back ends (e.g. Local or even Google). Am I missing something here? If not, it would be great to see more of a roadmap about where you see this going because it's not clear from the code here what direction it could take. The concept of running/orchestrating Spark jobs via Cromwell is an important one and could take many, many forms so clarifying which approach this is taking (and making that clear in naming/location/etc) is important. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132#issuecomment-231173340
https://github.com/broadinstitute/cromwell/pull/1135#issuecomment-231378728:29,Modifiability,config,config,29,"really clear comments in the config file, nice",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1135#issuecomment-231378728
https://github.com/broadinstitute/cromwell/pull/1135#issuecomment-231378728:7,Usability,clear,clear,7,"really clear comments in the config file, nice",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1135#issuecomment-231378728
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986:112,Testability,assert,assert,112,"Sounds like some syntactic sugars are expected to simplify the fail method declaration. In addition, will such 'assert' be dynamic (in run-time) or static (in parse-time, before running any task)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986:50,Usability,simpl,simplify,50,"Sounds like some syntactic sugars are expected to simplify the fail method declaration. In addition, will such 'assert' be dynamic (in run-time) or static (in parse-time, before running any task)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:366,Availability,failure,failure,366,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:68,Security,validat,validation,68,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:190,Security,validat,validation,190,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:331,Security,validat,validate,331,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:25,Testability,assert,assert,25,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:355,Testability,assert,assertion,355,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:579,Testability,assert,assert,579,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089
https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:508,Usability,simpl,simplify,508,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089
https://github.com/broadinstitute/cromwell/issues/1191#issuecomment-234341313:130,Usability,clear,clearly,130,"@cjllanwarne Is anything listening to them currently? If not, is this the right answer or would it be to remove the acks? They're clearly not being used by the WEA or the WMA for anything.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1191#issuecomment-234341313
https://github.com/broadinstitute/cromwell/pull/1192#issuecomment-234070556:174,Usability,undo,undo,174,Welcoming all comers as this is hefty but @Horneth in particular - a lot of the rebase-a-palooza involved changes you had made so an extra pair of eyes to make sure I didn't undo something would be good.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1192#issuecomment-234070556
https://github.com/broadinstitute/cromwell/pull/1198#issuecomment-235020776:67,Usability,feedback,feedback,67,👍 assuming Travis is happy. Re-assign back if/when requesting more feedback. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1198/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1198#issuecomment-235020776
https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511142579:43,Usability,clear,clear,43,"@ctjoreilly Sorry, I should have been more clear, that change wouldn't affect your problem as it's WDL only. While it's a known issue for CWL support it doesn't look like there's an issue for **that** problem. It's worth opening a new ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1210#issuecomment-511142579
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235976923:19,Usability,clear,clear,19,"@jainh Just so I'm clear, the WDL example that @cjllanwarne posted above would still work correctly, right? (I believe that's the case, just being cautious here)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235976923
https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236012262:18,Usability,clear,clear,18,"@jainh just to be clear. I don't think your code breaks anything but I also don't think it can be comprehensively correct. Here's why- Imagine two nearly the same WDL files:. ```; workflow ordered {; call a; call b { input: i = a.i }; }; ```. and:. ```; workflow unordered {; call b { input: i = a.i }; call a; }; ```. Now, if I understand you correctly, in both cases you want to pass to the initialiser the same list: List(a, b) since a should in initialised before b. However, if you just create a ListMap as your code does, it will pick the same order from the WDL file. So here's what happens:. | Workflow | List that you want | List that your code makes | Match |; | --- | --- | --- | --- |; | ordered | `List(a, b)` | `List(a, b)` | 🆗 |; | unordered | `List(a, b)` | `List(b, a)` | 🚫 |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236012262
https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897:759,Integrability,message,messages,759,"More details that emerged over the past couple weeks:. In the past, the Local and JES backends extended the ABJEA. When the Local and JES backends were merged into Standard, there was a lot of work in the Standard not to mess with the existing ABJEA. This turns out to have not been necessary. The only known extension of the ABJEA is now the Standard implementation, the `StandardAsyncExecutionActor` (SAEA). Thus, one is now free to merge the SAEA and ABJEA, and remove the promises-of-results, futures-of-handles, etc. with a simplified actor/fsm. Similarly, while there is a Standard actor that extends the `BackendJobExecutionActor` (BJEA), this isn't necessary either! As the actors are untyped, any implementation that receives and responds to correct messages will work. Thus the `StandardSyncExecutionActor` (SSEA) could be refactored also, and even merging the SSEA and the SAEA into a single actor/fsm. Also, when considering rewrites, it was also noted that the term ""sync/async"" in the SSEA/SAEA names were confusing. Possible alternatives were StandardFutureExecutionActor / StandardMessagingExecutionActor. Again this may be moot if the actors are merged into a single actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897
https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897:95,Modifiability,extend,extended,95,"More details that emerged over the past couple weeks:. In the past, the Local and JES backends extended the ABJEA. When the Local and JES backends were merged into Standard, there was a lot of work in the Standard not to mess with the existing ABJEA. This turns out to have not been necessary. The only known extension of the ABJEA is now the Standard implementation, the `StandardAsyncExecutionActor` (SAEA). Thus, one is now free to merge the SAEA and ABJEA, and remove the promises-of-results, futures-of-handles, etc. with a simplified actor/fsm. Similarly, while there is a Standard actor that extends the `BackendJobExecutionActor` (BJEA), this isn't necessary either! As the actors are untyped, any implementation that receives and responds to correct messages will work. Thus the `StandardSyncExecutionActor` (SSEA) could be refactored also, and even merging the SSEA and the SAEA into a single actor/fsm. Also, when considering rewrites, it was also noted that the term ""sync/async"" in the SSEA/SAEA names were confusing. Possible alternatives were StandardFutureExecutionActor / StandardMessagingExecutionActor. Again this may be moot if the actors are merged into a single actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897
https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897:599,Modifiability,extend,extends,599,"More details that emerged over the past couple weeks:. In the past, the Local and JES backends extended the ABJEA. When the Local and JES backends were merged into Standard, there was a lot of work in the Standard not to mess with the existing ABJEA. This turns out to have not been necessary. The only known extension of the ABJEA is now the Standard implementation, the `StandardAsyncExecutionActor` (SAEA). Thus, one is now free to merge the SAEA and ABJEA, and remove the promises-of-results, futures-of-handles, etc. with a simplified actor/fsm. Similarly, while there is a Standard actor that extends the `BackendJobExecutionActor` (BJEA), this isn't necessary either! As the actors are untyped, any implementation that receives and responds to correct messages will work. Thus the `StandardSyncExecutionActor` (SSEA) could be refactored also, and even merging the SSEA and the SAEA into a single actor/fsm. Also, when considering rewrites, it was also noted that the term ""sync/async"" in the SSEA/SAEA names were confusing. Possible alternatives were StandardFutureExecutionActor / StandardMessagingExecutionActor. Again this may be moot if the actors are merged into a single actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897
https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897:833,Modifiability,refactor,refactored,833,"More details that emerged over the past couple weeks:. In the past, the Local and JES backends extended the ABJEA. When the Local and JES backends were merged into Standard, there was a lot of work in the Standard not to mess with the existing ABJEA. This turns out to have not been necessary. The only known extension of the ABJEA is now the Standard implementation, the `StandardAsyncExecutionActor` (SAEA). Thus, one is now free to merge the SAEA and ABJEA, and remove the promises-of-results, futures-of-handles, etc. with a simplified actor/fsm. Similarly, while there is a Standard actor that extends the `BackendJobExecutionActor` (BJEA), this isn't necessary either! As the actors are untyped, any implementation that receives and responds to correct messages will work. Thus the `StandardSyncExecutionActor` (SSEA) could be refactored also, and even merging the SSEA and the SAEA into a single actor/fsm. Also, when considering rewrites, it was also noted that the term ""sync/async"" in the SSEA/SAEA names were confusing. Possible alternatives were StandardFutureExecutionActor / StandardMessagingExecutionActor. Again this may be moot if the actors are merged into a single actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897
https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897:937,Modifiability,rewrite,rewrites,937,"More details that emerged over the past couple weeks:. In the past, the Local and JES backends extended the ABJEA. When the Local and JES backends were merged into Standard, there was a lot of work in the Standard not to mess with the existing ABJEA. This turns out to have not been necessary. The only known extension of the ABJEA is now the Standard implementation, the `StandardAsyncExecutionActor` (SAEA). Thus, one is now free to merge the SAEA and ABJEA, and remove the promises-of-results, futures-of-handles, etc. with a simplified actor/fsm. Similarly, while there is a Standard actor that extends the `BackendJobExecutionActor` (BJEA), this isn't necessary either! As the actors are untyped, any implementation that receives and responds to correct messages will work. Thus the `StandardSyncExecutionActor` (SSEA) could be refactored also, and even merging the SSEA and the SAEA into a single actor/fsm. Also, when considering rewrites, it was also noted that the term ""sync/async"" in the SSEA/SAEA names were confusing. Possible alternatives were StandardFutureExecutionActor / StandardMessagingExecutionActor. Again this may be moot if the actors are merged into a single actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897
https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897:529,Usability,simpl,simplified,529,"More details that emerged over the past couple weeks:. In the past, the Local and JES backends extended the ABJEA. When the Local and JES backends were merged into Standard, there was a lot of work in the Standard not to mess with the existing ABJEA. This turns out to have not been necessary. The only known extension of the ABJEA is now the Standard implementation, the `StandardAsyncExecutionActor` (SAEA). Thus, one is now free to merge the SAEA and ABJEA, and remove the promises-of-results, futures-of-handles, etc. with a simplified actor/fsm. Similarly, while there is a Standard actor that extends the `BackendJobExecutionActor` (BJEA), this isn't necessary either! As the actors are untyped, any implementation that receives and responds to correct messages will work. Thus the `StandardSyncExecutionActor` (SSEA) could be refactored also, and even merging the SSEA and the SAEA into a single actor/fsm. Also, when considering rewrites, it was also noted that the term ""sync/async"" in the SSEA/SAEA names were confusing. Possible alternatives were StandardFutureExecutionActor / StandardMessagingExecutionActor. Again this may be moot if the actors are merged into a single actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-289853897
https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-344404227:102,Usability,simpl,simple,102,"As a **user developing a backend for Cromwell**, I want **the execution actor naming and system to be simple and concise**, so that I can **more easily work with Cromwell, and don't add unnecessary code**.; - Effort: **Medium**; - Risk: **Small**; - Business value: **Small**; - Not to devalue cleaning up tech debt, but there are higher value items on our docket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218#issuecomment-344404227
https://github.com/broadinstitute/cromwell/issues/1230#issuecomment-236367111:75,Usability,simpl,simple,75,"@kshakir noted in the design doc:. PLEASE be sure to not use the name as a simple string. It's a tuple3 of (namespace, repository, tag). For example ""ubuntu"" == ""library/ubuntu:latest"". Not specifying a slash implies ""library/"", and not specifying a colon implies "":latest"". . https://docs.docker.com/docker-hub/repos/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1230#issuecomment-236367111
https://github.com/broadinstitute/cromwell/pull/1240#issuecomment-236984808:59,Usability,clear,clear,59,"@kshakir I think I might be able to make the comments more clear, I'll give that a try. @cjllanwarne Can't be better than the Spirit of 1776. 🇺🇸 🎆",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1240#issuecomment-236984808
https://github.com/broadinstitute/cromwell/issues/1261#issuecomment-238340358:29,Usability,clear,clear,29,Ok. Perfect. There was not a clear disclaimer about Windows. Sorry,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261#issuecomment-238340358
https://github.com/broadinstitute/cromwell/pull/1286#issuecomment-239452983:8,Usability,clear,clear,8,👍 looks clear!. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1286/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1286#issuecomment-239452983
https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239593121:186,Energy Efficiency,schedul,schedulers,186,"Thank you!. A related question is - could the existing APIs be used to make custom implementations of ""task call""?; In particular, AWS ECS offers a very simple way to create custom task schedulers, which could process an API call with relative ease. One could then schedule a task on ECS instead of JES based on that API call.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239593121
https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239593121:265,Energy Efficiency,schedul,schedule,265,"Thank you!. A related question is - could the existing APIs be used to make custom implementations of ""task call""?; In particular, AWS ECS offers a very simple way to create custom task schedulers, which could process an API call with relative ease. One could then schedule a task on ECS instead of JES based on that API call.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239593121
https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239593121:153,Usability,simpl,simple,153,"Thank you!. A related question is - could the existing APIs be used to make custom implementations of ""task call""?; In particular, AWS ECS offers a very simple way to create custom task schedulers, which could process an API call with relative ease. One could then schedule a task on ECS instead of JES based on that API call.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239593121
https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-418414639:514,Usability,usab,usability,514,"@EvanTheB Without going too far into the details we do hope to see Azure support in the coming quarter or so. Regarding k8s, you might want to look at the work on the [TESK backend](https://github.com/broadinstitute/cromwell/pull/4008) which supports ELIXIR's [TESK](https://github.com/EMBL-EBI-TSI/TESK) project. It's still early days there (e.g. you are limited to using `FTP` based file transfer, which introduces a host of issues) but I'm sure they'd love more people helping them to move that project towards usability",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-418414639
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-328138653:326,Performance,cache,cache,326,"This one comes up *a lot*, and will even more once people can see how much they are spending on intermediate results in FireCloud. I wanted to ask about the effort -- why large? Alex has already implemented the logic for what to delete by looking at the workflow metadata, and if we simply had an endpoint to ""evict from call cache"" we could build this outside the system pretty easily. Basically, iterate through a workflow and for every ""output"" that is not part of the workflow ""output"", you rm the file and evict the call from the task that produced it (or maybe any task that uses that output?). What are you thinking of that I'm missing? Or maybe I'm interpreting ""Large"" as being bigger than you are?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-328138653
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-328138653:211,Testability,log,logic,211,"This one comes up *a lot*, and will even more once people can see how much they are spending on intermediate results in FireCloud. I wanted to ask about the effort -- why large? Alex has already implemented the logic for what to delete by looking at the workflow metadata, and if we simply had an endpoint to ""evict from call cache"" we could build this outside the system pretty easily. Basically, iterate through a workflow and for every ""output"" that is not part of the workflow ""output"", you rm the file and evict the call from the task that produced it (or maybe any task that uses that output?). What are you thinking of that I'm missing? Or maybe I'm interpreting ""Large"" as being bigger than you are?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-328138653
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-328138653:283,Usability,simpl,simply,283,"This one comes up *a lot*, and will even more once people can see how much they are spending on intermediate results in FireCloud. I wanted to ask about the effort -- why large? Alex has already implemented the logic for what to delete by looking at the workflow metadata, and if we simply had an endpoint to ""evict from call cache"" we could build this outside the system pretty easily. Basically, iterate through a workflow and for every ""output"" that is not part of the workflow ""output"", you rm the file and evict the call from the task that produced it (or maybe any task that uses that output?). What are you thinking of that I'm missing? Or maybe I'm interpreting ""Large"" as being bigger than you are?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-328138653
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329452283:434,Deployability,pipeline,pipeline,434,"Thinking about this a little more, I just added this comment to Kate's doc:; ""FWIW, I don’t necessarily need Cromwell to do the delete for me in my use case. I’m happy to do it if I have all the files at hand that I want to delete. This is important because I don’t really want an all-or-nothing delete. I usually want to delete the call cached intermediate results, but not the final outputs of the workflow (and I note that not all pipeline developers list the outputs in the wdl -- I just learned this yesterday from Megan -- so I would like some control over the delete in many cases).""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329452283
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329452283:338,Performance,cache,cached,338,"Thinking about this a little more, I just added this comment to Kate's doc:; ""FWIW, I don’t necessarily need Cromwell to do the delete for me in my use case. I’m happy to do it if I have all the files at hand that I want to delete. This is important because I don’t really want an all-or-nothing delete. I usually want to delete the call cached intermediate results, but not the final outputs of the workflow (and I note that not all pipeline developers list the outputs in the wdl -- I just learned this yesterday from Megan -- so I would like some control over the delete in many cases).""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329452283
https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329452283:492,Usability,learn,learned,492,"Thinking about this a little more, I just added this comment to Kate's doc:; ""FWIW, I don’t necessarily need Cromwell to do the delete for me in my use case. I’m happy to do it if I have all the files at hand that I want to delete. This is important because I don’t really want an all-or-nothing delete. I usually want to delete the call cached intermediate results, but not the final outputs of the workflow (and I note that not all pipeline developers list the outputs in the wdl -- I just learned this yesterday from Megan -- so I would like some control over the delete in many cases).""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292#issuecomment-329452283
https://github.com/broadinstitute/cromwell/issues/1294#issuecomment-240113302:71,Usability,usab,usable,71,What's the state of backends that support this API? Is the RI complete/usable?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1294#issuecomment-240113302
https://github.com/broadinstitute/cromwell/pull/1310#issuecomment-241476680:112,Testability,test,tests,112,"@cjllanwarne not sure what you mean? This isn't really a fix so much as a simplification, and there are already tests in `WdlValueBuilderSpec` which thankfully continue to pass. 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1310#issuecomment-241476680
https://github.com/broadinstitute/cromwell/pull/1310#issuecomment-241476680:74,Usability,simpl,simplification,74,"@cjllanwarne not sure what you mean? This isn't really a fix so much as a simplification, and there are already tests in `WdlValueBuilderSpec` which thankfully continue to pass. 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1310#issuecomment-241476680
https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324205740:551,Testability,log,logic,551,"Since I couldn't remember what these endpoints even *did* I had to go [dig into ye olde wayback machine, aka gmail](https://github.com/broadinstitute/cromwell/pull/306/files). As far as i'm concerned this is along the lines of those ""PBE TODO"" and similar tickets - I don't remember why we never put this piece of the puzzle back together but clearly it isn't some burning desire. I think I've heard people ask about this functionality once or twice and it gets met with a fuzzy ""yeah, i think we used to have that?"" but as I said, don't remember our logic. My $0.02 is to just close this until such a day that people are asking for this again. I realize that's a shockingly hot take considering I just likened it to my stance on other PBE TODO type things :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324205740
https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324205740:343,Usability,clear,clearly,343,"Since I couldn't remember what these endpoints even *did* I had to go [dig into ye olde wayback machine, aka gmail](https://github.com/broadinstitute/cromwell/pull/306/files). As far as i'm concerned this is along the lines of those ""PBE TODO"" and similar tickets - I don't remember why we never put this piece of the puzzle back together but clearly it isn't some burning desire. I think I've heard people ask about this functionality once or twice and it gets met with a fuzzy ""yeah, i think we used to have that?"" but as I said, don't remember our logic. My $0.02 is to just close this until such a day that people are asking for this again. I realize that's a shockingly hot take considering I just likened it to my stance on other PBE TODO type things :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-324205740
https://github.com/broadinstitute/cromwell/pull/1338#issuecomment-242276606:10,Usability,feedback,feedback,10,"Ready for feedback, but don't merge until #1326 is merged and conflicts are resolved.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1338#issuecomment-242276606
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:129,Availability,robust,robust,129,"Well if it's doing it only when the job is finished I would even make it `warn` instead of `info`. > Right now a workflow is not robust enough to run it on a/our HPC, see also on gitte:; > Peter van 't Hof @ffinfo Aug 26 19:05; > ye I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that'",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1456,Availability,recover,recovering,1456,"ymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless loop he will never retry. In it's current state it's for us not yet usable but If you open to it I can think/test things then there is a improvement on this. I can even try to get some time to do some developing but that I can't promise di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1197,Energy Efficiency,reduce,reduce,1197," I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless lo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:979,Performance,queue,queue,979,"Well if it's doing it only when the job is finished I would even make it `warn` instead of `info`. > Right now a workflow is not robust enough to run it on a/our HPC, see also on gitte:; > Peter van 't Hof @ffinfo Aug 26 19:05; > ye I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that'",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1208,Performance,load,load,1208," I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless lo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:367,Safety,detect,detect,367,"Well if it's doing it only when the job is finished I would even make it `warn` instead of `info`. > Right now a workflow is not robust enough to run it on a/our HPC, see also on gitte:; > Peter van 't Hof @ffinfo Aug 26 19:05; > ye I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that'",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1456,Safety,recover,recovering,1456,"ymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless loop he will never retry. In it's current state it's for us not yet usable but If you open to it I can think/test things then there is a improvement on this. I can even try to get some time to do some developing but that I can't promise di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1859,Testability,log,logs,1859," > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless loop he will never retry. In it's current state it's for us not yet usable but If you open to it I can think/test things then there is a improvement on this. I can even try to get some time to do some developing but that I can't promise directly. I can try to look into a lfs.drama backend. If it's possible to check running jobs after restart this would be nice to have, but need to look into this. What to do with this PR i leave up to you guys, merging it or not does not really matter for me at this point ;).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:2340,Testability,test,test,2340," > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless loop he will never retry. In it's current state it's for us not yet usable but If you open to it I can think/test things then there is a improvement on this. I can even try to get some time to do some developing but that I can't promise directly. I can try to look into a lfs.drama backend. If it's possible to check running jobs after restart this would be nice to have, but need to look into this. What to do with this PR i leave up to you guys, merging it or not does not really matter for me at this point ;).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:2299,Usability,usab,usable,2299," > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless loop he will never retry. In it's current state it's for us not yet usable but If you open to it I can think/test things then there is a improvement on this. I can even try to get some time to do some developing but that I can't promise directly. I can try to look into a lfs.drama backend. If it's possible to check running jobs after restart this would be nice to have, but need to look into this. What to do with this PR i leave up to you guys, merging it or not does not really matter for me at this point ;).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243137051:333,Modifiability,config,config-file,333,"~~@ffinfo just to be clear - unless the script explodes in a **really** unusual way, you should _always_ eventually get a return code file. So for us, waiting for it to appear is sufficient. (e.g. even if a command fails, we still get the return code file).~~. ~~In fact, in previous Cromwell versions (before making these backends ""config-file based"") our hard-coded local and SGE backends waited for the return code file and never checked `isAlive` apart from at restart-time.~~. ~~Just so I can understand, is there a specific reason that you don't believe that an `rc` file will eventually be produced for your situation? (and if so, could we maybe try to protect that instead?)~~. EDIT: I just read the gitter log you copied above. I think I see now!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243137051
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243137051:715,Testability,log,log,715,"~~@ffinfo just to be clear - unless the script explodes in a **really** unusual way, you should _always_ eventually get a return code file. So for us, waiting for it to appear is sufficient. (e.g. even if a command fails, we still get the return code file).~~. ~~In fact, in previous Cromwell versions (before making these backends ""config-file based"") our hard-coded local and SGE backends waited for the return code file and never checked `isAlive` apart from at restart-time.~~. ~~Just so I can understand, is there a specific reason that you don't believe that an `rc` file will eventually be produced for your situation? (and if so, could we maybe try to protect that instead?)~~. EDIT: I just read the gitter log you copied above. I think I see now!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243137051
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243137051:21,Usability,clear,clear,21,"~~@ffinfo just to be clear - unless the script explodes in a **really** unusual way, you should _always_ eventually get a return code file. So for us, waiting for it to appear is sufficient. (e.g. even if a command fails, we still get the return code file).~~. ~~In fact, in previous Cromwell versions (before making these backends ""config-file based"") our hard-coded local and SGE backends waited for the return code file and never checked `isAlive` apart from at restart-time.~~. ~~Just so I can understand, is there a specific reason that you don't believe that an `rc` file will eventually be produced for your situation? (and if so, could we maybe try to protect that instead?)~~. EDIT: I just read the gitter log you copied above. I think I see now!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243137051
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:518,Availability,recover,recover,518,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:594,Availability,alive,alive,594,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:653,Availability,alive,alive,653,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:677,Availability,alive,alive,677,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:731,Availability,alive,alive,731,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:765,Availability,alive,alive,765,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2553,Availability,error,error,2553,"lways appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design there instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:56,Deployability,patch,patch,56,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:828,Deployability,configurat,configuration,828,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2446,Deployability,configurat,configuration,2446,"lways appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design there instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:528,Energy Efficiency,schedul,scheduleOnce,528,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:684,Energy Efficiency,schedul,scheduleOnce,684,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:923,Energy Efficiency,schedul,scheduled,923,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:974,Energy Efficiency,schedul,scheduled,974,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1918,Energy Efficiency,schedul,scheduleOnce,1918," - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2270,Energy Efficiency,schedul,scheduler,2270,"- `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2474,Energy Efficiency,schedul,scheduling,2474,"lways appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design there instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:544,Integrability,message,message,544,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:613,Integrability,message,message,613,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:700,Integrability,message,message,700,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:933,Integrability,message,messages,933,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:957,Integrability,message,message,957,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:828,Modifiability,config,configuration,828,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2446,Modifiability,config,configuration,2446,"lways appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design there instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2500,Modifiability,config,configurable,2500,"lways appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design there instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2596,Modifiability,config,configurable,2596,"lways appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design there instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1240,Performance,cache,cache,1240,"he moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1259,Performance,cache,cache,1259,"unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1304,Performance,cache,cache,1304," look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatt",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1329,Performance,cache,cache,1329,"in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:518,Safety,recover,recover,518,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1943,Testability,test,tests,1943,"eduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2021,Testability,test,tests,2021,"few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suit",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2384,Usability,simpl,simplify,2384,"- `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238
https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-424954064:344,Performance,optimiz,optimization,344,"@LeeTL1220 whats the driving force behind wanting to run with GCS files locally? It would seem most cost effective (by not having to pay egress at all) to run compute to in GCP. However, if you want to use on-prem resources specifically, then simply make a copy a local copy of workflow inputs to start off with. I think this is an interesting optimization but not really a widespread use case and adds more complexity to how Cromwell has to handle localization strategies. I'm inclined to close this issue but feel free to re-open if this essential to your flow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-424954064
https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-424954064:243,Usability,simpl,simply,243,"@LeeTL1220 whats the driving force behind wanting to run with GCS files locally? It would seem most cost effective (by not having to pay egress at all) to run compute to in GCP. However, if you want to use on-prem resources specifically, then simply make a copy a local copy of workflow inputs to start off with. I think this is an interesting optimization but not really a widespread use case and adds more complexity to how Cromwell has to handle localization strategies. I'm inclined to close this issue but feel free to re-open if this essential to your flow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-424954064
https://github.com/broadinstitute/cromwell/pull/1350#issuecomment-242883334:93,Performance,cache,cache,93,"Sorry, I'm not sure what you mean. For context, Ruchi has some forthcoming work for the call cache result stuff that will want to do exactly the same db simpleton -> WdlValueSimpleton conversions as the job store.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1350#issuecomment-242883334
https://github.com/broadinstitute/cromwell/pull/1350#issuecomment-242883334:153,Usability,simpl,simpleton,153,"Sorry, I'm not sure what you mean. For context, Ruchi has some forthcoming work for the call cache result stuff that will want to do exactly the same db simpleton -> WdlValueSimpleton conversions as the job store.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1350#issuecomment-242883334
https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244488817:81,Usability,guid,guide-,81,Actually now that I look at http://doc.akka.io/docs/akka/2.4.0/project/migration-guide-2.3.x-2.4.x.html I might take a spin around our codebase before merging,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244488817
https://github.com/broadinstitute/cromwell/issues/1412#issuecomment-246819803:36,Usability,clear,clear,36,That name was just to make my point clear. I don't care what it's called.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1412#issuecomment-246819803
https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848:190,Integrability,wrap,wrapped,190,"Regarding the supervision/exceptions-- for better or worse, our akka Actors are still using a lot of scala Futures. In the case of the call to `copyCachedOutputs`, the entire method call is wrapped in a `Future.apply()` via `BackendCacheHitCopyingActor.receive`:. ``` scala; def receive: Receive = LoggingReceive {; case CopyOutputsCommand(simpletons, jobDetritus, returnCode) =>; performActionThenRespond(Future(copyCachedOutputs(simpletons, jobDetritus, returnCode)), onFailure = cachingFailed, andThen = context stop self); case AbortJobCommand =>; abort(); context.parent ! AbortedResponse(jobDescriptor.key); context stop self; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848
https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848:381,Performance,perform,performActionThenRespond,381,"Regarding the supervision/exceptions-- for better or worse, our akka Actors are still using a lot of scala Futures. In the case of the call to `copyCachedOutputs`, the entire method call is wrapped in a `Future.apply()` via `BackendCacheHitCopyingActor.receive`:. ``` scala; def receive: Receive = LoggingReceive {; case CopyOutputsCommand(simpletons, jobDetritus, returnCode) =>; performActionThenRespond(Future(copyCachedOutputs(simpletons, jobDetritus, returnCode)), onFailure = cachingFailed, andThen = context stop self); case AbortJobCommand =>; abort(); context.parent ! AbortedResponse(jobDescriptor.key); context stop self; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848
https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848:552,Safety,abort,abort,552,"Regarding the supervision/exceptions-- for better or worse, our akka Actors are still using a lot of scala Futures. In the case of the call to `copyCachedOutputs`, the entire method call is wrapped in a `Future.apply()` via `BackendCacheHitCopyingActor.receive`:. ``` scala; def receive: Receive = LoggingReceive {; case CopyOutputsCommand(simpletons, jobDetritus, returnCode) =>; performActionThenRespond(Future(copyCachedOutputs(simpletons, jobDetritus, returnCode)), onFailure = cachingFailed, andThen = context stop self); case AbortJobCommand =>; abort(); context.parent ! AbortedResponse(jobDescriptor.key); context stop self; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848
https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848:340,Usability,simpl,simpletons,340,"Regarding the supervision/exceptions-- for better or worse, our akka Actors are still using a lot of scala Futures. In the case of the call to `copyCachedOutputs`, the entire method call is wrapped in a `Future.apply()` via `BackendCacheHitCopyingActor.receive`:. ``` scala; def receive: Receive = LoggingReceive {; case CopyOutputsCommand(simpletons, jobDetritus, returnCode) =>; performActionThenRespond(Future(copyCachedOutputs(simpletons, jobDetritus, returnCode)), onFailure = cachingFailed, andThen = context stop self); case AbortJobCommand =>; abort(); context.parent ! AbortedResponse(jobDescriptor.key); context stop self; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848
https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848:431,Usability,simpl,simpletons,431,"Regarding the supervision/exceptions-- for better or worse, our akka Actors are still using a lot of scala Futures. In the case of the call to `copyCachedOutputs`, the entire method call is wrapped in a `Future.apply()` via `BackendCacheHitCopyingActor.receive`:. ``` scala; def receive: Receive = LoggingReceive {; case CopyOutputsCommand(simpletons, jobDetritus, returnCode) =>; performActionThenRespond(Future(copyCachedOutputs(simpletons, jobDetritus, returnCode)), onFailure = cachingFailed, andThen = context stop self); case AbortJobCommand =>; abort(); context.parent ! AbortedResponse(jobDescriptor.key); context stop self; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1439#issuecomment-248343848
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:11398,Availability,down,downstream,11398,"fasta} \; --disable_all_read_filters ${disable_all_read_filters} --interval_set_rule UNION --interval_padding 0 \; --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation ${disable_sequence_dictionary_validation} \; --createOutputBamIndex true --help false --version false --verbosity INFO --QUIET false; \; else touch ${entity_id}.coverage.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; }. #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Calculate coverage on Whole Genome Sequence using Spark.; # This task automatically creates a target output file.; task WholeGenomeCoverage {; String entity_id; File coverage_file ; File target_file; File input_bam; File bam_idx; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; String gatk_jar; Boolean isWGS; Int wgsBinSize; Int mem. # If isWGS is set to true, the task produces WGS coverage and targets that are passed to downstream tasks; # If not, coverage and target files (received from upstream) for WES are passed downstream; command {; if [ ${isWGS} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \; --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \; else cp ${coverage_file} ${entity_id}.coverage.tsv; cp ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; File gatk_target_file = ""${entity_id}.coverage.tsv.targets.tsv""; }; }. # Add new columns to an existing target table with various targets; # Note that this task is optional ; task AnnotateTargets {; String entity_id; File target_file; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then an empty file gets passed downstream; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:11496,Availability,down,downstream,11496,"fasta} \; --disable_all_read_filters ${disable_all_read_filters} --interval_set_rule UNION --interval_padding 0 \; --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation ${disable_sequence_dictionary_validation} \; --createOutputBamIndex true --help false --version false --verbosity INFO --QUIET false; \; else touch ${entity_id}.coverage.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; }. #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Calculate coverage on Whole Genome Sequence using Spark.; # This task automatically creates a target output file.; task WholeGenomeCoverage {; String entity_id; File coverage_file ; File target_file; File input_bam; File bam_idx; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; String gatk_jar; Boolean isWGS; Int wgsBinSize; Int mem. # If isWGS is set to true, the task produces WGS coverage and targets that are passed to downstream tasks; # If not, coverage and target files (received from upstream) for WES are passed downstream; command {; if [ ${isWGS} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \; --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \; else cp ${coverage_file} ${entity_id}.coverage.tsv; cp ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; File gatk_target_file = ""${entity_id}.coverage.tsv.targets.tsv""; }; }. # Add new columns to an existing target table with various targets; # Note that this task is optional ; task AnnotateTargets {; String entity_id; File target_file; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then an empty file gets passed downstream; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:12341,Availability,down,downstream,12341,"eam tasks; # If not, coverage and target files (received from upstream) for WES are passed downstream; command {; if [ ${isWGS} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \; --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \; else cp ${coverage_file} ${entity_id}.coverage.tsv; cp ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; File gatk_target_file = ""${entity_id}.coverage.tsv.targets.tsv""; }; }. # Add new columns to an existing target table with various targets; # Note that this task is optional ; task AnnotateTargets {; String entity_id; File target_file; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then an empty file gets passed downstream; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \; else touch ${entity_id}.annotated.tsv; \; fi; }. output {; File annotated_targets = ""${entity_id}.annotated.tsv""; }; }. # Correct coverage for sample-specific GC bias effects; # Note that this task is optional ; task CorrectGCBias {; String entity_id; File coverage_file; File annotated_targets; String gatk_jar; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then the coverage file gets passed downstream unchanged; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \; --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \; else cp ${coverage_file} ${entity_id}.gc_corrected_coverage.tsv; \; fi; }. output {; File gatk_cnv_coverage_file_gcbias = ""${entity_id}.gc_corrected_coverage.tsv""; }; }. # Perform tangent normaliz",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:12970,Availability,down,downstream,12970,"existing target table with various targets; # Note that this task is optional ; task AnnotateTargets {; String entity_id; File target_file; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then an empty file gets passed downstream; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \; else touch ${entity_id}.annotated.tsv; \; fi; }. output {; File annotated_targets = ""${entity_id}.annotated.tsv""; }; }. # Correct coverage for sample-specific GC bias effects; # Note that this task is optional ; task CorrectGCBias {; String entity_id; File coverage_file; File annotated_targets; String gatk_jar; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then the coverage file gets passed downstream unchanged; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \; --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \; else cp ${coverage_file} ${entity_id}.gc_corrected_coverage.tsv; \; fi; }. output {; File gatk_cnv_coverage_file_gcbias = ""${entity_id}.gc_corrected_coverage.tsv""; }; }. # Perform tangent normalization (noise reduction) on the proportional coverage file.; task NormalizeSomaticReadCounts {; String entity_id; File coverage_file; File padded_target_file; File pon; String gatk_jar; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} NormalizeSomaticReadCounts --input ${coverage_file} \; --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \; --betaHatsOutput ${entity_id}.betaHats.tsv --preTangentNormalized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:666,Modifiability,variab,variable,666,"Tell me if you need me to post the metadata ...it is very big for github. WDL is below... ```; #; # Case sample workflow for a list of pairs of case-control samples. Includes GATK CNV and ACNV. Supports both WGS and WES samples. This was tested on a3c7368 commit of gatk-protected.; #; # Notes:; #; # - the input file(input_bam_list) must contain a list of tab separated values in the following format(one or more lines must be supplied):; # tumor_entity tumor_bam tumor_bam_index normal_entity normal_bam normal_bam_index <--first input; # tumor_entity tumor_bam tumor_bam_index normal_entity normal_bam normal_bam_index <--second input; # etc...; #; # - set isWGS variable to true or false to specify whether to run a WGS or WES workflow respectively; #; # - file names will use the entity ID specified, but inside the file, the bam SM tag will typically be used.; #; # - target file (which must be in tsv format) is only used with WES workflow, WGS workflow generates its own targets (so user can pass any string as an argument); #; # - the WGS PoN must be generated with WGS samples; # ; # - THIS SCRIPT SHOULD BE CONSIDERED OF ""BETA"" QUALITY; #; # - Example invocation:; # java -jar cromwell.jar run case_gatk_acnv_workflow.wdl myParameters.json; # - See case_gatk_acnv_workflow_template.json for a template json file to modify with your own parameters (please save; # your modified version with a different filename and do not commit to gatk-protected repo).; #; # - Some call inputs might seem out of place - consult with the comments in task definitions for details; #; #############. workflow case_gatk_acnv_workflow {; # Workflow input files; File target_file; File ref_fasta; File ref_fasta_dict; File ref_fasta_fai; File common_snp_list; File input_bam_list; Array[Array[String]] bam_list_array = read_tsv(input_bam_list); File PoN; String gatk_jar. # Input parameters of the PerformSegmentation tool; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_min",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:238,Testability,test,tested,238,"Tell me if you need me to post the metadata ...it is very big for github. WDL is below... ```; #; # Case sample workflow for a list of pairs of case-control samples. Includes GATK CNV and ACNV. Supports both WGS and WES samples. This was tested on a3c7368 commit of gatk-protected.; #; # Notes:; #; # - the input file(input_bam_list) must contain a list of tab separated values in the following format(one or more lines must be supplied):; # tumor_entity tumor_bam tumor_bam_index normal_entity normal_bam normal_bam_index <--first input; # tumor_entity tumor_bam tumor_bam_index normal_entity normal_bam normal_bam_index <--second input; # etc...; #; # - set isWGS variable to true or false to specify whether to run a WGS or WES workflow respectively; #; # - file names will use the entity ID specified, but inside the file, the bam SM tag will typically be used.; #; # - target file (which must be in tsv format) is only used with WES workflow, WGS workflow generates its own targets (so user can pass any string as an argument); #; # - the WGS PoN must be generated with WGS samples; # ; # - THIS SCRIPT SHOULD BE CONSIDERED OF ""BETA"" QUALITY; #; # - Example invocation:; # java -jar cromwell.jar run case_gatk_acnv_workflow.wdl myParameters.json; # - See case_gatk_acnv_workflow_template.json for a template json file to modify with your own parameters (please save; # your modified version with a different filename and do not commit to gatk-protected repo).; #; # - Some call inputs might seem out of place - consult with the comments in task definitions for details; #; #############. workflow case_gatk_acnv_workflow {; # Workflow input files; File target_file; File ref_fasta; File ref_fasta_dict; File ref_fasta_fai; File common_snp_list; File input_bam_list; Array[Array[String]] bam_list_array = read_tsv(input_bam_list); File PoN; String gatk_jar. # Input parameters of the PerformSegmentation tool; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_min",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:14942,Usability,undo,undoSplits,14942,"alized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nmin; Float seg_param_eta; Float seg_param_trim; String seg_param_undoSplits; Float seg_param_undoPrune; Int seg_param_undoSD; String gatk_jar; File tn_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \; --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \; --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \; --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \; --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \; --verbosity INFO --QUIET false; }. output {; File seg_file = ""${entity_id}.seg""; }; }. # Make calls (amp, neutral, or deleted) on each segment.; task Caller {; String entity_id; String gatk_jar; File tn_file; File seg_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \; --segments ${seg_file} --output ${entity_id}.called --legacy false \; --help false --version false --verbosity INFO --QUIET false; }. output {; File called_file=""${entity_id}.called""; }; }. # Call heterozygous SNPs in the normal and then count the reads in the tumor for each called position.; # Entity IDs can be the same value; task HetPulldown {; String entity_id_tumor; String entity_id_normal; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; File tumor_bam; File tumor_bam_idx; File",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:14982,Usability,undo,undoPrune,14982,"alized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nmin; Float seg_param_eta; Float seg_param_trim; String seg_param_undoSplits; Float seg_param_undoPrune; Int seg_param_undoSD; String gatk_jar; File tn_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \; --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \; --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \; --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \; --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \; --verbosity INFO --QUIET false; }. output {; File seg_file = ""${entity_id}.seg""; }; }. # Make calls (amp, neutral, or deleted) on each segment.; task Caller {; String entity_id; String gatk_jar; File tn_file; File seg_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \; --segments ${seg_file} --output ${entity_id}.called --legacy false \; --help false --version false --verbosity INFO --QUIET false; }. output {; File called_file=""${entity_id}.called""; }; }. # Call heterozygous SNPs in the normal and then count the reads in the tumor for each called position.; # Entity IDs can be the same value; task HetPulldown {; String entity_id_tumor; String entity_id_normal; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; File tumor_bam; File tumor_bam_idx; File",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151:15017,Usability,undo,undoSD,15017,"alized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nmin; Float seg_param_eta; Float seg_param_trim; String seg_param_undoSplits; Float seg_param_undoPrune; Int seg_param_undoSD; String gatk_jar; File tn_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \; --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \; --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \; --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \; --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \; --verbosity INFO --QUIET false; }. output {; File seg_file = ""${entity_id}.seg""; }; }. # Make calls (amp, neutral, or deleted) on each segment.; task Caller {; String entity_id; String gatk_jar; File tn_file; File seg_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \; --segments ${seg_file} --output ${entity_id}.called --legacy false \; --help false --version false --verbosity INFO --QUIET false; }. output {; File called_file=""${entity_id}.called""; }; }. # Call heterozygous SNPs in the normal and then count the reads in the tumor for each called position.; # Entity IDs can be the same value; task HetPulldown {; String entity_id_tumor; String entity_id_normal; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; File tumor_bam; File tumor_bam_idx; File",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488#issuecomment-249696151
https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-325470216:104,Usability,simpl,simplify,104,"Agree with @Horneth in that my interpretation was that this ticket was looking for an outside script to simplify running things, not cromwellian behavior itself",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-325470216
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:369,Availability,alive,alive,369,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:426,Availability,alive,alive,426,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:466,Availability,alive,alive,466,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:527,Availability,alive,alive,527,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:101,Energy Efficiency,schedul,schedulers,101,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:135,Energy Efficiency,schedul,scheduler,135,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:608,Testability,test,test,608,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:13,Usability,simpl,simple,13,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:787,Availability,alive,alive-or-dead,787,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:846,Availability,alive,alive,846,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:248,Deployability,configurat,configuration,248,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:717,Energy Efficiency,schedul,scheduler,717,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:769,Energy Efficiency,schedul,scheduler,769,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:817,Energy Efficiency,schedul,scheduler,817,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:248,Modifiability,config,configuration,248,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:853,Safety,abort,aborts,853,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:518,Usability,simpl,simple,518,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578
https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-406950091:44,Usability,simpl,simple,44,FYI Our current workaround is running [this simple script](https://gist.github.com/delocalizer/6b4d97158044e1331f3c4393c9e05586) once an hour as a cron job. The details may differ slightly for SGE/SLURM/etc backends.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-406950091
https://github.com/broadinstitute/cromwell/pull/1500#issuecomment-250574493:123,Testability,log,logic,123,@geoffjentry We could certainly move the token dispenser into a per-BE model (and then only have one tokenPool to make the logic simpler. Hooray!). Also FWIW I don't believe the EJEA is a hairball. It's just getting large enough to merit decomposing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1500#issuecomment-250574493
https://github.com/broadinstitute/cromwell/pull/1500#issuecomment-250574493:129,Usability,simpl,simpler,129,@geoffjentry We could certainly move the token dispenser into a per-BE model (and then only have one tokenPool to make the logic simpler. Hooray!). Also FWIW I don't believe the EJEA is a hairball. It's just getting large enough to merit decomposing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1500#issuecomment-250574493
https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037:36,Deployability,rolling,rolling,36,"Yeah, it's definitely easier to get rolling. I found pretty quickly in Centaur that I was rolling my own extensions to Ficus (typeclasses IIRC?) which seemed to defeat the point. I switched to configs as it gives you more stuff out of the box. The monad returns I've both enjoyed and shaken my fist at depending on the situation. I liked these two better than any of the rest I saw. . I didn't look at the PR but if you didn't need to do any extensions I suppose it doesn't matter for now. . Edit: To be clear, ""I suppose it doesn't matter"" means ""carry on"", ficus was definitely easier to use until I found myself making custom typeclasses and wondering if i should stick them in Lenthall.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037
https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037:90,Deployability,rolling,rolling,90,"Yeah, it's definitely easier to get rolling. I found pretty quickly in Centaur that I was rolling my own extensions to Ficus (typeclasses IIRC?) which seemed to defeat the point. I switched to configs as it gives you more stuff out of the box. The monad returns I've both enjoyed and shaken my fist at depending on the situation. I liked these two better than any of the rest I saw. . I didn't look at the PR but if you didn't need to do any extensions I suppose it doesn't matter for now. . Edit: To be clear, ""I suppose it doesn't matter"" means ""carry on"", ficus was definitely easier to use until I found myself making custom typeclasses and wondering if i should stick them in Lenthall.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037
https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037:302,Integrability,depend,depending,302,"Yeah, it's definitely easier to get rolling. I found pretty quickly in Centaur that I was rolling my own extensions to Ficus (typeclasses IIRC?) which seemed to defeat the point. I switched to configs as it gives you more stuff out of the box. The monad returns I've both enjoyed and shaken my fist at depending on the situation. I liked these two better than any of the rest I saw. . I didn't look at the PR but if you didn't need to do any extensions I suppose it doesn't matter for now. . Edit: To be clear, ""I suppose it doesn't matter"" means ""carry on"", ficus was definitely easier to use until I found myself making custom typeclasses and wondering if i should stick them in Lenthall.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037
https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037:193,Modifiability,config,configs,193,"Yeah, it's definitely easier to get rolling. I found pretty quickly in Centaur that I was rolling my own extensions to Ficus (typeclasses IIRC?) which seemed to defeat the point. I switched to configs as it gives you more stuff out of the box. The monad returns I've both enjoyed and shaken my fist at depending on the situation. I liked these two better than any of the rest I saw. . I didn't look at the PR but if you didn't need to do any extensions I suppose it doesn't matter for now. . Edit: To be clear, ""I suppose it doesn't matter"" means ""carry on"", ficus was definitely easier to use until I found myself making custom typeclasses and wondering if i should stick them in Lenthall.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037
https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037:504,Usability,clear,clear,504,"Yeah, it's definitely easier to get rolling. I found pretty quickly in Centaur that I was rolling my own extensions to Ficus (typeclasses IIRC?) which seemed to defeat the point. I switched to configs as it gives you more stuff out of the box. The monad returns I've both enjoyed and shaken my fist at depending on the situation. I liked these two better than any of the rest I saw. . I didn't look at the PR but if you didn't need to do any extensions I suppose it doesn't matter for now. . Edit: To be clear, ""I suppose it doesn't matter"" means ""carry on"", ficus was definitely easier to use until I found myself making custom typeclasses and wondering if i should stick them in Lenthall.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1554#issuecomment-252431037
https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327723019:165,Performance,queue,queue,165,"Hi,; Kate brought me here by this thread in the [forum](https://gatkforums.broadinstitute.org/wdl/discussion/10296/prioritize-workflows-which-are-allready-in-server-queue#latest) . It mostly covers the features @kcibul already requested. In addition, I would like to have an API command which forces a workflow directly to start by sending an actual running workflow to sleep/ pause. Maybe this could go hand in hand with call-caching for the sleeping workflow?. Greetings Selonka / EADG",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327723019
https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327723019:377,Usability,pause,pause,377,"Hi,; Kate brought me here by this thread in the [forum](https://gatkforums.broadinstitute.org/wdl/discussion/10296/prioritize-workflows-which-are-allready-in-server-queue#latest) . It mostly covers the features @kcibul already requested. In addition, I would like to have an API command which forces a workflow directly to start by sending an actual running workflow to sleep/ pause. Maybe this could go hand in hand with call-caching for the sleeping workflow?. Greetings Selonka / EADG",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327723019
https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257119375:14,Usability,clear,clear,14,"@kcibul To be clear you mean that you only exercised the /batch endpoint, right? There's no weird reliance on that (I can't imagine how that even would be the case otherwise)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1571#issuecomment-257119375
https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-328295266:16,Usability,clear,clear,16,@patmagee to be clear i'm not *too* worried about that implementation statement I made. It's just that I think this particular concept is trending in that direction so I'd like us to be careful.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-328295266
https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-328279623:134,Integrability,interface,interface,134,"hands on keyboard effort is relatively low. the real work is defining how to specify it, although now that we have the snazzy new CLI interface it's probably pretty simple to do so",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-328279623
https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-328279623:165,Usability,simpl,simple,165,"hands on keyboard effort is relatively low. the real work is defining how to specify it, although now that we have the snazzy new CLI interface it's probably pretty simple to do so",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1578#issuecomment-328279623
https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992:417,Security,password,password,417,"I feel uneasy recommending an unencrypted connection to a database, especially when the MySQL team went out of their way to start warning about this issue. That said, the simplest copypasta you can use to remove that warning is to pass the specified parameter in your database url:. In this stanza, change the url from:. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```. To:. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?useSSL=false""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```. Or if there are already other params already on your url, append using ""&"" instead of ""?"":. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?other=param&useSSL=false""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992
https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992:584,Security,password,password,584,"I feel uneasy recommending an unencrypted connection to a database, especially when the MySQL team went out of their way to start warning about this issue. That said, the simplest copypasta you can use to remove that warning is to pass the specified parameter in your database url:. In this stanza, change the url from:. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```. To:. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?useSSL=false""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```. Or if there are already other params already on your url, append using ""&"" instead of ""?"":. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?other=param&useSSL=false""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992
https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992:850,Security,password,password,850,"I feel uneasy recommending an unencrypted connection to a database, especially when the MySQL team went out of their way to start warning about this issue. That said, the simplest copypasta you can use to remove that warning is to pass the specified parameter in your database url:. In this stanza, change the url from:. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```. To:. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?useSSL=false""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```. Or if there are already other params already on your url, append using ""&"" instead of ""?"":. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?other=param&useSSL=false""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992
https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992:171,Usability,simpl,simplest,171,"I feel uneasy recommending an unencrypted connection to a database, especially when the MySQL team went out of their way to start warning about this issue. That said, the simplest copypasta you can use to remove that warning is to pass the specified parameter in your database url:. In this stanza, change the url from:. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```. To:. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?useSSL=false""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```. Or if there are already other params already on your url, append using ""&"" instead of ""?"":. ```; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?other=param&useSSL=false""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992
https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254525171:573,Security,password,password,573,"This is good for casual users, thanks @kshakir. On Tue, Oct 18, 2016 at 10:25 AM, kshakir notifications@github.com wrote:. > I feel uneasy recommending an unencrypted connection to a database,; > especially when the MySQL team went out of their way to start warning about; > this issue.; > ; > That said, the simplest copypasta you can use to remove that warning is to; > pass the specified parameter in your database url:; > ; > In this stanza, change the url from:; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > To:; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell?useSSL=false""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > Or if there are already other params already on your url, append using ""&""; > instead of ""?"":; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell?other=param&useSSL=false""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk53aShDWYxlKJyOGeGW4XyTntKdFks5q1NbxgaJpZM4KZ1eV; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254525171
https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254525171:754,Security,password,password,754,"This is good for casual users, thanks @kshakir. On Tue, Oct 18, 2016 at 10:25 AM, kshakir notifications@github.com wrote:. > I feel uneasy recommending an unencrypted connection to a database,; > especially when the MySQL team went out of their way to start warning about; > this issue.; > ; > That said, the simplest copypasta you can use to remove that warning is to; > pass the specified parameter in your database url:; > ; > In this stanza, change the url from:; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > To:; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell?useSSL=false""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > Or if there are already other params already on your url, append using ""&""; > instead of ""?"":; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell?other=param&useSSL=false""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk53aShDWYxlKJyOGeGW4XyTntKdFks5q1NbxgaJpZM4KZ1eV; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254525171
https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254525171:1037,Security,password,password,1037,"This is good for casual users, thanks @kshakir. On Tue, Oct 18, 2016 at 10:25 AM, kshakir notifications@github.com wrote:. > I feel uneasy recommending an unencrypted connection to a database,; > especially when the MySQL team went out of their way to start warning about; > this issue.; > ; > That said, the simplest copypasta you can use to remove that warning is to; > pass the specified parameter in your database url:; > ; > In this stanza, change the url from:; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > To:; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell?useSSL=false""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > Or if there are already other params already on your url, append using ""&""; > instead of ""?"":; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell?other=param&useSSL=false""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk53aShDWYxlKJyOGeGW4XyTntKdFks5q1NbxgaJpZM4KZ1eV; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254525171
https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254525171:309,Usability,simpl,simplest,309,"This is good for casual users, thanks @kshakir. On Tue, Oct 18, 2016 at 10:25 AM, kshakir notifications@github.com wrote:. > I feel uneasy recommending an unencrypted connection to a database,; > especially when the MySQL team went out of their way to start warning about; > this issue.; > ; > That said, the simplest copypasta you can use to remove that warning is to; > pass the specified parameter in your database url:; > ; > In this stanza, change the url from:; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > To:; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell?useSSL=false""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > Or if there are already other params already on your url, append using ""&""; > instead of ""?"":; > ; > db {; > driver = ""com.mysql.jdbc.Driver""; > url = ""jdbc:mysql://host/cromwell?other=param&useSSL=false""; > user = ""user""; > password = ""pass""; > connectionTimeout = 5000; > }; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254523992,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk53aShDWYxlKJyOGeGW4XyTntKdFks5q1NbxgaJpZM4KZ1eV; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591#issuecomment-254525171
https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277885430:13,Security,hash,hashCode,13,"At the time `hashCode` was all over the Ctrl-\ thread dumps. After seeing all the expensive computation in `hashCode` and considering that keys should have been comparable via instance equality, it became clear that was the problem. It sounds like that's not what you're seeing now?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277885430
https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277885430:108,Security,hash,hashCode,108,"At the time `hashCode` was all over the Ctrl-\ thread dumps. After seeing all the expensive computation in `hashCode` and considering that keys should have been comparable via instance equality, it became clear that was the problem. It sounds like that's not what you're seeing now?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277885430
https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277885430:205,Usability,clear,clear,205,"At the time `hashCode` was all over the Ctrl-\ thread dumps. After seeing all the expensive computation in `hashCode` and considering that keys should have been comparable via instance equality, it became clear that was the problem. It sounds like that's not what you're seeing now?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-277885430
https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-255139898:137,Usability,simpl,simple,137,"is overloading the right thing to do here?. i think so, but as this is a DSL it's worth focus grouping to make ure it is still viewed as simple",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-255139898
https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270205111:273,Availability,down,down,273,"@knoblett I would imagine you can combine a `size()` with a `range()` to scatter over the elements in a list (I think). I don't mind the overloading but would `length()` and `size()` be simpler here? I'm not really keen on stuff like `array_size` because then I usually go down the road of what other `X_size` exist, what do they do, and why are they special but I might just not be normal.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270205111
https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270205111:186,Usability,simpl,simpler,186,"@knoblett I would imagine you can combine a `size()` with a `range()` to scatter over the elements in a list (I think). I don't mind the overloading but would `length()` and `size()` be simpler here? I'm not really keen on stuff like `array_size` because then I usually go down the road of what other `X_size` exist, what do they do, and why are they special but I might just not be normal.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270205111
https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270723595:68,Usability,feedback,feedback,68,"@knoblett That's exactly the reason we try to spread a wide net for feedback on these syntactical choices, I feel you all have the pulse of that sort of user a lot better than we do. So keep on doing what you're doing :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270723595
https://github.com/broadinstitute/cromwell/issues/1608#issuecomment-255594077:49,Usability,simpl,simple,49,"Yes, this comes from a user trying to run a very simple wdl, but because of the way localization / docker / path transformation works in cromwell something like that doesn't work:. ```; task foo {; File myinputfile # let's say this is /Users/me/myfile.txt. command {; dosomething -using ${myinputfile} > ${myinputfile}.samples; // evaluates to dosomething -using /Users/me/cromwell_executions/.../inputs/Users/me/myfile.txt > /Users/me/cromwell_executions/.../inputs/Users/me/myfile.txt.samples; }. output {; File out = ${myinputfile}.samples # this doesn't work because myinputfile resolves to /Users/me/myfile.txt.samples; }; }; ```. Now that I write this I realize that maybe the real issue is actually fixing the above behaviour ?. Regardless, what the user really wanted to do here is write the output of his command to a file having the same name as his input file + "".samples"", which it turns out is a lot more difficult than it should.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1608#issuecomment-255594077
https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256199538:87,Usability,clear,clearly,87,"I should add that I was using expression a little too liberally, I was also referring (clearly, based on my examples) to what we call engine functions",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1618#issuecomment-256199538
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-345849355:636,Safety,risk,risks,636,">The issue is that she's asking to have everything dumped into one location. In many workflows I have a task like this:; ```. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; And I apply this task to all final outputs of my workflows because my colleagues do not want to go into super-nested folder structure with many debugging files (like logs and so on), they just want to get the final results! Having a flat-copy feature will save me from copy-pasting this task everywhere =) . Regarding overwrite risks, I think they are exaggerated:; 1) Usually, it takes you multiple runs until you get everything working, however, after that you switch to another dataset and point other members of the team to the folder they should go to pick the results from you. As I understand the copying of the workflow output happens only when the workflow succeeded.; 2) The final output folder is assigned in the options. That means that for another run you can simply change it.; 3) It is possible to put rewriting only if last file is newer than previous.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-345849355
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-345849355:474,Testability,log,logs,474,">The issue is that she's asking to have everything dumped into one location. In many workflows I have a task like this:; ```. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; And I apply this task to all final outputs of my workflows because my colleagues do not want to go into super-nested folder structure with many debugging files (like logs and so on), they just want to get the final results! Having a flat-copy feature will save me from copy-pasting this task everywhere =) . Regarding overwrite risks, I think they are exaggerated:; 1) Usually, it takes you multiple runs until you get everything working, however, after that you switch to another dataset and point other members of the team to the folder they should go to pick the results from you. As I understand the copying of the workflow output happens only when the workflow succeeded.; 2) The final output folder is assigned in the options. That means that for another run you can simply change it.; 3) It is possible to put rewriting only if last file is newer than previous.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-345849355
https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-345849355:1081,Usability,simpl,simply,1081,">The issue is that she's asking to have everything dumped into one location. In many workflows I have a task like this:; ```. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; And I apply this task to all final outputs of my workflows because my colleagues do not want to go into super-nested folder structure with many debugging files (like logs and so on), they just want to get the final results! Having a flat-copy feature will save me from copy-pasting this task everywhere =) . Regarding overwrite risks, I think they are exaggerated:; 1) Usually, it takes you multiple runs until you get everything working, however, after that you switch to another dataset and point other members of the team to the folder they should go to pick the results from you. As I understand the copying of the workflow output happens only when the workflow succeeded.; 2) The final output folder is assigned in the options. That means that for another run you can simply change it.; 3) It is possible to put rewriting only if last file is newer than previous.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-345849355
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265755507:41,Usability,simpl,simply,41,@geoffjentry Any ideas? Looks like it is simply not exiting.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265755507
https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266028389:57,Usability,clear,clear,57,That's awesome!! It's great that the actual issue is now clear too.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266028389
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531:956,Deployability,update,update,956,"Discussed with @katevoss - as we don't have our long range documentation plan hashed out yet for now we'll be:; - Making a separate doc (name tbd) with two initial bits. One is the content from @delocalizer describing a user based setup. We'll make it clear that this is their set up and not ours, both from the indemnity angle that @cjllanwarne was concerned about but more importantly because it helps to demonstrate the vibrant community which is building around Cromwell; - I'll add a second section describing Firecloud's security model; - That doc will be linked from the README; - We'll set up a blog post on the main site describing security/auth options in Cromwell and directly referencing this doc. Readers/users will be encouraged to ask questions, provide alternate suggestions, etc. The security doc (for lack of a better word atm) will be more of a living doc. @delocalizer ... I don't want to make extra work for you here. If you wanted to update this PR to reflect the first and third bullet points great, otherwise I can pick up this PR and do first and third while i'm doing the second. If you're going with the former hold off until I confirm the name of the file :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531:78,Security,hash,hashed,78,"Discussed with @katevoss - as we don't have our long range documentation plan hashed out yet for now we'll be:; - Making a separate doc (name tbd) with two initial bits. One is the content from @delocalizer describing a user based setup. We'll make it clear that this is their set up and not ours, both from the indemnity angle that @cjllanwarne was concerned about but more importantly because it helps to demonstrate the vibrant community which is building around Cromwell; - I'll add a second section describing Firecloud's security model; - That doc will be linked from the README; - We'll set up a blog post on the main site describing security/auth options in Cromwell and directly referencing this doc. Readers/users will be encouraged to ask questions, provide alternate suggestions, etc. The security doc (for lack of a better word atm) will be more of a living doc. @delocalizer ... I don't want to make extra work for you here. If you wanted to update this PR to reflect the first and third bullet points great, otherwise I can pick up this PR and do first and third while i'm doing the second. If you're going with the former hold off until I confirm the name of the file :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531:527,Security,secur,security,527,"Discussed with @katevoss - as we don't have our long range documentation plan hashed out yet for now we'll be:; - Making a separate doc (name tbd) with two initial bits. One is the content from @delocalizer describing a user based setup. We'll make it clear that this is their set up and not ours, both from the indemnity angle that @cjllanwarne was concerned about but more importantly because it helps to demonstrate the vibrant community which is building around Cromwell; - I'll add a second section describing Firecloud's security model; - That doc will be linked from the README; - We'll set up a blog post on the main site describing security/auth options in Cromwell and directly referencing this doc. Readers/users will be encouraged to ask questions, provide alternate suggestions, etc. The security doc (for lack of a better word atm) will be more of a living doc. @delocalizer ... I don't want to make extra work for you here. If you wanted to update this PR to reflect the first and third bullet points great, otherwise I can pick up this PR and do first and third while i'm doing the second. If you're going with the former hold off until I confirm the name of the file :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531:641,Security,secur,security,641,"Discussed with @katevoss - as we don't have our long range documentation plan hashed out yet for now we'll be:; - Making a separate doc (name tbd) with two initial bits. One is the content from @delocalizer describing a user based setup. We'll make it clear that this is their set up and not ours, both from the indemnity angle that @cjllanwarne was concerned about but more importantly because it helps to demonstrate the vibrant community which is building around Cromwell; - I'll add a second section describing Firecloud's security model; - That doc will be linked from the README; - We'll set up a blog post on the main site describing security/auth options in Cromwell and directly referencing this doc. Readers/users will be encouraged to ask questions, provide alternate suggestions, etc. The security doc (for lack of a better word atm) will be more of a living doc. @delocalizer ... I don't want to make extra work for you here. If you wanted to update this PR to reflect the first and third bullet points great, otherwise I can pick up this PR and do first and third while i'm doing the second. If you're going with the former hold off until I confirm the name of the file :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531:801,Security,secur,security,801,"Discussed with @katevoss - as we don't have our long range documentation plan hashed out yet for now we'll be:; - Making a separate doc (name tbd) with two initial bits. One is the content from @delocalizer describing a user based setup. We'll make it clear that this is their set up and not ours, both from the indemnity angle that @cjllanwarne was concerned about but more importantly because it helps to demonstrate the vibrant community which is building around Cromwell; - I'll add a second section describing Firecloud's security model; - That doc will be linked from the README; - We'll set up a blog post on the main site describing security/auth options in Cromwell and directly referencing this doc. Readers/users will be encouraged to ask questions, provide alternate suggestions, etc. The security doc (for lack of a better word atm) will be more of a living doc. @delocalizer ... I don't want to make extra work for you here. If you wanted to update this PR to reflect the first and third bullet points great, otherwise I can pick up this PR and do first and third while i'm doing the second. If you're going with the former hold off until I confirm the name of the file :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531
https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531:252,Usability,clear,clear,252,"Discussed with @katevoss - as we don't have our long range documentation plan hashed out yet for now we'll be:; - Making a separate doc (name tbd) with two initial bits. One is the content from @delocalizer describing a user based setup. We'll make it clear that this is their set up and not ours, both from the indemnity angle that @cjllanwarne was concerned about but more importantly because it helps to demonstrate the vibrant community which is building around Cromwell; - I'll add a second section describing Firecloud's security model; - That doc will be linked from the README; - We'll set up a blog post on the main site describing security/auth options in Cromwell and directly referencing this doc. Readers/users will be encouraged to ask questions, provide alternate suggestions, etc. The security doc (for lack of a better word atm) will be more of a living doc. @delocalizer ... I don't want to make extra work for you here. If you wanted to update this PR to reflect the first and third bullet points great, otherwise I can pick up this PR and do first and third while i'm doing the second. If you're going with the former hold off until I confirm the name of the file :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259449531
https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260527636:209,Performance,load,load,209,"@seandavi Ah, it's something @kcibul whipped up which runs on google app engine which presents the JES API for trivial tasks - we've been using it to be able to run things which test the cromwell engine under load w/o having need to run up a large bill (or run into quota issues!) on JES. I've been starting to use it heavily and have run into some weird issues, such as this ticket and those unexpected actor death notifications from the other issue. I've been theorizing that they're due to responses from appengine which we don't see in JES but i need to verify that - and clearly that's not the case w/ the unexpected actor death ones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260527636
https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260527636:178,Testability,test,test,178,"@seandavi Ah, it's something @kcibul whipped up which runs on google app engine which presents the JES API for trivial tasks - we've been using it to be able to run things which test the cromwell engine under load w/o having need to run up a large bill (or run into quota issues!) on JES. I've been starting to use it heavily and have run into some weird issues, such as this ticket and those unexpected actor death notifications from the other issue. I've been theorizing that they're due to responses from appengine which we don't see in JES but i need to verify that - and clearly that's not the case w/ the unexpected actor death ones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260527636
https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260527636:576,Usability,clear,clearly,576,"@seandavi Ah, it's something @kcibul whipped up which runs on google app engine which presents the JES API for trivial tasks - we've been using it to be able to run things which test the cromwell engine under load w/o having need to run up a large bill (or run into quota issues!) on JES. I've been starting to use it heavily and have run into some weird issues, such as this ticket and those unexpected actor death notifications from the other issue. I've been theorizing that they're due to responses from appengine which we don't see in JES but i need to verify that - and clearly that's not the case w/ the unexpected actor death ones.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662#issuecomment-260527636
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260120082:298,Availability,alive,alive,298,"I was able to track partway back on the Google side of things. In short, Cromwell reported jobs in the ""Running"" state. These were associated with Google Genomics API operations, also listed as ""Running"". However, the instances that were supposed to be associated with those GG operations were not alive. I noticed in my billing statement that there were credits to offset compute instances, so I suspect that something happened at Google that was unexpected. I'll dig a bit further. . In the meantime, it would definitely help to have a way to differentiate exceptions that are ""expected"" from those that are not. In this case, I assumed that a restart of a service after an exception was probably OK, but it was associated with ""dead"" jobs that now appear to have been on the google side of things and not with Cromwell directly. I'll follow up with whatever else I can learn.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260120082
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260120082:872,Usability,learn,learn,872,"I was able to track partway back on the Google side of things. In short, Cromwell reported jobs in the ""Running"" state. These were associated with Google Genomics API operations, also listed as ""Running"". However, the instances that were supposed to be associated with those GG operations were not alive. I noticed in my billing statement that there were credits to offset compute instances, so I suspect that something happened at Google that was unexpected. I'll dig a bit further. . In the meantime, it would definitely help to have a way to differentiate exceptions that are ""expected"" from those that are not. In this case, I assumed that a restart of a service after an exception was probably OK, but it was associated with ""dead"" jobs that now appear to have been on the google side of things and not with Cromwell directly. I'll follow up with whatever else I can learn.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260120082
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:1000,Availability,mask,masking,1000,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:255,Performance,latency,latency,255,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:316,Safety,detect,detecting,316,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:1051,Testability,log,logging,1051,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:1332,Testability,log,logging,1332,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:1393,Testability,log,log,1393,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:51,Usability,clear,clear,51,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711
https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260719088:750,Usability,simpl,simplify,750,"@geoffjentry I agree with crawling before running, and have definitely seen how busy you are through the evolution of Cromwell over the years, which will always be appreciated. I do not disagree that the Google folks understand how to parallelize their products/services, but that does not mean they are the only ones to understand the concepts and how to actually implement it. I agree it is not an overnight thing, as it took me many years to put the pieces together. I think by opening these discussions with the larger Community here, you might be surprised how many more people can synergistically help you and your team out. Vetting design decisions from the todo list before they approach an implementation stage, can sometimes streamline and simplify multiple requests from the list. I understand that Cromwell is a mix of features to satisfy sometimes users' legacy preferences, but if an implemented backend of Cromwell has the capability to scale in such a way as to become a paradigm shift in the way one does analysis, I am sure users would welcome that. Again I do not disagree with crawling, though I feel the Community would welcome the opportunity to help pave paths with the transition to running :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260719088
https://github.com/broadinstitute/cromwell/issues/1669#issuecomment-325686560:53,Usability,clear,clear,53,IMHO since the instructions to reproduce this are so clear it might be more appropriate for us to check this rather than putting that on @kcibul. It might even be something for the current bug rotation person to look at... 🙂,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1669#issuecomment-325686560
https://github.com/broadinstitute/cromwell/pull/1686#issuecomment-261366667:166,Usability,simpl,simpler,166,@cjllanwarne re retry: i went back and forth on that topic and managed to come up with an argument I felt was valid the other way. This definition ended up being far simpler so I chose the short path.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1686#issuecomment-261366667
https://github.com/broadinstitute/cromwell/pull/1693#issuecomment-261994351:139,Usability,simpl,simple,139,"LGTM. Some sort of comment in the docs about when/why I'd want to use this might be nice. My thinking is, if I just want to ""try it out as simple as possible"" I probably don't want to be scared off by all these options?. Anyway, that's just a ToL so 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1693/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1693#issuecomment-261994351
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586:169,Performance,cache,cache,169,"@geoffjentry Is this at the time of running the workflow, or after the fact (like #1670)?. As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **avoid reusing bad or old cache results**. ; - Effort: **Small**; - Risk: **Small to Mediume**; - This should not be a runtime attribute; - Make sure users don't overuse this feature and eliminate the benefits of call caching (i.e. clearly state when users should use this); - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586:187,Performance,cache,cache,187,"@geoffjentry Is this at the time of running the workflow, or after the fact (like #1670)?. As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **avoid reusing bad or old cache results**. ; - Effort: **Small**; - Risk: **Small to Mediume**; - This should not be a runtime attribute; - Make sure users don't overuse this feature and eliminate the benefits of call caching (i.e. clearly state when users should use this); - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586:240,Performance,cache,cache,240,"@geoffjentry Is this at the time of running the workflow, or after the fact (like #1670)?. As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **avoid reusing bad or old cache results**. ; - Effort: **Small**; - Risk: **Small to Mediume**; - This should not be a runtime attribute; - Make sure users don't overuse this feature and eliminate the benefits of call caching (i.e. clearly state when users should use this); - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586:215,Safety,avoid,avoid,215,"@geoffjentry Is this at the time of running the workflow, or after the fact (like #1670)?. As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **avoid reusing bad or old cache results**. ; - Effort: **Small**; - Risk: **Small to Mediume**; - This should not be a runtime attribute; - Make sure users don't overuse this feature and eliminate the benefits of call caching (i.e. clearly state when users should use this); - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586
https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586:446,Usability,clear,clearly,446,"@geoffjentry Is this at the time of running the workflow, or after the fact (like #1670)?. As a **workflow runner**, I want **to be able to select certain tasks to call cache or not call cache on**, so that I can **avoid reusing bad or old cache results**. ; - Effort: **Small**; - Risk: **Small to Mediume**; - This should not be a runtime attribute; - Make sure users don't overuse this feature and eliminate the benefits of call caching (i.e. clearly state when users should use this); - Business value: **Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-327932586
https://github.com/broadinstitute/cromwell/pull/1704#issuecomment-263444662:342,Usability,simpl,simply,342,"@cjllanwarne @delocalizer I don't have a strong opinion either way although I think left/right looks nicer. I'm not concerned about expandability into tuples. Chris what you said is pretty much exactly what we laid out in the ""but what if we want tuples?"" discussion when talking about Pairs. My take on that is if that happens then the pair simply becomes semantic sugar over a tuple2 at which point we can map left/right to whatever representation a tuple2 uses.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1704#issuecomment-263444662
https://github.com/broadinstitute/cromwell/issues/1705#issuecomment-267833828:115,Usability,learn,learning,115,"Closing this for now due to the combo of the ""I don't know if it works like that"" i noted as well as the more I am learning the less I think it works like that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1705#issuecomment-267833828
https://github.com/broadinstitute/cromwell/issues/1736#issuecomment-264643571:112,Usability,simpl,simplest,112,Yes it was. There was internal debate on how to handle version numbering in general and dropping the 0. Was the simplest path to our original intention (plus matched how they were colloquially referred to),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1736#issuecomment-264643571
https://github.com/broadinstitute/cromwell/issues/1736#issuecomment-264643631:30,Usability,clear,clearing,30,"@geoffjentry Cool, thanks for clearing that up. I'll merge 23 to Homebrew now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1736#issuecomment-264643631
https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328208741:93,Usability,feedback,feedback,93,"Of the folks who have expressed in SLURM support, has anyone had a chance to try it out? Any feedback (or recommended documentation) would be great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328208741
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294300861:171,Testability,log,log,171,One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294300861
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294300861:117,Usability,feedback,feedback,117,One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294300861
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364:88,Deployability,pipeline,pipeline,88,"Error... not reading the whole file probably will not produce the right behavior in the pipeline being run. > On Apr 15, 2017, at 11:41 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364:364,Testability,log,log,364,"Error... not reading the whole file probably will not produce the right behavior in the pipeline being run. > On Apr 15, 2017, at 11:41 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364:310,Usability,feedback,feedback,310,"Error... not reading the whole file probably will not produce the right behavior in the pipeline being run. > On Apr 15, 2017, at 11:41 AM, Jeff Gentry <notifications@github.com> wrote:; > ; > One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294302364
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637:147,Availability,error,error,147,"I would support spec-mandated minimums, supplemented by knobs in Cromwell. . For the user experience, a key thing you can do is write really clear error messages. Ie don't make it die with just ""File was too big""; add a note in there about where to get more info/what can be done to get past this. @katevoss can help with this; she has strong feelings about microcopy as I'm sure you know by now ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637:153,Integrability,message,messages,153,"I would support spec-mandated minimums, supplemented by knobs in Cromwell. . For the user experience, a key thing you can do is write really clear error messages. Ie don't make it die with just ""File was too big""; add a note in there about where to get more info/what can be done to get past this. @katevoss can help with this; she has strong feelings about microcopy as I'm sure you know by now ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637:85,Usability,user experience,user experience,85,"I would support spec-mandated minimums, supplemented by knobs in Cromwell. . For the user experience, a key thing you can do is write really clear error messages. Ie don't make it die with just ""File was too big""; add a note in there about where to get more info/what can be done to get past this. @katevoss can help with this; she has strong feelings about microcopy as I'm sure you know by now ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637:141,Usability,clear,clear,141,"I would support spec-mandated minimums, supplemented by knobs in Cromwell. . For the user experience, a key thing you can do is write really clear error messages. Ie don't make it die with just ""File was too big""; add a note in there about where to get more info/what can be done to get past this. @katevoss can help with this; she has strong feelings about microcopy as I'm sure you know by now ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096:73,Deployability,configurat,configuration,73,"To summarize, these will be the spec mandated minimums. There'll also be configuration parameters in Cromwell to tune these higher if one wants. Cromwell will attempt to check file size *prior* to reading it or pulling it across the network for cloud filesystems. Error messages should be very clear and checked past @katevoss . `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K ; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096:270,Integrability,message,messages,270,"To summarize, these will be the spec mandated minimums. There'll also be configuration parameters in Cromwell to tune these higher if one wants. Cromwell will attempt to check file size *prior* to reading it or pulling it across the network for cloud filesystems. Error messages should be very clear and checked past @katevoss . `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K ; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096:73,Modifiability,config,configuration,73,"To summarize, these will be the spec mandated minimums. There'll also be configuration parameters in Cromwell to tune these higher if one wants. Cromwell will attempt to check file size *prior* to reading it or pulling it across the network for cloud filesystems. Error messages should be very clear and checked past @katevoss . `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K ; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096:113,Performance,tune,tune,113,"To summarize, these will be the spec mandated minimums. There'll also be configuration parameters in Cromwell to tune these higher if one wants. Cromwell will attempt to check file size *prior* to reading it or pulling it across the network for cloud filesystems. Error messages should be very clear and checked past @katevoss . `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K ; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096
https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096:294,Usability,clear,clear,294,"To summarize, these will be the spec mandated minimums. There'll also be configuration parameters in Cromwell to tune these higher if one wants. Cromwell will attempt to check file size *prior* to reading it or pulling it across the network for cloud filesystems. Error messages should be very clear and checked past @katevoss . `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K ; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096
https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-266563690:137,Deployability,rolling,rolling,137,"@Horneth True, I can set that up quick to see how it goes. . If it does go well, this might end up being a good simple use case to start rolling w/ streams: https://groups.google.com/forum/#!topic/akka-user/v01YeU6Zb-o",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-266563690
https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-266563690:112,Usability,simpl,simple,112,"@Horneth True, I can set that up quick to see how it goes. . If it does go well, this might end up being a good simple use case to start rolling w/ streams: https://groups.google.com/forum/#!topic/akka-user/v01YeU6Zb-o",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-266563690
https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-295908844:62,Usability,simpl,simple,62,"Based on when I was likely using either 20k, 40k or 200k wide simple scatters and just watching jprofiler and such",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1776#issuecomment-295908844
https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066:395,Availability,down,down,395,"@cjllanwarne I'm going to take this the complete opposite direction but I'll give you all the credit as it was what you said that led me here. Specifically it started with your statement "" a WDL author can decide when they want their globs to be exploded ..."" my reaction was ""hell no!"" as the point here is system sanctity and not user desire. If the point is to not allow a large glob to take down a cromwell server then we should not allow users to be the ones deciding what's getting processed in a more/less efficient manner. To bend an old gem of wisdom a bit beyond its original meaning, ""never trust the client"". That got me thinking that I think all of this (including my original post) is coming at this all wrong. To the WDL user they should only ever have to think in terms of `File` and `Array[File]`, but that should imply no specific implementation under the hood. I think some of this goes to how tightly coupled WDL is to implementation in Cromwell and how that fact tends to guide our thinking in certain directions. If instead we *always* treated `Array[File]` as a FOFN behind the scenes in a completely invisible to the user manner we'd be able to keep the simplistic sugar of WDL but go even beyond glob situations when it comes to memory savings. That way both `File` and `Array[File]` are internally managed as just a single file path. For obvious reasons (including your last statement and others that we both made) that wouldn't be a tiny change but I don't think it'd be monumental either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066
https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066:513,Energy Efficiency,efficient,efficient,513,"@cjllanwarne I'm going to take this the complete opposite direction but I'll give you all the credit as it was what you said that led me here. Specifically it started with your statement "" a WDL author can decide when they want their globs to be exploded ..."" my reaction was ""hell no!"" as the point here is system sanctity and not user desire. If the point is to not allow a large glob to take down a cromwell server then we should not allow users to be the ones deciding what's getting processed in a more/less efficient manner. To bend an old gem of wisdom a bit beyond its original meaning, ""never trust the client"". That got me thinking that I think all of this (including my original post) is coming at this all wrong. To the WDL user they should only ever have to think in terms of `File` and `Array[File]`, but that should imply no specific implementation under the hood. I think some of this goes to how tightly coupled WDL is to implementation in Cromwell and how that fact tends to guide our thinking in certain directions. If instead we *always* treated `Array[File]` as a FOFN behind the scenes in a completely invisible to the user manner we'd be able to keep the simplistic sugar of WDL but go even beyond glob situations when it comes to memory savings. That way both `File` and `Array[File]` are internally managed as just a single file path. For obvious reasons (including your last statement and others that we both made) that wouldn't be a tiny change but I don't think it'd be monumental either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066
https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066:993,Usability,guid,guide,993,"@cjllanwarne I'm going to take this the complete opposite direction but I'll give you all the credit as it was what you said that led me here. Specifically it started with your statement "" a WDL author can decide when they want their globs to be exploded ..."" my reaction was ""hell no!"" as the point here is system sanctity and not user desire. If the point is to not allow a large glob to take down a cromwell server then we should not allow users to be the ones deciding what's getting processed in a more/less efficient manner. To bend an old gem of wisdom a bit beyond its original meaning, ""never trust the client"". That got me thinking that I think all of this (including my original post) is coming at this all wrong. To the WDL user they should only ever have to think in terms of `File` and `Array[File]`, but that should imply no specific implementation under the hood. I think some of this goes to how tightly coupled WDL is to implementation in Cromwell and how that fact tends to guide our thinking in certain directions. If instead we *always* treated `Array[File]` as a FOFN behind the scenes in a completely invisible to the user manner we'd be able to keep the simplistic sugar of WDL but go even beyond glob situations when it comes to memory savings. That way both `File` and `Array[File]` are internally managed as just a single file path. For obvious reasons (including your last statement and others that we both made) that wouldn't be a tiny change but I don't think it'd be monumental either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066
https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066:1178,Usability,simpl,simplistic,1178,"@cjllanwarne I'm going to take this the complete opposite direction but I'll give you all the credit as it was what you said that led me here. Specifically it started with your statement "" a WDL author can decide when they want their globs to be exploded ..."" my reaction was ""hell no!"" as the point here is system sanctity and not user desire. If the point is to not allow a large glob to take down a cromwell server then we should not allow users to be the ones deciding what's getting processed in a more/less efficient manner. To bend an old gem of wisdom a bit beyond its original meaning, ""never trust the client"". That got me thinking that I think all of this (including my original post) is coming at this all wrong. To the WDL user they should only ever have to think in terms of `File` and `Array[File]`, but that should imply no specific implementation under the hood. I think some of this goes to how tightly coupled WDL is to implementation in Cromwell and how that fact tends to guide our thinking in certain directions. If instead we *always* treated `Array[File]` as a FOFN behind the scenes in a completely invisible to the user manner we'd be able to keep the simplistic sugar of WDL but go even beyond glob situations when it comes to memory savings. That way both `File` and `Array[File]` are internally managed as just a single file path. For obvious reasons (including your last statement and others that we both made) that wouldn't be a tiny change but I don't think it'd be monumental either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259:150,Deployability,configurat,configuration,150,"So we need an uber api that tracks preemptions across registered Cromwell; servers and then uses machine learning to make predictions of lowest cost; configuration that meets system resources. Simple.... (joke). On Dec 18, 2016 12:39, ""Jeff Gentry"" <notifications@github.com> wrote:. > @pgrosu <https://github.com/pgrosu> IIRC we use central to avoid some of; > their other large customers in other zones. However note that we *are*; > one of their large customers so choosing the same zone as us might not be; > the best plan for success in avoiding preemptions :); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE0Z7yysmspq1G1F35bUZzr4Cy7wzks5rJW_LgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259:150,Modifiability,config,configuration,150,"So we need an uber api that tracks preemptions across registered Cromwell; servers and then uses machine learning to make predictions of lowest cost; configuration that meets system resources. Simple.... (joke). On Dec 18, 2016 12:39, ""Jeff Gentry"" <notifications@github.com> wrote:. > @pgrosu <https://github.com/pgrosu> IIRC we use central to avoid some of; > their other large customers in other zones. However note that we *are*; > one of their large customers so choosing the same zone as us might not be; > the best plan for success in avoiding preemptions :); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE0Z7yysmspq1G1F35bUZzr4Cy7wzks5rJW_LgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259:122,Safety,predict,predictions,122,"So we need an uber api that tracks preemptions across registered Cromwell; servers and then uses machine learning to make predictions of lowest cost; configuration that meets system resources. Simple.... (joke). On Dec 18, 2016 12:39, ""Jeff Gentry"" <notifications@github.com> wrote:. > @pgrosu <https://github.com/pgrosu> IIRC we use central to avoid some of; > their other large customers in other zones. However note that we *are*; > one of their large customers so choosing the same zone as us might not be; > the best plan for success in avoiding preemptions :); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE0Z7yysmspq1G1F35bUZzr4Cy7wzks5rJW_LgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259:345,Safety,avoid,avoid,345,"So we need an uber api that tracks preemptions across registered Cromwell; servers and then uses machine learning to make predictions of lowest cost; configuration that meets system resources. Simple.... (joke). On Dec 18, 2016 12:39, ""Jeff Gentry"" <notifications@github.com> wrote:. > @pgrosu <https://github.com/pgrosu> IIRC we use central to avoid some of; > their other large customers in other zones. However note that we *are*; > one of their large customers so choosing the same zone as us might not be; > the best plan for success in avoiding preemptions :); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE0Z7yysmspq1G1F35bUZzr4Cy7wzks5rJW_LgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259:542,Safety,avoid,avoiding,542,"So we need an uber api that tracks preemptions across registered Cromwell; servers and then uses machine learning to make predictions of lowest cost; configuration that meets system resources. Simple.... (joke). On Dec 18, 2016 12:39, ""Jeff Gentry"" <notifications@github.com> wrote:. > @pgrosu <https://github.com/pgrosu> IIRC we use central to avoid some of; > their other large customers in other zones. However note that we *are*; > one of their large customers so choosing the same zone as us might not be; > the best plan for success in avoiding preemptions :); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE0Z7yysmspq1G1F35bUZzr4Cy7wzks5rJW_LgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259
https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259:105,Usability,learn,learning,105,"So we need an uber api that tracks preemptions across registered Cromwell; servers and then uses machine learning to make predictions of lowest cost; configuration that meets system resources. Simple.... (joke). On Dec 18, 2016 12:39, ""Jeff Gentry"" <notifications@github.com> wrote:. > @pgrosu <https://github.com/pgrosu> IIRC we use central to avoid some of; > their other large customers in other zones. However note that we *are*; > one of their large customers so choosing the same zone as us might not be; > the best plan for success in avoiding preemptions :); >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267834836>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAFpE0Z7yysmspq1G1F35bUZzr4Cy7wzks5rJW_LgaJpZM4LPeJB>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1795#issuecomment-267836259
https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267824365:208,Modifiability,config,config,208,"Hey @geoffjentry this is a great change to make, didn't make sense to have a non-changeable hardcoded default!. ToL: What struck me in this PR was that it took changing of 11 files to wire a default from the config file to where it is used. I struggled with this same thing when adding in the alternate compute service account. Felt like a lot of boilerplate. As a future refactoring, it would be nice if this was simplified overall",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267824365
https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267824365:372,Modifiability,refactor,refactoring,372,"Hey @geoffjentry this is a great change to make, didn't make sense to have a non-changeable hardcoded default!. ToL: What struck me in this PR was that it took changing of 11 files to wire a default from the config file to where it is used. I struggled with this same thing when adding in the alternate compute service account. Felt like a lot of boilerplate. As a future refactoring, it would be nice if this was simplified overall",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267824365
https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267824365:414,Usability,simpl,simplified,414,"Hey @geoffjentry this is a great change to make, didn't make sense to have a non-changeable hardcoded default!. ToL: What struck me in this PR was that it took changing of 11 files to wire a default from the config file to where it is used. I struggled with this same thing when adding in the alternate compute service account. Felt like a lot of boilerplate. As a future refactoring, it would be nice if this was simplified overall",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1797#issuecomment-267824365
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268931134:43,Deployability,update,update,43,"When we get back, please also make sure to update the documentation as well. This is a breaking change (which is essential for consistency) but we should be super clear about that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268931134
https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268931134:163,Usability,clear,clear,163,"When we get back, please also make sure to update the documentation as well. This is a breaking change (which is essential for consistency) but we should be super clear about that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268931134
https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268610787:122,Usability,simpl,simplistic,122,"@kcibul - I've been increasingly thinking that we should rethink runtime attrs altogether, they've always seemed a little simplistic for what can be a fairly complex situation (e.g. what if 2 backends interpret the same RA differently and we're in a multi-backend situation?).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268610787
https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268615173:414,Usability,simpl,simplistic,414,":+1: I was worried about interpretation across backends. Since I just; found out that the SGE backend does not support docker. That could have; been an unpleasant surprise... On Wed, Dec 21, 2016 at 2:01 PM, Jeff Gentry <notifications@github.com>; wrote:. > @kcibul <https://github.com/kcibul> - I've been increasingly thinking; > that we should rethink runtime attrs altogether, they've always seemed a; > little simplistic for what can be a fairly complex situation (e.g. what if; > 2 backends interpret the same RA differently and we're in a multi-backend; > situation?).; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268610787>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2BYpbGzGuj7izW7zJZsKRrwE0Bcks5rKXeagaJpZM4LTPm1>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268615173
https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-467170389:320,Usability,clear,clear,320,"It was lost in some of the comments but the way to go (IMO) would be to have a master `whenUnhandled` to cover the cases where nothing useful is being done. . There are other cases where something useful **is** done, which isn't as bad but I think we should switch to use local `whenUnhandled` there to make things more clear",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-467170389
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269339796:106,Usability,simpl,simple,106,"Is that a db level setting giving you bulk insert via magic? . Doing it by hand turns out to not be super simple to make it not suck when it comes to our metadata, although I think I came up with something that would work after I quit poking at it. I won't bother fiddling with it if there's potential magic though. Btw I think this might be behind a lot of the various things I've been tracking, the pattern is pretty much the same. Well, more generally I think this is a pattern we use a lot (not just the single writes but firing these things off as fast as possible due to the futures) but this is the #1 culprit by far",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269339796
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855:195,Modifiability,config,config,195,Gave up trying to combat the performance vs memory tradeoff (which seems unlikely to be winnable anyways) and the relatively simple solution works. We can have the metadata write batch size be a config option and tell users that setting the number lower uses less memory but drops performance under higher load and vice versa,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855:29,Performance,perform,performance,29,Gave up trying to combat the performance vs memory tradeoff (which seems unlikely to be winnable anyways) and the relatively simple solution works. We can have the metadata write batch size be a config option and tell users that setting the number lower uses less memory but drops performance under higher load and vice versa,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855:281,Performance,perform,performance,281,Gave up trying to combat the performance vs memory tradeoff (which seems unlikely to be winnable anyways) and the relatively simple solution works. We can have the metadata write batch size be a config option and tell users that setting the number lower uses less memory but drops performance under higher load and vice versa,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855:306,Performance,load,load,306,Gave up trying to combat the performance vs memory tradeoff (which seems unlikely to be winnable anyways) and the relatively simple solution works. We can have the metadata write batch size be a config option and tell users that setting the number lower uses less memory but drops performance under higher load and vice versa,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855
https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855:125,Usability,simpl,simple,125,Gave up trying to combat the performance vs memory tradeoff (which seems unlikely to be winnable anyways) and the relatively simple solution works. We can have the metadata write batch size be a config option and tell users that setting the number lower uses less memory but drops performance under higher load and vice versa,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269428855
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:301,Deployability,patch,patch,301,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:484,Deployability,update,updated,484,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:1084,Deployability,patch,patches,1084,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:338,Integrability,bridg,bridge,338,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:1514,Integrability,message,messages,1514,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:1654,Safety,timeout,timeouts,1654,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:232,Testability,test,tested,232,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:862,Testability,test,testing,862,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:917,Testability,test,test,917,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:1128,Testability,test,test,1128,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:1138,Testability,test,tests,1138,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:89,Usability,feedback,feedback,89,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:152,Usability,feedback,feedback,152,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case … => { … }` could be `case … => …`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295
https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285378599:118,Testability,test,test,118,"Ahha, that is useful information. I think this WDL didn't work for multiple reasons. Is there a simple example WDL or test that uses declarations in a scatter?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285378599
https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285378599:96,Usability,simpl,simple,96,"Ahha, that is useful information. I think this WDL didn't work for multiple reasons. Is there a simple example WDL or test that uses declarations in a scatter?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285378599
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:19,Deployability,configurat,configuration,19,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:288,Deployability,configurat,configuration,288,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:19,Modifiability,config,configuration,19,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:222,Modifiability,config,config,222,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:229,Modifiability,variab,variable,229,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:288,Modifiability,config,configuration,288,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:890,Modifiability,config,config,890,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:982,Modifiability,config,config,982,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:1036,Modifiability,config,config,1036,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:484,Performance,load,loaded,484,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258:731,Usability,simpl,simplest,731,"> This is a common configuration that prohibits docker. ""Local"" and SGE/HPC are two separate issues. SGE (and all HPC) backends can already run without docker. When setting up the backend, just don't add a `submit-docker` config variable nor a `docker` runtime attribute to the backend's configuration. Docs for new local/HPC backends are documented under the title ""[SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/)"". Separately, there is the issue that cromwell is pre-loaded with a default ""Local"" backend. This ""Local"" backend [is docker enabled](https://github.com/broadinstitute/cromwell/blob/a3c5e055a5a4c6793a526689d38577c2f122bc95/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L34). The simplest workaround today is to create another backend ""Local-NoDocker"" or similar. Or if one wants to just change the existing ""Local"" backend they can use a config like:. ```hocon; include required(classpath(""application"")); backend.providers.Local.config.runtime-attributes=""""; backend.providers.Local.config.submit-docker=null; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-423240258
https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-342191752:56,Usability,clear,clear,56,"Thanks for that additional info, Jeff. Just so I can be clear: other than ""age"" and ""more detail,"" is there an essential difference between the aims of what David linked to this thread and the aims of the OP? The goal of the OP is to have the WDL spec change to support optional outputs, correct? And David is essentially adding a +1 [or +10, if that's legal :) ] to this. Regardless, we're happy to put forth effort towards the OpenWDL forums as you suggest, and possibly even contribute code when possible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-342191752
https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-520618084:6,Usability,clear,clear,6,"To be clear, this is actually the current cromwell behavior, even though it's not in the spec, right?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-520618084
https://github.com/broadinstitute/cromwell/issues/1858#issuecomment-272594504:171,Usability,clear,clear,171,"Depending on your definition of `actual` it is actual, the devs themselves confirmed this :) Whether or not it can ever add up to be meaningful for us, who knows. . To be clear I think at most this would be a tiny effect, it just seemed like something which could take 5 mins to do as opposed to some of our real problems ;) As PO I wouldn't worry too much about this ticket :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1858#issuecomment-272594504
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334478731:251,Safety,abort,aborts,251,"just want to also be sure to make it clear there are more issues related to that forum post than this specific issue - this one is on making sure the task statuses reach a final state when the workflow ends in a terminal state - that's different than aborts not working - both are an issue, but different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334478731
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334478731:37,Usability,clear,clear,37,"just want to also be sure to make it clear there are more issues related to that forum post than this specific issue - this one is on making sure the task statuses reach a final state when the workflow ends in a terminal state - that's different than aborts not working - both are an issue, but different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334478731
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334526788:272,Safety,abort,aborted,272,"There are two tables in the cromwell database that are out of sync. Unofficially, if one knows what they're looking for, one can edit the values directly in the database. Often the case is the `WorkflowStoreEntry` doesn't have a record for the workflow, thus it cannot be aborted or resumed-on-restart, yet the last row in `MetadataEntry` says the workflow is still `Running`. A ""reconciler"" could write a final row into `MetadataEntry` with Aborted/Success/Fail. This could be called:; - Manually by a user/service; - Automatically whenever a user requests status of a workflow; - Automatically by a MetadataEntry sweeper looking for workflows w/o a finalization and no row in WorkflowStoreEntry; - Other?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334526788
https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334526788:283,Usability,resume,resumed-on-restart,283,"There are two tables in the cromwell database that are out of sync. Unofficially, if one knows what they're looking for, one can edit the values directly in the database. Often the case is the `WorkflowStoreEntry` doesn't have a record for the workflow, thus it cannot be aborted or resumed-on-restart, yet the last row in `MetadataEntry` says the workflow is still `Running`. A ""reconciler"" could write a final row into `MetadataEntry` with Aborted/Success/Fail. This could be called:; - Manually by a user/service; - Automatically whenever a user requests status of a workflow; - Automatically by a MetadataEntry sweeper looking for workflows w/o a finalization and no row in WorkflowStoreEntry; - Other?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334526788
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-274082302:527,Integrability,message,messages,527,"Was talking about this sort of thing with @vdauwera just yesterday. . I feel the fact that Cromwell has both multiple distinct use cases (e.g. multiuser server, person running on the command line, etc) as well as multiple distinct user personas that this is tough to manage. By and large the stuff spewing out to the logs is intended to be, well, logs. In other words stuff that you can look at to figure out what went wrong, where ""you"" is more of a developer audience. That's kind of a necessary thing to have, IMO. Thus the messages tend to be highly specific and quite literal. On the other hand, particularly when looking at people running as a single user from the command line most of the stuff which is emitted is at best completely useless and at worst unnecessarily frightening. Your concern falls somewhere in the middle ;). I'm happy to leave this open as a placeholder but also happy to close for now if telling you that a) we're aware of the situation, b) the solution isn't going to be as simple as ""reword some things"" and c) it's likely to be a while makes you say ""eh, good enough""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-274082302
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-274082302:317,Testability,log,logs,317,"Was talking about this sort of thing with @vdauwera just yesterday. . I feel the fact that Cromwell has both multiple distinct use cases (e.g. multiuser server, person running on the command line, etc) as well as multiple distinct user personas that this is tough to manage. By and large the stuff spewing out to the logs is intended to be, well, logs. In other words stuff that you can look at to figure out what went wrong, where ""you"" is more of a developer audience. That's kind of a necessary thing to have, IMO. Thus the messages tend to be highly specific and quite literal. On the other hand, particularly when looking at people running as a single user from the command line most of the stuff which is emitted is at best completely useless and at worst unnecessarily frightening. Your concern falls somewhere in the middle ;). I'm happy to leave this open as a placeholder but also happy to close for now if telling you that a) we're aware of the situation, b) the solution isn't going to be as simple as ""reword some things"" and c) it's likely to be a while makes you say ""eh, good enough""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-274082302
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-274082302:347,Testability,log,logs,347,"Was talking about this sort of thing with @vdauwera just yesterday. . I feel the fact that Cromwell has both multiple distinct use cases (e.g. multiuser server, person running on the command line, etc) as well as multiple distinct user personas that this is tough to manage. By and large the stuff spewing out to the logs is intended to be, well, logs. In other words stuff that you can look at to figure out what went wrong, where ""you"" is more of a developer audience. That's kind of a necessary thing to have, IMO. Thus the messages tend to be highly specific and quite literal. On the other hand, particularly when looking at people running as a single user from the command line most of the stuff which is emitted is at best completely useless and at worst unnecessarily frightening. Your concern falls somewhere in the middle ;). I'm happy to leave this open as a placeholder but also happy to close for now if telling you that a) we're aware of the situation, b) the solution isn't going to be as simple as ""reword some things"" and c) it's likely to be a while makes you say ""eh, good enough""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-274082302
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-274082302:1004,Usability,simpl,simple,1004,"Was talking about this sort of thing with @vdauwera just yesterday. . I feel the fact that Cromwell has both multiple distinct use cases (e.g. multiuser server, person running on the command line, etc) as well as multiple distinct user personas that this is tough to manage. By and large the stuff spewing out to the logs is intended to be, well, logs. In other words stuff that you can look at to figure out what went wrong, where ""you"" is more of a developer audience. That's kind of a necessary thing to have, IMO. Thus the messages tend to be highly specific and quite literal. On the other hand, particularly when looking at people running as a single user from the command line most of the stuff which is emitted is at best completely useless and at worst unnecessarily frightening. Your concern falls somewhere in the middle ;). I'm happy to leave this open as a placeholder but also happy to close for now if telling you that a) we're aware of the situation, b) the solution isn't going to be as simple as ""reword some things"" and c) it's likely to be a while makes you say ""eh, good enough""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-274082302
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983:49,Integrability,message,messages,49,"Suggestion: Use GATK Queue as a guideline. Those messages were reasonable. On Wed, Feb 22, 2017 at 2:17 PM, Jeff Gentry <notifications@github.com>; wrote:. > this has come up a few times in a few different issues and that multitude; > actually makes the larger point here. Internally we've been discussing how; > to handle this as an upcoming project. In particular the problem is that we; > have too many different user personas and trying to have a single form of; > log meet all of their needs is going to be useless. Log levels doesn't; > quite capture all of the variables that might be in play here as often what; > happens is that someone 99% of the time only wants to see form X but once; > in a while *really* needs to see form Y and it's useless if Y wasn't; > captured at all.; >; > We're going to be moving towards some sort of system where there are; > different sorts of logs and then everyone can be happy, or at least; > happier. That's probably at least a ""next quarter"" level of project,; > however.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_a13fmpLUm3gESPL3BzZfNNvuzIks5rfInSgaJpZM4LpV_C>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983:568,Modifiability,variab,variables,568,"Suggestion: Use GATK Queue as a guideline. Those messages were reasonable. On Wed, Feb 22, 2017 at 2:17 PM, Jeff Gentry <notifications@github.com>; wrote:. > this has come up a few times in a few different issues and that multitude; > actually makes the larger point here. Internally we've been discussing how; > to handle this as an upcoming project. In particular the problem is that we; > have too many different user personas and trying to have a single form of; > log meet all of their needs is going to be useless. Log levels doesn't; > quite capture all of the variables that might be in play here as often what; > happens is that someone 99% of the time only wants to see form X but once; > in a while *really* needs to see form Y and it's useless if Y wasn't; > captured at all.; >; > We're going to be moving towards some sort of system where there are; > different sorts of logs and then everyone can be happy, or at least; > happier. That's probably at least a ""next quarter"" level of project,; > however.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_a13fmpLUm3gESPL3BzZfNNvuzIks5rfInSgaJpZM4LpV_C>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983:469,Testability,log,log,469,"Suggestion: Use GATK Queue as a guideline. Those messages were reasonable. On Wed, Feb 22, 2017 at 2:17 PM, Jeff Gentry <notifications@github.com>; wrote:. > this has come up a few times in a few different issues and that multitude; > actually makes the larger point here. Internally we've been discussing how; > to handle this as an upcoming project. In particular the problem is that we; > have too many different user personas and trying to have a single form of; > log meet all of their needs is going to be useless. Log levels doesn't; > quite capture all of the variables that might be in play here as often what; > happens is that someone 99% of the time only wants to see form X but once; > in a while *really* needs to see form Y and it's useless if Y wasn't; > captured at all.; >; > We're going to be moving towards some sort of system where there are; > different sorts of logs and then everyone can be happy, or at least; > happier. That's probably at least a ""next quarter"" level of project,; > however.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_a13fmpLUm3gESPL3BzZfNNvuzIks5rfInSgaJpZM4LpV_C>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983:885,Testability,log,logs,885,"Suggestion: Use GATK Queue as a guideline. Those messages were reasonable. On Wed, Feb 22, 2017 at 2:17 PM, Jeff Gentry <notifications@github.com>; wrote:. > this has come up a few times in a few different issues and that multitude; > actually makes the larger point here. Internally we've been discussing how; > to handle this as an upcoming project. In particular the problem is that we; > have too many different user personas and trying to have a single form of; > log meet all of their needs is going to be useless. Log levels doesn't; > quite capture all of the variables that might be in play here as often what; > happens is that someone 99% of the time only wants to see form X but once; > in a while *really* needs to see form Y and it's useless if Y wasn't; > captured at all.; >; > We're going to be moving towards some sort of system where there are; > different sorts of logs and then everyone can be happy, or at least; > happier. That's probably at least a ""next quarter"" level of project,; > however.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_a13fmpLUm3gESPL3BzZfNNvuzIks5rfInSgaJpZM4LpV_C>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983
https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983:32,Usability,guid,guideline,32,"Suggestion: Use GATK Queue as a guideline. Those messages were reasonable. On Wed, Feb 22, 2017 at 2:17 PM, Jeff Gentry <notifications@github.com>; wrote:. > this has come up a few times in a few different issues and that multitude; > actually makes the larger point here. Internally we've been discussing how; > to handle this as an upcoming project. In particular the problem is that we; > have too many different user personas and trying to have a single form of; > log meet all of their needs is going to be useless. Log levels doesn't; > quite capture all of the variables that might be in play here as often what; > happens is that someone 99% of the time only wants to see form X but once; > in a while *really* needs to see form Y and it's useless if Y wasn't; > captured at all.; >; > We're going to be moving towards some sort of system where there are; > different sorts of logs and then everyone can be happy, or at least; > happier. That's probably at least a ""next quarter"" level of project,; > however.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281771971>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk_a13fmpLUm3gESPL3BzZfNNvuzIks5rfInSgaJpZM4LpV_C>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883#issuecomment-281774983
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-287458911:105,Deployability,update,update,105,"@dheiman this is actually more of a feature request as this feature has not yet been implemented. I will update the issue accordingly, thanks for your feedback!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-287458911
https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-287458911:151,Usability,feedback,feedback,151,"@dheiman this is actually more of a feature request as this feature has not yet been implemented. I will update the issue accordingly, thanks for your feedback!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-287458911
https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275517495:105,Usability,user-friendly,user-friendly,105,"@geoffjentry Admittedly, despite some inaccuracies you might have, the documentation on the site is very user-friendly, special thanks, hopefully, the style itself won't change. I turned to the spec because I needed simpler examples for my own purposes. Nonetheless, this issue is resolved, thank you :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275517495
https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275517495:216,Usability,simpl,simpler,216,"@geoffjentry Admittedly, despite some inaccuracies you might have, the documentation on the site is very user-friendly, special thanks, hopefully, the style itself won't change. I turned to the spec because I needed simpler examples for my own purposes. Nonetheless, this issue is resolved, thank you :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275517495
https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275521773:30,Availability,ping,ping,30,@anton-khodak Cool. I'm gonna ping @katevoss and @vdauwera so they see the feedback. Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275521773
https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275521773:75,Usability,feedback,feedback,75,@anton-khodak Cool. I'm gonna ping @katevoss and @vdauwera so they see the feedback. Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-275521773
https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-276415323:173,Deployability,update,update,173,"@anton-khodak I'm glad to hear that you find the docs user-friendly, can you elaborate what you find particularly helpful? I want to make sure I preserve it as I review and update the docs. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-276415323
https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-276415323:54,Usability,user-friendly,user-friendly,54,"@anton-khodak I'm glad to hear that you find the docs user-friendly, can you elaborate what you find particularly helpful? I want to make sure I preserve it as I review and update the docs. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1908#issuecomment-276415323
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436:57,Availability,error,error,57,"As a **user running workflows on P.API**, I want **clear error messages when I put in an invalid zone**, so that **I know to change the zone and Cromwell doesn't infinitely retry (and spend all my money).**. - Effort: Small; - Risk: Small; - Business value: Small to Medium; - @ruchim have you heard of users running into this issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436:63,Integrability,message,messages,63,"As a **user running workflows on P.API**, I want **clear error messages when I put in an invalid zone**, so that **I know to change the zone and Cromwell doesn't infinitely retry (and spend all my money).**. - Effort: Small; - Risk: Small; - Business value: Small to Medium; - @ruchim have you heard of users running into this issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436
https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436:51,Usability,clear,clear,51,"As a **user running workflows on P.API**, I want **clear error messages when I put in an invalid zone**, so that **I know to change the zone and Cromwell doesn't infinitely retry (and spend all my money).**. - Effort: Small; - Risk: Small; - Business value: Small to Medium; - @ruchim have you heard of users running into this issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-344986436
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276773341:13,Usability,clear,clear,13,"Sorry, to be clear, the second task itself is not slow, but submitting it is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276773341
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277107529:77,Usability,simpl,simple,77,"Can't recreate this locally, albeit JES backend and not the same WDL, just a simple 10k wide scatter. (@yfarjoun this isn't an accusation to you, just a note on the ticket). I did notice that MaterializeWorkflowDescriptorActor takes *way* longer to build up w/ my wide scatter wdl than it used to",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277107529
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277521031:164,Usability,clear,clear,164,"I'm not trying to say that's it (I haven't thought at all about the FC case) but it's possible. What I saw was that'd it would often take a half hour to an hour to clear out a 40-50k pool of jobs due to quotas. Now imagine if there were hundreds of thousands of jobs being tracked by FC, the problem is way worse. 2 things: I doubt that this was the cas in your example and yes google is aware that this sucks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277521031
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275:252,Availability,down,down,252,"I don't know about the actual difference. I also have to admit that there is another possible variable here which is that when we ran this originally the farm was free and clear so the jobs ran very quickly. Now the farm is quite busy which is slowing down the workflow, but this is not the fault of cromwell. . Maybe @yfarjoun would have a better sense of timing boost.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275:291,Availability,fault,fault,291,"I don't know about the actual difference. I also have to admit that there is another possible variable here which is that when we ran this originally the farm was free and clear so the jobs ran very quickly. Now the farm is quite busy which is slowing down the workflow, but this is not the fault of cromwell. . Maybe @yfarjoun would have a better sense of timing boost.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275:94,Modifiability,variab,variable,94,"I don't know about the actual difference. I also have to admit that there is another possible variable here which is that when we ran this originally the farm was free and clear so the jobs ran very quickly. Now the farm is quite busy which is slowing down the workflow, but this is not the fault of cromwell. . Maybe @yfarjoun would have a better sense of timing boost.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275:172,Usability,clear,clear,172,"I don't know about the actual difference. I also have to admit that there is another possible variable here which is that when we ran this originally the farm was free and clear so the jobs ran very quickly. Now the farm is quite busy which is slowing down the workflow, but this is not the fault of cromwell. . Maybe @yfarjoun would have a better sense of timing boost.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289103663:39,Usability,clear,clear,39,With some help from @Horneth it became clear that I was using the wrong jar when I ran with file path call caching. Everything seems to be working much faster! Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289103663
https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-1330458010:88,Usability,clear,clear,88,I met the same problem. How did you solve it?. > With some help from @Horneth it became clear that I was using the wrong jar when I ran with file path call caching. Everything seems to be working much faster! Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-1330458010
https://github.com/broadinstitute/cromwell/issues/1945#issuecomment-277806238:702,Availability,echo,echo,702,"Per discussion with @ruchim and @cjllanwarne, the coercion from `File` to `String` in task declarations can't possibly yield a localized path since wdl4s coercions simply don't have the information required to do this. We're beginning to think File to String coercions probably shouldn't be allowed at all, but I'll keep the issue of disallowing them out of this ticket. There's still an issue with the optional ""filling"" not being transferred between the optional File and optional String. Here's a WDL that reproduces the issue. `coerced_int` and `coerced_string` stringify as empty:. ```; task strings {; Int? int; String? string. Int? coerced_int = string; String? coerced_string = int. command {; echo int: ${int} string: ${string} coerced_int: ${coerced_int} coerced_string: ${coerced_string}; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow w {; call strings { input: int = 1, string = ""2"" }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1945#issuecomment-277806238
https://github.com/broadinstitute/cromwell/issues/1945#issuecomment-277806238:164,Usability,simpl,simply,164,"Per discussion with @ruchim and @cjllanwarne, the coercion from `File` to `String` in task declarations can't possibly yield a localized path since wdl4s coercions simply don't have the information required to do this. We're beginning to think File to String coercions probably shouldn't be allowed at all, but I'll keep the issue of disallowing them out of this ticket. There's still an issue with the optional ""filling"" not being transferred between the optional File and optional String. Here's a WDL that reproduces the issue. `coerced_int` and `coerced_string` stringify as empty:. ```; task strings {; Int? int; String? string. Int? coerced_int = string; String? coerced_string = int. command {; echo int: ${int} string: ${string} coerced_int: ${coerced_int} coerced_string: ${coerced_string}; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow w {; call strings { input: int = 1, string = ""2"" }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1945#issuecomment-277806238
https://github.com/broadinstitute/cromwell/issues/1946#issuecomment-277272371:126,Usability,simpl,simple,126,"As far as I can tell, Cromwell should just throw this out as invalid WDL, and definitely not trip over itself. Thanks for the simple example though, that's going to make it much easier to debug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1946#issuecomment-277272371
https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-344357584:71,Usability,intuit,intuition,71,If I understand what's being asked I think so. I'm saying that more on intuition than hard evidence however,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-344357584
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509:53,Safety,abort,aborted,53,"I've seen this again on Cromwell 25-f80282a, after I aborted a workflow. Rebooting does NOT seem to have cleared it up this time. The workflow bucket doesn't exist. As far as I can tell, there is no mention of the workflow in Cromwell logs (weirdly).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509:235,Testability,log,logs,235,"I've seen this again on Cromwell 25-f80282a, after I aborted a workflow. Rebooting does NOT seem to have cleared it up this time. The workflow bucket doesn't exist. As far as I can tell, there is no mention of the workflow in Cromwell logs (weirdly).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509
https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509:105,Usability,clear,cleared,105,"I've seen this again on Cromwell 25-f80282a, after I aborted a workflow. Rebooting does NOT seem to have cleared it up this time. The workflow bucket doesn't exist. As far as I can tell, there is no mention of the workflow in Cromwell logs (weirdly).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509
https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280815262:89,Availability,error,error,89,"The Local build did fail, due to an unrelated hiccup. A restart of the build cleared the error. Meanwhile, **J**ES Centaur failed due to #1717, but **T**ES passed just fine. Everything looking great. Thanks again for all of your contributions!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280815262
https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280815262:77,Usability,clear,cleared,77,"The Local build did fail, due to an unrelated hiccup. A restart of the build cleared the error. Meanwhile, **J**ES Centaur failed due to #1717, but **T**ES passed just fine. Everything looking great. Thanks again for all of your contributions!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280815262
https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-426913487:165,Availability,error,error,165,"+1 ; The runtime.backend parameter is completely undocumented from what I can tell, but mentioned variously in forums and here on github. I tried to use it, saw the error, got extremely confused, and here I am hours later.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-426913487
https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-426913487:49,Usability,undo,undocumented,49,"+1 ; The runtime.backend parameter is completely undocumented from what I can tell, but mentioned variously in forums and here on github. I tried to use it, saw the error, got extremely confused, and here I am hours later.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1997#issuecomment-426913487
https://github.com/broadinstitute/cromwell/issues/2020#issuecomment-281811584:50,Usability,clear,clear,50,This sounds like a low priority bug as there is a clear workaround.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2020#issuecomment-281811584
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:301,Availability,down,down,301,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:387,Modifiability,config,config,387,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:466,Modifiability,config,config,466,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:145,Performance,concurren,concurrent-job-limit,145,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:476,Performance,concurren,concurrent-job-limit,476,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:728,Safety,detect,detect,728,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:343,Usability,pause,pause,343,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:349,Usability,resume,resume,349,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970:682,Performance,cache,cache,682,"We have users with the same use case as the second paragraph: ""most of your workflow worked well but fails in the end. You figure out the problem and you do not want to start it from the very beginning as intermediate results are already recorded to the Cromwell-execution folder"". More specifically, the harder case is when you have a scatter (say 100 way wide) and 98 succeed, but the other two fail. You figure out what changes to your command/docker that would fix this and you want to resume those with this change. Since Cromwell would only rerun for these failed tasks it wouldn't cause call caching confusion (rerun the succeeded tasks again), and these new runs would call cache to different hashes than had the original cached. If you were to run this same workflow again with the same data, those 98 that succeeded would not be able to call cache because you changed your task or docker, but I feel this is fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970:730,Performance,cache,cached,730,"We have users with the same use case as the second paragraph: ""most of your workflow worked well but fails in the end. You figure out the problem and you do not want to start it from the very beginning as intermediate results are already recorded to the Cromwell-execution folder"". More specifically, the harder case is when you have a scatter (say 100 way wide) and 98 succeed, but the other two fail. You figure out what changes to your command/docker that would fix this and you want to resume those with this change. Since Cromwell would only rerun for these failed tasks it wouldn't cause call caching confusion (rerun the succeeded tasks again), and these new runs would call cache to different hashes than had the original cached. If you were to run this same workflow again with the same data, those 98 that succeeded would not be able to call cache because you changed your task or docker, but I feel this is fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970:852,Performance,cache,cache,852,"We have users with the same use case as the second paragraph: ""most of your workflow worked well but fails in the end. You figure out the problem and you do not want to start it from the very beginning as intermediate results are already recorded to the Cromwell-execution folder"". More specifically, the harder case is when you have a scatter (say 100 way wide) and 98 succeed, but the other two fail. You figure out what changes to your command/docker that would fix this and you want to resume those with this change. Since Cromwell would only rerun for these failed tasks it wouldn't cause call caching confusion (rerun the succeeded tasks again), and these new runs would call cache to different hashes than had the original cached. If you were to run this same workflow again with the same data, those 98 that succeeded would not be able to call cache because you changed your task or docker, but I feel this is fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970:701,Security,hash,hashes,701,"We have users with the same use case as the second paragraph: ""most of your workflow worked well but fails in the end. You figure out the problem and you do not want to start it from the very beginning as intermediate results are already recorded to the Cromwell-execution folder"". More specifically, the harder case is when you have a scatter (say 100 way wide) and 98 succeed, but the other two fail. You figure out what changes to your command/docker that would fix this and you want to resume those with this change. Since Cromwell would only rerun for these failed tasks it wouldn't cause call caching confusion (rerun the succeeded tasks again), and these new runs would call cache to different hashes than had the original cached. If you were to run this same workflow again with the same data, those 98 that succeeded would not be able to call cache because you changed your task or docker, but I feel this is fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970:490,Usability,resume,resume,490,"We have users with the same use case as the second paragraph: ""most of your workflow worked well but fails in the end. You figure out the problem and you do not want to start it from the very beginning as intermediate results are already recorded to the Cromwell-execution folder"". More specifically, the harder case is when you have a scatter (say 100 way wide) and 98 succeed, but the other two fail. You figure out what changes to your command/docker that would fix this and you want to resume those with this change. Since Cromwell would only rerun for these failed tasks it wouldn't cause call caching confusion (rerun the succeeded tasks again), and these new runs would call cache to different hashes than had the original cached. If you were to run this same workflow again with the same data, those 98 that succeeded would not be able to call cache because you changed your task or docker, but I feel this is fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-360594970
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626:26,Performance,cache,cache,26,A resume option with call cache skipping would be very useful. Another feature which could be related to this option is unit test capabilities. for example I had run a workflow before (successfully). now I have changed a specific part of it and just want to test it with the inputs provided till that step.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626:125,Testability,test,test,125,A resume option with call cache skipping would be very useful. Another feature which could be related to this option is unit test capabilities. for example I had run a workflow before (successfully). now I have changed a specific part of it and just want to test it with the inputs provided till that step.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626:258,Testability,test,test,258,A resume option with call cache skipping would be very useful. Another feature which could be related to this option is unit test capabilities. for example I had run a workflow before (successfully). now I have changed a specific part of it and just want to test it with the inputs provided till that step.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626
https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626:2,Usability,resume,resume,2,A resume option with call cache skipping would be very useful. Another feature which could be related to this option is unit test capabilities. for example I had run a workflow before (successfully). now I have changed a specific part of it and just want to test it with the inputs provided till that step.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-1201794626
https://github.com/broadinstitute/cromwell/pull/2027#issuecomment-282367470:31,Usability,undo,undocumented,31,"The siblling knob is currently undocumented, I thought perhaps intentionally? I can certainly add to the reference.conf.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2027#issuecomment-282367470
https://github.com/broadinstitute/cromwell/pull/2027#issuecomment-282391104:54,Usability,undo,undocumented,54,"In terms of documenting the knob, I'm fine leaving it undocumented for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2027#issuecomment-282391104
https://github.com/broadinstitute/cromwell/pull/2027#issuecomment-282771079:82,Usability,undo,undocumented,82,👍... but do we at least have a list somewhere (or institutional knowledge) of our undocumented options?. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2027/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2027#issuecomment-282771079
https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283056934:25,Usability,clear,clearer,25,It would be nice to have clearer docs on Spark backend (what should be put to wdl and what should be given when running java -jar cromwell.jar instead of default application.conf),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2033#issuecomment-283056934
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631:147,Availability,failure,failures,147,"Pinging @Horneth and @kshakir on thoughts on triviality of this. I think we'd need a query to convert everything that looks like: ; ```; blah:blah:failures[123]:causedBy:causedBy:message; ```; into something like:; ```; blah:blah:failures[123456]:message; ```. AFAIK there's not a simple SQL query that'd do that so we'd need to look more like the scala-hooky metadata migration script? How trivial was that? Also, would it take another O(migration) to complete it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631:230,Availability,failure,failures,230,"Pinging @Horneth and @kshakir on thoughts on triviality of this. I think we'd need a query to convert everything that looks like: ; ```; blah:blah:failures[123]:causedBy:causedBy:message; ```; into something like:; ```; blah:blah:failures[123456]:message; ```. AFAIK there's not a simple SQL query that'd do that so we'd need to look more like the scala-hooky metadata migration script? How trivial was that? Also, would it take another O(migration) to complete it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631:179,Integrability,message,message,179,"Pinging @Horneth and @kshakir on thoughts on triviality of this. I think we'd need a query to convert everything that looks like: ; ```; blah:blah:failures[123]:causedBy:causedBy:message; ```; into something like:; ```; blah:blah:failures[123456]:message; ```. AFAIK there's not a simple SQL query that'd do that so we'd need to look more like the scala-hooky metadata migration script? How trivial was that? Also, would it take another O(migration) to complete it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631:247,Integrability,message,message,247,"Pinging @Horneth and @kshakir on thoughts on triviality of this. I think we'd need a query to convert everything that looks like: ; ```; blah:blah:failures[123]:causedBy:causedBy:message; ```; into something like:; ```; blah:blah:failures[123456]:message; ```. AFAIK there's not a simple SQL query that'd do that so we'd need to look more like the scala-hooky metadata migration script? How trivial was that? Also, would it take another O(migration) to complete it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631
https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631:281,Usability,simpl,simple,281,"Pinging @Horneth and @kshakir on thoughts on triviality of this. I think we'd need a query to convert everything that looks like: ; ```; blah:blah:failures[123]:causedBy:causedBy:message; ```; into something like:; ```; blah:blah:failures[123456]:message; ```. AFAIK there's not a simple SQL query that'd do that so we'd need to look more like the scala-hooky metadata migration script? How trivial was that? Also, would it take another O(migration) to complete it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:379,Performance,cache,cache,379,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment 😢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:448,Performance,cache,cache,448,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment 😢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:494,Performance,cache,cache,494,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment 😢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:60,Security,hash,hash,60,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment 😢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:374,Security,hash,hash,374,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment 😢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:443,Security,hash,hash,443,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment 😢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:482,Security,hash,hash,482,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment 😢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:545,Security,hash,hashes,545,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment 😢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797
https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797:437,Usability,clear,clear,437,"Looks awesome to me. Thinking out loud.....:. - (PO q) If a hash lookup fails (i.e. I mean the dockerhub request, not the expression evaluation), not even starting the workflow might be the expected behaviour, because ""don't waste my money starting an entire analysis when in 2 minutes I can resubmit and get CC""?; - (Separate ticket?) Should we also have a ""disable docker hash cache"" option, for Lee's very-fast iterations? Or just a ""clear hash cache"" REST endpoint?; - ToL: The hash lookup cache may also need to be aware of local vs remote hashes; - ToL: one step further away from dynamic backend assignment 😢",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-283975797
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016:551,Deployability,release,release,551,"![load_avg_with_and_without_sync](https://cloud.githubusercontent.com/assets/817809/23653377/0b318f3e-0378-11e7-9b24-a2cbe2b3e2dd.png). This is a plot of load average on our machine that hosts cromwell running as server - the box has 48 cores and 256G RAM. Mounted filesystems are local disk, an NFS share, and Lustre filesystem that hosts cromwell executions directory. The chart covers two identical submissions of a batch of about 1500 workflows. The period on the left from 5:45 to 7:30 with load (gray area) peaking up around 800 is running with release 25 jar. The period on the right from 9:15 to 10:30 is running with a build that is identical to release 25 but without [this sync](https://github.com/broadinstitute/cromwell/blob/fac784dd4078b8cc12fb4ca6c9abdbb05072990b/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L192).; It's a big difference, and very noticeable in the responsiveness of the server at the two different times.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016:655,Deployability,release,release,655,"![load_avg_with_and_without_sync](https://cloud.githubusercontent.com/assets/817809/23653377/0b318f3e-0378-11e7-9b24-a2cbe2b3e2dd.png). This is a plot of load average on our machine that hosts cromwell running as server - the box has 48 cores and 256G RAM. Mounted filesystems are local disk, an NFS share, and Lustre filesystem that hosts cromwell executions directory. The chart covers two identical submissions of a batch of about 1500 workflows. The period on the left from 5:45 to 7:30 with load (gray area) peaking up around 800 is running with release 25 jar. The period on the right from 9:15 to 10:30 is running with a build that is identical to release 25 but without [this sync](https://github.com/broadinstitute/cromwell/blob/fac784dd4078b8cc12fb4ca6c9abdbb05072990b/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L192).; It's a big difference, and very noticeable in the responsiveness of the server at the two different times.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016:154,Performance,load,load,154,"![load_avg_with_and_without_sync](https://cloud.githubusercontent.com/assets/817809/23653377/0b318f3e-0378-11e7-9b24-a2cbe2b3e2dd.png). This is a plot of load average on our machine that hosts cromwell running as server - the box has 48 cores and 256G RAM. Mounted filesystems are local disk, an NFS share, and Lustre filesystem that hosts cromwell executions directory. The chart covers two identical submissions of a batch of about 1500 workflows. The period on the left from 5:45 to 7:30 with load (gray area) peaking up around 800 is running with release 25 jar. The period on the right from 9:15 to 10:30 is running with a build that is identical to release 25 but without [this sync](https://github.com/broadinstitute/cromwell/blob/fac784dd4078b8cc12fb4ca6c9abdbb05072990b/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L192).; It's a big difference, and very noticeable in the responsiveness of the server at the two different times.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016:496,Performance,load,load,496,"![load_avg_with_and_without_sync](https://cloud.githubusercontent.com/assets/817809/23653377/0b318f3e-0378-11e7-9b24-a2cbe2b3e2dd.png). This is a plot of load average on our machine that hosts cromwell running as server - the box has 48 cores and 256G RAM. Mounted filesystems are local disk, an NFS share, and Lustre filesystem that hosts cromwell executions directory. The chart covers two identical submissions of a batch of about 1500 workflows. The period on the left from 5:45 to 7:30 with load (gray area) peaking up around 800 is running with release 25 jar. The period on the right from 9:15 to 10:30 is running with a build that is identical to release 25 but without [this sync](https://github.com/broadinstitute/cromwell/blob/fac784dd4078b8cc12fb4ca6c9abdbb05072990b/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L192).; It's a big difference, and very noticeable in the responsiveness of the server at the two different times.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016:920,Usability,responsiv,responsiveness,920,"![load_avg_with_and_without_sync](https://cloud.githubusercontent.com/assets/817809/23653377/0b318f3e-0378-11e7-9b24-a2cbe2b3e2dd.png). This is a plot of load average on our machine that hosts cromwell running as server - the box has 48 cores and 256G RAM. Mounted filesystems are local disk, an NFS share, and Lustre filesystem that hosts cromwell executions directory. The chart covers two identical submissions of a batch of about 1500 workflows. The period on the left from 5:45 to 7:30 with load (gray area) peaking up around 800 is running with release 25 jar. The period on the right from 9:15 to 10:30 is running with a build that is identical to release 25 but without [this sync](https://github.com/broadinstitute/cromwell/blob/fac784dd4078b8cc12fb4ca6c9abdbb05072990b/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L192).; It's a big difference, and very noticeable in the responsiveness of the server at the two different times.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284691016
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284748023:25,Performance,load,load,25,"Good point - that is the load on cromwell server itself; default backend is PBS (very like SGE) but some of the simple tasks in the workflow (`mkdir`, `uuidgen`) specify `backend: ""Local""` in the WDL `runtime` block.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284748023
https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284748023:112,Usability,simpl,simple,112,"Good point - that is the load on cromwell server itself; default backend is PBS (very like SGE) but some of the simple tasks in the workflow (`mkdir`, `uuidgen`) specify `backend: ""Local""` in the WDL `runtime` block.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284748023
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:192,Deployability,configurat,configurations,192,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > …; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:269,Deployability,configurat,configuration,269,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > …; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:192,Modifiability,config,configurations,192,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > …; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:269,Modifiability,config,configuration,269,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > …; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:500,Modifiability,extend,extends,500,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > …; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:1390,Modifiability,config,config,1390,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > …; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:1707,Modifiability,portab,portable,1707,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > …; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090:127,Usability,clear,clearer,127,"In the [README](https://github.com/broadinstitute/cromwell/blob/25/README.md#sun-gridengine-backend), it's buried and could be clearer, but it says:. > There are two special runtime attribute configurations, cpu, and memory_\<unit\>.; > …; > When the runtime attribute configuration Int memory_\<unit\> or Float memory_\<unit\> is specified, it is provided to submit by the runtime attribute in WDL memory. (slightly better formatting in the README). There's more ""what"" in the README, but the ""why"" extends from the fact that JES, TES, and other backends all use a common `memory` runtime attribute. Using this common runtime attribute name, `memory`, increases the chance that a WDL will be runnable on a different backends. In a WDL run on standard backends, `memory` is specified as a WDL string, parsed by the backends into a [`MemorySize`](https://github.com/broadinstitute/cromwell/blob/25/backend/src/main/scala/cromwell/backend/MemorySize.scala#L39). However, the Config backend used for SLURM, SGE, Local, etc., needs to covert the `MemorySize` back into a string, for embedding into the custom `submit` string. The `_<unit>` in `memory_<unit>` is how the `MemoryUnit` gets converted into a string. Say someone defines a WDL originally intended to run on the JES backend, containing a task with `memory: ""2 GB""`. If instead, this same WDL will be run on a Config backend, and the config specifies `Int memory_mb`, the string value of `memory_mb` passed into `submit` will expand to `2000`. Let us know if you have more questions and/or suggestions, or if this resolves this particular issue for now?. **TL;DR memory_\<unit\> is one of the reserved runtime-attribute names, meant to make WDLs more portable.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-287266090
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-289041622:97,Usability,guid,guide,97,"Yes, this explains my issue. I suggest including a description of runtime parameters in the user guide: https://software.broadinstitute.org/wdl/userguide/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-289041622
https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-289053131:28,Usability,feedback,feedback,28,"@MatthewMah thanks for your feedback, I'm in the process of updating the Cromwell docs and information about the runtime parameters will be a part of that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068#issuecomment-289053131
https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961:14,Deployability,upgrade,upgrade,14,"So when users upgrade to this version of Cromwell, they will need to re-configure their Cromwell or they will go back to using Local? That will need to be clearly included in the release notes, changelog, and possibly a blog post so users are less likely to miss the update.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961
https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961:179,Deployability,release,release,179,"So when users upgrade to this version of Cromwell, they will need to re-configure their Cromwell or they will go back to using Local? That will need to be clearly included in the release notes, changelog, and possibly a blog post so users are less likely to miss the update.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961
https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961:267,Deployability,update,update,267,"So when users upgrade to this version of Cromwell, they will need to re-configure their Cromwell or they will go back to using Local? That will need to be clearly included in the release notes, changelog, and possibly a blog post so users are less likely to miss the update.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961
https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961:72,Modifiability,config,configure,72,"So when users upgrade to this version of Cromwell, they will need to re-configure their Cromwell or they will go back to using Local? That will need to be clearly included in the release notes, changelog, and possibly a blog post so users are less likely to miss the update.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961
https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961:155,Usability,clear,clearly,155,"So when users upgrade to this version of Cromwell, they will need to re-configure their Cromwell or they will go back to using Local? That will need to be clearly included in the release notes, changelog, and possibly a blog post so users are less likely to miss the update.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2069#issuecomment-288176961
https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289546870:105,Availability,avail,available,105,"Hey Thibault, can you let me know your timeline for this? I'm going on vacation in a week but want to be available to you to discuss anything you have questions about and also available to give feedback.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289546870
https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289546870:176,Availability,avail,available,176,"Hey Thibault, can you let me know your timeline for this? I'm going on vacation in a week but want to be available to you to discuss anything you have questions about and also available to give feedback.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289546870
https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289546870:194,Usability,feedback,feedback,194,"Hey Thibault, can you let me know your timeline for this? I'm going on vacation in a week but want to be available to you to discuss anything you have questions about and also available to give feedback.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289546870
https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289556076:143,Usability,feedback,feedback,143,"Hey, I have started thinking about it but haven't written anything yet. I'll see if I can draft a first version this week at least to get your feedback on it before you go on vacation :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289556076
https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288765933:441,Availability,failure,failures,441,"I think the real solution would be to make WDLExpression evaluation truly asynchronous, which is not trivial.; The simpler solution to retry evaluations as a whole I would say is not that much work, although it happens in a bunch of places and some might be trickier than others. I would say it's small enough that we could try to squeeze it in 26, which would be nice because we've been seeing an increasingly large number of transient gcs failures lately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288765933
https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288765933:115,Usability,simpl,simpler,115,"I think the real solution would be to make WDLExpression evaluation truly asynchronous, which is not trivial.; The simpler solution to retry evaluations as a whole I would say is not that much work, although it happens in a bunch of places and some might be trickier than others. I would say it's small enough that we could try to squeeze it in 26, which would be nice because we've been seeing an increasingly large number of transient gcs failures lately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288765933
https://github.com/broadinstitute/cromwell/issues/2085#issuecomment-288824517:18,Integrability,depend,dependency,18,"You mentioned the dependency of job store simpletons on job store entries, is that still a complication here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2085#issuecomment-288824517
https://github.com/broadinstitute/cromwell/issues/2085#issuecomment-288824517:42,Usability,simpl,simpletons,42,"You mentioned the dependency of job store simpletons on job store entries, is that still a complication here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2085#issuecomment-288824517
https://github.com/broadinstitute/cromwell/issues/2085#issuecomment-288825795:211,Usability,simpl,simpletons,211,"Yes, it makes it more difficult to batch but I'm sure there's some slick magic that can make it happen ?; But yeah pretty much those events arrive in the form of. `JobStoreEntry -> List(JobStoreSimpleton)`. The simpletons have a FK mapping to their job store entry, which means job store entries have to be inserted first and the PK collected before simpletons can be inserted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2085#issuecomment-288825795
https://github.com/broadinstitute/cromwell/issues/2085#issuecomment-288825795:350,Usability,simpl,simpletons,350,"Yes, it makes it more difficult to batch but I'm sure there's some slick magic that can make it happen ?; But yeah pretty much those events arrive in the form of. `JobStoreEntry -> List(JobStoreSimpleton)`. The simpletons have a FK mapping to their job store entry, which means job store entries have to be inserted first and the PK collected before simpletons can be inserted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2085#issuecomment-288825795
https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292595363:46,Deployability,hotfix,hotfix,46,We should probably be clear (particularly for hotfix accepters) that this is unlikely to fix-fix aborts but specifically what we believe it'll resolve.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292595363
https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292595363:97,Safety,abort,aborts,97,We should probably be clear (particularly for hotfix accepters) that this is unlikely to fix-fix aborts but specifically what we believe it'll resolve.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292595363
https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292595363:22,Usability,clear,clear,22,We should probably be clear (particularly for hotfix accepters) that this is unlikely to fix-fix aborts but specifically what we believe it'll resolve.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-292595363
https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-294230149:306,Usability,simpl,simply,306,"That might be a reasonable place to start. Part of the confusion (that I just want someone to get right!) is what that list should be. There's v2 and v3, which are ""right"" but only about 1000 shards. Then there are several flavors of what was used to do the 20k, which is the ~20k shards. . So the task is simply getting someone who knows to (a) pick the best starting point (something with the right territory and somewhat even) and (b) do the splitting . Thinking out loud... Do you think that the balance of the shards changes with the number of samples? That is... could you take a 100-sample call set and use it to empirically balance the shards and then that would scale to 50k? or does the distribution change significantly as you add more samples?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2175#issuecomment-294230149
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412188725:18,Usability,clear,clear,18,"Hi @vsoch - to be clear, what I mean is this ... If I'm writing a WDL and I want to put some container in the `runtime` block, should **I** be opinionated as to if it's singularity or docker or should that be up to the person running the WDL? I used to view it as the former, but now I think it's the latter?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412188725
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412201049:164,Usability,clear,clear,164,"Wouldn't it be up to the person running the wdl? If it's not up to me, how I am empowered to say I am using slurm vs a container environment like kubernetes? to be clear I've only used Cromwell a day and a half so I'm not the right person to answer this question. I'm trying to understand how Singularity would fit in beyond being a binary executable (that might work in several environments). I think @bek might be able to weigh in?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412201049
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412897542:256,Deployability,pipeline,pipeline,256,"Cool thanks! So just to verify - I don't actually need to touch any scala, this is just a custom backend.conf for singularity (most of which I've already got a good start on?) This would simplify things quite a bit! Is this then provided in the workflow / pipeline or with cromwell here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412897542
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412897542:187,Usability,simpl,simplify,187,"Cool thanks! So just to verify - I don't actually need to touch any scala, this is just a custom backend.conf for singularity (most of which I've already got a good start on?) This would simplify things quite a bit! Is this then provided in the workflow / pipeline or with cromwell here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412897542
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:3909,Availability,down,down,3909," (via its API). but for now, without a clean API for services, only the first two really make sense. Singularity is not special. It's just a binary. ## Why has it been so confusing?. We get Singularity confused with Docker, because they are both containers. Same thing right? Sort of, but not exactly. Docker is a container technology, but actually it's older and has had time to develop a full API for services. It meets the criteria for both a backend and an executable, and this is because it can be conceptualized as both ""a thing that you run"" and ""the thing that is the container you run in."" But it's confusing. The distinction is that although Singularity is also a container, Singularity is **not** like Docker because it doesn't have the fully developed services API (yet!). This problem is hard because the language for Singularity containers communicating between one another, and even to the host, is not completely implemented yet. This comes down to OCI compliance, and having a way for some host to manage all of its Singularity containers. Right now we just have start and stop, but we can't connect containers, define ports, or even easily get a PID. It could (sort of?) be hacked, but we would be better off waiting for that nice standard. ## Reproducible Binary (Workflow Step) vs. Environment. There is also a distinction that I haven't completely wrapped my head around. Docker is very commonly used as an environment - you put a bunch of software (e.g., samtools, bwa aligner, etc.) and then issue commands to the container with custom things. Singularity, in my mind, to be truly a reproducible thing is more of the workflow step or script. It will have the software inside, but better should have those same commands represented with internal modularity. I could arguably completely do away with the external workflow dependency if a single binary told me how to run itself, and then had more than one entrypoint defined for each step. I wouldn't need to care about the softwa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6690,Availability,down,down,6690,"ularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that same format. A new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has several file formats under it) but it revolutionizing how brain imaging analysis is done. (e.g, take a look at [https://www.openneuro.org](https://www.openneuro.org). # Development of my Thinking; Finally, I want to share how I came to the thinking above. Here are the steps that I've taken in the last few weeks, and resulting thoughts from them. I started with this issue board actually, and a general goal to ""Add Singularity to Cromwell."" Ok. ### Question 1: How do I develop Cromwell?; It first was hard for me",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:9916,Availability,echo,echo,9916,"additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, is this making life easier for the scientist? If I'm a programmer person, and this is the minimum I am allowed for this to just run a simple container, what happens when it gets harder? I realized that without a proper services API, singularity is no more special than python, bash, samtools, it's just a binary. . And I realize also that it's easy to get caught up in details like ""Should we use Travis or Circle?"" Does it work on Amazon with this kind of input? And there will always be bugs! But I think the forest is being a bit lost for the trees. . ## Question 4: What is the direction to go in?. You can probably take what I'm saying with a grain of salt because I'm new to this entire universe, and there is so much invested there is no turning back or rethinking. But all of this seems too complicated, and too hard. What is needed is a solution that is just really stupid and simple. You have a container that understands its data. You point the container at a dataset and run it. You outsource the w",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6188,Deployability,configurat,configuration,6188," ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken""). The first is harder because you represent more things (more containers), but the second isn't reproducible because if you lose ""by chicken"" you've lost the entire workflow. Does that make sense?. ## What can/should we do now?. So there are two things to think about. With the current representation of a workflow, we would want Singularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that same format. A new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has severa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6283,Deployability,integrat,integrate,6283," ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken""). The first is harder because you represent more things (more containers), but the second isn't reproducible because if you lose ""by chicken"" you've lost the entire workflow. Does that make sense?. ## What can/should we do now?. So there are two things to think about. With the current representation of a workflow, we would want Singularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that same format. A new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has severa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:8579,Deployability,pipeline,pipeline,8579," general goal to ""Add Singularity to Cromwell."" Ok. ### Question 1: How do I develop Cromwell?; It first was hard for me to know where to start to develop Cromwell, because the docs just went into how to compile it on a host. So it made sense to make it easy for the developer to develop Cromwell so I made a Dockerfile to do that:; - https://github.com/broadinstitute/cromwell/pull/4002. Woohoo merged! We needed to have tests too, so I followed up on that:; - https://github.com/broadinstitute/cromwell/pull/4015 . But unfortunately it was decided that CircleCI was too new / needed to learn stuff (this is ok!) so it's going to be closed. . ## Question 2: How do we add a Singularity backend?. But this is actually ok, because we realize that we don't need to add Singularity to Cromwell proper, it can just be a backend! But I didn't understand wdl, or any of the formats, so my crew in Cherry lab gave me a solid repo to startwith, and then it started to click!; - https://github.com/vsoch/wgbs-pipeline/pull/1. I was waiting for the Dockerfile test PR to pass, but realized it probably wouldn't, so I jumped on adding the example backend workflows (still without totally understanding what/why/how, but figuring out as I went):; - https://github.com/broadinstitute/cromwell/pull/4039. ## Question 3: But what about Cromwell+Singularity on Travis?. I got confused again when there were [requests for additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But i",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:9784,Deployability,configurat,configuration,9784,"additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, is this making life easier for the scientist? If I'm a programmer person, and this is the minimum I am allowed for this to just run a simple container, what happens when it gets harder? I realized that without a proper services API, singularity is no more special than python, bash, samtools, it's just a binary. . And I realize also that it's easy to get caught up in details like ""Should we use Travis or Circle?"" Does it work on Amazon with this kind of input? And there will always be bugs! But I think the forest is being a bit lost for the trees. . ## Question 4: What is the direction to go in?. You can probably take what I'm saying with a grain of salt because I'm new to this entire universe, and there is so much invested there is no turning back or rethinking. But all of this seems too complicated, and too hard. What is needed is a solution that is just really stupid and simple. You have a container that understands its data. You point the container at a dataset and run it. You outsource the w",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:4321,Integrability,wrap,wrapped,4321," older and has had time to develop a full API for services. It meets the criteria for both a backend and an executable, and this is because it can be conceptualized as both ""a thing that you run"" and ""the thing that is the container you run in."" But it's confusing. The distinction is that although Singularity is also a container, Singularity is **not** like Docker because it doesn't have the fully developed services API (yet!). This problem is hard because the language for Singularity containers communicating between one another, and even to the host, is not completely implemented yet. This comes down to OCI compliance, and having a way for some host to manage all of its Singularity containers. Right now we just have start and stop, but we can't connect containers, define ports, or even easily get a PID. It could (sort of?) be hacked, but we would be better off waiting for that nice standard. ## Reproducible Binary (Workflow Step) vs. Environment. There is also a distinction that I haven't completely wrapped my head around. Docker is very commonly used as an environment - you put a bunch of software (e.g., samtools, bwa aligner, etc.) and then issue commands to the container with custom things. Singularity, in my mind, to be truly a reproducible thing is more of the workflow step or script. It will have the software inside, but better should have those same commands represented with internal modularity. I could arguably completely do away with the external workflow dependency if a single binary told me how to run itself, and then had more than one entrypoint defined for each step. I wouldn't need to care about the software or components inside because my host just needs to run Singularity. A container should almost be more like a hard coded binary step instead of a ""come into the environment and play around, the water's fine!"" It's a little bit like the ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") inst",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:4795,Integrability,depend,dependency,4795,"etween one another, and even to the host, is not completely implemented yet. This comes down to OCI compliance, and having a way for some host to manage all of its Singularity containers. Right now we just have start and stop, but we can't connect containers, define ports, or even easily get a PID. It could (sort of?) be hacked, but we would be better off waiting for that nice standard. ## Reproducible Binary (Workflow Step) vs. Environment. There is also a distinction that I haven't completely wrapped my head around. Docker is very commonly used as an environment - you put a bunch of software (e.g., samtools, bwa aligner, etc.) and then issue commands to the container with custom things. Singularity, in my mind, to be truly a reproducible thing is more of the workflow step or script. It will have the software inside, but better should have those same commands represented with internal modularity. I could arguably completely do away with the external workflow dependency if a single binary told me how to run itself, and then had more than one entrypoint defined for each step. I wouldn't need to care about the software or components inside because my host just needs to run Singularity. A container should almost be more like a hard coded binary step instead of a ""come into the environment and play around, the water's fine!"" It's a little bit like the ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken""). The first is harder because you represent more things (more containers), but the second isn't reproducible because if you lose ""by chicken"" you've lost the entire workflow. Does that make sense?. ## What can/should we do now?. So there are two things to think about. With the current representation of a workflow, we would want Singularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity its",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6283,Integrability,integrat,integrate,6283," ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken""). The first is harder because you represent more things (more containers), but the second isn't reproducible because if you lose ""by chicken"" you've lost the entire workflow. Does that make sense?. ## What can/should we do now?. So there are two things to think about. With the current representation of a workflow, we would want Singularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that same format. A new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has severa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6188,Modifiability,config,configuration,6188," ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken""). The first is harder because you represent more things (more containers), but the second isn't reproducible because if you lose ""by chicken"" you've lost the entire workflow. Does that make sense?. ## What can/should we do now?. So there are two things to think about. With the current representation of a workflow, we would want Singularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that same format. A new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has severa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:9217,Modifiability,layers,layers,9217,"is is ok!) so it's going to be closed. . ## Question 2: How do we add a Singularity backend?. But this is actually ok, because we realize that we don't need to add Singularity to Cromwell proper, it can just be a backend! But I didn't understand wdl, or any of the formats, so my crew in Cherry lab gave me a solid repo to startwith, and then it started to click!; - https://github.com/vsoch/wgbs-pipeline/pull/1. I was waiting for the Dockerfile test PR to pass, but realized it probably wouldn't, so I jumped on adding the example backend workflows (still without totally understanding what/why/how, but figuring out as I went):; - https://github.com/broadinstitute/cromwell/pull/4039. ## Question 3: But what about Cromwell+Singularity on Travis?. I got confused again when there were [requests for additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, is this making life easier for the scientist? If I'm a programmer person, and this is the minimum I am allowed for this to just run a simple container, what happens when it gets harder? I realized that witho",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:9784,Modifiability,config,configuration,9784,"additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, is this making life easier for the scientist? If I'm a programmer person, and this is the minimum I am allowed for this to just run a simple container, what happens when it gets harder? I realized that without a proper services API, singularity is no more special than python, bash, samtools, it's just a binary. . And I realize also that it's easy to get caught up in details like ""Should we use Travis or Circle?"" Does it work on Amazon with this kind of input? And there will always be bugs! But I think the forest is being a bit lost for the trees. . ## Question 4: What is the direction to go in?. You can probably take what I'm saying with a grain of salt because I'm new to this entire universe, and there is so much invested there is no turning back or rethinking. But all of this seems too complicated, and too hard. What is needed is a solution that is just really stupid and simple. You have a container that understands its data. You point the container at a dataset and run it. You outsource the w",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6027,Performance,load,load,6027,"t needs to run Singularity. A container should almost be more like a hard coded binary step instead of a ""come into the environment and play around, the water's fine!"" It's a little bit like the ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken""). The first is harder because you represent more things (more containers), but the second isn't reproducible because if you lose ""by chicken"" you've lost the entire workflow. Does that make sense?. ## What can/should we do now?. So there are two things to think about. With the current representation of a workflow, we would want Singularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:2575,Safety,redund,redundancy,2575,"ry, etc. that the scientist has written all the magic into, that takes some input arguments (data, poutputs, thresholds, etc.) and ""does the scientific thing"" to return to the workflow manager (cromwell) that is controlling its run via the backend. ## What does Singularity + Cromwell look like?. People keep saying these two together, and I've been struggling to figure it out. I've been doing a lot of work trying to do that. What does it mean for Singularity to be a part of Cromwell. I first logically thought it would mean a backend, because the basic exec / run commands for Singularity don't change much (but arguments do!). But it doesn't fit well here because it's missing that API to make it a fully fledged service. To those familiar with Singularity, this is the instance command group (and not running containers as images). Then I thought it was really more of a workflow executable. But if this is the case, why is it special at all? It doesn't really fit because there is still going to be a lot of redundancy in specifying the ""singularity run <container> <args> bit over and over again. So I think (eventually) all these use cases could fit into cromwell,. - running a singularity container as an executable with a backend like slurm; - running a singularity container as an executable on with Local (host) backend; - running a container as a backend as a container instance (via its API). but for now, without a clean API for services, only the first two really make sense. Singularity is not special. It's just a binary. ## Why has it been so confusing?. We get Singularity confused with Docker, because they are both containers. Same thing right? Sort of, but not exactly. Docker is a container technology, but actually it's older and has had time to develop a full API for services. It meets the criteria for both a backend and an executable, and this is because it can be conceptualized as both ""a thing that you run"" and ""the thing that is the container you run in."" But it's c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6889,Security,validat,validated,6889,"ant this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that same format. A new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has several file formats under it) but it revolutionizing how brain imaging analysis is done. (e.g, take a look at [https://www.openneuro.org](https://www.openneuro.org). # Development of my Thinking; Finally, I want to share how I came to the thinking above. Here are the steps that I've taken in the last few weeks, and resulting thoughts from them. I started with this issue board actually, and a general goal to ""Add Singularity to Cromwell."" Ok. ### Question 1: How do I develop Cromwell?; It first was hard for me to know where to start to develop Cromwell, because the docs just went into how to compile it on a host. So it made sense to make it easy for the deve",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:54,Testability,test,testing,54,"Hey everyone!. I've been thinking more about this and testing, and I want to offer my thoughts here. ; I think overall my conclusions are:. - We are trying to shove Singularity in as a backend **and** a workflow component, it's one or the other; - It's probably more appropriately the latter - a command you would put into a workflow (e.g., like python, any binary really) because services and standards (OCI) aren't fully developed.; - The time is soon, but it's not now, to define a Singularity backend; - For now, give users examples of just using containers as executables, nothing special. TLDR let's not try shoving a dog into a cat hole because the ears look similar. They are two different technologies, the latter (Singularity) is probably going to do great things for Cromwell **because** it is a single binary (and not a collection of tarballs) but we need that version 3.0 with OCI compliance to really have a well formulated language for Cromwell to talk to, period. I can go into more detail. First, let's define the parties involved:. ## Definitions. - **cromwell** is a workflow executor. It understands backends, and workflows. The backends run the workflows, and cromwell is just a manager for that.; - **backend** is an API really for services. The basic needs for this API are generally ""start, ""stop"", ""status,"" etc., and other kinds of ""controller"" commands for a particular executable. You have to be able to list what is going on, and get PIDs, and issue stop and status commands for the guts inside.; - **executable** is a script, binary, etc. that the scientist has written all the magic into, that takes some input arguments (data, poutputs, thresholds, etc.) and ""does the scientific thing"" to return to the workflow manager (cromwell) that is controlling its run via the backend. ## What does Singularity + Cromwell look like?. People keep saying these two together, and I've been struggling to figure it out. I've been doing a lot of work trying to do that. What does it ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:2056,Testability,log,logically,2056," backends, and workflows. The backends run the workflows, and cromwell is just a manager for that.; - **backend** is an API really for services. The basic needs for this API are generally ""start, ""stop"", ""status,"" etc., and other kinds of ""controller"" commands for a particular executable. You have to be able to list what is going on, and get PIDs, and issue stop and status commands for the guts inside.; - **executable** is a script, binary, etc. that the scientist has written all the magic into, that takes some input arguments (data, poutputs, thresholds, etc.) and ""does the scientific thing"" to return to the workflow manager (cromwell) that is controlling its run via the backend. ## What does Singularity + Cromwell look like?. People keep saying these two together, and I've been struggling to figure it out. I've been doing a lot of work trying to do that. What does it mean for Singularity to be a part of Cromwell. I first logically thought it would mean a backend, because the basic exec / run commands for Singularity don't change much (but arguments do!). But it doesn't fit well here because it's missing that API to make it a fully fledged service. To those familiar with Singularity, this is the instance command group (and not running containers as images). Then I thought it was really more of a workflow executable. But if this is the case, why is it special at all? It doesn't really fit because there is still going to be a lot of redundancy in specifying the ""singularity run <container> <args> bit over and over again. So I think (eventually) all these use cases could fit into cromwell,. - running a singularity container as an executable with a backend like slurm; - running a singularity container as an executable on with Local (host) backend; - running a container as a backend as a container instance (via its API). but for now, without a clean API for services, only the first two really make sense. Singularity is not special. It's just a binary. ## Why has it been",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:8001,Testability,test,tests,8001," new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has several file formats under it) but it revolutionizing how brain imaging analysis is done. (e.g, take a look at [https://www.openneuro.org](https://www.openneuro.org). # Development of my Thinking; Finally, I want to share how I came to the thinking above. Here are the steps that I've taken in the last few weeks, and resulting thoughts from them. I started with this issue board actually, and a general goal to ""Add Singularity to Cromwell."" Ok. ### Question 1: How do I develop Cromwell?; It first was hard for me to know where to start to develop Cromwell, because the docs just went into how to compile it on a host. So it made sense to make it easy for the developer to develop Cromwell so I made a Dockerfile to do that:; - https://github.com/broadinstitute/cromwell/pull/4002. Woohoo merged! We needed to have tests too, so I followed up on that:; - https://github.com/broadinstitute/cromwell/pull/4015 . But unfortunately it was decided that CircleCI was too new / needed to learn stuff (this is ok!) so it's going to be closed. . ## Question 2: How do we add a Singularity backend?. But this is actually ok, because we realize that we don't need to add Singularity to Cromwell proper, it can just be a backend! But I didn't understand wdl, or any of the formats, so my crew in Cherry lab gave me a solid repo to startwith, and then it started to click!; - https://github.com/vsoch/wgbs-pipeline/pull/1. I was waiting for the Dockerfile test PR to pass, but realized it probably wouldn't, so I jumped on adding the example backend workflows (still without totally understanding what/why/how, but figuring out as I went):; - https://github.com/broadinstitute/cromwell/pull/4039. ## Question 3: But what about Cromwell+Singularity on Travis?. I got confused again when there were [requests for additional tests](https://gi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:8629,Testability,test,test,8629," to start to develop Cromwell, because the docs just went into how to compile it on a host. So it made sense to make it easy for the developer to develop Cromwell so I made a Dockerfile to do that:; - https://github.com/broadinstitute/cromwell/pull/4002. Woohoo merged! We needed to have tests too, so I followed up on that:; - https://github.com/broadinstitute/cromwell/pull/4015 . But unfortunately it was decided that CircleCI was too new / needed to learn stuff (this is ok!) so it's going to be closed. . ## Question 2: How do we add a Singularity backend?. But this is actually ok, because we realize that we don't need to add Singularity to Cromwell proper, it can just be a backend! But I didn't understand wdl, or any of the formats, so my crew in Cherry lab gave me a solid repo to startwith, and then it started to click!; - https://github.com/vsoch/wgbs-pipeline/pull/1. I was waiting for the Dockerfile test PR to pass, but realized it probably wouldn't, so I jumped on adding the example backend workflows (still without totally understanding what/why/how, but figuring out as I went):; - https://github.com/broadinstitute/cromwell/pull/4039. ## Question 3: But what about Cromwell+Singularity on Travis?. I got confused again when there were [requests for additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:8995,Testability,test,tests,8995," merged! We needed to have tests too, so I followed up on that:; - https://github.com/broadinstitute/cromwell/pull/4015 . But unfortunately it was decided that CircleCI was too new / needed to learn stuff (this is ok!) so it's going to be closed. . ## Question 2: How do we add a Singularity backend?. But this is actually ok, because we realize that we don't need to add Singularity to Cromwell proper, it can just be a backend! But I didn't understand wdl, or any of the formats, so my crew in Cherry lab gave me a solid repo to startwith, and then it started to click!; - https://github.com/vsoch/wgbs-pipeline/pull/1. I was waiting for the Dockerfile test PR to pass, but realized it probably wouldn't, so I jumped on adding the example backend workflows (still without totally understanding what/why/how, but figuring out as I went):; - https://github.com/broadinstitute/cromwell/pull/4039. ## Question 3: But what about Cromwell+Singularity on Travis?. I got confused again when there were [requests for additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:8167,Usability,learn,learn,8167," data structure) that (has several file formats under it) but it revolutionizing how brain imaging analysis is done. (e.g, take a look at [https://www.openneuro.org](https://www.openneuro.org). # Development of my Thinking; Finally, I want to share how I came to the thinking above. Here are the steps that I've taken in the last few weeks, and resulting thoughts from them. I started with this issue board actually, and a general goal to ""Add Singularity to Cromwell."" Ok. ### Question 1: How do I develop Cromwell?; It first was hard for me to know where to start to develop Cromwell, because the docs just went into how to compile it on a host. So it made sense to make it easy for the developer to develop Cromwell so I made a Dockerfile to do that:; - https://github.com/broadinstitute/cromwell/pull/4002. Woohoo merged! We needed to have tests too, so I followed up on that:; - https://github.com/broadinstitute/cromwell/pull/4015 . But unfortunately it was decided that CircleCI was too new / needed to learn stuff (this is ok!) so it's going to be closed. . ## Question 2: How do we add a Singularity backend?. But this is actually ok, because we realize that we don't need to add Singularity to Cromwell proper, it can just be a backend! But I didn't understand wdl, or any of the formats, so my crew in Cherry lab gave me a solid repo to startwith, and then it started to click!; - https://github.com/vsoch/wgbs-pipeline/pull/1. I was waiting for the Dockerfile test PR to pass, but realized it probably wouldn't, so I jumped on adding the example backend workflows (still without totally understanding what/why/how, but figuring out as I went):; - https://github.com/broadinstitute/cromwell/pull/4039. ## Question 3: But what about Cromwell+Singularity on Travis?. I got confused again when there were [requests for additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:10109,Usability,simpl,simple,10109,"additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, is this making life easier for the scientist? If I'm a programmer person, and this is the minimum I am allowed for this to just run a simple container, what happens when it gets harder? I realized that without a proper services API, singularity is no more special than python, bash, samtools, it's just a binary. . And I realize also that it's easy to get caught up in details like ""Should we use Travis or Circle?"" Does it work on Amazon with this kind of input? And there will always be bugs! But I think the forest is being a bit lost for the trees. . ## Question 4: What is the direction to go in?. You can probably take what I'm saying with a grain of salt because I'm new to this entire universe, and there is so much invested there is no turning back or rethinking. But all of this seems too complicated, and too hard. What is needed is a solution that is just really stupid and simple. You have a container that understands its data. You point the container at a dataset and run it. You outsource the w",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:10861,Usability,simpl,simple,10861," Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, is this making life easier for the scientist? If I'm a programmer person, and this is the minimum I am allowed for this to just run a simple container, what happens when it gets harder? I realized that without a proper services API, singularity is no more special than python, bash, samtools, it's just a binary. . And I realize also that it's easy to get caught up in details like ""Should we use Travis or Circle?"" Does it work on Amazon with this kind of input? And there will always be bugs! But I think the forest is being a bit lost for the trees. . ## Question 4: What is the direction to go in?. You can probably take what I'm saying with a grain of salt because I'm new to this entire universe, and there is so much invested there is no turning back or rethinking. But all of this seems too complicated, and too hard. What is needed is a solution that is just really stupid and simple. You have a container that understands its data. You point the container at a dataset and run it. You outsource the workflow part to the technologies that big players are building already. This definitely isn't a ""throw hands in the air"" sort of deal, because most of this stuff is working already it seems? I don't know if this perspective is useful, but as a new person (outsider) I wanted to offer it because if I'm confused and find this hard, probably others are too. And minimally it's good for awareness and discussion? I'm definitely happy to help however I can! But I'd really like to not try shoving dogs into cat holes, it's a very messy business. :cat: :dog: :hole: :sos:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:6023,Availability,down,down,6023," clean API for services, only the first two really; > make sense. Singularity is not special. It's just a binary.; > Why has it been so confusing?; >; > We get Singularity confused with Docker, because they are both containers.; > Same thing right? Sort of, but not exactly. Docker is a container; > technology, but actually it's older and has had time to develop a full API; > for services. It meets the criteria for both a backend and an executable,; > and this is because it can be conceptualized as both ""a thing that you run""; > and ""the thing that is the container you run in."" But it's confusing. The; > distinction is that although Singularity is also a container, Singularity; > is *not* like Docker because it doesn't have the fully developed services; > API (yet!). This problem is hard because the language for Singularity; > containers communicating between one another, and even to the host, is not; > completely implemented yet. This comes down to OCI compliance, and having a; > way for some host to manage all of its Singularity containers. Right now we; > just have start and stop, but we can't connect containers, define ports, or; > even easily get a PID. It could (sort of?) be hacked, but we would be; > better off waiting for that nice standard.; > Reproducible Binary (Workflow Step) vs. Environment; >; > There is also a distinction that I haven't completely wrapped my head; > around. Docker is very commonly used as an environment - you put a bunch of; > software (e.g., samtools, bwa aligner, etc.) and then issue commands to the; > container with custom things. Singularity, in my mind, to be truly a; > reproducible thing is more of the workflow step or script. It will have the; > software inside, but better should have those same commands represented; > with internal modularity. I could arguably completely do away with the; > external workflow dependency if a single binary told me how to run itself,; > and then had more than one entrypoint defined for each step. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8924,Availability,down,down,8924,"ropose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (brain imaging data structure); > that (has several file formats under it) but it revolutionizing how brain; > imaging analysis is done. (e.g, take a look at https://www.openneuro.org.; > Development of my Thinking; >; > Finally, I want to share how I came to the thinking above. Here are the; > steps that I've taken in the last few weeks, and resulting thoughts from; > them. I started with this issue board actually, and a general goal to ""Add; > Singularity to Cromwell."" Ok.; > Question 1: How do I develop Cromwell?; >; > It first w",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:12312,Availability,echo,echo,12312,"e were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub://vsoch/hello-world; >; > turned into needing Cromwell (java and the jar), an inputs json file, a; > wdl specification, a backend configuration, and a runtime command that I; > can't seem to remember, and then the entire thing takes much longer than an; > instance to echo a tiny Rawwwwr! If this is the goal we are going for, is; > this making life easier for the scientist? If I'm a programmer person, and; > this is the minimum I am allowed for this to just run a simple container,; > what happens when it gets harder? I realized that without a proper services; > API, singularity is no more special than python, bash, samtools, it's just; > a binary.; >; > And I realize also that it's easy to get caught up in details like ""Should; > we use Travis or Circle?"" Does it work on Amazon with this kind of input?; > And there will always be bugs! But I think the forest is being a bit lost; > for the trees.; > Question 4: What is the direction to go in?; >; > You can probably take what I'm saying with a grain of salt because I'm new; > to this entire universe, and there is so much invested there is no turning; > back or rethinking. But all of this seems too complicated, and too hard.; > What is needed is a solution that is just really stupid and simple. You; > have a container t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8398,Deployability,configurat,configuration,8398,"o every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (bra",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8496,Deployability,integrat,integrate,8496,"o every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (bra",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:10872,Deployability,pipeline,pipeline,10872,"taken in the last few weeks, and resulting thoughts from; > them. I started with this issue board actually, and a general goal to ""Add; > Singularity to Cromwell."" Ok.; > Question 1: How do I develop Cromwell?; >; > It first was hard for me to know where to start to develop Cromwell,; > because the docs just went into how to compile it on a host. So it made; > sense to make it easy for the developer to develop Cromwell so I made a; > Dockerfile to do that:; >; > - #4002 <https://github.com/broadinstitute/cromwell/pull/4002>; >; > Woohoo merged! We needed to have tests too, so I followed up on that:; >; > - #4015 <https://github.com/broadinstitute/cromwell/pull/4015>; >; > But unfortunately it was decided that CircleCI was too new / needed to; > learn stuff (this is ok!) so it's going to be closed.; > Question 2: How do we add a Singularity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+Singularity on Travis?; >; > I got confused again when there were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any k",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:10914,Deployability,pipeline,pipeline,10914," on a host. So it made; > sense to make it easy for the developer to develop Cromwell so I made a; > Dockerfile to do that:; >; > - #4002 <https://github.com/broadinstitute/cromwell/pull/4002>; >; > Woohoo merged! We needed to have tests too, so I followed up on that:; >; > - #4015 <https://github.com/broadinstitute/cromwell/pull/4015>; >; > But unfortunately it was decided that CircleCI was too new / needed to; > learn stuff (this is ok!) so it's going to be closed.; > Question 2: How do we add a Singularity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+Singularity on Travis?; >; > I got confused again when there were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub:/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:12174,Deployability,configurat,configuration,12174,"e were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub://vsoch/hello-world; >; > turned into needing Cromwell (java and the jar), an inputs json file, a; > wdl specification, a backend configuration, and a runtime command that I; > can't seem to remember, and then the entire thing takes much longer than an; > instance to echo a tiny Rawwwwr! If this is the goal we are going for, is; > this making life easier for the scientist? If I'm a programmer person, and; > this is the minimum I am allowed for this to just run a simple container,; > what happens when it gets harder? I realized that without a proper services; > API, singularity is no more special than python, bash, samtools, it's just; > a binary.; >; > And I realize also that it's easy to get caught up in details like ""Should; > we use Travis or Circle?"" Does it work on Amazon with this kind of input?; > And there will always be bugs! But I think the forest is being a bit lost; > for the trees.; > Question 4: What is the direction to go in?; >; > You can probably take what I'm saying with a grain of salt because I'm new; > to this entire universe, and there is so much invested there is no turning; > back or rethinking. But all of this seems too complicated, and too hard.; > What is needed is a solution that is just really stupid and simple. You; > have a container t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:1206,Integrability,wrap,wrapper,1206,"y could have a role as a backend for; workflow systems, but it's ineffective to take that idea as a starting; point. I really agree that it's best to lay that idea to rest and focus on; the biggest impact / low hanging fruit . To be honest, Singularity as a workflow componetn is exactly the way I've; been using Singularity in real life, whereas the idea to use it as a; workflow backbone always remained ... just an idea. This is not because; Singularity lacks potential there, but mostly because workflow backbones; have complex requirements, and trying to fit a new tool to them that wasn't; made for it in the first place is not trivial. Moving Singularity out of the role of the backend and into the role of a; workflow component, more specifically a container that understands its; data, also introduces the room to give it its own subfunctions, variables,; metadata, tags, etc. This makes the starting point plainly obvious. You can just take the; location where you mention the location of the executable, and put the; wrapper to your singularity image there. I bet this is what most people do; anyway. A next step would be to give it its own section within the workflow; components. Maybe the comment of oneillkza is a high impact one, just; define Singularity as a CAP/ISOblablabla compliant workflow component; within Cromwell. Another take (and not per se mutually exclusive from the take mentioned; above) would be to, again, fix Singularity as a workflow component, and; create a set of options and functions around it that focus on abstraction; of data access etcetera. Very curious where this will go, and thanks so much Vanessa for rethinking; the approach!. Gr. Pim. On Tue, Aug 28, 2018 at 3:12 AM Vanessa Sochat <notifications@github.com>; wrote:. > Hey everyone!; >; > I've been thinking more about this and testing, and I want to offer my; > thoughts here.; > I think overall my conclusions are:; >; > - We are trying to shove Singularity in as a backend *and* a workflow; > co",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:6452,Integrability,wrap,wrapped,6452,"lop a full API; > for services. It meets the criteria for both a backend and an executable,; > and this is because it can be conceptualized as both ""a thing that you run""; > and ""the thing that is the container you run in."" But it's confusing. The; > distinction is that although Singularity is also a container, Singularity; > is *not* like Docker because it doesn't have the fully developed services; > API (yet!). This problem is hard because the language for Singularity; > containers communicating between one another, and even to the host, is not; > completely implemented yet. This comes down to OCI compliance, and having a; > way for some host to manage all of its Singularity containers. Right now we; > just have start and stop, but we can't connect containers, define ports, or; > even easily get a PID. It could (sort of?) be hacked, but we would be; > better off waiting for that nice standard.; > Reproducible Binary (Workflow Step) vs. Environment; >; > There is also a distinction that I haven't completely wrapped my head; > around. Docker is very commonly used as an environment - you put a bunch of; > software (e.g., samtools, bwa aligner, etc.) and then issue commands to the; > container with custom things. Singularity, in my mind, to be truly a; > reproducible thing is more of the workflow step or script. It will have the; > software inside, but better should have those same commands represented; > with internal modularity. I could arguably completely do away with the; > external workflow dependency if a single binary told me how to run itself,; > and then had more than one entrypoint defined for each step. I wouldn't; > need to care about the software or components inside because my host just; > needs to run Singularity. A container should almost be more like a hard; > coded binary step instead of a ""come into the environment and play around,; > the water's fine!"" It's a little bit like the ICD 10 decision to give a; > unique id to every combination of things (",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:6947,Integrability,depend,dependency,6947," is not; > completely implemented yet. This comes down to OCI compliance, and having a; > way for some host to manage all of its Singularity containers. Right now we; > just have start and stop, but we can't connect containers, define ports, or; > even easily get a PID. It could (sort of?) be hacked, but we would be; > better off waiting for that nice standard.; > Reproducible Binary (Workflow Step) vs. Environment; >; > There is also a distinction that I haven't completely wrapped my head; > around. Docker is very commonly used as an environment - you put a bunch of; > software (e.g., samtools, bwa aligner, etc.) and then issue commands to the; > container with custom things. Singularity, in my mind, to be truly a; > reproducible thing is more of the workflow step or script. It will have the; > software inside, but better should have those same commands represented; > with internal modularity. I could arguably completely do away with the; > external workflow dependency if a single binary told me how to run itself,; > and then had more than one entrypoint defined for each step. I wouldn't; > need to care about the software or components inside because my host just; > needs to run Singularity. A container should almost be more like a hard; > coded binary step instead of a ""come into the environment and play around,; > the water's fine!"" It's a little bit like the ICD 10 decision to give a; > unique id to every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8496,Integrability,integrat,integrate,8496,"o every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (bra",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:1031,Modifiability,variab,variables,1031,"I fully agree Vanessa!!! I don't think this is surrendering, it's finding; the solution that has been standing in plain sight all the time. At some point in the future Singularity could have a role as a backend for; workflow systems, but it's ineffective to take that idea as a starting; point. I really agree that it's best to lay that idea to rest and focus on; the biggest impact / low hanging fruit . To be honest, Singularity as a workflow componetn is exactly the way I've; been using Singularity in real life, whereas the idea to use it as a; workflow backbone always remained ... just an idea. This is not because; Singularity lacks potential there, but mostly because workflow backbones; have complex requirements, and trying to fit a new tool to them that wasn't; made for it in the first place is not trivial. Moving Singularity out of the role of the backend and into the role of a; workflow component, more specifically a container that understands its; data, also introduces the room to give it its own subfunctions, variables,; metadata, tags, etc. This makes the starting point plainly obvious. You can just take the; location where you mention the location of the executable, and put the; wrapper to your singularity image there. I bet this is what most people do; anyway. A next step would be to give it its own section within the workflow; components. Maybe the comment of oneillkza is a high impact one, just; define Singularity as a CAP/ISOblablabla compliant workflow component; within Cromwell. Another take (and not per se mutually exclusive from the take mentioned; above) would be to, again, fix Singularity as a workflow component, and; create a set of options and functions around it that focus on abstraction; of data access etcetera. Very curious where this will go, and thanks so much Vanessa for rethinking; the approach!. Gr. Pim. On Tue, Aug 28, 2018 at 3:12 AM Vanessa Sochat <notifications@github.com>; wrote:. > Hey everyone!; >; > I've been thinking more about th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8398,Modifiability,config,configuration,8398,"o every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (bra",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:11592,Modifiability,layers,layers,11592,"ity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+Singularity on Travis?; >; > I got confused again when there were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub://vsoch/hello-world; >; > turned into needing Cromwell (java and the jar), an inputs json file, a; > wdl specification, a backend configuration, and a runtime command that I; > can't seem to remember, and then the entire thing takes much longer than an; > instance to echo a tiny Rawwwwr! If this is the goal we are going for, is; > this making life easier for the scientist? If I'm a programmer person, and; > this is the minimum I am allowed for this to just run a simple container,; > what happens when it get",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:12174,Modifiability,config,configuration,12174,"e were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub://vsoch/hello-world; >; > turned into needing Cromwell (java and the jar), an inputs json file, a; > wdl specification, a backend configuration, and a runtime command that I; > can't seem to remember, and then the entire thing takes much longer than an; > instance to echo a tiny Rawwwwr! If this is the goal we are going for, is; > this making life easier for the scientist? If I'm a programmer person, and; > this is the minimum I am allowed for this to just run a simple container,; > what happens when it gets harder? I realized that without a proper services; > API, singularity is no more special than python, bash, samtools, it's just; > a binary.; >; > And I realize also that it's easy to get caught up in details like ""Should; > we use Travis or Circle?"" Does it work on Amazon with this kind of input?; > And there will always be bugs! But I think the forest is being a bit lost; > for the trees.; > Question 4: What is the direction to go in?; >; > You can probably take what I'm saying with a grain of salt because I'm new; > to this entire universe, and there is so much invested there is no turning; > back or rethinking. But all of this seems too complicated, and too hard.; > What is needed is a solution that is just really stupid and simple. You; > have a container t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8228,Performance,load,load,8228,"should almost be more like a hard; > coded binary step instead of a ""come into the environment and play around,; > the water's fine!"" It's a little bit like the ICD 10 decision to give a; > unique id to every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:4643,Safety,redund,redundancy,4643,"written all the magic into, that takes some input arguments (data,; > poutputs, thresholds, etc.) and ""does the scientific thing"" to return to; > the workflow manager (cromwell) that is controlling its run via the backend.; >; > What does Singularity + Cromwell look like?; >; > People keep saying these two together, and I've been struggling to figure; > it out. I've been doing a lot of work trying to do that. What does it mean; > for Singularity to be a part of Cromwell. I first logically thought it; > would mean a backend, because the basic exec / run commands for Singularity; > don't change much (but arguments do!). But it doesn't fit well here because; > it's missing that API to make it a fully fledged service. To those familiar; > with Singularity, this is the instance command group (and not running; > containers as images). Then I thought it was really more of a workflow; > executable. But if this is the case, why is it special at all? It doesn't; > really fit because there is still going to be a lot of redundancy in; > specifying the ""singularity run bit over and over again. So I think; > (eventually) all these use cases could fit into cromwell,; >; > - running a singularity container as an executable with a backend like; > slurm; > - running a singularity container as an executable on with Local; > (host) backend; > - running a container as a backend as a container instance (via its; > API); >; > but for now, without a clean API for services, only the first two really; > make sense. Singularity is not special. It's just a binary.; > Why has it been so confusing?; >; > We get Singularity confused with Docker, because they are both containers.; > Same thing right? Sort of, but not exactly. Docker is a container; > technology, but actually it's older and has had time to develop a full API; > for services. It meets the criteria for both a backend and an executable,; > and this is because it can be conceptualized as both ""a thing that you run""; > and ""the thing th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:1747,Security,access,access,1747," potential there, but mostly because workflow backbones; have complex requirements, and trying to fit a new tool to them that wasn't; made for it in the first place is not trivial. Moving Singularity out of the role of the backend and into the role of a; workflow component, more specifically a container that understands its; data, also introduces the room to give it its own subfunctions, variables,; metadata, tags, etc. This makes the starting point plainly obvious. You can just take the; location where you mention the location of the executable, and put the; wrapper to your singularity image there. I bet this is what most people do; anyway. A next step would be to give it its own section within the workflow; components. Maybe the comment of oneillkza is a high impact one, just; define Singularity as a CAP/ISOblablabla compliant workflow component; within Cromwell. Another take (and not per se mutually exclusive from the take mentioned; above) would be to, again, fix Singularity as a workflow component, and; create a set of options and functions around it that focus on abstraction; of data access etcetera. Very curious where this will go, and thanks so much Vanessa for rethinking; the approach!. Gr. Pim. On Tue, Aug 28, 2018 at 3:12 AM Vanessa Sochat <notifications@github.com>; wrote:. > Hey everyone!; >; > I've been thinking more about this and testing, and I want to offer my; > thoughts here.; > I think overall my conclusions are:; >; > - We are trying to shove Singularity in as a backend *and* a workflow; > component, it's one or the other; > - It's probably more appropriately the latter - a command you would; > put into a workflow (e.g., like python, any binary really) because services; > and standards (OCI) aren't fully developed.; > - The time is soon, but it's not now, to define a Singularity backend; > - For now, give users examples of just using containers as; > executables, nothing special.; >; > TLDR let's not try shoving a dog into a cat hole because the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:9132,Security,validat,validated,9132," to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (brain imaging data structure); > that (has several file formats under it) but it revolutionizing how brain; > imaging analysis is done. (e.g, take a look at https://www.openneuro.org.; > Development of my Thinking; >; > Finally, I want to share how I came to the thinking above. Here are the; > steps that I've taken in the last few weeks, and resulting thoughts from; > them. I started with this issue board actually, and a general goal to ""Add; > Singularity to Cromwell."" Ok.; > Question 1: How do I develop Cromwell?; >; > It first was hard for me to know where to start to develop Cromwell,; > because the docs just went into how to compile it on a host. So it made; > sense to make it eas",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:2008,Testability,test,testing,2008,"give it its own subfunctions, variables,; metadata, tags, etc. This makes the starting point plainly obvious. You can just take the; location where you mention the location of the executable, and put the; wrapper to your singularity image there. I bet this is what most people do; anyway. A next step would be to give it its own section within the workflow; components. Maybe the comment of oneillkza is a high impact one, just; define Singularity as a CAP/ISOblablabla compliant workflow component; within Cromwell. Another take (and not per se mutually exclusive from the take mentioned; above) would be to, again, fix Singularity as a workflow component, and; create a set of options and functions around it that focus on abstraction; of data access etcetera. Very curious where this will go, and thanks so much Vanessa for rethinking; the approach!. Gr. Pim. On Tue, Aug 28, 2018 at 3:12 AM Vanessa Sochat <notifications@github.com>; wrote:. > Hey everyone!; >; > I've been thinking more about this and testing, and I want to offer my; > thoughts here.; > I think overall my conclusions are:; >; > - We are trying to shove Singularity in as a backend *and* a workflow; > component, it's one or the other; > - It's probably more appropriately the latter - a command you would; > put into a workflow (e.g., like python, any binary really) because services; > and standards (OCI) aren't fully developed.; > - The time is soon, but it's not now, to define a Singularity backend; > - For now, give users examples of just using containers as; > executables, nothing special.; >; > TLDR let's not try shoving a dog into a cat hole because the ears look; > similar. They are two different technologies, the latter (Singularity) is; > probably going to do great things for Cromwell *because* it is a single; > binary (and not a collection of tarballs) but we need that version 3.0 with; > OCI compliance to really have a well formulated language for Cromwell to; > talk to, period.; >; > I can go into more",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:4103,Testability,log,logically,4103,"n the workflows, and cromwell is just a manager; > for that.; > - *backend* is an API really for services. The basic needs for this; > API are generally ""start, ""stop"", ""status,"" etc., and other kinds of; > ""controller"" commands for a particular executable. You have to be able to; > list what is going on, and get PIDs, and issue stop and status commands for; > the guts inside.; > - *executable* is a script, binary, etc. that the scientist has; > written all the magic into, that takes some input arguments (data,; > poutputs, thresholds, etc.) and ""does the scientific thing"" to return to; > the workflow manager (cromwell) that is controlling its run via the backend.; >; > What does Singularity + Cromwell look like?; >; > People keep saying these two together, and I've been struggling to figure; > it out. I've been doing a lot of work trying to do that. What does it mean; > for Singularity to be a part of Cromwell. I first logically thought it; > would mean a backend, because the basic exec / run commands for Singularity; > don't change much (but arguments do!). But it doesn't fit well here because; > it's missing that API to make it a fully fledged service. To those familiar; > with Singularity, this is the instance command group (and not running; > containers as images). Then I thought it was really more of a workflow; > executable. But if this is the case, why is it special at all? It doesn't; > really fit because there is still going to be a lot of redundancy in; > specifying the ""singularity run bit over and over again. So I think; > (eventually) all these use cases could fit into cromwell,; >; > - running a singularity container as an executable with a backend like; > slurm; > - running a singularity container as an executable on with Local; > (host) backend; > - running a container as a backend as a container instance (via its; > API); >; > but for now, without a clean API for services, only the first two really; > make sense. Singularity is not special. It's ju",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:10276,Testability,test,tests,10276,"; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (brain imaging data structure); > that (has several file formats under it) but it revolutionizing how brain; > imaging analysis is done. (e.g, take a look at https://www.openneuro.org.; > Development of my Thinking; >; > Finally, I want to share how I came to the thinking above. Here are the; > steps that I've taken in the last few weeks, and resulting thoughts from; > them. I started with this issue board actually, and a general goal to ""Add; > Singularity to Cromwell."" Ok.; > Question 1: How do I develop Cromwell?; >; > It first was hard for me to know where to start to develop Cromwell,; > because the docs just went into how to compile it on a host. So it made; > sense to make it easy for the developer to develop Cromwell so I made a; > Dockerfile to do that:; >; > - #4002 <https://github.com/broadinstitute/cromwell/pull/4002>; >; > Woohoo merged! We needed to have tests too, so I followed up on that:; >; > - #4015 <https://github.com/broadinstitute/cromwell/pull/4015>; >; > But unfortunately it was decided that CircleCI was too new / needed to; > learn stuff (this is ok!) so it's going to be closed.; > Question 2: How do we add a Singularity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:10970,Testability,test,test,10970," on a host. So it made; > sense to make it easy for the developer to develop Cromwell so I made a; > Dockerfile to do that:; >; > - #4002 <https://github.com/broadinstitute/cromwell/pull/4002>; >; > Woohoo merged! We needed to have tests too, so I followed up on that:; >; > - #4015 <https://github.com/broadinstitute/cromwell/pull/4015>; >; > But unfortunately it was decided that CircleCI was too new / needed to; > learn stuff (this is ok!) so it's going to be closed.; > Question 2: How do we add a Singularity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+Singularity on Travis?; >; > I got confused again when there were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub:/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:11361,Testability,test,tests,11361,"o I followed up on that:; >; > - #4015 <https://github.com/broadinstitute/cromwell/pull/4015>; >; > But unfortunately it was decided that CircleCI was too new / needed to; > learn stuff (this is ok!) so it's going to be closed.; > Question 2: How do we add a Singularity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+Singularity on Travis?; >; > I got confused again when there were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub://vsoch/hello-world; >; > turned into needing Cromwell (java and the jar), an inputs json file, a; > wdl specification, a backend configuration, and a runtime command that I; > can't seem to remember, and then the entire thing takes much longer",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:10462,Usability,learn,learn,10462,"that (has several file formats under it) but it revolutionizing how brain; > imaging analysis is done. (e.g, take a look at https://www.openneuro.org.; > Development of my Thinking; >; > Finally, I want to share how I came to the thinking above. Here are the; > steps that I've taken in the last few weeks, and resulting thoughts from; > them. I started with this issue board actually, and a general goal to ""Add; > Singularity to Cromwell."" Ok.; > Question 1: How do I develop Cromwell?; >; > It first was hard for me to know where to start to develop Cromwell,; > because the docs just went into how to compile it on a host. So it made; > sense to make it easy for the developer to develop Cromwell so I made a; > Dockerfile to do that:; >; > - #4002 <https://github.com/broadinstitute/cromwell/pull/4002>; >; > Woohoo merged! We needed to have tests too, so I followed up on that:; >; > - #4015 <https://github.com/broadinstitute/cromwell/pull/4015>; >; > But unfortunately it was decided that CircleCI was too new / needed to; > learn stuff (this is ok!) so it's going to be closed.; > Question 2: How do we add a Singularity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+Singularity on Travis?; >; > I got confused again when there were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issue",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:12511,Usability,simpl,simple,12511,"e were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub://vsoch/hello-world; >; > turned into needing Cromwell (java and the jar), an inputs json file, a; > wdl specification, a backend configuration, and a runtime command that I; > can't seem to remember, and then the entire thing takes much longer than an; > instance to echo a tiny Rawwwwr! If this is the goal we are going for, is; > this making life easier for the scientist? If I'm a programmer person, and; > this is the minimum I am allowed for this to just run a simple container,; > what happens when it gets harder? I realized that without a proper services; > API, singularity is no more special than python, bash, samtools, it's just; > a binary.; >; > And I realize also that it's easy to get caught up in details like ""Should; > we use Travis or Circle?"" Does it work on Amazon with this kind of input?; > And there will always be bugs! But I think the forest is being a bit lost; > for the trees.; > Question 4: What is the direction to go in?; >; > You can probably take what I'm saying with a grain of salt because I'm new; > to this entire universe, and there is so much invested there is no turning; > back or rethinking. But all of this seems too complicated, and too hard.; > What is needed is a solution that is just really stupid and simple. You; > have a container t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:13297,Usability,simpl,simple,13297,"ng takes much longer than an; > instance to echo a tiny Rawwwwr! If this is the goal we are going for, is; > this making life easier for the scientist? If I'm a programmer person, and; > this is the minimum I am allowed for this to just run a simple container,; > what happens when it gets harder? I realized that without a proper services; > API, singularity is no more special than python, bash, samtools, it's just; > a binary.; >; > And I realize also that it's easy to get caught up in details like ""Should; > we use Travis or Circle?"" Does it work on Amazon with this kind of input?; > And there will always be bugs! But I think the forest is being a bit lost; > for the trees.; > Question 4: What is the direction to go in?; >; > You can probably take what I'm saying with a grain of salt because I'm new; > to this entire universe, and there is so much invested there is no turning; > back or rethinking. But all of this seems too complicated, and too hard.; > What is needed is a solution that is just really stupid and simple. You; > have a container that understands its data. You point the container at a; > dataset and run it. You outsource the workflow part to the technologies; > that big players are building already.; >; > This definitely isn't a ""throw hands in the air"" sort of deal, because; > most of this stuff is working already it seems? I don't know if this; > perspective is useful, but as a new person (outsider) I wanted to offer it; > because if I'm confused and find this hard, probably others are too. And; > minimally it's good for awareness and discussion? I'm definitely happy to; > help however I can! But I'd really like to not try shoving dogs into cat; > holes, it's a very messy business. 🐱 🐶 🕳 🆘; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214>,; > or mute the thread; > <https://github.com/notifications/unsubscri",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516:97,Availability,error,error,97,"Just to be clear the ""tasks"" referred to here are Slick tasks and not Cromwell / WDL tasks (that error message is produced by the Slick library). I'm speculating a bit but it may be that the unrestricted query was tying up the database for so long that too many tasks backed up behind it and overflowed the Slick task queue of size 1000. More restrictive server-side filtering like you're doing now definitely seems like a good idea. 🙂",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516:103,Integrability,message,message,103,"Just to be clear the ""tasks"" referred to here are Slick tasks and not Cromwell / WDL tasks (that error message is produced by the Slick library). I'm speculating a bit but it may be that the unrestricted query was tying up the database for so long that too many tasks backed up behind it and overflowed the Slick task queue of size 1000. More restrictive server-side filtering like you're doing now definitely seems like a good idea. 🙂",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516:318,Performance,queue,queue,318,"Just to be clear the ""tasks"" referred to here are Slick tasks and not Cromwell / WDL tasks (that error message is produced by the Slick library). I'm speculating a bit but it may be that the unrestricted query was tying up the database for so long that too many tasks backed up behind it and overflowed the Slick task queue of size 1000. More restrictive server-side filtering like you're doing now definitely seems like a good idea. 🙂",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516
https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516:11,Usability,clear,clear,11,"Just to be clear the ""tasks"" referred to here are Slick tasks and not Cromwell / WDL tasks (that error message is produced by the Slick library). I'm speculating a bit but it may be that the unrestricted query was tying up the database for so long that too many tasks backed up behind it and overflowed the Slick task queue of size 1000. More restrictive server-side filtering like you're doing now definitely seems like a good idea. 🙂",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516
https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298047072:205,Usability,simpl,simple,205,"The issue is in the reference.conf. It still uses the old convention. Even if you override the values in your own conf file, the reference.conf will trigger the deprecation exception. This issue should be simple as correcting the reference.conf.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298047072
https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-303595634:43,Usability,clear,clear,43,"@geoffjentry Unclear, I was only aiming to clear the closed PR from the board. Reopening and I'll let @danbills close this whenever appropriate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2246#issuecomment-303595634
https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-322892247:1897,Modifiability,config,config,1897,"ssue. PR #2280 fixes part of the problem, but only for the local backend. I am trying to use the official Picard docker:; https://hub.docker.com/r/broadinstitute/picard/~/dockerfile/. Here is a simplified version of my WDL for illustration purposes:. task picard{; command {; CollectAlignmentSummaryMetrics; }; runtime{; docker:""broadinstitute/picard""; }; }. workflow picardwf {; call picard; }. If I locally do; `docker run broadinstitute/picard CollectAlignmentSummaryMetrics`. I get the behavior I expect -- in this simplified example, it prints out the help command for CollectAlignmentSummaryMetrics. (The Dockerfile specifies an entrypoint that calls a script in the container and passes ""CollectAlignmentSummaryMetrics"" to it.). When running with the JES backend, however, I get:; '/tmp/ggp-298770331' is not a valid command. See PicardCommandLine -h for more information.'. Cromwell has inserted a file path immediately after the image name in the docker run command:; `Running command: docker run -v /tmp/ggp-298770331:/tmp/ggp-298770331 -v /mnt/local-disk:/cromwell_root -e exec=/cromwell_root/exec.sh -e picard-rc.txt=/cromwell_root/picard-rc.txt -e __extra_config_gcs_path=gs://<path_redacted> broadinstitute/picard@sha256:1ddf5888182718c55054c96dca6a5d65d23a703f98e66832400e4837d04854a7 /tmp/ggp-298770331`. I don't think this default behavior makes sense for any backend. It should either override the entrypoint by default (as the local backend now does) or -- this would be much better -- it would work with the existing entrypoint. A lot of published containers specify an entrypoint, which means when running in Cromwell they can't really be used as intended with that entrypoint. To get them running at all requires changing Cromwell's config to ignore the entrypoint, and then if you want to use the entrypoint you have to re-specify it as part of the command. Could you please consider changing the default behavior to make it compatible with entrypoints out of the box?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-322892247
https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-322892247:336,Usability,simpl,simplified,336,"Greetings, my illustrious Cromwellian friends!. I find your workflow engine indispensable but am still running into problems related to this issue. PR #2280 fixes part of the problem, but only for the local backend. I am trying to use the official Picard docker:; https://hub.docker.com/r/broadinstitute/picard/~/dockerfile/. Here is a simplified version of my WDL for illustration purposes:. task picard{; command {; CollectAlignmentSummaryMetrics; }; runtime{; docker:""broadinstitute/picard""; }; }. workflow picardwf {; call picard; }. If I locally do; `docker run broadinstitute/picard CollectAlignmentSummaryMetrics`. I get the behavior I expect -- in this simplified example, it prints out the help command for CollectAlignmentSummaryMetrics. (The Dockerfile specifies an entrypoint that calls a script in the container and passes ""CollectAlignmentSummaryMetrics"" to it.). When running with the JES backend, however, I get:; '/tmp/ggp-298770331' is not a valid command. See PicardCommandLine -h for more information.'. Cromwell has inserted a file path immediately after the image name in the docker run command:; `Running command: docker run -v /tmp/ggp-298770331:/tmp/ggp-298770331 -v /mnt/local-disk:/cromwell_root -e exec=/cromwell_root/exec.sh -e picard-rc.txt=/cromwell_root/picard-rc.txt -e __extra_config_gcs_path=gs://<path_redacted> broadinstitute/picard@sha256:1ddf5888182718c55054c96dca6a5d65d23a703f98e66832400e4837d04854a7 /tmp/ggp-298770331`. I don't think this default behavior makes sense for any backend. It should either override the entrypoint by default (as the local backend now does) or -- this would be much better -- it would work with the existing entrypoint. A lot of published containers specify an entrypoint, which means when running in Cromwell they can't really be used as intended with that entrypoint. To get them running at all requires changing Cromwell's config to ignore the entrypoint, and then if you want to use the entrypoint you have to re-specify it as",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-322892247
https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-322892247:661,Usability,simpl,simplified,661,"Greetings, my illustrious Cromwellian friends!. I find your workflow engine indispensable but am still running into problems related to this issue. PR #2280 fixes part of the problem, but only for the local backend. I am trying to use the official Picard docker:; https://hub.docker.com/r/broadinstitute/picard/~/dockerfile/. Here is a simplified version of my WDL for illustration purposes:. task picard{; command {; CollectAlignmentSummaryMetrics; }; runtime{; docker:""broadinstitute/picard""; }; }. workflow picardwf {; call picard; }. If I locally do; `docker run broadinstitute/picard CollectAlignmentSummaryMetrics`. I get the behavior I expect -- in this simplified example, it prints out the help command for CollectAlignmentSummaryMetrics. (The Dockerfile specifies an entrypoint that calls a script in the container and passes ""CollectAlignmentSummaryMetrics"" to it.). When running with the JES backend, however, I get:; '/tmp/ggp-298770331' is not a valid command. See PicardCommandLine -h for more information.'. Cromwell has inserted a file path immediately after the image name in the docker run command:; `Running command: docker run -v /tmp/ggp-298770331:/tmp/ggp-298770331 -v /mnt/local-disk:/cromwell_root -e exec=/cromwell_root/exec.sh -e picard-rc.txt=/cromwell_root/picard-rc.txt -e __extra_config_gcs_path=gs://<path_redacted> broadinstitute/picard@sha256:1ddf5888182718c55054c96dca6a5d65d23a703f98e66832400e4837d04854a7 /tmp/ggp-298770331`. I don't think this default behavior makes sense for any backend. It should either override the entrypoint by default (as the local backend now does) or -- this would be much better -- it would work with the existing entrypoint. A lot of published containers specify an entrypoint, which means when running in Cromwell they can't really be used as intended with that entrypoint. To get them running at all requires changing Cromwell's config to ignore the entrypoint, and then if you want to use the entrypoint you have to re-specify it as",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2256#issuecomment-322892247
https://github.com/broadinstitute/cromwell/pull/2257#issuecomment-300918183:75,Usability,usab,usability,75,"Other than my pie-in-the-sky ""wouldn't it be nice"" comment about the API's usability, looks good 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2257/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2257#issuecomment-300918183
https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-302190815:89,Deployability,release,release,89,@tomkinsc What we're going to do is time allowing try to do it by hand for this upcoming release and either way make it part of our automated process for the following release. I'm assuming you're willing to provide a helpful hand if we need some guidance on the specifics?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-302190815
https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-302190815:168,Deployability,release,release,168,@tomkinsc What we're going to do is time allowing try to do it by hand for this upcoming release and either way make it part of our automated process for the following release. I'm assuming you're willing to provide a helpful hand if we need some guidance on the specifics?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-302190815
https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-302190815:247,Usability,guid,guidance,247,@tomkinsc What we're going to do is time allowing try to do it by hand for this upcoming release and either way make it part of our automated process for the following release. I'm assuming you're willing to provide a helpful hand if we need some guidance on the specifics?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258#issuecomment-302190815
https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301213292:117,Deployability,release,release,117,@vdauwera Can you explain the situation? I'm not clear what the exact feature request is. It won't make it into this release but we can see about next (Cromwell 28).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301213292
https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301213292:49,Usability,clear,clear,49,@vdauwera Can you explain the situation? I'm not clear what the exact feature request is. It won't make it into this release but we can see about next (Cromwell 28).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301213292
https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301274387:159,Usability,learn,learn,159,"A suggestion I have would be to get people using expressions for memory and; disk size. I believe we already have a size expression for files, don't; we? Then learn what people are doing commonly and make that easy!. Magic is hard to do well -- discovering some common patterns and making; that is is... easier. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Sat, May 13, 2017 at 1:34 PM, Jeff Gentry <notifications@github.com>; wrote:. > It sounded like they wanted something more automagical but yeah; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301262719>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4gw1uBQmH-POG0YZKp4XLsuf8p_V9ks5r5em6gaJpZM4NZ55B>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2267#issuecomment-301274387
https://github.com/broadinstitute/cromwell/issues/2268#issuecomment-301481174:103,Performance,cache,cache,103,"This is from the point of view of the need on the FireCloud side:; In my opinion, Cromwell should call cache based upon it's best effort to know everything that is different. Clearly you could write a WDL that does the same thing in the end with different inputs (e.g. more memory is given than before), but I'd rather err on the side of no false positives than trying to make sure Cromwell is too clever. This really feels more like sugar in workbench where we could make some best guesses based upon what our users do or simply organize things in a way that it's less confusing/less effort for our users.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2268#issuecomment-301481174
https://github.com/broadinstitute/cromwell/issues/2268#issuecomment-301481174:523,Usability,simpl,simply,523,"This is from the point of view of the need on the FireCloud side:; In my opinion, Cromwell should call cache based upon it's best effort to know everything that is different. Clearly you could write a WDL that does the same thing in the end with different inputs (e.g. more memory is given than before), but I'd rather err on the side of no false positives than trying to make sure Cromwell is too clever. This really feels more like sugar in workbench where we could make some best guesses based upon what our users do or simply organize things in a way that it's less confusing/less effort for our users.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2268#issuecomment-301481174
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317858944:97,Security,access,accessory,97,"FYI this was a key item in feedback from our WDL sessions in the UK workshops; having to specify accessory files is a big source of annoyance. Not that it's any surprise, but we're definitely getting confirmation from real users.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317858944
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317858944:27,Usability,feedback,feedback,27,"FYI this was a key item in feedback from our WDL sessions in the UK workshops; having to specify accessory files is a big source of annoyance. Not that it's any surprise, but we're definitely getting confirmation from real users.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317858944
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317859657:45,Usability,learn,learned,45,We should certainly heed the lesson that CWL learned to provide both the concepts of directory and secondary files. They wound up implementing the former because people were also trying to do that and shoehorning it into the latter.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317859657
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317861526:388,Usability,learn,learned,388,"+100 on support for secondary files! BAM + Index, VCF + Index would be; super helpful!. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Tue, Jul 25, 2017 at 4:18 PM, Jeff Gentry <notifications@github.com>; wrote:. > We should certainly heed the lesson that CWL learned to provide both the; > concepts of directory and secondary files. They wound up implementing the; > former because people were also trying to do that and shoehorning it into; > the latter.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317859657>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g1ghUrXGroGI7qjHMH_N5z75osBjks5sRk2sgaJpZM4NZ6CY>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317861526
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-463843884:135,Usability,simpl,simpler,135,"Is there any progress on secondary files, I see we have structs which I could probably use but I'm looking for a concept that makes it simpler to pick up index files rather than writing more globs and more mappings. We got directory support in WDL (https://github.com/openwdl/wdl/pull/241) and in Cromwell (https://github.com/broadinstitute/cromwell/pull/3980). . I understand that the language and the engine are different, but Cromwell has some concept of these secondary files as the CWL implementation supports it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-463843884
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464284636:216,Usability,clear,clear,216,"@illusional While I cannot speak on behalf of the cromwell team on what they are implementing, I can say that there has been no discussions around secondary files for WDL. My inclination is that we will try to steer clear of it within WDL. However I encourage you to create an issue or make a PR in the WDL repo suggesting this change and we can allow the community to determine wtheher or not it should be something supported.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464284636
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564:159,Modifiability,portab,portable,159,I would agree w/ @patmagee that this is a matter for the OpenWDL group. Any Cromwell-level constructs to get at the underlying functionality would require non-portable WDLs to be written. I'll tag @cjllanwarne in case he has any clever ideas on how to express the concept in portable WDL in a less sucky way. I disagree with @patmagee that WDL should steer clear of the concept - IMO not doing this in the first place was one of the larger mistakes we made in the early days of WDL. Perhaps something with `Object`. We're seeing something similar play out in GA4GH land w/ DRS ... the concept of a file bundle seems inescapable and it's not quite the same thing as `Directory`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564:275,Modifiability,portab,portable,275,I would agree w/ @patmagee that this is a matter for the OpenWDL group. Any Cromwell-level constructs to get at the underlying functionality would require non-portable WDLs to be written. I'll tag @cjllanwarne in case he has any clever ideas on how to express the concept in portable WDL in a less sucky way. I disagree with @patmagee that WDL should steer clear of the concept - IMO not doing this in the first place was one of the larger mistakes we made in the early days of WDL. Perhaps something with `Object`. We're seeing something similar play out in GA4GH land w/ DRS ... the concept of a file bundle seems inescapable and it's not quite the same thing as `Directory`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564:357,Usability,clear,clear,357,I would agree w/ @patmagee that this is a matter for the OpenWDL group. Any Cromwell-level constructs to get at the underlying functionality would require non-portable WDLs to be written. I'll tag @cjllanwarne in case he has any clever ideas on how to express the concept in portable WDL in a less sucky way. I disagree with @patmagee that WDL should steer clear of the concept - IMO not doing this in the first place was one of the larger mistakes we made in the early days of WDL. Perhaps something with `Object`. We're seeing something similar play out in GA4GH land w/ DRS ... the concept of a file bundle seems inescapable and it's not quite the same thing as `Directory`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464373736:410,Modifiability,portab,portable,410,"Yeah from an end user POV it is still a pain not to have a file bundle; concept, and it is something I wish we had at the WDL level. On Sat, Feb 16, 2019 at 9:32 AM Jeff Gentry <notifications@github.com>; wrote:. > I would agree w/ @patmagee <https://github.com/patmagee> that this is a; > matter for the OpenWDL group. Any Cromwell-level constructs to get at the; > underlying functionality would require non-portable WDLs to be written.; > I'll tag @cjllanwarne <https://github.com/cjllanwarne> in case he has any; > clever ideas on how to express the concept in portable WDL in a less sucky; > way.; >; > I disagree with @patmagee <https://github.com/patmagee> that WDL should; > steer clear of the concept - IMO not doing this in the first place was one; > of the larger mistakes we made in the early days of WDL. Perhaps something; > with Object. We're seeing something similar play out in GA4GH land w/ DRS; > ... the concept of a file bundle seems inescapable and it's not quite the; > same thing as Directory; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnwEyiT-eFgW3NMiY39SRU8MvDG3L6Gks5vOBZpgaJpZM4NZ6CY>; > .; >; -- ; Geraldine A. Van der Auwera, Ph.D.; Associate Director of Outreach and Communications; Data Sciences Platform; Broad Institute",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464373736
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464373736:565,Modifiability,portab,portable,565,"Yeah from an end user POV it is still a pain not to have a file bundle; concept, and it is something I wish we had at the WDL level. On Sat, Feb 16, 2019 at 9:32 AM Jeff Gentry <notifications@github.com>; wrote:. > I would agree w/ @patmagee <https://github.com/patmagee> that this is a; > matter for the OpenWDL group. Any Cromwell-level constructs to get at the; > underlying functionality would require non-portable WDLs to be written.; > I'll tag @cjllanwarne <https://github.com/cjllanwarne> in case he has any; > clever ideas on how to express the concept in portable WDL in a less sucky; > way.; >; > I disagree with @patmagee <https://github.com/patmagee> that WDL should; > steer clear of the concept - IMO not doing this in the first place was one; > of the larger mistakes we made in the early days of WDL. Perhaps something; > with Object. We're seeing something similar play out in GA4GH land w/ DRS; > ... the concept of a file bundle seems inescapable and it's not quite the; > same thing as Directory; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnwEyiT-eFgW3NMiY39SRU8MvDG3L6Gks5vOBZpgaJpZM4NZ6CY>; > .; >; -- ; Geraldine A. Van der Auwera, Ph.D.; Associate Director of Outreach and Communications; Data Sciences Platform; Broad Institute",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464373736
https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464373736:689,Usability,clear,clear,689,"Yeah from an end user POV it is still a pain not to have a file bundle; concept, and it is something I wish we had at the WDL level. On Sat, Feb 16, 2019 at 9:32 AM Jeff Gentry <notifications@github.com>; wrote:. > I would agree w/ @patmagee <https://github.com/patmagee> that this is a; > matter for the OpenWDL group. Any Cromwell-level constructs to get at the; > underlying functionality would require non-portable WDLs to be written.; > I'll tag @cjllanwarne <https://github.com/cjllanwarne> in case he has any; > clever ideas on how to express the concept in portable WDL in a less sucky; > way.; >; > I disagree with @patmagee <https://github.com/patmagee> that WDL should; > steer clear of the concept - IMO not doing this in the first place was one; > of the larger mistakes we made in the early days of WDL. Perhaps something; > with Object. We're seeing something similar play out in GA4GH land w/ DRS; > ... the concept of a file bundle seems inescapable and it's not quite the; > same thing as Directory; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464351564>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnwEyiT-eFgW3NMiY39SRU8MvDG3L6Gks5vOBZpgaJpZM4NZ6CY>; > .; >; -- ; Geraldine A. Van der Auwera, Ph.D.; Associate Director of Outreach and Communications; Data Sciences Platform; Broad Institute",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-464373736
https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330323656:537,Energy Efficiency,efficient,efficiently,537,"@geoffjentry if the use case is really intended for validation criteria on objects which the user sets, I feel the same as you, that this is an abstraction that should not be handled withing wdl/cromwell. . While I understand the use case (we also have toyed around with the idea of this as a feature request) it adds unnecessary boundaries to object types that should be handled at the level of execution and not job submition. . I think what might be of use in these instances, for users (like myself) is using the parameter meta more efficiently to define in writing what constitutes valid entries. . Going back to the idea of objects as typed key Value pairs, I still think this is a valid idea, that has real use cases and purposes. In many cases data must be paired with other corresponding datasets and values. In a scatter operation having these types of structured objects would greatly simplify how we can group data together",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330323656
https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330323656:52,Security,validat,validation,52,"@geoffjentry if the use case is really intended for validation criteria on objects which the user sets, I feel the same as you, that this is an abstraction that should not be handled withing wdl/cromwell. . While I understand the use case (we also have toyed around with the idea of this as a feature request) it adds unnecessary boundaries to object types that should be handled at the level of execution and not job submition. . I think what might be of use in these instances, for users (like myself) is using the parameter meta more efficiently to define in writing what constitutes valid entries. . Going back to the idea of objects as typed key Value pairs, I still think this is a valid idea, that has real use cases and purposes. In many cases data must be paired with other corresponding datasets and values. In a scatter operation having these types of structured objects would greatly simplify how we can group data together",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330323656
https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330323656:896,Usability,simpl,simplify,896,"@geoffjentry if the use case is really intended for validation criteria on objects which the user sets, I feel the same as you, that this is an abstraction that should not be handled withing wdl/cromwell. . While I understand the use case (we also have toyed around with the idea of this as a feature request) it adds unnecessary boundaries to object types that should be handled at the level of execution and not job submition. . I think what might be of use in these instances, for users (like myself) is using the parameter meta more efficiently to define in writing what constitutes valid entries. . Going back to the idea of objects as typed key Value pairs, I still think this is a valid idea, that has real use cases and purposes. In many cases data must be paired with other corresponding datasets and values. In a scatter operation having these types of structured objects would greatly simplify how we can group data together",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330323656
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:1218,Availability,robust,robust,1218,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:873,Deployability,pipeline,pipeline,873,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:1492,Performance,cache,cache,1492,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:1619,Performance,cache,cache-miss,1619,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:1609,Safety,avoid,avoid,1609,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:1755,Safety,avoid,avoidance,1755,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:593,Security,access,access,593,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:1728,Security,hash,hashed,1728,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901:779,Usability,resume,resume,779,"@geoffjentry asked me to clarify, so here I am!. Currently, PAPI doesn't understand FOFN... so they are really just a File that contains strings. Often they are created by taking the file output of a scatter call as an array and writing it to an array like. ```; Array[File] vcfs = PreviousTask.output ...; File fofn = write_lines(vcfs); ```. Then that FOFN is used as the parameter to the task, and used by the tool in the command directly. The only thing that gets localized by PAPI is the FOFN itself. Keep in mind right now that the only scenario where this works is where your docker has access to the file, which on Google means when you're running in service account mode, but hopefully we can overcome that in the future. Just for context, my use case here is more like 'resume' than call caching. I don't expect to find results from some previous/other run of the pipeline. It's really that something broke, I tweaked the WDL, and now want to basically pick up where I left off. That's the specific problem I have (and any methods developer will have with a FOFN step). There are two ways I can think of going about this:. 1. Fix call caching to handle FOFNs specifically. This is tricky I think, but is most robust. In this case, I want Cromwell to understand a File of File references as a specific type but just for call caching purposes. 2. Change call caching to re-use files rather than copying, thus the path of the file doesn't changes, the FOFN doesn't change, and the call cache hits. This is how I ended up working around this by splitting the WDL into pieces where I supply the inputs to avoid the cache-miss step. I believe we have this option in the SFS?. In your proposal @cjllanwarne a FileRef would be hashed like a file for job avoidance, but treated like a string for all other purposes (e.g. passing to PAPI, etc)? I think that could work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305977901
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305979013:1197,Security,hash,hashes,1197,"On your last question, yeah that's basically it (although there was a request by Lee to refer to it as a `Path` and there was even some discussion that perhaps it should be cloud URL only so e.g. `CloudPath`). Despite the nomenclature the idea would be a type which would be treated as a String except that Cromwell would understand that it represents a file for things like call caching and handle them appropriately. Unlike a `File` the underlying path is never localized, it's always maintained as-is. Coercion between `File` and this new type would be seamless. I'll use `FileRef` for examples w/o necessarily endorsing that term. In a much simpler example where you have a single `FileRef` it'd be treated just like a `String` when it came to a command block, e.g. task foo {; FileRef bar; File baz. command {; grep ${bar} ${baz}; }; }. This would be grepping the delocalized path referenced by `bar` in the contents of the localized file `baz`. For the `Array[FileRef]`/`writelines()` examples, the belief was that if the writelines was in the command block (or the declaration? crap, now I forget which) that call caching would work as desired as the individual `FileRef`s would have their hashes checked *prior* to the FOFN generation. Moving away from your specific situation, I alluded to `CloudPath` above. The reason this came up in a separate context was e.g. GATK4's NIO capability where one doesn't want to be localizing files but does want call caching to be in effect. So there was a request for this concept of a file like thing which stays where it originally is. Some tricky edge cases start coming up when you're talking about actual local files for this and all the examples people came up with were people using cloud based storage, thus .....",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305979013
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305979013:645,Usability,simpl,simpler,645,"On your last question, yeah that's basically it (although there was a request by Lee to refer to it as a `Path` and there was even some discussion that perhaps it should be cloud URL only so e.g. `CloudPath`). Despite the nomenclature the idea would be a type which would be treated as a String except that Cromwell would understand that it represents a file for things like call caching and handle them appropriately. Unlike a `File` the underlying path is never localized, it's always maintained as-is. Coercion between `File` and this new type would be seamless. I'll use `FileRef` for examples w/o necessarily endorsing that term. In a much simpler example where you have a single `FileRef` it'd be treated just like a `String` when it came to a command block, e.g. task foo {; FileRef bar; File baz. command {; grep ${bar} ${baz}; }; }. This would be grepping the delocalized path referenced by `bar` in the contents of the localized file `baz`. For the `Array[FileRef]`/`writelines()` examples, the belief was that if the writelines was in the command block (or the declaration? crap, now I forget which) that call caching would work as desired as the individual `FileRef`s would have their hashes checked *prior* to the FOFN generation. Moving away from your specific situation, I alluded to `CloudPath` above. The reason this came up in a separate context was e.g. GATK4's NIO capability where one doesn't want to be localizing files but does want call caching to be in effect. So there was a request for this concept of a file like thing which stays where it originally is. Some tricky edge cases start coming up when you're talking about actual local files for this and all the examples people came up with were people using cloud based storage, thus .....",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-305979013
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516:542,Security,hash,hash,542,"The ""resume"" part in @kcibul explanation made me think that we could actually separate the ""resume"" case from the ""I use call caching to simulate resume"" case, although it's probably not the solution for this and I'm not even sure it's a good idea at all.; We could imagine a resume endpoint that takes a workflowId and a WDL, and same way call caching works now, re-uses succeeded task outputs (except this time no need to copy them because we're still running the same workflow technically) from this workflow previous run as long as their hash matches the one of the new WDL, and when they don't or/and the job was failed, run it again. It'd be very similar to ""call caching without copying files"" in the end, just conceptually a bit different as it would be the same workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516:5,Usability,resume,resume,5,"The ""resume"" part in @kcibul explanation made me think that we could actually separate the ""resume"" case from the ""I use call caching to simulate resume"" case, although it's probably not the solution for this and I'm not even sure it's a good idea at all.; We could imagine a resume endpoint that takes a workflowId and a WDL, and same way call caching works now, re-uses succeeded task outputs (except this time no need to copy them because we're still running the same workflow technically) from this workflow previous run as long as their hash matches the one of the new WDL, and when they don't or/and the job was failed, run it again. It'd be very similar to ""call caching without copying files"" in the end, just conceptually a bit different as it would be the same workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516:92,Usability,resume,resume,92,"The ""resume"" part in @kcibul explanation made me think that we could actually separate the ""resume"" case from the ""I use call caching to simulate resume"" case, although it's probably not the solution for this and I'm not even sure it's a good idea at all.; We could imagine a resume endpoint that takes a workflowId and a WDL, and same way call caching works now, re-uses succeeded task outputs (except this time no need to copy them because we're still running the same workflow technically) from this workflow previous run as long as their hash matches the one of the new WDL, and when they don't or/and the job was failed, run it again. It'd be very similar to ""call caching without copying files"" in the end, just conceptually a bit different as it would be the same workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516:146,Usability,resume,resume,146,"The ""resume"" part in @kcibul explanation made me think that we could actually separate the ""resume"" case from the ""I use call caching to simulate resume"" case, although it's probably not the solution for this and I'm not even sure it's a good idea at all.; We could imagine a resume endpoint that takes a workflowId and a WDL, and same way call caching works now, re-uses succeeded task outputs (except this time no need to copy them because we're still running the same workflow technically) from this workflow previous run as long as their hash matches the one of the new WDL, and when they don't or/and the job was failed, run it again. It'd be very similar to ""call caching without copying files"" in the end, just conceptually a bit different as it would be the same workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516
https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516:276,Usability,resume,resume,276,"The ""resume"" part in @kcibul explanation made me think that we could actually separate the ""resume"" case from the ""I use call caching to simulate resume"" case, although it's probably not the solution for this and I'm not even sure it's a good idea at all.; We could imagine a resume endpoint that takes a workflowId and a WDL, and same way call caching works now, re-uses succeeded task outputs (except this time no need to copy them because we're still running the same workflow technically) from this workflow previous run as long as their hash matches the one of the new WDL, and when they don't or/and the job was failed, run it again. It'd be very similar to ""call caching without copying files"" in the end, just conceptually a bit different as it would be the same workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065516
https://github.com/broadinstitute/cromwell/issues/2313#issuecomment-305589042:408,Availability,down,down,408,"What I was going to suggest was looking at [WES](https://github.com/ga4gh/workflow-execution-schemas). . - Conceptually it's what we need to do anyways (""Hey, here's a workflow and it's of type X""); - We've signed on to support WES at a future date anyways. It's possible that it doesn't map particularly cleanly and/or doesn't make sense for some other reason but IMO it'd be good to at least poke at going down this path first. There's protobuf & swagger in that repo I linked. If there are complaints/feedback about the API and not the Cromwell implications of the API, let me know and we can surface them w/ the appropriate folks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2313#issuecomment-305589042
https://github.com/broadinstitute/cromwell/issues/2313#issuecomment-305589042:504,Usability,feedback,feedback,504,"What I was going to suggest was looking at [WES](https://github.com/ga4gh/workflow-execution-schemas). . - Conceptually it's what we need to do anyways (""Hey, here's a workflow and it's of type X""); - We've signed on to support WES at a future date anyways. It's possible that it doesn't map particularly cleanly and/or doesn't make sense for some other reason but IMO it'd be good to at least poke at going down this path first. There's protobuf & swagger in that repo I linked. If there are complaints/feedback about the API and not the Cromwell implications of the API, let me know and we can surface them w/ the appropriate folks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2313#issuecomment-305589042
https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-332204672:44,Usability,guid,guide,44,"Per @davidbernick's comments on Slack, this guide should include how Cromwell administrators should set up Authorization, Authentication, Encryption, and Persistent Databases. This guide will be created during the [Doc-A-Thon](https://docs.google.com/document/d/1M5u-ESSpt_eM0ORsvIu2AoOtvYXImPmAKoyvXFo4p9s/edit) in November.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-332204672
https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-332204672:181,Usability,guid,guide,181,"Per @davidbernick's comments on Slack, this guide should include how Cromwell administrators should set up Authorization, Authentication, Encryption, and Persistent Databases. This guide will be created during the [Doc-A-Thon](https://docs.google.com/document/d/1M5u-ESSpt_eM0ORsvIu2AoOtvYXImPmAKoyvXFo4p9s/edit) in November.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-332204672
https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-345271005:25,Usability,guid,guide,25,The Cromwell Developer's guide is here!; - [Quick Start](http://cromwell.readthedocs.io/en/develop/tutorials/FiveMinuteIntro/); - [How to Configure](http://cromwell.readthedocs.io/en/develop/tutorials/ConfigurationFiles/); - [Google Pipelines API quick start](http://cromwell.readthedocs.io/en/develop/tutorials/PipelinesApi101/); - [Timing Diagrams](http://cromwell.readthedocs.io/en/develop/tutorials/TimingDiagrams/); - [Setup a persistent server](http://cromwell.readthedocs.io/en/develop/tutorials/PersistentServer/); - [Setup on HPC cluster](http://cromwell.readthedocs.io/en/develop/tutorials/HPCIntro/); - [Run in Server Mode](http://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/). Checkout the full [Cromwell Docs!](http://cromwell.rtfd.io),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2327#issuecomment-345271005
https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021:508,Availability,echo,echo,508,"This is something that would be quite useful. There's a lot of simple things that are just easier (and cheaper!) to run or debug locally, but doing so right now is a bit of a mess. Several times I have had Cromwell tasks [get sigkilled on local runs](https://github.com/aofarrel/analysis_pipeline_WDL/issues/5), or even choke up Docker to the point of being unable to enter containers both within (containerized tasks get stuck on WaitingForReturn code but will never anything within the task, even a simple echo) and outside of Cromwell (docker run, etc) without a full restart of the Docker system. I am not certain if limiting resources would solve my specfic probems, but at the very least it would be closer to the cloud experience without the associated cost. It also could be used to test out rough approximations of memory and disk requirements for things would normally be used on the cloud, again saving money.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021
https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021:791,Testability,test,test,791,"This is something that would be quite useful. There's a lot of simple things that are just easier (and cheaper!) to run or debug locally, but doing so right now is a bit of a mess. Several times I have had Cromwell tasks [get sigkilled on local runs](https://github.com/aofarrel/analysis_pipeline_WDL/issues/5), or even choke up Docker to the point of being unable to enter containers both within (containerized tasks get stuck on WaitingForReturn code but will never anything within the task, even a simple echo) and outside of Cromwell (docker run, etc) without a full restart of the Docker system. I am not certain if limiting resources would solve my specfic probems, but at the very least it would be closer to the cloud experience without the associated cost. It also could be used to test out rough approximations of memory and disk requirements for things would normally be used on the cloud, again saving money.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021
https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021:63,Usability,simpl,simple,63,"This is something that would be quite useful. There's a lot of simple things that are just easier (and cheaper!) to run or debug locally, but doing so right now is a bit of a mess. Several times I have had Cromwell tasks [get sigkilled on local runs](https://github.com/aofarrel/analysis_pipeline_WDL/issues/5), or even choke up Docker to the point of being unable to enter containers both within (containerized tasks get stuck on WaitingForReturn code but will never anything within the task, even a simple echo) and outside of Cromwell (docker run, etc) without a full restart of the Docker system. I am not certain if limiting resources would solve my specfic probems, but at the very least it would be closer to the cloud experience without the associated cost. It also could be used to test out rough approximations of memory and disk requirements for things would normally be used on the cloud, again saving money.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021
https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021:501,Usability,simpl,simple,501,"This is something that would be quite useful. There's a lot of simple things that are just easier (and cheaper!) to run or debug locally, but doing so right now is a bit of a mess. Several times I have had Cromwell tasks [get sigkilled on local runs](https://github.com/aofarrel/analysis_pipeline_WDL/issues/5), or even choke up Docker to the point of being unable to enter containers both within (containerized tasks get stuck on WaitingForReturn code but will never anything within the task, even a simple echo) and outside of Cromwell (docker run, etc) without a full restart of the Docker system. I am not certain if limiting resources would solve my specfic probems, but at the very least it would be closer to the cloud experience without the associated cost. It also could be used to test out rough approximations of memory and disk requirements for things would normally be used on the cloud, again saving money.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311446204:9,Usability,clear,clear,9,So to be clear: moving docs from the forum back to here is a step in the right direction in any case,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311446204
https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311461964:85,Usability,feedback,feedback,85,"@vdauwera I'd be happy to go over the changes in person, and I'd also appreciate any feedback you have on the changes to the `README.md` here. 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311461964
https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336165593:3,Usability,intuit,intuition,3,"My intuition is what @Horneth said, that this is an artifact of something we've seen before. It is possible to sorta-fix-it-fix-it (e.g. write out a ""i'm totally done"" file at the very end and not read anything until then) but that has its own problems, including some which lead to the ""sorta-"" prefix there. It's worth noting that this isn't a problem unique to Cromwell, it pops up fairly regularly with these things.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2378#issuecomment-336165593
https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-333241563:83,Security,secur,security,83,Because the alternative is to use personal github tokens which is not great from a security and re-usability perspective,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-333241563
https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-333241563:99,Usability,usab,usability,99,Because the alternative is to use personal github tokens which is not great from a security and re-usability perspective,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2402#issuecomment-333241563
https://github.com/broadinstitute/cromwell/issues/2423#issuecomment-345259634:428,Usability,guid,guide,428,"Related to switching to circe, and printing JSON in general, pretty-vs-compact printing has been up for debate. Here is an example of how to pretty print in circe: https://github.com/broadinstitute/clio/commit/4b78f9a5d2f5a0aea389dc404859d5f91e07321c#diff-4543387a548c3521400bdaa006943757. The above always pretty-prints. It may be the case that one wants the pretty-vs-compact printing to be an [option](https://www.elastic.co/guide/en/elasticsearch/reference/current/common-options.html). Because this particular solution uses implicits, one _could_ turn this. ```scala; implicit val akkaHttpJsonPrinter: Printer; ```. into. ```scala; implicit def akkaHttpJsonPrinter()(implicit cromwellPrintType: CromwellPrintType): Printer; ```. and then implicitly provide an enum set by a query string parameter.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2423#issuecomment-345259634
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:227,Availability,error,error,227,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:958,Availability,error,error,958,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:984,Availability,error,error,984,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:1071,Availability,down,down,1071,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:1110,Availability,error,error,1110,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:922,Integrability,message,message,922,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:964,Integrability,message,message,964,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094
https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:763,Usability,resume,resume,763,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094
https://github.com/broadinstitute/cromwell/pull/2470#issuecomment-316727394:59,Usability,clear,clear,59,"@cjllanwarne Totally agree. One of the things which became clear to me toot sweet was that was that we have no docs around the service registry in general, e.g. how to make a new service. All that sorta stuff is on my radar atm.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2470#issuecomment-316727394
https://github.com/broadinstitute/cromwell/pull/2471#issuecomment-316752639:371,Security,hash,hashDifferential,371,"I haven't used this endpoint and I'm not entirely sure what the use case is. However, this new version seems less usable to me. I can no longer look up the differential by a key I'm interested in - I have to iterate over all elements looking for the one I want. This seems worse -- why use a map?. Also I don't really know why it's in an array. Why isn't it just:. ```; ""hashDifferential"": {; ""output expression:String hi”: ; {; ""callA"": ""935C6E7EB2068B83C40B788575747EFB”, ; ""callB"": “0183144CF6617D5341681C6B2F756046""; },; ""output thing:blah blah"": { ... },; ...; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2471#issuecomment-316752639
https://github.com/broadinstitute/cromwell/pull/2471#issuecomment-316752639:114,Usability,usab,usable,114,"I haven't used this endpoint and I'm not entirely sure what the use case is. However, this new version seems less usable to me. I can no longer look up the differential by a key I'm interested in - I have to iterate over all elements looking for the one I want. This seems worse -- why use a map?. Also I don't really know why it's in an array. Why isn't it just:. ```; ""hashDifferential"": {; ""output expression:String hi”: ; {; ""callA"": ""935C6E7EB2068B83C40B788575747EFB”, ; ""callB"": “0183144CF6617D5341681C6B2F756046""; },; ""output thing:blah blah"": { ... },; ...; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2471#issuecomment-316752639
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002:70,Availability,error,error,70,"Thanks @pshapiro4broad . Yes, I agree that this should be fixed. That error is being caught by our ""oops we have no idea what happened"" when clearly there's some repeatable code path causing an error here where we *should* know what's going on. It'd be good to fix this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002:194,Availability,error,error,194,"Thanks @pshapiro4broad . Yes, I agree that this should be fixed. That error is being caught by our ""oops we have no idea what happened"" when clearly there's some repeatable code path causing an error here where we *should* know what's going on. It'd be good to fix this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002:141,Usability,clear,clearly,141,"Thanks @pshapiro4broad . Yes, I agree that this should be fixed. That error is being caught by our ""oops we have no idea what happened"" when clearly there's some repeatable code path causing an error here where we *should* know what's going on. It'd be good to fix this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426414:359,Availability,error,error,359,"+1. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences Platform; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Jul 27, 2017 at 1:09 PM, Jeff Gentry <notifications@github.com>; wrote:. > Thanks @pshapiro4broad <https://github.com/pshapiro4broad>; >; > Yes, I agree that this should be fixed. That error is being caught by our; > ""oops we have no idea what happened"" when clearly there's some repeatable; > code path causing an error here where we *should* know what's going on.; > It'd be good to fix this.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g_YmtNolUq9osOY-f_PNa3wEoPHrks5sSMQzgaJpZM4OlcEp>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426414
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426414:489,Availability,error,error,489,"+1. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences Platform; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Jul 27, 2017 at 1:09 PM, Jeff Gentry <notifications@github.com>; wrote:. > Thanks @pshapiro4broad <https://github.com/pshapiro4broad>; >; > Yes, I agree that this should be fixed. That error is being caught by our; > ""oops we have no idea what happened"" when clearly there's some repeatable; > code path causing an error here where we *should* know what's going on.; > It'd be good to fix this.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g_YmtNolUq9osOY-f_PNa3wEoPHrks5sSMQzgaJpZM4OlcEp>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426414
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426414:433,Usability,clear,clearly,433,"+1. -------------------------------; Kristian Cibulskis; Engineering Director, Data Sciences Platform; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Thu, Jul 27, 2017 at 1:09 PM, Jeff Gentry <notifications@github.com>; wrote:. > Thanks @pshapiro4broad <https://github.com/pshapiro4broad>; >; > Yes, I agree that this should be fixed. That error is being caught by our; > ""oops we have no idea what happened"" when clearly there's some repeatable; > code path causing an error here where we *should* know what's going on.; > It'd be good to fix this.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABW4g_YmtNolUq9osOY-f_PNa3wEoPHrks5sSMQzgaJpZM4OlcEp>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426414
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:65,Availability,error,error-message-the-job-was-aborted-from-outside-cromwell,65,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:301,Availability,failure,failures,301,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:332,Availability,error,error,332,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:937,Availability,error,error,937,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:1214,Availability,failure,failure,1214,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:621,Energy Efficiency,reduce,reduce,621,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:71,Integrability,message,message-the-job-was-aborted-from-outside-cromwell,71,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:130,Integrability,message,message,130,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:338,Integrability,message,message,338,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:459,Integrability,message,message,459,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:1075,Integrability,message,message,1075,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:91,Safety,abort,aborted-from-outside-cromwell,91,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:147,Safety,abort,aborted,147,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:411,Safety,timeout,timeout,411,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:1108,Usability,simpl,simply,1108,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266
https://github.com/broadinstitute/cromwell/pull/2566#issuecomment-324120498:180,Safety,abort,abort,180,"@cjllanwarne there was one `ignore`d test about default runtime attributes that pretty clearly seemed to be covered by newer tests, so I've deleted that as well. There's one other abort test we should definitely continue to feel bad about, and another test for a ""taskless workflow"" for which I couldn't readily find an equivalent.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2566#issuecomment-324120498
https://github.com/broadinstitute/cromwell/pull/2566#issuecomment-324120498:37,Testability,test,test,37,"@cjllanwarne there was one `ignore`d test about default runtime attributes that pretty clearly seemed to be covered by newer tests, so I've deleted that as well. There's one other abort test we should definitely continue to feel bad about, and another test for a ""taskless workflow"" for which I couldn't readily find an equivalent.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2566#issuecomment-324120498
https://github.com/broadinstitute/cromwell/pull/2566#issuecomment-324120498:125,Testability,test,tests,125,"@cjllanwarne there was one `ignore`d test about default runtime attributes that pretty clearly seemed to be covered by newer tests, so I've deleted that as well. There's one other abort test we should definitely continue to feel bad about, and another test for a ""taskless workflow"" for which I couldn't readily find an equivalent.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2566#issuecomment-324120498
https://github.com/broadinstitute/cromwell/pull/2566#issuecomment-324120498:186,Testability,test,test,186,"@cjllanwarne there was one `ignore`d test about default runtime attributes that pretty clearly seemed to be covered by newer tests, so I've deleted that as well. There's one other abort test we should definitely continue to feel bad about, and another test for a ""taskless workflow"" for which I couldn't readily find an equivalent.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2566#issuecomment-324120498
https://github.com/broadinstitute/cromwell/pull/2566#issuecomment-324120498:252,Testability,test,test,252,"@cjllanwarne there was one `ignore`d test about default runtime attributes that pretty clearly seemed to be covered by newer tests, so I've deleted that as well. There's one other abort test we should definitely continue to feel bad about, and another test for a ""taskless workflow"" for which I couldn't readily find an equivalent.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2566#issuecomment-324120498
https://github.com/broadinstitute/cromwell/pull/2566#issuecomment-324120498:87,Usability,clear,clearly,87,"@cjllanwarne there was one `ignore`d test about default runtime attributes that pretty clearly seemed to be covered by newer tests, so I've deleted that as well. There's one other abort test we should definitely continue to feel bad about, and another test for a ""taskless workflow"" for which I couldn't readily find an equivalent.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2566#issuecomment-324120498
https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324791741:19,Testability,test,tests,19,"Re:. > Beyond unit tests, how was this tested?. The DBMS CI tests are testing fresh MySQL databases for both engine and metadata. I manually started running a workflow on develop with MySql. Stopped Cromwell, switched to this branch, and restarted. Liquibase ran successfully, marking the change_logs as ""MARK_RAN"" vs. ""EXECUTED"". The jobs/workflow resumed and succeeded.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324791741
https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324791741:39,Testability,test,tested,39,"Re:. > Beyond unit tests, how was this tested?. The DBMS CI tests are testing fresh MySQL databases for both engine and metadata. I manually started running a workflow on develop with MySql. Stopped Cromwell, switched to this branch, and restarted. Liquibase ran successfully, marking the change_logs as ""MARK_RAN"" vs. ""EXECUTED"". The jobs/workflow resumed and succeeded.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324791741
https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324791741:60,Testability,test,tests,60,"Re:. > Beyond unit tests, how was this tested?. The DBMS CI tests are testing fresh MySQL databases for both engine and metadata. I manually started running a workflow on develop with MySql. Stopped Cromwell, switched to this branch, and restarted. Liquibase ran successfully, marking the change_logs as ""MARK_RAN"" vs. ""EXECUTED"". The jobs/workflow resumed and succeeded.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324791741
https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324791741:70,Testability,test,testing,70,"Re:. > Beyond unit tests, how was this tested?. The DBMS CI tests are testing fresh MySQL databases for both engine and metadata. I manually started running a workflow on develop with MySql. Stopped Cromwell, switched to this branch, and restarted. Liquibase ran successfully, marking the change_logs as ""MARK_RAN"" vs. ""EXECUTED"". The jobs/workflow resumed and succeeded.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324791741
https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324791741:349,Usability,resume,resumed,349,"Re:. > Beyond unit tests, how was this tested?. The DBMS CI tests are testing fresh MySQL databases for both engine and metadata. I manually started running a workflow on develop with MySql. Stopped Cromwell, switched to this branch, and restarted. Liquibase ran successfully, marking the change_logs as ""MARK_RAN"" vs. ""EXECUTED"". The jobs/workflow resumed and succeeded.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324791741
https://github.com/broadinstitute/cromwell/issues/2606#issuecomment-347902943:30,Integrability,wrap,wrapper,30,"Luckily, we now have a simple wrapper that can turn `WomValue`s into `WomExpression`s (which simply return the value given to them), so runtime attributes can be WomExpression, and the values given in the default runtime attributes can be easily converted into expressions",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2606#issuecomment-347902943
https://github.com/broadinstitute/cromwell/issues/2606#issuecomment-347902943:23,Usability,simpl,simple,23,"Luckily, we now have a simple wrapper that can turn `WomValue`s into `WomExpression`s (which simply return the value given to them), so runtime attributes can be WomExpression, and the values given in the default runtime attributes can be easily converted into expressions",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2606#issuecomment-347902943
https://github.com/broadinstitute/cromwell/issues/2606#issuecomment-347902943:93,Usability,simpl,simply,93,"Luckily, we now have a simple wrapper that can turn `WomValue`s into `WomExpression`s (which simply return the value given to them), so runtime attributes can be WomExpression, and the values given in the default runtime attributes can be easily converted into expressions",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2606#issuecomment-347902943
https://github.com/broadinstitute/cromwell/issues/2627#issuecomment-414084761:37,Usability,clear,clearly,37,Closing this as it's been a year and clearly no one is going to bother to do this,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2627#issuecomment-414084761
https://github.com/broadinstitute/cromwell/pull/2657#issuecomment-332631165:237,Availability,error,error-reference,237,# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/2657?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@370f3e3`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## develop #2657 +/- ##; ==========================================; Coverage ? 64.03% ; ==========================================; Files ? 381 ; Lines ? 8893 ; Branches ? 193 ; ==========================================; Hits ? 5695 ; Misses ? 3198 ; Partials ? 0; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/2657?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [...backend/standard/StandardAsyncExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/2657?src=pr&el=tree#diff-YmFja2VuZC9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL3N0YW5kYXJkL1N0YW5kYXJkQXN5bmNFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `67.44% <100%> (ø)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2657#issuecomment-332631165
https://github.com/broadinstitute/cromwell/pull/2657#issuecomment-332631165:185,Usability,learn,learn,185,# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/2657?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@370f3e3`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `100%`. ```diff; @@ Coverage Diff @@; ## develop #2657 +/- ##; ==========================================; Coverage ? 64.03% ; ==========================================; Files ? 381 ; Lines ? 8893 ; Branches ? 193 ; ==========================================; Hits ? 5695 ; Misses ? 3198 ; Partials ? 0; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/2657?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [...backend/standard/StandardAsyncExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/2657?src=pr&el=tree#diff-YmFja2VuZC9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL3N0YW5kYXJkL1N0YW5kYXJkQXN5bmNFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `67.44% <100%> (ø)` | |,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2657#issuecomment-332631165
https://github.com/broadinstitute/cromwell/pull/2663#issuecomment-332620801:237,Availability,error,error-reference,237,# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/2663?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@839ea1e`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## develop #2663 +/- ##; ==========================================; Coverage ? 64.32% ; ==========================================; Files ? 381 ; Lines ? 8892 ; Branches ? 195 ; ==========================================; Hits ? 5720 ; Misses ? 3172 ; Partials ? 0; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2663#issuecomment-332620801
https://github.com/broadinstitute/cromwell/pull/2663#issuecomment-332620801:185,Usability,learn,learn,185,# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/2663?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@839ea1e`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## develop #2663 +/- ##; ==========================================; Coverage ? 64.32% ; ==========================================; Files ? 381 ; Lines ? 8892 ; Branches ? 195 ; ==========================================; Hits ? 5720 ; Misses ? 3172 ; Partials ? 0; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2663#issuecomment-332620801
https://github.com/broadinstitute/cromwell/issues/2679#issuecomment-338283948:30,Usability,simpl,simpler,30,Good news is it should become simpler no ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2679#issuecomment-338283948
https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829:677,Availability,echo,echo,677,"There are multiple related issues in this ticket. The common thread is: a WDL author expects to be able to write compound WDL statements in a task `output` section. However, there are certain WDL statements that fail parsing when strung together, but _will_ work if the statement is broken into multiple variables. The original issue identifies problems in `output` using `glob()` or `Map[,]`. In terms of a fix, I'm guessing for the right dev this is a medium<sup>1</sup> sized task, but would probably be lower on the list of TODOs as there exists a workaround. This ""workaround"" works, where all three `output` variables are relatively simple:; ```wdl; task x {; command {; echo 0 > intFile.txt; echo hello > outFile.txt; }; runtime { docker: ""ubuntu"" }; output {; Int intermediateInt = read_int(""intFile.txt""); Array[File] intermediateOuts = glob(""outFile.txt""); File out = intermediateOuts[intermediateInt]; }; }. workflow glob_indexing { call x }; ```. Starting to compress the output block into two statements, where the latter is a compound expression, this still parses and runs:; ```wdl; output {; Int intermediateInt = read_int(""intFile.txt""); File out = glob(""outFile.txt"")[intermediateInt]; }; ```. Regarding the problems with `Map[,]` this _does_ work:; ```wdl; output {; Map[String, File] intermediateMap = {""a"": ""outFile.txt""}; File out = intermediateMap[""a""]; }; ```. HOWEVER, this doesn't work, currently failing with the error `Workflow input processing failed: <string:8:20 lbrace ""ew==""> (of class wdl4s.parser.WdlParser$Terminal)`:. ```wdl; output {; File out = {""a"": ""outFile.txt""}[""a""]; }; ```. And going back to globbing, the error with globs is _slightly_ better. This doesn't work, either:; ```wdl; output {; File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; }; ```. And fails with the ""prettier"" message at the moment:. ```; ERROR: Unexpected symbol (line 8, col 48) when parsing 'e'. Expected rsquare, got (. File out = glob(""outFile.txt"")[read_int(""intFile.txt"")];",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829
https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829:699,Availability,echo,echo,699,"There are multiple related issues in this ticket. The common thread is: a WDL author expects to be able to write compound WDL statements in a task `output` section. However, there are certain WDL statements that fail parsing when strung together, but _will_ work if the statement is broken into multiple variables. The original issue identifies problems in `output` using `glob()` or `Map[,]`. In terms of a fix, I'm guessing for the right dev this is a medium<sup>1</sup> sized task, but would probably be lower on the list of TODOs as there exists a workaround. This ""workaround"" works, where all three `output` variables are relatively simple:; ```wdl; task x {; command {; echo 0 > intFile.txt; echo hello > outFile.txt; }; runtime { docker: ""ubuntu"" }; output {; Int intermediateInt = read_int(""intFile.txt""); Array[File] intermediateOuts = glob(""outFile.txt""); File out = intermediateOuts[intermediateInt]; }; }. workflow glob_indexing { call x }; ```. Starting to compress the output block into two statements, where the latter is a compound expression, this still parses and runs:; ```wdl; output {; Int intermediateInt = read_int(""intFile.txt""); File out = glob(""outFile.txt"")[intermediateInt]; }; ```. Regarding the problems with `Map[,]` this _does_ work:; ```wdl; output {; Map[String, File] intermediateMap = {""a"": ""outFile.txt""}; File out = intermediateMap[""a""]; }; ```. HOWEVER, this doesn't work, currently failing with the error `Workflow input processing failed: <string:8:20 lbrace ""ew==""> (of class wdl4s.parser.WdlParser$Terminal)`:. ```wdl; output {; File out = {""a"": ""outFile.txt""}[""a""]; }; ```. And going back to globbing, the error with globs is _slightly_ better. This doesn't work, either:; ```wdl; output {; File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; }; ```. And fails with the ""prettier"" message at the moment:. ```; ERROR: Unexpected symbol (line 8, col 48) when parsing 'e'. Expected rsquare, got (. File out = glob(""outFile.txt"")[read_int(""intFile.txt"")];",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829
https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829:1440,Availability,error,error,1440,"medium<sup>1</sup> sized task, but would probably be lower on the list of TODOs as there exists a workaround. This ""workaround"" works, where all three `output` variables are relatively simple:; ```wdl; task x {; command {; echo 0 > intFile.txt; echo hello > outFile.txt; }; runtime { docker: ""ubuntu"" }; output {; Int intermediateInt = read_int(""intFile.txt""); Array[File] intermediateOuts = glob(""outFile.txt""); File out = intermediateOuts[intermediateInt]; }; }. workflow glob_indexing { call x }; ```. Starting to compress the output block into two statements, where the latter is a compound expression, this still parses and runs:; ```wdl; output {; Int intermediateInt = read_int(""intFile.txt""); File out = glob(""outFile.txt"")[intermediateInt]; }; ```. Regarding the problems with `Map[,]` this _does_ work:; ```wdl; output {; Map[String, File] intermediateMap = {""a"": ""outFile.txt""}; File out = intermediateMap[""a""]; }; ```. HOWEVER, this doesn't work, currently failing with the error `Workflow input processing failed: <string:8:20 lbrace ""ew==""> (of class wdl4s.parser.WdlParser$Terminal)`:. ```wdl; output {; File out = {""a"": ""outFile.txt""}[""a""]; }; ```. And going back to globbing, the error with globs is _slightly_ better. This doesn't work, either:; ```wdl; output {; File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; }; ```. And fails with the ""prettier"" message at the moment:. ```; ERROR: Unexpected symbol (line 8, col 48) when parsing 'e'. Expected rsquare, got (. File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; ^. $e = :identifier <=> :lparen $_gen18 :rparen -> FunctionCall( name=$0, params=$2 ); ; ```. ---. <sup>1</sup> The ""medium"" estimate is assuming this only needs to be fixed in the ~wdl4s~ cromwell-wdl project. If this is a problem lower down in the parser/grammar, then it might be harder for a developer to do. My note here is because Winstanley is also highlighting these ""bad"" examples as problematic with red-underlines, hinting that this may be a ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829
https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829:1651,Availability,error,error,1651," would probably be lower on the list of TODOs as there exists a workaround. This ""workaround"" works, where all three `output` variables are relatively simple:; ```wdl; task x {; command {; echo 0 > intFile.txt; echo hello > outFile.txt; }; runtime { docker: ""ubuntu"" }; output {; Int intermediateInt = read_int(""intFile.txt""); Array[File] intermediateOuts = glob(""outFile.txt""); File out = intermediateOuts[intermediateInt]; }; }. workflow glob_indexing { call x }; ```. Starting to compress the output block into two statements, where the latter is a compound expression, this still parses and runs:; ```wdl; output {; Int intermediateInt = read_int(""intFile.txt""); File out = glob(""outFile.txt"")[intermediateInt]; }; ```. Regarding the problems with `Map[,]` this _does_ work:; ```wdl; output {; Map[String, File] intermediateMap = {""a"": ""outFile.txt""}; File out = intermediateMap[""a""]; }; ```. HOWEVER, this doesn't work, currently failing with the error `Workflow input processing failed: <string:8:20 lbrace ""ew==""> (of class wdl4s.parser.WdlParser$Terminal)`:. ```wdl; output {; File out = {""a"": ""outFile.txt""}[""a""]; }; ```. And going back to globbing, the error with globs is _slightly_ better. This doesn't work, either:; ```wdl; output {; File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; }; ```. And fails with the ""prettier"" message at the moment:. ```; ERROR: Unexpected symbol (line 8, col 48) when parsing 'e'. Expected rsquare, got (. File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; ^. $e = :identifier <=> :lparen $_gen18 :rparen -> FunctionCall( name=$0, params=$2 ); ; ```. ---. <sup>1</sup> The ""medium"" estimate is assuming this only needs to be fixed in the ~wdl4s~ cromwell-wdl project. If this is a problem lower down in the parser/grammar, then it might be harder for a developer to do. My note here is because Winstanley is also highlighting these ""bad"" examples as problematic with red-underlines, hinting that this may be a lower level problem than I think.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829
https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829:2240,Availability,down,down,2240," would probably be lower on the list of TODOs as there exists a workaround. This ""workaround"" works, where all three `output` variables are relatively simple:; ```wdl; task x {; command {; echo 0 > intFile.txt; echo hello > outFile.txt; }; runtime { docker: ""ubuntu"" }; output {; Int intermediateInt = read_int(""intFile.txt""); Array[File] intermediateOuts = glob(""outFile.txt""); File out = intermediateOuts[intermediateInt]; }; }. workflow glob_indexing { call x }; ```. Starting to compress the output block into two statements, where the latter is a compound expression, this still parses and runs:; ```wdl; output {; Int intermediateInt = read_int(""intFile.txt""); File out = glob(""outFile.txt"")[intermediateInt]; }; ```. Regarding the problems with `Map[,]` this _does_ work:; ```wdl; output {; Map[String, File] intermediateMap = {""a"": ""outFile.txt""}; File out = intermediateMap[""a""]; }; ```. HOWEVER, this doesn't work, currently failing with the error `Workflow input processing failed: <string:8:20 lbrace ""ew==""> (of class wdl4s.parser.WdlParser$Terminal)`:. ```wdl; output {; File out = {""a"": ""outFile.txt""}[""a""]; }; ```. And going back to globbing, the error with globs is _slightly_ better. This doesn't work, either:; ```wdl; output {; File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; }; ```. And fails with the ""prettier"" message at the moment:. ```; ERROR: Unexpected symbol (line 8, col 48) when parsing 'e'. Expected rsquare, got (. File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; ^. $e = :identifier <=> :lparen $_gen18 :rparen -> FunctionCall( name=$0, params=$2 ); ; ```. ---. <sup>1</sup> The ""medium"" estimate is assuming this only needs to be fixed in the ~wdl4s~ cromwell-wdl project. If this is a problem lower down in the parser/grammar, then it might be harder for a developer to do. My note here is because Winstanley is also highlighting these ""bad"" examples as problematic with red-underlines, hinting that this may be a lower level problem than I think.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829
https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829:1831,Integrability,message,message,1831," would probably be lower on the list of TODOs as there exists a workaround. This ""workaround"" works, where all three `output` variables are relatively simple:; ```wdl; task x {; command {; echo 0 > intFile.txt; echo hello > outFile.txt; }; runtime { docker: ""ubuntu"" }; output {; Int intermediateInt = read_int(""intFile.txt""); Array[File] intermediateOuts = glob(""outFile.txt""); File out = intermediateOuts[intermediateInt]; }; }. workflow glob_indexing { call x }; ```. Starting to compress the output block into two statements, where the latter is a compound expression, this still parses and runs:; ```wdl; output {; Int intermediateInt = read_int(""intFile.txt""); File out = glob(""outFile.txt"")[intermediateInt]; }; ```. Regarding the problems with `Map[,]` this _does_ work:; ```wdl; output {; Map[String, File] intermediateMap = {""a"": ""outFile.txt""}; File out = intermediateMap[""a""]; }; ```. HOWEVER, this doesn't work, currently failing with the error `Workflow input processing failed: <string:8:20 lbrace ""ew==""> (of class wdl4s.parser.WdlParser$Terminal)`:. ```wdl; output {; File out = {""a"": ""outFile.txt""}[""a""]; }; ```. And going back to globbing, the error with globs is _slightly_ better. This doesn't work, either:; ```wdl; output {; File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; }; ```. And fails with the ""prettier"" message at the moment:. ```; ERROR: Unexpected symbol (line 8, col 48) when parsing 'e'. Expected rsquare, got (. File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; ^. $e = :identifier <=> :lparen $_gen18 :rparen -> FunctionCall( name=$0, params=$2 ); ; ```. ---. <sup>1</sup> The ""medium"" estimate is assuming this only needs to be fixed in the ~wdl4s~ cromwell-wdl project. If this is a problem lower down in the parser/grammar, then it might be harder for a developer to do. My note here is because Winstanley is also highlighting these ""bad"" examples as problematic with red-underlines, hinting that this may be a lower level problem than I think.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829
https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829:304,Modifiability,variab,variables,304,"There are multiple related issues in this ticket. The common thread is: a WDL author expects to be able to write compound WDL statements in a task `output` section. However, there are certain WDL statements that fail parsing when strung together, but _will_ work if the statement is broken into multiple variables. The original issue identifies problems in `output` using `glob()` or `Map[,]`. In terms of a fix, I'm guessing for the right dev this is a medium<sup>1</sup> sized task, but would probably be lower on the list of TODOs as there exists a workaround. This ""workaround"" works, where all three `output` variables are relatively simple:; ```wdl; task x {; command {; echo 0 > intFile.txt; echo hello > outFile.txt; }; runtime { docker: ""ubuntu"" }; output {; Int intermediateInt = read_int(""intFile.txt""); Array[File] intermediateOuts = glob(""outFile.txt""); File out = intermediateOuts[intermediateInt]; }; }. workflow glob_indexing { call x }; ```. Starting to compress the output block into two statements, where the latter is a compound expression, this still parses and runs:; ```wdl; output {; Int intermediateInt = read_int(""intFile.txt""); File out = glob(""outFile.txt"")[intermediateInt]; }; ```. Regarding the problems with `Map[,]` this _does_ work:; ```wdl; output {; Map[String, File] intermediateMap = {""a"": ""outFile.txt""}; File out = intermediateMap[""a""]; }; ```. HOWEVER, this doesn't work, currently failing with the error `Workflow input processing failed: <string:8:20 lbrace ""ew==""> (of class wdl4s.parser.WdlParser$Terminal)`:. ```wdl; output {; File out = {""a"": ""outFile.txt""}[""a""]; }; ```. And going back to globbing, the error with globs is _slightly_ better. This doesn't work, either:; ```wdl; output {; File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; }; ```. And fails with the ""prettier"" message at the moment:. ```; ERROR: Unexpected symbol (line 8, col 48) when parsing 'e'. Expected rsquare, got (. File out = glob(""outFile.txt"")[read_int(""intFile.txt"")];",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829
https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829:614,Modifiability,variab,variables,614,"There are multiple related issues in this ticket. The common thread is: a WDL author expects to be able to write compound WDL statements in a task `output` section. However, there are certain WDL statements that fail parsing when strung together, but _will_ work if the statement is broken into multiple variables. The original issue identifies problems in `output` using `glob()` or `Map[,]`. In terms of a fix, I'm guessing for the right dev this is a medium<sup>1</sup> sized task, but would probably be lower on the list of TODOs as there exists a workaround. This ""workaround"" works, where all three `output` variables are relatively simple:; ```wdl; task x {; command {; echo 0 > intFile.txt; echo hello > outFile.txt; }; runtime { docker: ""ubuntu"" }; output {; Int intermediateInt = read_int(""intFile.txt""); Array[File] intermediateOuts = glob(""outFile.txt""); File out = intermediateOuts[intermediateInt]; }; }. workflow glob_indexing { call x }; ```. Starting to compress the output block into two statements, where the latter is a compound expression, this still parses and runs:; ```wdl; output {; Int intermediateInt = read_int(""intFile.txt""); File out = glob(""outFile.txt"")[intermediateInt]; }; ```. Regarding the problems with `Map[,]` this _does_ work:; ```wdl; output {; Map[String, File] intermediateMap = {""a"": ""outFile.txt""}; File out = intermediateMap[""a""]; }; ```. HOWEVER, this doesn't work, currently failing with the error `Workflow input processing failed: <string:8:20 lbrace ""ew==""> (of class wdl4s.parser.WdlParser$Terminal)`:. ```wdl; output {; File out = {""a"": ""outFile.txt""}[""a""]; }; ```. And going back to globbing, the error with globs is _slightly_ better. This doesn't work, either:; ```wdl; output {; File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; }; ```. And fails with the ""prettier"" message at the moment:. ```; ERROR: Unexpected symbol (line 8, col 48) when parsing 'e'. Expected rsquare, got (. File out = glob(""outFile.txt"")[read_int(""intFile.txt"")];",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829
https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829:639,Usability,simpl,simple,639,"There are multiple related issues in this ticket. The common thread is: a WDL author expects to be able to write compound WDL statements in a task `output` section. However, there are certain WDL statements that fail parsing when strung together, but _will_ work if the statement is broken into multiple variables. The original issue identifies problems in `output` using `glob()` or `Map[,]`. In terms of a fix, I'm guessing for the right dev this is a medium<sup>1</sup> sized task, but would probably be lower on the list of TODOs as there exists a workaround. This ""workaround"" works, where all three `output` variables are relatively simple:; ```wdl; task x {; command {; echo 0 > intFile.txt; echo hello > outFile.txt; }; runtime { docker: ""ubuntu"" }; output {; Int intermediateInt = read_int(""intFile.txt""); Array[File] intermediateOuts = glob(""outFile.txt""); File out = intermediateOuts[intermediateInt]; }; }. workflow glob_indexing { call x }; ```. Starting to compress the output block into two statements, where the latter is a compound expression, this still parses and runs:; ```wdl; output {; Int intermediateInt = read_int(""intFile.txt""); File out = glob(""outFile.txt"")[intermediateInt]; }; ```. Regarding the problems with `Map[,]` this _does_ work:; ```wdl; output {; Map[String, File] intermediateMap = {""a"": ""outFile.txt""}; File out = intermediateMap[""a""]; }; ```. HOWEVER, this doesn't work, currently failing with the error `Workflow input processing failed: <string:8:20 lbrace ""ew==""> (of class wdl4s.parser.WdlParser$Terminal)`:. ```wdl; output {; File out = {""a"": ""outFile.txt""}[""a""]; }; ```. And going back to globbing, the error with globs is _slightly_ better. This doesn't work, either:; ```wdl; output {; File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; }; ```. And fails with the ""prettier"" message at the moment:. ```; ERROR: Unexpected symbol (line 8, col 48) when parsing 'e'. Expected rsquare, got (. File out = glob(""outFile.txt"")[read_int(""intFile.txt"")];",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829
https://github.com/broadinstitute/cromwell/issues/2708#issuecomment-345252091:226,Modifiability,config,configurable,226,"Pretty easy: https://doc.akka.io/docs/akka-http/current/scala/http/common/json-support.html#pretty-printing. I'm picking up a discussion on what the workbench-wide policy for such things should be (e.g. always-on, always-off, configurable). Once that's decided the path for this ticket will be clear.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2708#issuecomment-345252091
https://github.com/broadinstitute/cromwell/issues/2708#issuecomment-345252091:294,Usability,clear,clear,294,"Pretty easy: https://doc.akka.io/docs/akka-http/current/scala/http/common/json-support.html#pretty-printing. I'm picking up a discussion on what the workbench-wide policy for such things should be (e.g. always-on, always-off, configurable). Once that's decided the path for this ticket will be clear.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2708#issuecomment-345252091
https://github.com/broadinstitute/cromwell/pull/2770#issuecomment-338699689:59,Usability,clear,clear,59,"👍 I don't particularly like ""common"" since it's not at all clear what now goes in ""common"" vs ""core"" but 🤷‍♂️ I can't think of anything better. [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2770/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2770#issuecomment-338699689
https://github.com/broadinstitute/cromwell/issues/2771#issuecomment-338328584:56,Usability,simpl,simpler,56,"Note that I don’t really care but it seems like it’d be simpler for us and it’s not being skeezy or anything, that was literally how they intended it to work",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2771#issuecomment-338328584
https://github.com/broadinstitute/cromwell/pull/2803#issuecomment-341213876:30,Usability,simpl,simple,30,This PR will return when less simple `if`s are added too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2803#issuecomment-341213876
https://github.com/broadinstitute/cromwell/issues/2870#issuecomment-344348677:121,Usability,clear,clearly,121,"Having been recently bitten by this, I agree with Eddie's suggestion to document the (somewhat surprising) behavior more clearly rather than making assumptions about how the command block will treat octothorpes and the content that trails them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2870#issuecomment-344348677
https://github.com/broadinstitute/cromwell/issues/2912#issuecomment-490940850:801,Usability,guid,guidance,801,"I ran in this issue this week and it requires some quite nasty and ugly workarounds. So it would be great if this could be fixed. What I can grok from the code in Cromwell is that the node structure only considers tasks as `ExternalInputNode`. These have inputs that can be overwritten. Other nodes may be considered `OuterGraphInputNode`. Basically everything that is not an ExternalInputNode can not be considered for options that can be overridden. I feel there is a lot of technical decisions being made in this piece of code. Without knowing all the reasons why decisions are made it will be very hard to solve this in a PR for an outside contributor. It is hard to find the place in which to edit code in this case. I would love to take some work of cromwell's developers hands, but I need more guidance to do so. @ruchim can I help out in any way in the effort to solve this issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2912#issuecomment-490940850
https://github.com/broadinstitute/cromwell/pull/2935#issuecomment-347208372:37,Usability,simpl,simple,37,"@cjllanwarne in this case it'll be a simple find/replace text change, the underlying API in terms of data structures remains the same.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2935#issuecomment-347208372
https://github.com/broadinstitute/cromwell/pull/2948#issuecomment-347844423:171,Energy Efficiency,efficient,efficient,171,Yeah I recall Jeff tried a simpler setup like this first and then had to resort to nesting Cromwells when that didn't work. But clearly Cromwell has become that much more efficient so now it does work hooray!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2948#issuecomment-347844423
https://github.com/broadinstitute/cromwell/pull/2948#issuecomment-347844423:27,Usability,simpl,simpler,27,Yeah I recall Jeff tried a simpler setup like this first and then had to resort to nesting Cromwells when that didn't work. But clearly Cromwell has become that much more efficient so now it does work hooray!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2948#issuecomment-347844423
https://github.com/broadinstitute/cromwell/pull/2948#issuecomment-347844423:128,Usability,clear,clearly,128,Yeah I recall Jeff tried a simpler setup like this first and then had to resort to nesting Cromwells when that didn't work. But clearly Cromwell has become that much more efficient so now it does work hooray!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2948#issuecomment-347844423
https://github.com/broadinstitute/cromwell/pull/2951#issuecomment-348069333:84,Availability,failure,failures,84,That certainly looks reasonable despite all the Travis redness. I restarted all the failures and assuming everything clears up :+1:. [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2951/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2951#issuecomment-348069333
https://github.com/broadinstitute/cromwell/pull/2951#issuecomment-348069333:117,Usability,clear,clears,117,That certainly looks reasonable despite all the Travis redness. I restarted all the failures and assuming everything clears up :+1:. [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2951/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2951#issuecomment-348069333
https://github.com/broadinstitute/cromwell/pull/2971#issuecomment-349068493:23,Usability,feedback,feedback,23,Thanks for the helpful feedback! I've addressed all of the comments if you want to take another look! @cjllanwarne @mcovarr,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2971#issuecomment-349068493
https://github.com/broadinstitute/cromwell/issues/2990#issuecomment-349335796:580,Modifiability,config,configs,580,"Isn't the `runtime` section supposed to be backend/engine specific? Maybe this discussion belongs more in the openWDL board, but having a section that is simply defined as key-value pairs that allows expressions, the implementation of which is at least partially engine specific (as an engine may implement for multiple backends) makes a lot of sense to me. While there are plenty of key-values that are easily generalized, and can be fixed for such a section, backends almost invariably have specialized specific options that are best controlled on the fly rather than via fixed configs. As an analogy, The DRMAA API allowed for this with SGE and similar run-execution engines, by generalizing common functions, and allowing pass-through of specific ones (e.g. soft memory limits, if my vague memory serves).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2990#issuecomment-349335796
https://github.com/broadinstitute/cromwell/issues/2990#issuecomment-349335796:154,Usability,simpl,simply,154,"Isn't the `runtime` section supposed to be backend/engine specific? Maybe this discussion belongs more in the openWDL board, but having a section that is simply defined as key-value pairs that allows expressions, the implementation of which is at least partially engine specific (as an engine may implement for multiple backends) makes a lot of sense to me. While there are plenty of key-values that are easily generalized, and can be fixed for such a section, backends almost invariably have specialized specific options that are best controlled on the fly rather than via fixed configs. As an analogy, The DRMAA API allowed for this with SGE and similar run-execution engines, by generalizing common functions, and allowing pass-through of specific ones (e.g. soft memory limits, if my vague memory serves).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2990#issuecomment-349335796
https://github.com/broadinstitute/cromwell/issues/3007#issuecomment-349689182:89,Availability,fault,fault,89,"Hey @leepc12 it turns out that you do have a bug in your WDL and that Cromwell 29 was at fault for not highlighting it too. I'll submit a PR to include a better error message, which will be along the lines of:; ```; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$scatter_2': Unable to build WOM node for WdlTaskCall 't3': Invalid indexing target. You cannot index a value of type'Array[Int]?'; ```. Notice that in order to access `t2.out` you're looking up inside another `if` block, which means that the output has to be treated as optional. . - Given the structure of *this* workflow you could move the `if ( b1 && b2 )` inside the `if (b1)` (and simplify the conditional expression). ; - If that's not possible in your real workflow you can use `select_first` to get the value out, eg `call t0 as t3 { input: i=select_first([t2.out])[i] }` (NB this is only valid because `if (b1 && b2)` implies `if (b1)` must have been run, so the `select_first` is known to succeed)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007#issuecomment-349689182
https://github.com/broadinstitute/cromwell/issues/3007#issuecomment-349689182:161,Availability,error,error,161,"Hey @leepc12 it turns out that you do have a bug in your WDL and that Cromwell 29 was at fault for not highlighting it too. I'll submit a PR to include a better error message, which will be along the lines of:; ```; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$scatter_2': Unable to build WOM node for WdlTaskCall 't3': Invalid indexing target. You cannot index a value of type'Array[Int]?'; ```. Notice that in order to access `t2.out` you're looking up inside another `if` block, which means that the output has to be treated as optional. . - Given the structure of *this* workflow you could move the `if ( b1 && b2 )` inside the `if (b1)` (and simplify the conditional expression). ; - If that's not possible in your real workflow you can use `select_first` to get the value out, eg `call t0 as t3 { input: i=select_first([t2.out])[i] }` (NB this is only valid because `if (b1 && b2)` implies `if (b1)` must have been run, so the `select_first` is known to succeed)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007#issuecomment-349689182
https://github.com/broadinstitute/cromwell/issues/3007#issuecomment-349689182:167,Integrability,message,message,167,"Hey @leepc12 it turns out that you do have a bug in your WDL and that Cromwell 29 was at fault for not highlighting it too. I'll submit a PR to include a better error message, which will be along the lines of:; ```; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$scatter_2': Unable to build WOM node for WdlTaskCall 't3': Invalid indexing target. You cannot index a value of type'Array[Int]?'; ```. Notice that in order to access `t2.out` you're looking up inside another `if` block, which means that the output has to be treated as optional. . - Given the structure of *this* workflow you could move the `if ( b1 && b2 )` inside the `if (b1)` (and simplify the conditional expression). ; - If that's not possible in your real workflow you can use `select_first` to get the value out, eg `call t0 as t3 { input: i=select_first([t2.out])[i] }` (NB this is only valid because `if (b1 && b2)` implies `if (b1)` must have been run, so the `select_first` is known to succeed)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007#issuecomment-349689182
https://github.com/broadinstitute/cromwell/issues/3007#issuecomment-349689182:456,Security,access,access,456,"Hey @leepc12 it turns out that you do have a bug in your WDL and that Cromwell 29 was at fault for not highlighting it too. I'll submit a PR to include a better error message, which will be along the lines of:; ```; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$scatter_2': Unable to build WOM node for WdlTaskCall 't3': Invalid indexing target. You cannot index a value of type'Array[Int]?'; ```. Notice that in order to access `t2.out` you're looking up inside another `if` block, which means that the output has to be treated as optional. . - Given the structure of *this* workflow you could move the `if ( b1 && b2 )` inside the `if (b1)` (and simplify the conditional expression). ; - If that's not possible in your real workflow you can use `select_first` to get the value out, eg `call t0 as t3 { input: i=select_first([t2.out])[i] }` (NB this is only valid because `if (b1 && b2)` implies `if (b1)` must have been run, so the `select_first` is known to succeed)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007#issuecomment-349689182
https://github.com/broadinstitute/cromwell/issues/3007#issuecomment-349689182:682,Usability,simpl,simplify,682,"Hey @leepc12 it turns out that you do have a bug in your WDL and that Cromwell 29 was at fault for not highlighting it too. I'll submit a PR to include a better error message, which will be along the lines of:; ```; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$scatter_2': Unable to build WOM node for WdlTaskCall 't3': Invalid indexing target. You cannot index a value of type'Array[Int]?'; ```. Notice that in order to access `t2.out` you're looking up inside another `if` block, which means that the output has to be treated as optional. . - Given the structure of *this* workflow you could move the `if ( b1 && b2 )` inside the `if (b1)` (and simplify the conditional expression). ; - If that's not possible in your real workflow you can use `select_first` to get the value out, eg `call t0 as t3 { input: i=select_first([t2.out])[i] }` (NB this is only valid because `if (b1 && b2)` implies `if (b1)` must have been run, so the `select_first` is known to succeed)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007#issuecomment-349689182
https://github.com/broadinstitute/cromwell/issues/3093#issuecomment-361298736:142,Usability,simpl,simpler,142,"Thank you @cjllanwarne . A `Struct` would be indeed a nicer way to fix this as you can define all the items. An `object` can then be used for simpler things. Extra methods just add clutter. I will check it out. EDIT: on second thought, best to leave this issue open until structs are actually there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3093#issuecomment-361298736
https://github.com/broadinstitute/cromwell/issues/3151#issuecomment-359951122:116,Usability,simpl,simpleton,116,"I'll note that WDL only supports primitive keys. If CWL is the same, this problem might be a type fix rather than a simpleton fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3151#issuecomment-359951122
https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358657812:155,Modifiability,variab,variable,155,"Hmm you're right, I had simplified the workflow too much to reproduce the blocking but going back to your workflow I still can get it stuck with the local variable.; Looking into it now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358657812
https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358657812:24,Usability,simpl,simplified,24,"Hmm you're right, I had simplified the workflow too much to reproduce the blocking but going back to your workflow I still can get it stuck with the local variable.; Looking into it now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358657812
https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427:42,Availability,failure,failures,42,"**Update: Non-additive retry counts**. If failures due to preemption can be clearly distinguished from failures due to other causes, I would prefer the failed_task_retries count to be independent of the preemptible count, rather than additive. For example, with failed_task_retries: 2 and preemptible: 2, I would expect the following behavior:; - try 1: preemptible machine, got preempted; - try 2: preemptible machine, other error (not preemption); - try 3: non-preemptible machine, error; - try 4: non-preemptible machine, error; - task fails. We have only retried 3 times here, because one of the non-preemption retries was ""used up"" when try 2 failed. (With additive behavior, we would have retried 4 times.). This behavior would allow users to independently set the retries due to preemption from those due to other causes, to more finely tune the desired behavior. However, if this can't be accommodated, this feature would still be very valuable with the additive behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427
https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427:103,Availability,failure,failures,103,"**Update: Non-additive retry counts**. If failures due to preemption can be clearly distinguished from failures due to other causes, I would prefer the failed_task_retries count to be independent of the preemptible count, rather than additive. For example, with failed_task_retries: 2 and preemptible: 2, I would expect the following behavior:; - try 1: preemptible machine, got preempted; - try 2: preemptible machine, other error (not preemption); - try 3: non-preemptible machine, error; - try 4: non-preemptible machine, error; - task fails. We have only retried 3 times here, because one of the non-preemption retries was ""used up"" when try 2 failed. (With additive behavior, we would have retried 4 times.). This behavior would allow users to independently set the retries due to preemption from those due to other causes, to more finely tune the desired behavior. However, if this can't be accommodated, this feature would still be very valuable with the additive behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427
https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427:426,Availability,error,error,426,"**Update: Non-additive retry counts**. If failures due to preemption can be clearly distinguished from failures due to other causes, I would prefer the failed_task_retries count to be independent of the preemptible count, rather than additive. For example, with failed_task_retries: 2 and preemptible: 2, I would expect the following behavior:; - try 1: preemptible machine, got preempted; - try 2: preemptible machine, other error (not preemption); - try 3: non-preemptible machine, error; - try 4: non-preemptible machine, error; - task fails. We have only retried 3 times here, because one of the non-preemption retries was ""used up"" when try 2 failed. (With additive behavior, we would have retried 4 times.). This behavior would allow users to independently set the retries due to preemption from those due to other causes, to more finely tune the desired behavior. However, if this can't be accommodated, this feature would still be very valuable with the additive behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427
https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427:484,Availability,error,error,484,"**Update: Non-additive retry counts**. If failures due to preemption can be clearly distinguished from failures due to other causes, I would prefer the failed_task_retries count to be independent of the preemptible count, rather than additive. For example, with failed_task_retries: 2 and preemptible: 2, I would expect the following behavior:; - try 1: preemptible machine, got preempted; - try 2: preemptible machine, other error (not preemption); - try 3: non-preemptible machine, error; - try 4: non-preemptible machine, error; - task fails. We have only retried 3 times here, because one of the non-preemption retries was ""used up"" when try 2 failed. (With additive behavior, we would have retried 4 times.). This behavior would allow users to independently set the retries due to preemption from those due to other causes, to more finely tune the desired behavior. However, if this can't be accommodated, this feature would still be very valuable with the additive behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427
https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427:525,Availability,error,error,525,"**Update: Non-additive retry counts**. If failures due to preemption can be clearly distinguished from failures due to other causes, I would prefer the failed_task_retries count to be independent of the preemptible count, rather than additive. For example, with failed_task_retries: 2 and preemptible: 2, I would expect the following behavior:; - try 1: preemptible machine, got preempted; - try 2: preemptible machine, other error (not preemption); - try 3: non-preemptible machine, error; - try 4: non-preemptible machine, error; - task fails. We have only retried 3 times here, because one of the non-preemption retries was ""used up"" when try 2 failed. (With additive behavior, we would have retried 4 times.). This behavior would allow users to independently set the retries due to preemption from those due to other causes, to more finely tune the desired behavior. However, if this can't be accommodated, this feature would still be very valuable with the additive behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427
https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427:844,Performance,tune,tune,844,"**Update: Non-additive retry counts**. If failures due to preemption can be clearly distinguished from failures due to other causes, I would prefer the failed_task_retries count to be independent of the preemptible count, rather than additive. For example, with failed_task_retries: 2 and preemptible: 2, I would expect the following behavior:; - try 1: preemptible machine, got preempted; - try 2: preemptible machine, other error (not preemption); - try 3: non-preemptible machine, error; - try 4: non-preemptible machine, error; - task fails. We have only retried 3 times here, because one of the non-preemption retries was ""used up"" when try 2 failed. (With additive behavior, we would have retried 4 times.). This behavior would allow users to independently set the retries due to preemption from those due to other causes, to more finely tune the desired behavior. However, if this can't be accommodated, this feature would still be very valuable with the additive behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427
https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427:76,Usability,clear,clearly,76,"**Update: Non-additive retry counts**. If failures due to preemption can be clearly distinguished from failures due to other causes, I would prefer the failed_task_retries count to be independent of the preemptible count, rather than additive. For example, with failed_task_retries: 2 and preemptible: 2, I would expect the following behavior:; - try 1: preemptible machine, got preempted; - try 2: preemptible machine, other error (not preemption); - try 3: non-preemptible machine, error; - try 4: non-preemptible machine, error; - task fails. We have only retried 3 times here, because one of the non-preemption retries was ""used up"" when try 2 failed. (With additive behavior, we would have retried 4 times.). This behavior would allow users to independently set the retries due to preemption from those due to other causes, to more finely tune the desired behavior. However, if this can't be accommodated, this feature would still be very valuable with the additive behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358986427
https://github.com/broadinstitute/cromwell/issues/3210#issuecomment-382814261:55,Usability,simpl,simply,55,To what extent can we merge the two? Is it possible to simply override where necessary on the cromiam side?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3210#issuecomment-382814261
https://github.com/broadinstitute/cromwell/issues/3210#issuecomment-429449186:95,Usability,clear,clearly,95,Discussed in person. As the long term plan is to do away with CromIAM as a separate server and clearly any deltas aren't bothering anyone at the moment we're closing this,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3210#issuecomment-429449186
https://github.com/broadinstitute/cromwell/pull/3229#issuecomment-363646996:100,Integrability,rout,route,100,Closing for now. I'll use my lessons learned here to advise the trajectory of the tech debt paydown route.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3229#issuecomment-363646996
https://github.com/broadinstitute/cromwell/pull/3229#issuecomment-363646996:37,Usability,learn,learned,37,Closing for now. I'll use my lessons learned here to advise the trajectory of the tech debt paydown route.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3229#issuecomment-363646996
https://github.com/broadinstitute/cromwell/pull/3234#issuecomment-363647088:127,Integrability,rout,route,127,Closing for now. Thank you all for looking. I'll use my lessons learned here to advise the trajectory of the tech debt paydown route.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3234#issuecomment-363647088
https://github.com/broadinstitute/cromwell/pull/3234#issuecomment-363647088:64,Usability,learn,learned,64,Closing for now. Thank you all for looking. I'll use my lessons learned here to advise the trajectory of the tech debt paydown route.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3234#issuecomment-363647088
https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367423915:95,Deployability,pipeline,pipelines,95,"@geoffjentry in such case I have not idea how I can do anything other than super-simple linear pipelines with wdl: tsv-s with headers cannot be read, read_json does not work, loops do not work, I cannot make even simpliest preprocessing of input arrays or maps with wdl!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367423915
https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367423915:81,Usability,simpl,simple,81,"@geoffjentry in such case I have not idea how I can do anything other than super-simple linear pipelines with wdl: tsv-s with headers cannot be read, read_json does not work, loops do not work, I cannot make even simpliest preprocessing of input arrays or maps with wdl!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367423915
https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367423915:213,Usability,simpl,simpliest,213,"@geoffjentry in such case I have not idea how I can do anything other than super-simple linear pipelines with wdl: tsv-s with headers cannot be read, read_json does not work, loops do not work, I cannot make even simpliest preprocessing of input arrays or maps with wdl!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367423915
https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367449569:394,Energy Efficiency,power,powerful,394,"@cjllanwarne you are totally right, if read/write_json worked it would not be such a pain, I would simple write everything to json, give it to a task with some Scala (or whatever language I want) script that return json and then read it to a cromwell Map. >All WDL values are immutable as an early design choice for the language. I do not mind it, I am used to it in Scala, but in Scala I have powerful filter/map/flatMap/foldLeft are you going to give any of them to WDL?. >You can get something similar by using the implicit gather on a scatter. eg I could map over an array to calculate the ""values plus one"" array like this:. Thanks, I did not know that such thing is possible, I thought that all variables declared inside loops/scatter are not visible from outside",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367449569
https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367449569:701,Modifiability,variab,variables,701,"@cjllanwarne you are totally right, if read/write_json worked it would not be such a pain, I would simple write everything to json, give it to a task with some Scala (or whatever language I want) script that return json and then read it to a cromwell Map. >All WDL values are immutable as an early design choice for the language. I do not mind it, I am used to it in Scala, but in Scala I have powerful filter/map/flatMap/foldLeft are you going to give any of them to WDL?. >You can get something similar by using the implicit gather on a scatter. eg I could map over an array to calculate the ""values plus one"" array like this:. Thanks, I did not know that such thing is possible, I thought that all variables declared inside loops/scatter are not visible from outside",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367449569
https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367449569:99,Usability,simpl,simple,99,"@cjllanwarne you are totally right, if read/write_json worked it would not be such a pain, I would simple write everything to json, give it to a task with some Scala (or whatever language I want) script that return json and then read it to a cromwell Map. >All WDL values are immutable as an early design choice for the language. I do not mind it, I am used to it in Scala, but in Scala I have powerful filter/map/flatMap/foldLeft are you going to give any of them to WDL?. >You can get something similar by using the implicit gather on a scatter. eg I could map over an array to calculate the ""values plus one"" array like this:. Thanks, I did not know that such thing is possible, I thought that all variables declared inside loops/scatter are not visible from outside",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367449569
https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374722237:419,Deployability,configurat,configuration,419,"@carolynlawrence Just to double check, are you all using Docker in your workflow tasks? The only reason we're fiddling with permissions is due to Docker, and I don't know of anyone using Docker w/ Cromwell in an HPC environment - so my thought was that we could simply disable that permission activity for tasks which are not using docker. . To tie it into what @danbills suggested, perhaps **that** should be what the configuration flag is doing, just to be sure it's not breaking anyone's reliance on current behavior either way.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374722237
https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374722237:419,Modifiability,config,configuration,419,"@carolynlawrence Just to double check, are you all using Docker in your workflow tasks? The only reason we're fiddling with permissions is due to Docker, and I don't know of anyone using Docker w/ Cromwell in an HPC environment - so my thought was that we could simply disable that permission activity for tasks which are not using docker. . To tie it into what @danbills suggested, perhaps **that** should be what the configuration flag is doing, just to be sure it's not breaking anyone's reliance on current behavior either way.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374722237
https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374722237:262,Usability,simpl,simply,262,"@carolynlawrence Just to double check, are you all using Docker in your workflow tasks? The only reason we're fiddling with permissions is due to Docker, and I don't know of anyone using Docker w/ Cromwell in an HPC environment - so my thought was that we could simply disable that permission activity for tasks which are not using docker. . To tie it into what @danbills suggested, perhaps **that** should be what the configuration flag is doing, just to be sure it's not breaking anyone's reliance on current behavior either way.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-374722237
https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-376196463:59,Modifiability,config,config,59,"Debrief here from a face-to-face w/ @geoffjentry :. So the config flag should be set _AND_ the call should be using docker to do the override. To be clear, the truth table here is . Using Docker on this call | Configuration Flag is set to true | Should reassign; --|--|--; T|T|T; T|F|F; F|T|F; F|F|F. Config flag should be `docker.override_umask_when_creating_directories`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-376196463
https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-376196463:149,Usability,clear,clear,149,"Debrief here from a face-to-face w/ @geoffjentry :. So the config flag should be set _AND_ the call should be using docker to do the override. To be clear, the truth table here is . Using Docker on this call | Configuration Flag is set to true | Should reassign; --|--|--; T|T|T; T|F|F; F|T|F; F|F|F. Config flag should be `docker.override_umask_when_creating_directories`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333#issuecomment-376196463
https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371703999:44,Modifiability,config,config,44,There's code now on develop that looks at a config value. It's not clear from the discussion above if that's enough to satisfy this ticket?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371703999
https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371703999:67,Usability,clear,clear,67,There's code now on develop that looks at a config value. It's not clear from the discussion above if that's enough to satisfy this ticket?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371703999
https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371786829:224,Modifiability,config,config,224,"Let’s talk after standup. In this case I think the port that Cromwell itself is listening on works. > On Mar 8, 2018, at 10:40 PM, mcovarr <notifications@github.com> wrote:; > ; > There's code now on develop that looks at a config value. It's not clear from the discussion above if that's enough to satisfy this ticket?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371786829
https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371786829:247,Usability,clear,clear,247,"Let’s talk after standup. In this case I think the port that Cromwell itself is listening on works. > On Mar 8, 2018, at 10:40 PM, mcovarr <notifications@github.com> wrote:; > ; > There's code now on develop that looks at a config value. It's not clear from the discussion above if that's enough to satisfy this ticket?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3340#issuecomment-371786829
https://github.com/broadinstitute/cromwell/issues/3384#issuecomment-371697922:22,Usability,clear,clear,22,"@yfarjoun just so i'm clear are you providing the version number to suggest that it used to work, or just because it's generally a useful thing to include?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3384#issuecomment-371697922
https://github.com/broadinstitute/cromwell/issues/3384#issuecomment-371866116:15,Usability,simpl,simple,15,This should be simple enough to fix for 31 (I think I still have time...). interestingly the if/then/else do already short circuit so this should work:; ```wdl; if (if defined(optional_int) then select_first([optional_int]) == 2 else false) { ... }; ```; ... on the other hand the nested if is probably an even easier workaround,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3384#issuecomment-371866116
https://github.com/broadinstitute/cromwell/pull/3438#issuecomment-374914130:40,Testability,test,test,40,@mcovarr could we make a simple centaur test? Or even a PR into CWL’s suite?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3438#issuecomment-374914130
https://github.com/broadinstitute/cromwell/pull/3438#issuecomment-374914130:25,Usability,simpl,simple,25,@mcovarr could we make a simple centaur test? Or even a PR into CWL’s suite?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3438#issuecomment-374914130
https://github.com/broadinstitute/cromwell/pull/3443#issuecomment-375080865:229,Modifiability,refactor,refactor,229,"Discussed in person, a better way could be to do this work as a separate graph node. It would make the wiring a bit more complicated and the immediate gain isn't clear so I'll leave as is for now and we can revisit later when we refactor everything to finally achieve Cromwell singularity.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3443#issuecomment-375080865
https://github.com/broadinstitute/cromwell/pull/3443#issuecomment-375080865:162,Usability,clear,clear,162,"Discussed in person, a better way could be to do this work as a separate graph node. It would make the wiring a bit more complicated and the immediate gain isn't clear so I'll leave as is for now and we can revisit later when we refactor everything to finally achieve Cromwell singularity.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3443#issuecomment-375080865
https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-378866720:189,Testability,test,test,189,"Hi @geoffjentry - thanks for the prompt and detailed response. It makes sense to submit using curl instead. That might save most of the 2.5 seconds. But since your fast-polling hello world test still took 7 seconds from pickup to completion, I still think this would be frustratingly slow. I'm familiar with Snakemake (and about two dozen other similar tools to be found scattered across the web) but I think WDL is better in a number of ways. The main points that I like are (1) that the workflow language is clearly separated from the execution engine, (2) that the language is much more human-readable than CWL, and (3) the possibility to serialize/deserialize values, e.g., so that (3a) simple operations, e.g., concatenation of different results can be done in the workflow language and (3b) one can do things like mapping a task over a set of values coming out of another task. It is also nice that (4) there is a rather solid set of backing organizations behind WDL and Cromwell. As far as I know, this combination of properties gives WDL a rather unique value proposition. Maybe there is room for a simpler local-only WDL execution engine focused on speed and rapid iteration. That could be used for development and then the execution of production workflows on various cloud backends etc could still be done by Cromwell. I was dabbling one weekend a few months ago with a local-only Python compiler/execution engine for WDL. If optimal execution speed is not a must, it does not look too hard to support most of the WDL spec on a local machine. I'm closing this issue for now. Thank you again for the input!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-378866720
https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-378866720:510,Usability,clear,clearly,510,"Hi @geoffjentry - thanks for the prompt and detailed response. It makes sense to submit using curl instead. That might save most of the 2.5 seconds. But since your fast-polling hello world test still took 7 seconds from pickup to completion, I still think this would be frustratingly slow. I'm familiar with Snakemake (and about two dozen other similar tools to be found scattered across the web) but I think WDL is better in a number of ways. The main points that I like are (1) that the workflow language is clearly separated from the execution engine, (2) that the language is much more human-readable than CWL, and (3) the possibility to serialize/deserialize values, e.g., so that (3a) simple operations, e.g., concatenation of different results can be done in the workflow language and (3b) one can do things like mapping a task over a set of values coming out of another task. It is also nice that (4) there is a rather solid set of backing organizations behind WDL and Cromwell. As far as I know, this combination of properties gives WDL a rather unique value proposition. Maybe there is room for a simpler local-only WDL execution engine focused on speed and rapid iteration. That could be used for development and then the execution of production workflows on various cloud backends etc could still be done by Cromwell. I was dabbling one weekend a few months ago with a local-only Python compiler/execution engine for WDL. If optimal execution speed is not a must, it does not look too hard to support most of the WDL spec on a local machine. I'm closing this issue for now. Thank you again for the input!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-378866720
https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-378866720:691,Usability,simpl,simple,691,"Hi @geoffjentry - thanks for the prompt and detailed response. It makes sense to submit using curl instead. That might save most of the 2.5 seconds. But since your fast-polling hello world test still took 7 seconds from pickup to completion, I still think this would be frustratingly slow. I'm familiar with Snakemake (and about two dozen other similar tools to be found scattered across the web) but I think WDL is better in a number of ways. The main points that I like are (1) that the workflow language is clearly separated from the execution engine, (2) that the language is much more human-readable than CWL, and (3) the possibility to serialize/deserialize values, e.g., so that (3a) simple operations, e.g., concatenation of different results can be done in the workflow language and (3b) one can do things like mapping a task over a set of values coming out of another task. It is also nice that (4) there is a rather solid set of backing organizations behind WDL and Cromwell. As far as I know, this combination of properties gives WDL a rather unique value proposition. Maybe there is room for a simpler local-only WDL execution engine focused on speed and rapid iteration. That could be used for development and then the execution of production workflows on various cloud backends etc could still be done by Cromwell. I was dabbling one weekend a few months ago with a local-only Python compiler/execution engine for WDL. If optimal execution speed is not a must, it does not look too hard to support most of the WDL spec on a local machine. I'm closing this issue for now. Thank you again for the input!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-378866720
https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-378866720:1107,Usability,simpl,simpler,1107,"Hi @geoffjentry - thanks for the prompt and detailed response. It makes sense to submit using curl instead. That might save most of the 2.5 seconds. But since your fast-polling hello world test still took 7 seconds from pickup to completion, I still think this would be frustratingly slow. I'm familiar with Snakemake (and about two dozen other similar tools to be found scattered across the web) but I think WDL is better in a number of ways. The main points that I like are (1) that the workflow language is clearly separated from the execution engine, (2) that the language is much more human-readable than CWL, and (3) the possibility to serialize/deserialize values, e.g., so that (3a) simple operations, e.g., concatenation of different results can be done in the workflow language and (3b) one can do things like mapping a task over a set of values coming out of another task. It is also nice that (4) there is a rather solid set of backing organizations behind WDL and Cromwell. As far as I know, this combination of properties gives WDL a rather unique value proposition. Maybe there is room for a simpler local-only WDL execution engine focused on speed and rapid iteration. That could be used for development and then the execution of production workflows on various cloud backends etc could still be done by Cromwell. I was dabbling one weekend a few months ago with a local-only Python compiler/execution engine for WDL. If optimal execution speed is not a must, it does not look too hard to support most of the WDL spec on a local machine. I'm closing this issue for now. Thank you again for the input!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-378866720
https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-379006890:376,Usability,simpl,simple,376,"Hi @rasmuse That's fair, and again I'm not going to try too hard to sway you away from WDL :). I'd be very much in favor of more WDL runner implementations out there if that's a path you wanted to explore. . It's pretty out of date but you might find some inspiration in [PyWDL](https://github.com/broadinstitute/pywdl). If you go back far enough in its history it was even a simple python based WDL runner. If you're interested in Go, there's also [Maple](https://github.com/scottfrazer/maple) from the originator of WDL, although it's certainly out of date w/ the latest WDL features.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3477#issuecomment-379006890
https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-390838429:80,Testability,test,test,80,@lbergelson -- is this a reproducible issue? Perhaps we can create a simplified test case for our debugging.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-390838429
https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-390838429:69,Usability,simpl,simplified,69,@lbergelson -- is this a reproducible issue? Perhaps we can create a simplified test case for our debugging.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-390838429
https://github.com/broadinstitute/cromwell/pull/3580#issuecomment-386051165:39,Usability,simpl,simple,39,"Ah, ok, I didn't know that it was that simple! I always leave these off if intellij is happy so on that basis leaving more off works for me 👍 . [![Approved with PullApprove](https://img.shields.io/badge/one_reviewer-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/3580/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell) [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/3580/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3580#issuecomment-386051165
https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386700970:185,Performance,perform,performs,185,"Glad that helped !; Regarding HSQL vs MySQL, the main reason is that we've rarely used HSQL and there may be some corner cases that we don't support (and don't know about); It probably performs better too on the long run as your DB grows.; But it's definitely good to have some feedback on how Cromwell behaves with HSQL too.; For the `null` hash, something weird is going on so I'd keep the issue open. If it's not immediately blocking you anymore it might get slightly de-prioritized but we'll definitely look into it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386700970
https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386700970:342,Security,hash,hash,342,"Glad that helped !; Regarding HSQL vs MySQL, the main reason is that we've rarely used HSQL and there may be some corner cases that we don't support (and don't know about); It probably performs better too on the long run as your DB grows.; But it's definitely good to have some feedback on how Cromwell behaves with HSQL too.; For the `null` hash, something weird is going on so I'd keep the issue open. If it's not immediately blocking you anymore it might get slightly de-prioritized but we'll definitely look into it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386700970
https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386700970:278,Usability,feedback,feedback,278,"Glad that helped !; Regarding HSQL vs MySQL, the main reason is that we've rarely used HSQL and there may be some corner cases that we don't support (and don't know about); It probably performs better too on the long run as your DB grows.; But it's definitely good to have some feedback on how Cromwell behaves with HSQL too.; For the `null` hash, something weird is going on so I'd keep the issue open. If it's not immediately blocking you anymore it might get slightly de-prioritized but we'll definitely look into it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386700970
https://github.com/broadinstitute/cromwell/issues/3673#issuecomment-391869122:319,Usability,resume,resumed,319,"Hi Matt. Without looking at your DB I can't be sure exactly what the states of your workflows are, but I don't think it would hurt to try a more recent 32 snapshot with the fixes for this issue to see if that helps. If the workflows are still in the workflow store there's at least a chance they would be picked up and resumed on a Cromwell restart.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673#issuecomment-391869122
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:37,Availability,error,error,37,"I had the same issue. I got the same error message:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:84,Availability,error,error,84,"I had the same issue. I got the same error message:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:1870,Availability,error,error,1870," {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this means. If I remove `Requester pays` from the bucket I can get the WDL to work using `scheme = ""application_default""`, as long as I do not export `GOOGLE_APPLICATION_CREDENTIALS` first. But if I use `Requester pays` on the bucket, using `scheme = ""application_default""` causes error:; ```; [2020-07-27 23:19:31,90] [info] WorkflowManagerActor Workflow 4c8a642a-19a6-486b-acad-e0adf3168820 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs fo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:1906,Availability,error,error,1906," = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this means. If I remove `Requester pays` from the bucket I can get the WDL to work using `scheme = ""application_default""`, as long as I do not export `GOOGLE_APPLICATION_CREDENTIALS` first. But if I use `Requester pays` on the bucket, using `scheme = ""application_default""` causes error:; ```; [2020-07-27 23:19:31,90] [info] WorkflowManagerActor Workflow 4c8a642a-19a6-486b-acad-e0adf3168820 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/"": cp failed: gsutil -h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:2166,Availability,error,error,2166," and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this means. If I remove `Requester pays` from the bucket I can get the WDL to work using `scheme = ""application_default""`, as long as I do not export `GOOGLE_APPLICATION_CREDENTIALS` first. But if I use `Requester pays` on the bucket, using `scheme = ""application_default""` causes error:; ```; [2020-07-27 23:19:31,90] [info] WorkflowManagerActor Workflow 4c8a642a-19a6-486b-acad-e0adf3168820 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/, command failed: BadRequestException: 400 Buc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:2545,Availability,error,error,2545,"e = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this means. If I remove `Requester pays` from the bucket I can get the WDL to work using `scheme = ""application_default""`, as long as I do not export `GOOGLE_APPLICATION_CREDENTIALS` first. But if I use `Requester pays` on the bucket, using `scheme = ""application_default""` causes error:; ```; [2020-07-27 23:19:31,90] [info] WorkflowManagerActor Workflow 4c8a642a-19a6-486b-acad-e0adf3168820 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```. So I still have not found a way to run the WDL with `Requester pays` on. I wish Cromwell could give errors explaining what steps to take to solve the issue ... I know that with `gsutil` I can specify the user project with `-u xxx` but I have no idea how to do that with Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:2805,Availability,error,error,2805,"e = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this means. If I remove `Requester pays` from the bucket I can get the WDL to work using `scheme = ""application_default""`, as long as I do not export `GOOGLE_APPLICATION_CREDENTIALS` first. But if I use `Requester pays` on the bucket, using `scheme = ""application_default""` causes error:; ```; [2020-07-27 23:19:31,90] [info] WorkflowManagerActor Workflow 4c8a642a-19a6-486b-acad-e0adf3168820 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```. So I still have not found a way to run the WDL with `Requester pays` on. I wish Cromwell could give errors explaining what steps to take to solve the issue ... I know that with `gsutil` I can specify the user project with `-u xxx` but I have no idea how to do that with Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:3335,Availability,error,errors,3335,"e = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this means. If I remove `Requester pays` from the bucket I can get the WDL to work using `scheme = ""application_default""`, as long as I do not export `GOOGLE_APPLICATION_CREDENTIALS` first. But if I use `Requester pays` on the bucket, using `scheme = ""application_default""` causes error:; ```; [2020-07-27 23:19:31,90] [info] WorkflowManagerActor Workflow 4c8a642a-19a6-486b-acad-e0adf3168820 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```. So I still have not found a way to run the WDL with `Requester pays` on. I wish Cromwell could give errors explaining what steps to take to solve the issue ... I know that with `gsutil` I can specify the user project with `-u xxx` but I have no idea how to do that with Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:676,Deployability,configurat,configuration,676,"I had the same issue. I got the same error message:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:1254,Deployability,configurat,configuration,1254,"(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this m",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:43,Integrability,message,message,43,"I had the same issue. I got the same error message:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:676,Modifiability,config,configuration,676,"I had the same issue. I got the same error message:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:1254,Modifiability,config,configuration,1254,"(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this m",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:366,Security,access,access,366,"I had the same issue. I got the same error message:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:529,Security,access,access,529,"I had the same issue. I got the same error message:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:1156,Security,authoriz,authorization,1156,"lo:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command fin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:1079,Testability,log,logic,1079,"0,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:2856,Testability,log,logs,2856,"e = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this means. If I remove `Requester pays` from the bucket I can get the WDL to work using `scheme = ""application_default""`, as long as I do not export `GOOGLE_APPLICATION_CREDENTIALS` first. But if I use `Requester pays` on the bucket, using `scheme = ""application_default""` causes error:; ```; [2020-07-27 23:19:31,90] [info] WorkflowManagerActor Workflow 4c8a642a-19a6-486b-acad-e0adf3168820 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```. So I still have not found a way to run the WDL with `Requester pays` on. I wish Cromwell could give errors explaining what steps to take to solve the issue ... I know that with `gsutil` I can specify the user project with `-u xxx` but I have no idea how to do that with Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:3013,Testability,log,log,3013,"e = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this means. If I remove `Requester pays` from the bucket I can get the WDL to work using `scheme = ""application_default""`, as long as I do not export `GOOGLE_APPLICATION_CREDENTIALS` first. But if I use `Requester pays` on the bucket, using `scheme = ""application_default""` causes error:; ```; [2020-07-27 23:19:31,90] [info] WorkflowManagerActor Workflow 4c8a642a-19a6-486b-acad-e0adf3168820 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```. So I still have not found a way to run the WDL with `Requester pays` on. I wish Cromwell could give errors explaining what steps to take to solve the issue ... I know that with `gsutil` I can specify the user project with `-u xxx` but I have no idea how to do that with Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:3035,Testability,log,log,3035,"e = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9fe2-bf7cb2b2c3e6 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; I don't know what this means. If I remove `Requester pays` from the bucket I can get the WDL to work using `scheme = ""application_default""`, as long as I do not export `GOOGLE_APPLICATION_CREDENTIALS` first. But if I use `Requester pays` on the bucket, using `scheme = ""application_default""` causes error:; ```; [2020-07-27 23:19:31,90] [info] WorkflowManagerActor Workflow 4c8a642a-19a6-486b-acad-e0adf3168820 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/4c8a642a-19a6-486b-acad-e0adf3168820/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```. So I still have not found a way to run the WDL with `Requester pays` on. I wish Cromwell could give errors explaining what steps to take to solve the issue ... I know that with `gsutil` I can specify the user project with `-u xxx` but I have no idea how to do that with Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:1035,Usability,clear,clearly,1035,"I had the same issue. I got the same error message:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906
https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-395755673:497,Modifiability,config,config,497,"From a technical perspective - sure likely possible. My question is what do you want requests to that URL to return? 400,404, 50X? . The biggest challenge is how to make the change such that it does not break everything else, the current proxy setup is pretty simple ( / (slash - which is where /engine falls under) does one thing, /api does another). This will require adding a 3rd option around /engine/v1/stats - likely something with mod_rewrite. Will take me a bit to work through a workable config",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-395755673
https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-395755673:260,Usability,simpl,simple,260,"From a technical perspective - sure likely possible. My question is what do you want requests to that URL to return? 400,404, 50X? . The biggest challenge is how to make the change such that it does not break everything else, the current proxy setup is pretty simple ( / (slash - which is where /engine falls under) does one thing, /api does another). This will require adding a 3rd option around /engine/v1/stats - likely something with mod_rewrite. Will take me a bit to work through a workable config",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-395755673
https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435984778:124,Usability,simpl,simpler,124,"Nice one, glad you're having success with the gvcf_joint workflow. That has more parts and the svcaller one was meant to be simpler, so having that going is a good indication you've got most of the Cromwell parts in place. Really nice, I'm excited about having this going on GCP. Thanks again for all the work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435984778
https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395489025:52,Availability,error,error,52,Scratch that. Fresh git clone and build cleared the error for the non-AWS config. I must have screwed up the switch to develop from aws_backend branch. So my error **does** seem to be related to AWS code somehow! Sorry for the whipsaw...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395489025
https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395489025:158,Availability,error,error,158,Scratch that. Fresh git clone and build cleared the error for the non-AWS config. I must have screwed up the switch to develop from aws_backend branch. So my error **does** seem to be related to AWS code somehow! Sorry for the whipsaw...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395489025
https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395489025:74,Modifiability,config,config,74,Scratch that. Fresh git clone and build cleared the error for the non-AWS config. I must have screwed up the switch to develop from aws_backend branch. So my error **does** seem to be related to AWS code somehow! Sorry for the whipsaw...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395489025
https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395489025:40,Usability,clear,cleared,40,Scratch that. Fresh git clone and build cleared the error for the non-AWS config. I must have screwed up the switch to develop from aws_backend branch. So my error **does** seem to be related to AWS code somehow! Sorry for the whipsaw...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395489025
https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395554332:73,Performance,queue,queue,73,"Excellent! Thanks for the pointer. As far as compute environment and job queue, they need to be setup in advance. @delagoya is creating a CloudFormation template that will make this relatively simple, and I believe we're planning on putting that in the 101 docs at that time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395554332
https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395554332:193,Usability,simpl,simple,193,"Excellent! Thanks for the pointer. As far as compute environment and job queue, they need to be setup in advance. @delagoya is creating a CloudFormation template that will make this relatively simple, and I believe we're planning on putting that in the 101 docs at that time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395554332
https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-400859982:38,Availability,error,error,38,@tom-dyar fyi - I received this boxed error today as well. It eventually cleared when I did an sbt clean and rebuilt the source.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-400859982
https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-400859982:73,Usability,clear,cleared,73,@tom-dyar fyi - I received this boxed error today as well. It eventually cleared when I did an sbt clean and rebuilt the source.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-400859982
https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673:416,Availability,error,errors,416,"Hey Patrick, I just ran a tiny test and was able to confirm jobs getting aborted. ; - How many jobs were started from your workflow, and did any of the jobs from your workflow abort?; - Do you have a general sense at the stage your jobs were on when they were aborted? Were they all mostly executing the command when you aborted them? ; - Did Cromwell ever report the workflow to have been successfully Aborted? Any errors thrown in the server logs?. Would you mind posting the operation metadata from one of the jobs that you tried aborting using the rest endpoint? Or simply the events reported for that operation?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673
https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673:73,Safety,abort,aborted,73,"Hey Patrick, I just ran a tiny test and was able to confirm jobs getting aborted. ; - How many jobs were started from your workflow, and did any of the jobs from your workflow abort?; - Do you have a general sense at the stage your jobs were on when they were aborted? Were they all mostly executing the command when you aborted them? ; - Did Cromwell ever report the workflow to have been successfully Aborted? Any errors thrown in the server logs?. Would you mind posting the operation metadata from one of the jobs that you tried aborting using the rest endpoint? Or simply the events reported for that operation?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673
https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673:176,Safety,abort,abort,176,"Hey Patrick, I just ran a tiny test and was able to confirm jobs getting aborted. ; - How many jobs were started from your workflow, and did any of the jobs from your workflow abort?; - Do you have a general sense at the stage your jobs were on when they were aborted? Were they all mostly executing the command when you aborted them? ; - Did Cromwell ever report the workflow to have been successfully Aborted? Any errors thrown in the server logs?. Would you mind posting the operation metadata from one of the jobs that you tried aborting using the rest endpoint? Or simply the events reported for that operation?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673
https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673:260,Safety,abort,aborted,260,"Hey Patrick, I just ran a tiny test and was able to confirm jobs getting aborted. ; - How many jobs were started from your workflow, and did any of the jobs from your workflow abort?; - Do you have a general sense at the stage your jobs were on when they were aborted? Were they all mostly executing the command when you aborted them? ; - Did Cromwell ever report the workflow to have been successfully Aborted? Any errors thrown in the server logs?. Would you mind posting the operation metadata from one of the jobs that you tried aborting using the rest endpoint? Or simply the events reported for that operation?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673
https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673:321,Safety,abort,aborted,321,"Hey Patrick, I just ran a tiny test and was able to confirm jobs getting aborted. ; - How many jobs were started from your workflow, and did any of the jobs from your workflow abort?; - Do you have a general sense at the stage your jobs were on when they were aborted? Were they all mostly executing the command when you aborted them? ; - Did Cromwell ever report the workflow to have been successfully Aborted? Any errors thrown in the server logs?. Would you mind posting the operation metadata from one of the jobs that you tried aborting using the rest endpoint? Or simply the events reported for that operation?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673
https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673:533,Safety,abort,aborting,533,"Hey Patrick, I just ran a tiny test and was able to confirm jobs getting aborted. ; - How many jobs were started from your workflow, and did any of the jobs from your workflow abort?; - Do you have a general sense at the stage your jobs were on when they were aborted? Were they all mostly executing the command when you aborted them? ; - Did Cromwell ever report the workflow to have been successfully Aborted? Any errors thrown in the server logs?. Would you mind posting the operation metadata from one of the jobs that you tried aborting using the rest endpoint? Or simply the events reported for that operation?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673
https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673:31,Testability,test,test,31,"Hey Patrick, I just ran a tiny test and was able to confirm jobs getting aborted. ; - How many jobs were started from your workflow, and did any of the jobs from your workflow abort?; - Do you have a general sense at the stage your jobs were on when they were aborted? Were they all mostly executing the command when you aborted them? ; - Did Cromwell ever report the workflow to have been successfully Aborted? Any errors thrown in the server logs?. Would you mind posting the operation metadata from one of the jobs that you tried aborting using the rest endpoint? Or simply the events reported for that operation?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673
https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673:444,Testability,log,logs,444,"Hey Patrick, I just ran a tiny test and was able to confirm jobs getting aborted. ; - How many jobs were started from your workflow, and did any of the jobs from your workflow abort?; - Do you have a general sense at the stage your jobs were on when they were aborted? Were they all mostly executing the command when you aborted them? ; - Did Cromwell ever report the workflow to have been successfully Aborted? Any errors thrown in the server logs?. Would you mind posting the operation metadata from one of the jobs that you tried aborting using the rest endpoint? Or simply the events reported for that operation?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673
https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673:570,Usability,simpl,simply,570,"Hey Patrick, I just ran a tiny test and was able to confirm jobs getting aborted. ; - How many jobs were started from your workflow, and did any of the jobs from your workflow abort?; - Do you have a general sense at the stage your jobs were on when they were aborted? Were they all mostly executing the command when you aborted them? ; - Did Cromwell ever report the workflow to have been successfully Aborted? Any errors thrown in the server logs?. Would you mind posting the operation metadata from one of the jobs that you tried aborting using the rest endpoint? Or simply the events reported for that operation?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673
https://github.com/broadinstitute/cromwell/issues/3760#issuecomment-398846677:97,Availability,echo,echo,97,"Staging input/output files from/to S3 is not yet implemented. If the command from the task is an echo that redirects STDOUT to a file (e.g. `echo 'hello world' > output`) this will fail when Cromwell tries to retrieve the file from S3. . If the command is a simple echo (e.g. `echo ""hello world"") the STDOUT is retrieved from CloudWatch Logs and the process should succeed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760#issuecomment-398846677
https://github.com/broadinstitute/cromwell/issues/3760#issuecomment-398846677:141,Availability,echo,echo,141,"Staging input/output files from/to S3 is not yet implemented. If the command from the task is an echo that redirects STDOUT to a file (e.g. `echo 'hello world' > output`) this will fail when Cromwell tries to retrieve the file from S3. . If the command is a simple echo (e.g. `echo ""hello world"") the STDOUT is retrieved from CloudWatch Logs and the process should succeed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760#issuecomment-398846677
https://github.com/broadinstitute/cromwell/issues/3760#issuecomment-398846677:265,Availability,echo,echo,265,"Staging input/output files from/to S3 is not yet implemented. If the command from the task is an echo that redirects STDOUT to a file (e.g. `echo 'hello world' > output`) this will fail when Cromwell tries to retrieve the file from S3. . If the command is a simple echo (e.g. `echo ""hello world"") the STDOUT is retrieved from CloudWatch Logs and the process should succeed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760#issuecomment-398846677
https://github.com/broadinstitute/cromwell/issues/3760#issuecomment-398846677:277,Availability,echo,echo,277,"Staging input/output files from/to S3 is not yet implemented. If the command from the task is an echo that redirects STDOUT to a file (e.g. `echo 'hello world' > output`) this will fail when Cromwell tries to retrieve the file from S3. . If the command is a simple echo (e.g. `echo ""hello world"") the STDOUT is retrieved from CloudWatch Logs and the process should succeed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760#issuecomment-398846677
https://github.com/broadinstitute/cromwell/issues/3760#issuecomment-398846677:258,Usability,simpl,simple,258,"Staging input/output files from/to S3 is not yet implemented. If the command from the task is an echo that redirects STDOUT to a file (e.g. `echo 'hello world' > output`) this will fail when Cromwell tries to retrieve the file from S3. . If the command is a simple echo (e.g. `echo ""hello world"") the STDOUT is retrieved from CloudWatch Logs and the process should succeed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760#issuecomment-398846677
https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111:108,Availability,avail,available,108,"@cjllanwarne: I also encountered this issue. Until the mentioned upgrade script is released, is information available that highlights the changes necessary to migrate from draft 2 (or 3/1) to WDL 1.0? My files are in draft-2 format. Any sort of guidance about what's different between the versions would be helpful. Doing a visual diff of the `SPEC.md` files isn't ideal... Somewhat related: Is there an estimate of when womtool will have `-imports` exposed as a parameter?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111
https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111:65,Deployability,upgrade,upgrade,65,"@cjllanwarne: I also encountered this issue. Until the mentioned upgrade script is released, is information available that highlights the changes necessary to migrate from draft 2 (or 3/1) to WDL 1.0? My files are in draft-2 format. Any sort of guidance about what's different between the versions would be helpful. Doing a visual diff of the `SPEC.md` files isn't ideal... Somewhat related: Is there an estimate of when womtool will have `-imports` exposed as a parameter?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111
https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111:83,Deployability,release,released,83,"@cjllanwarne: I also encountered this issue. Until the mentioned upgrade script is released, is information available that highlights the changes necessary to migrate from draft 2 (or 3/1) to WDL 1.0? My files are in draft-2 format. Any sort of guidance about what's different between the versions would be helpful. Doing a visual diff of the `SPEC.md` files isn't ideal... Somewhat related: Is there an estimate of when womtool will have `-imports` exposed as a parameter?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111
https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111:450,Security,expose,exposed,450,"@cjllanwarne: I also encountered this issue. Until the mentioned upgrade script is released, is information available that highlights the changes necessary to migrate from draft 2 (or 3/1) to WDL 1.0? My files are in draft-2 format. Any sort of guidance about what's different between the versions would be helpful. Doing a visual diff of the `SPEC.md` files isn't ideal... Somewhat related: Is there an estimate of when womtool will have `-imports` exposed as a parameter?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111
https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111:245,Usability,guid,guidance,245,"@cjllanwarne: I also encountered this issue. Until the mentioned upgrade script is released, is information available that highlights the changes necessary to migrate from draft 2 (or 3/1) to WDL 1.0? My files are in draft-2 format. Any sort of guidance about what's different between the versions would be helpful. Doing a visual diff of the `SPEC.md` files isn't ideal... Somewhat related: Is there an estimate of when womtool will have `-imports` exposed as a parameter?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762#issuecomment-399565111
https://github.com/broadinstitute/cromwell/issues/3774#issuecomment-399490552:90,Usability,clear,clear,90,"This is a duplicate of #3804. I'll keep both open for the time being - that issue is more clear about what is going on at a high level, but this has good technical detail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774#issuecomment-399490552
https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194:2743,Deployability,update,update,2743,"of having multiple processes access the same embedded DB, is to research spinning up a background daemon db process, which do support multiple connections. Links to consider when defining acceptance criteria are below. . Re: our existing/proposed HSQLDB usage; - Cromwell's `database.metadata` and `database.engine` when absent both [fall back to the root `database` stanza.](https://github.com/broadinstitute/cromwell/blob/088e12d97dd18f463e6a387a6ffb002d9725cbe4/services/src/main/scala/cromwell/services/ServicesStore.scala#L12); - [""This allows each instance of a database object to use a clean, and different, in memory database.""](https://github.com/broadinstitute/cromwell/blob/a8a605ed1f2f2d2de2db9b05c395a2c87ebfc295/database/sql/src/main/scala/cromwell/database/sql/SqlDatabase.scala#L17-L39); - [""only one Java process at a time can make in-process connections to a given _file:_ database""](http://hsqldb.org/doc/guide/running-chapt.html#rgc_inprocess); - [""Several different programs can connect to the server and retrieve or update information.""](http://hsqldb.org/doc/2.0/guide/running-chapt.html#rgc_server_modes). Re: SQLite; - [""You can't add a constraint to existing table in SQLite""](https://stackoverflow.com/a/15498225); - [""use caution: this locking mechanism might not work correctly if the database file is kept on an NFS filesystem""](https://www.sqlite.org/faq.html#q5); - We often change our table uniqueness using [Liquibase](https://github.com/broadinstitute/cromwell/pull/3553/files#diff-76feec217bb5aaed111d4c3c11ead546). HSQLDB stopped supporting [unique indexes years ago](http://www.hsqldb.org/doc/1.8/guide/ch09.html#create_index-section), so Cromwell only uses their cousin the [unique constraint](https://community.oracle.com/thread/1033157). Meanwhile SQLite does not allow adding/dropping a unique constraint without [copying the whole table](https://stackoverflow.com/a/42013422) but does support [adding/dropping a unique index](https://www.sqlite.org/lang_crea",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194
https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194:125,Integrability,depend,depending,125,"Something like this will be great for users who just want something simple. We may need to add warnings to the Cromwell docs depending on how this ticket is implemented. The specific behavior this ticket aims to emulate / implement should be further refined with respect to HSQLDB. Plugging in `file:` will absolutely work for ""hello world"". But if one runs cromwell(s) the wrong way the db may become corrupted/deadlocked negating the ability to call-cache. Many databases have minimal to no support for sharing an embedded instance between concurrent procs. SQLite has the most ""support"" afaik but a) would require _a lot_ of custom Cromwell code, and b) still has other issues such as in NFS environments. Depending on whomever this ticket is aimed at, if they're using an HPC environment like our methods users do we'd have to be careful not to store a multiprocess embedded DB on NFS. Today with HSQLDB `mem:` cromwell uses a pair of ephemeral database connection pools. I'm not sure the behavior if both pools are pointed at the same HSQLDB `file:`, but I think it might work as the docs only warn of connecting from multi-process not multi-pool. The default config mentioned in this ticket may still consider using separate `file:` instances just in case. All issues above have workarounds with varying degrees of difficulty and/or documentation warnings. For example one could clarify the documentation with ""Cromwell only supports one instance connecting to the pair of default _file:_ databases at a time."" Or: ""Cromwell only supports call caching when running a workflow with the same name"" because we did something like generate the db file based on the workflow name. Another option, instead of having multiple processes access the same embedded DB, is to research spinning up a background daemon db process, which do support multiple connections. Links to consider when defining acceptance criteria are below. . Re: our existing/proposed HSQLDB usage; - Cromwell's `database.metadata` an",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194
https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194:1165,Modifiability,config,config,1165,"s to emulate / implement should be further refined with respect to HSQLDB. Plugging in `file:` will absolutely work for ""hello world"". But if one runs cromwell(s) the wrong way the db may become corrupted/deadlocked negating the ability to call-cache. Many databases have minimal to no support for sharing an embedded instance between concurrent procs. SQLite has the most ""support"" afaik but a) would require _a lot_ of custom Cromwell code, and b) still has other issues such as in NFS environments. Depending on whomever this ticket is aimed at, if they're using an HPC environment like our methods users do we'd have to be careful not to store a multiprocess embedded DB on NFS. Today with HSQLDB `mem:` cromwell uses a pair of ephemeral database connection pools. I'm not sure the behavior if both pools are pointed at the same HSQLDB `file:`, but I think it might work as the docs only warn of connecting from multi-process not multi-pool. The default config mentioned in this ticket may still consider using separate `file:` instances just in case. All issues above have workarounds with varying degrees of difficulty and/or documentation warnings. For example one could clarify the documentation with ""Cromwell only supports one instance connecting to the pair of default _file:_ databases at a time."" Or: ""Cromwell only supports call caching when running a workflow with the same name"" because we did something like generate the db file based on the workflow name. Another option, instead of having multiple processes access the same embedded DB, is to research spinning up a background daemon db process, which do support multiple connections. Links to consider when defining acceptance criteria are below. . Re: our existing/proposed HSQLDB usage; - Cromwell's `database.metadata` and `database.engine` when absent both [fall back to the root `database` stanza.](https://github.com/broadinstitute/cromwell/blob/088e12d97dd18f463e6a387a6ffb002d9725cbe4/services/src/main/scala/cromwell/serv",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194
https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194:452,Performance,cache,cache,452,"Something like this will be great for users who just want something simple. We may need to add warnings to the Cromwell docs depending on how this ticket is implemented. The specific behavior this ticket aims to emulate / implement should be further refined with respect to HSQLDB. Plugging in `file:` will absolutely work for ""hello world"". But if one runs cromwell(s) the wrong way the db may become corrupted/deadlocked negating the ability to call-cache. Many databases have minimal to no support for sharing an embedded instance between concurrent procs. SQLite has the most ""support"" afaik but a) would require _a lot_ of custom Cromwell code, and b) still has other issues such as in NFS environments. Depending on whomever this ticket is aimed at, if they're using an HPC environment like our methods users do we'd have to be careful not to store a multiprocess embedded DB on NFS. Today with HSQLDB `mem:` cromwell uses a pair of ephemeral database connection pools. I'm not sure the behavior if both pools are pointed at the same HSQLDB `file:`, but I think it might work as the docs only warn of connecting from multi-process not multi-pool. The default config mentioned in this ticket may still consider using separate `file:` instances just in case. All issues above have workarounds with varying degrees of difficulty and/or documentation warnings. For example one could clarify the documentation with ""Cromwell only supports one instance connecting to the pair of default _file:_ databases at a time."" Or: ""Cromwell only supports call caching when running a workflow with the same name"" because we did something like generate the db file based on the workflow name. Another option, instead of having multiple processes access the same embedded DB, is to research spinning up a background daemon db process, which do support multiple connections. Links to consider when defining acceptance criteria are below. . Re: our existing/proposed HSQLDB usage; - Cromwell's `database.metadata` an",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194
https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194:542,Performance,concurren,concurrent,542,"Something like this will be great for users who just want something simple. We may need to add warnings to the Cromwell docs depending on how this ticket is implemented. The specific behavior this ticket aims to emulate / implement should be further refined with respect to HSQLDB. Plugging in `file:` will absolutely work for ""hello world"". But if one runs cromwell(s) the wrong way the db may become corrupted/deadlocked negating the ability to call-cache. Many databases have minimal to no support for sharing an embedded instance between concurrent procs. SQLite has the most ""support"" afaik but a) would require _a lot_ of custom Cromwell code, and b) still has other issues such as in NFS environments. Depending on whomever this ticket is aimed at, if they're using an HPC environment like our methods users do we'd have to be careful not to store a multiprocess embedded DB on NFS. Today with HSQLDB `mem:` cromwell uses a pair of ephemeral database connection pools. I'm not sure the behavior if both pools are pointed at the same HSQLDB `file:`, but I think it might work as the docs only warn of connecting from multi-process not multi-pool. The default config mentioned in this ticket may still consider using separate `file:` instances just in case. All issues above have workarounds with varying degrees of difficulty and/or documentation warnings. For example one could clarify the documentation with ""Cromwell only supports one instance connecting to the pair of default _file:_ databases at a time."" Or: ""Cromwell only supports call caching when running a workflow with the same name"" because we did something like generate the db file based on the workflow name. Another option, instead of having multiple processes access the same embedded DB, is to research spinning up a background daemon db process, which do support multiple connections. Links to consider when defining acceptance criteria are below. . Re: our existing/proposed HSQLDB usage; - Cromwell's `database.metadata` an",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194
https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194:1734,Security,access,access,1734,"ng an HPC environment like our methods users do we'd have to be careful not to store a multiprocess embedded DB on NFS. Today with HSQLDB `mem:` cromwell uses a pair of ephemeral database connection pools. I'm not sure the behavior if both pools are pointed at the same HSQLDB `file:`, but I think it might work as the docs only warn of connecting from multi-process not multi-pool. The default config mentioned in this ticket may still consider using separate `file:` instances just in case. All issues above have workarounds with varying degrees of difficulty and/or documentation warnings. For example one could clarify the documentation with ""Cromwell only supports one instance connecting to the pair of default _file:_ databases at a time."" Or: ""Cromwell only supports call caching when running a workflow with the same name"" because we did something like generate the db file based on the workflow name. Another option, instead of having multiple processes access the same embedded DB, is to research spinning up a background daemon db process, which do support multiple connections. Links to consider when defining acceptance criteria are below. . Re: our existing/proposed HSQLDB usage; - Cromwell's `database.metadata` and `database.engine` when absent both [fall back to the root `database` stanza.](https://github.com/broadinstitute/cromwell/blob/088e12d97dd18f463e6a387a6ffb002d9725cbe4/services/src/main/scala/cromwell/services/ServicesStore.scala#L12); - [""This allows each instance of a database object to use a clean, and different, in memory database.""](https://github.com/broadinstitute/cromwell/blob/a8a605ed1f2f2d2de2db9b05c395a2c87ebfc295/database/sql/src/main/scala/cromwell/database/sql/SqlDatabase.scala#L17-L39); - [""only one Java process at a time can make in-process connections to a given _file:_ database""](http://hsqldb.org/doc/guide/running-chapt.html#rgc_inprocess); - [""Several different programs can connect to the server and retrieve or update information.""](http:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194
https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194:68,Usability,simpl,simple,68,"Something like this will be great for users who just want something simple. We may need to add warnings to the Cromwell docs depending on how this ticket is implemented. The specific behavior this ticket aims to emulate / implement should be further refined with respect to HSQLDB. Plugging in `file:` will absolutely work for ""hello world"". But if one runs cromwell(s) the wrong way the db may become corrupted/deadlocked negating the ability to call-cache. Many databases have minimal to no support for sharing an embedded instance between concurrent procs. SQLite has the most ""support"" afaik but a) would require _a lot_ of custom Cromwell code, and b) still has other issues such as in NFS environments. Depending on whomever this ticket is aimed at, if they're using an HPC environment like our methods users do we'd have to be careful not to store a multiprocess embedded DB on NFS. Today with HSQLDB `mem:` cromwell uses a pair of ephemeral database connection pools. I'm not sure the behavior if both pools are pointed at the same HSQLDB `file:`, but I think it might work as the docs only warn of connecting from multi-process not multi-pool. The default config mentioned in this ticket may still consider using separate `file:` instances just in case. All issues above have workarounds with varying degrees of difficulty and/or documentation warnings. For example one could clarify the documentation with ""Cromwell only supports one instance connecting to the pair of default _file:_ databases at a time."" Or: ""Cromwell only supports call caching when running a workflow with the same name"" because we did something like generate the db file based on the workflow name. Another option, instead of having multiple processes access the same embedded DB, is to research spinning up a background daemon db process, which do support multiple connections. Links to consider when defining acceptance criteria are below. . Re: our existing/proposed HSQLDB usage; - Cromwell's `database.metadata` an",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194
https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194:2629,Usability,guid,guide,2629,"erate the db file based on the workflow name. Another option, instead of having multiple processes access the same embedded DB, is to research spinning up a background daemon db process, which do support multiple connections. Links to consider when defining acceptance criteria are below. . Re: our existing/proposed HSQLDB usage; - Cromwell's `database.metadata` and `database.engine` when absent both [fall back to the root `database` stanza.](https://github.com/broadinstitute/cromwell/blob/088e12d97dd18f463e6a387a6ffb002d9725cbe4/services/src/main/scala/cromwell/services/ServicesStore.scala#L12); - [""This allows each instance of a database object to use a clean, and different, in memory database.""](https://github.com/broadinstitute/cromwell/blob/a8a605ed1f2f2d2de2db9b05c395a2c87ebfc295/database/sql/src/main/scala/cromwell/database/sql/SqlDatabase.scala#L17-L39); - [""only one Java process at a time can make in-process connections to a given _file:_ database""](http://hsqldb.org/doc/guide/running-chapt.html#rgc_inprocess); - [""Several different programs can connect to the server and retrieve or update information.""](http://hsqldb.org/doc/2.0/guide/running-chapt.html#rgc_server_modes). Re: SQLite; - [""You can't add a constraint to existing table in SQLite""](https://stackoverflow.com/a/15498225); - [""use caution: this locking mechanism might not work correctly if the database file is kept on an NFS filesystem""](https://www.sqlite.org/faq.html#q5); - We often change our table uniqueness using [Liquibase](https://github.com/broadinstitute/cromwell/pull/3553/files#diff-76feec217bb5aaed111d4c3c11ead546). HSQLDB stopped supporting [unique indexes years ago](http://www.hsqldb.org/doc/1.8/guide/ch09.html#create_index-section), so Cromwell only uses their cousin the [unique constraint](https://community.oracle.com/thread/1033157). Meanwhile SQLite does not allow adding/dropping a unique constraint without [copying the whole table](https://stackoverflow.com/a/42013422) but does su",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194
https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194:2791,Usability,guid,guide,2791,"nd daemon db process, which do support multiple connections. Links to consider when defining acceptance criteria are below. . Re: our existing/proposed HSQLDB usage; - Cromwell's `database.metadata` and `database.engine` when absent both [fall back to the root `database` stanza.](https://github.com/broadinstitute/cromwell/blob/088e12d97dd18f463e6a387a6ffb002d9725cbe4/services/src/main/scala/cromwell/services/ServicesStore.scala#L12); - [""This allows each instance of a database object to use a clean, and different, in memory database.""](https://github.com/broadinstitute/cromwell/blob/a8a605ed1f2f2d2de2db9b05c395a2c87ebfc295/database/sql/src/main/scala/cromwell/database/sql/SqlDatabase.scala#L17-L39); - [""only one Java process at a time can make in-process connections to a given _file:_ database""](http://hsqldb.org/doc/guide/running-chapt.html#rgc_inprocess); - [""Several different programs can connect to the server and retrieve or update information.""](http://hsqldb.org/doc/2.0/guide/running-chapt.html#rgc_server_modes). Re: SQLite; - [""You can't add a constraint to existing table in SQLite""](https://stackoverflow.com/a/15498225); - [""use caution: this locking mechanism might not work correctly if the database file is kept on an NFS filesystem""](https://www.sqlite.org/faq.html#q5); - We often change our table uniqueness using [Liquibase](https://github.com/broadinstitute/cromwell/pull/3553/files#diff-76feec217bb5aaed111d4c3c11ead546). HSQLDB stopped supporting [unique indexes years ago](http://www.hsqldb.org/doc/1.8/guide/ch09.html#create_index-section), so Cromwell only uses their cousin the [unique constraint](https://community.oracle.com/thread/1033157). Meanwhile SQLite does not allow adding/dropping a unique constraint without [copying the whole table](https://stackoverflow.com/a/42013422) but does support [adding/dropping a unique index](https://www.sqlite.org/lang_createindex.html).; - Additionally, SQLite will need a bunch of custom code for its [JDBC-lite imp",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194
https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194:3340,Usability,guid,guide,3340,".metadata` and `database.engine` when absent both [fall back to the root `database` stanza.](https://github.com/broadinstitute/cromwell/blob/088e12d97dd18f463e6a387a6ffb002d9725cbe4/services/src/main/scala/cromwell/services/ServicesStore.scala#L12); - [""This allows each instance of a database object to use a clean, and different, in memory database.""](https://github.com/broadinstitute/cromwell/blob/a8a605ed1f2f2d2de2db9b05c395a2c87ebfc295/database/sql/src/main/scala/cromwell/database/sql/SqlDatabase.scala#L17-L39); - [""only one Java process at a time can make in-process connections to a given _file:_ database""](http://hsqldb.org/doc/guide/running-chapt.html#rgc_inprocess); - [""Several different programs can connect to the server and retrieve or update information.""](http://hsqldb.org/doc/2.0/guide/running-chapt.html#rgc_server_modes). Re: SQLite; - [""You can't add a constraint to existing table in SQLite""](https://stackoverflow.com/a/15498225); - [""use caution: this locking mechanism might not work correctly if the database file is kept on an NFS filesystem""](https://www.sqlite.org/faq.html#q5); - We often change our table uniqueness using [Liquibase](https://github.com/broadinstitute/cromwell/pull/3553/files#diff-76feec217bb5aaed111d4c3c11ead546). HSQLDB stopped supporting [unique indexes years ago](http://www.hsqldb.org/doc/1.8/guide/ch09.html#create_index-section), so Cromwell only uses their cousin the [unique constraint](https://community.oracle.com/thread/1033157). Meanwhile SQLite does not allow adding/dropping a unique constraint without [copying the whole table](https://stackoverflow.com/a/42013422) but does support [adding/dropping a unique index](https://www.sqlite.org/lang_createindex.html).; - Additionally, SQLite will need a bunch of custom code for its [JDBC-lite implementation](https://stackoverflow.com/a/48643377). **TL;DR Be sure of the use case before verifying that ""hello world"" passes, and consider labeling any implementation as ""experimental""**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194
https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929:125,Availability,robust,robust,125,"@danbills I think I'm actually changing my mind and leaning towards doing the try/retry instead:; 1) It seems generally more robust to be able to fallback to that (compared to caching where if we can't get the info or we get it wrong we'd fail workflows); 2) Talking to @kshakir, things seem to be moving towards more generic implementations of filesystems. The retry logic could be lifted up to the generic implementation whenever it happens (which might be harder to do with a caching logic); 3) We can always add caching later if we see Cromwell struggling too much; 4) Unlike what I was thinking first, it actually simplifies the code a little and even more testing (testing that things get cached properly and for the right amount of time is a pain). Thoughts ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929
https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929:695,Performance,cache,cached,695,"@danbills I think I'm actually changing my mind and leaning towards doing the try/retry instead:; 1) It seems generally more robust to be able to fallback to that (compared to caching where if we can't get the info or we get it wrong we'd fail workflows); 2) Talking to @kshakir, things seem to be moving towards more generic implementations of filesystems. The retry logic could be lifted up to the generic implementation whenever it happens (which might be harder to do with a caching logic); 3) We can always add caching later if we see Cromwell struggling too much; 4) Unlike what I was thinking first, it actually simplifies the code a little and even more testing (testing that things get cached properly and for the right amount of time is a pain). Thoughts ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929
https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929:368,Testability,log,logic,368,"@danbills I think I'm actually changing my mind and leaning towards doing the try/retry instead:; 1) It seems generally more robust to be able to fallback to that (compared to caching where if we can't get the info or we get it wrong we'd fail workflows); 2) Talking to @kshakir, things seem to be moving towards more generic implementations of filesystems. The retry logic could be lifted up to the generic implementation whenever it happens (which might be harder to do with a caching logic); 3) We can always add caching later if we see Cromwell struggling too much; 4) Unlike what I was thinking first, it actually simplifies the code a little and even more testing (testing that things get cached properly and for the right amount of time is a pain). Thoughts ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929
https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929:487,Testability,log,logic,487,"@danbills I think I'm actually changing my mind and leaning towards doing the try/retry instead:; 1) It seems generally more robust to be able to fallback to that (compared to caching where if we can't get the info or we get it wrong we'd fail workflows); 2) Talking to @kshakir, things seem to be moving towards more generic implementations of filesystems. The retry logic could be lifted up to the generic implementation whenever it happens (which might be harder to do with a caching logic); 3) We can always add caching later if we see Cromwell struggling too much; 4) Unlike what I was thinking first, it actually simplifies the code a little and even more testing (testing that things get cached properly and for the right amount of time is a pain). Thoughts ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929
https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929:662,Testability,test,testing,662,"@danbills I think I'm actually changing my mind and leaning towards doing the try/retry instead:; 1) It seems generally more robust to be able to fallback to that (compared to caching where if we can't get the info or we get it wrong we'd fail workflows); 2) Talking to @kshakir, things seem to be moving towards more generic implementations of filesystems. The retry logic could be lifted up to the generic implementation whenever it happens (which might be harder to do with a caching logic); 3) We can always add caching later if we see Cromwell struggling too much; 4) Unlike what I was thinking first, it actually simplifies the code a little and even more testing (testing that things get cached properly and for the right amount of time is a pain). Thoughts ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929
https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929:671,Testability,test,testing,671,"@danbills I think I'm actually changing my mind and leaning towards doing the try/retry instead:; 1) It seems generally more robust to be able to fallback to that (compared to caching where if we can't get the info or we get it wrong we'd fail workflows); 2) Talking to @kshakir, things seem to be moving towards more generic implementations of filesystems. The retry logic could be lifted up to the generic implementation whenever it happens (which might be harder to do with a caching logic); 3) We can always add caching later if we see Cromwell struggling too much; 4) Unlike what I was thinking first, it actually simplifies the code a little and even more testing (testing that things get cached properly and for the right amount of time is a pain). Thoughts ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929
https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929:619,Usability,simpl,simplifies,619,"@danbills I think I'm actually changing my mind and leaning towards doing the try/retry instead:; 1) It seems generally more robust to be able to fallback to that (compared to caching where if we can't get the info or we get it wrong we'd fail workflows); 2) Talking to @kshakir, things seem to be moving towards more generic implementations of filesystems. The retry logic could be lifted up to the generic implementation whenever it happens (which might be harder to do with a caching logic); 3) We can always add caching later if we see Cromwell struggling too much; 4) Unlike what I was thinking first, it actually simplifies the code a little and even more testing (testing that things get cached properly and for the right amount of time is a pain). Thoughts ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401420929
https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401425590:15,Usability,simpl,simpler,15,* It does seem simpler; * not 100% sure how it plays into generic impl; * I think gsutil is getting smarter soon per the hint we got,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401425590
https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068:577,Deployability,configurat,configuration,577,"So I think I'm now clear regarding the options on this thread. One option, and the original one discussed, is a container entrypoint (""Orchestrator"" as @danbills put it). Another option, which is what I originally thought was being discussed, is an always-running sidecar. That entrypoint container would then launch our target container, similar to something like https://ohsu-comp-bio.github.io/funnel/ or https://github.com/delagoya/batch-task-runner. IMHO, the entrypoint/Orchestrator introduces some unnecessary complexity. It works for a simple case, but leaves a lot of configuration as TBD. To implement this properly we'd need to implement a standard task definition for the entrypoint (or Orchestrator) container (simple), but also pass to that container all the necessary docker parameters necessary for implementing the target container (much more difficult). There is also the question of supervision for the target container as the entrypoint/Orchestrator is hiding from batch and Cromwell the actual task's status. Also, the permissions issues I brought up in my comment above apply. I also have some concerns about the implementation of this approach within Cromwell, as I believe (but I'm not certain), this scheme would require some non-trivial changes to the StandardAsyncExecutionActor class as well as the AwsBatchAsyncExecutionActor logic (likely moreso on the later). I haven't investigated, but I also have some concerns over whether this mechanism would require significant rework of the process input/output and return code. It looks like @delagoya may have considered this in the batch-task-runner repo, but I'm not clear on this after the limited time I've had reviewing the repo. So, with that in mind, I spent a bit of time researching the ""always on sidecar"" approach, which I'll reference as ""cromwell agent"" moving forward for clarity. I took a look at the limitations of the permissions issue I mentioned above, and I believe I have a workable solution. The high leve",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068
https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068:3113,Deployability,configurat,configuration,3113,"aven't investigated, but I also have some concerns over whether this mechanism would require significant rework of the process input/output and return code. It looks like @delagoya may have considered this in the batch-task-runner repo, but I'm not clear on this after the limited time I've had reviewing the repo. So, with that in mind, I spent a bit of time researching the ""always on sidecar"" approach, which I'll reference as ""cromwell agent"" moving forward for clarity. I took a look at the limitations of the permissions issue I mentioned above, and I believe I have a workable solution. The high level process would work like this:. 0. Each host runs the cromwell agent container similar to the way the ecs-agent operates today; 1. The cromwell agent container listens to the system events as described above; 2. When a cromwell task is started, the cromwell agent container will pause the target container (cromwell task) immediately; 3. It can then inspect the container and use the ECS ""AWS_CONTAINER_CREDENTIALS_RELATIVE_URI"" in conjunction with the ecs-agent container credentials endpoint at 169.254.170.2 to fetch the **target container credentials**; 4. Using the target container credentials, we can localize inputs, then unpause the target container; 5. Upon completion of the task (we should see this from the system events stream), we can then delocalize outputs. This process feels workable with the following advantages:. * Minimal changes to the Cromwell code base/the agent can be developed and maintained separately; * Per-task IAM roles; * No changes needed to Cromwell task definitions or containers; * Cromwell task supervision stays within AWS Batch and Cromwell; * AWS Batch Job Definition configuration does not have to pass through an intermediary. Right now I have a POC to accomplish through step 3. Unless there are objections I'm going to continue to prototype to ensure 4 and 5 work as expected, then we can make a final call either on this thread or in a meeting.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068
https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068:577,Modifiability,config,configuration,577,"So I think I'm now clear regarding the options on this thread. One option, and the original one discussed, is a container entrypoint (""Orchestrator"" as @danbills put it). Another option, which is what I originally thought was being discussed, is an always-running sidecar. That entrypoint container would then launch our target container, similar to something like https://ohsu-comp-bio.github.io/funnel/ or https://github.com/delagoya/batch-task-runner. IMHO, the entrypoint/Orchestrator introduces some unnecessary complexity. It works for a simple case, but leaves a lot of configuration as TBD. To implement this properly we'd need to implement a standard task definition for the entrypoint (or Orchestrator) container (simple), but also pass to that container all the necessary docker parameters necessary for implementing the target container (much more difficult). There is also the question of supervision for the target container as the entrypoint/Orchestrator is hiding from batch and Cromwell the actual task's status. Also, the permissions issues I brought up in my comment above apply. I also have some concerns about the implementation of this approach within Cromwell, as I believe (but I'm not certain), this scheme would require some non-trivial changes to the StandardAsyncExecutionActor class as well as the AwsBatchAsyncExecutionActor logic (likely moreso on the later). I haven't investigated, but I also have some concerns over whether this mechanism would require significant rework of the process input/output and return code. It looks like @delagoya may have considered this in the batch-task-runner repo, but I'm not clear on this after the limited time I've had reviewing the repo. So, with that in mind, I spent a bit of time researching the ""always on sidecar"" approach, which I'll reference as ""cromwell agent"" moving forward for clarity. I took a look at the limitations of the permissions issue I mentioned above, and I believe I have a workable solution. The high leve",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068
https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068:3113,Modifiability,config,configuration,3113,"aven't investigated, but I also have some concerns over whether this mechanism would require significant rework of the process input/output and return code. It looks like @delagoya may have considered this in the batch-task-runner repo, but I'm not clear on this after the limited time I've had reviewing the repo. So, with that in mind, I spent a bit of time researching the ""always on sidecar"" approach, which I'll reference as ""cromwell agent"" moving forward for clarity. I took a look at the limitations of the permissions issue I mentioned above, and I believe I have a workable solution. The high level process would work like this:. 0. Each host runs the cromwell agent container similar to the way the ecs-agent operates today; 1. The cromwell agent container listens to the system events as described above; 2. When a cromwell task is started, the cromwell agent container will pause the target container (cromwell task) immediately; 3. It can then inspect the container and use the ECS ""AWS_CONTAINER_CREDENTIALS_RELATIVE_URI"" in conjunction with the ecs-agent container credentials endpoint at 169.254.170.2 to fetch the **target container credentials**; 4. Using the target container credentials, we can localize inputs, then unpause the target container; 5. Upon completion of the task (we should see this from the system events stream), we can then delocalize outputs. This process feels workable with the following advantages:. * Minimal changes to the Cromwell code base/the agent can be developed and maintained separately; * Per-task IAM roles; * No changes needed to Cromwell task definitions or containers; * Cromwell task supervision stays within AWS Batch and Cromwell; * AWS Batch Job Definition configuration does not have to pass through an intermediary. Right now I have a POC to accomplish through step 3. Unless there are objections I'm going to continue to prototype to ensure 4 and 5 work as expected, then we can make a final call either on this thread or in a meeting.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068
https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068:1355,Testability,log,logic,1355,"s an always-running sidecar. That entrypoint container would then launch our target container, similar to something like https://ohsu-comp-bio.github.io/funnel/ or https://github.com/delagoya/batch-task-runner. IMHO, the entrypoint/Orchestrator introduces some unnecessary complexity. It works for a simple case, but leaves a lot of configuration as TBD. To implement this properly we'd need to implement a standard task definition for the entrypoint (or Orchestrator) container (simple), but also pass to that container all the necessary docker parameters necessary for implementing the target container (much more difficult). There is also the question of supervision for the target container as the entrypoint/Orchestrator is hiding from batch and Cromwell the actual task's status. Also, the permissions issues I brought up in my comment above apply. I also have some concerns about the implementation of this approach within Cromwell, as I believe (but I'm not certain), this scheme would require some non-trivial changes to the StandardAsyncExecutionActor class as well as the AwsBatchAsyncExecutionActor logic (likely moreso on the later). I haven't investigated, but I also have some concerns over whether this mechanism would require significant rework of the process input/output and return code. It looks like @delagoya may have considered this in the batch-task-runner repo, but I'm not clear on this after the limited time I've had reviewing the repo. So, with that in mind, I spent a bit of time researching the ""always on sidecar"" approach, which I'll reference as ""cromwell agent"" moving forward for clarity. I took a look at the limitations of the permissions issue I mentioned above, and I believe I have a workable solution. The high level process would work like this:. 0. Each host runs the cromwell agent container similar to the way the ecs-agent operates today; 1. The cromwell agent container listens to the system events as described above; 2. When a cromwell task is starte",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068
https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068:19,Usability,clear,clear,19,"So I think I'm now clear regarding the options on this thread. One option, and the original one discussed, is a container entrypoint (""Orchestrator"" as @danbills put it). Another option, which is what I originally thought was being discussed, is an always-running sidecar. That entrypoint container would then launch our target container, similar to something like https://ohsu-comp-bio.github.io/funnel/ or https://github.com/delagoya/batch-task-runner. IMHO, the entrypoint/Orchestrator introduces some unnecessary complexity. It works for a simple case, but leaves a lot of configuration as TBD. To implement this properly we'd need to implement a standard task definition for the entrypoint (or Orchestrator) container (simple), but also pass to that container all the necessary docker parameters necessary for implementing the target container (much more difficult). There is also the question of supervision for the target container as the entrypoint/Orchestrator is hiding from batch and Cromwell the actual task's status. Also, the permissions issues I brought up in my comment above apply. I also have some concerns about the implementation of this approach within Cromwell, as I believe (but I'm not certain), this scheme would require some non-trivial changes to the StandardAsyncExecutionActor class as well as the AwsBatchAsyncExecutionActor logic (likely moreso on the later). I haven't investigated, but I also have some concerns over whether this mechanism would require significant rework of the process input/output and return code. It looks like @delagoya may have considered this in the batch-task-runner repo, but I'm not clear on this after the limited time I've had reviewing the repo. So, with that in mind, I spent a bit of time researching the ""always on sidecar"" approach, which I'll reference as ""cromwell agent"" moving forward for clarity. I took a look at the limitations of the permissions issue I mentioned above, and I believe I have a workable solution. The high leve",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068
https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068:544,Usability,simpl,simple,544,"So I think I'm now clear regarding the options on this thread. One option, and the original one discussed, is a container entrypoint (""Orchestrator"" as @danbills put it). Another option, which is what I originally thought was being discussed, is an always-running sidecar. That entrypoint container would then launch our target container, similar to something like https://ohsu-comp-bio.github.io/funnel/ or https://github.com/delagoya/batch-task-runner. IMHO, the entrypoint/Orchestrator introduces some unnecessary complexity. It works for a simple case, but leaves a lot of configuration as TBD. To implement this properly we'd need to implement a standard task definition for the entrypoint (or Orchestrator) container (simple), but also pass to that container all the necessary docker parameters necessary for implementing the target container (much more difficult). There is also the question of supervision for the target container as the entrypoint/Orchestrator is hiding from batch and Cromwell the actual task's status. Also, the permissions issues I brought up in my comment above apply. I also have some concerns about the implementation of this approach within Cromwell, as I believe (but I'm not certain), this scheme would require some non-trivial changes to the StandardAsyncExecutionActor class as well as the AwsBatchAsyncExecutionActor logic (likely moreso on the later). I haven't investigated, but I also have some concerns over whether this mechanism would require significant rework of the process input/output and return code. It looks like @delagoya may have considered this in the batch-task-runner repo, but I'm not clear on this after the limited time I've had reviewing the repo. So, with that in mind, I spent a bit of time researching the ""always on sidecar"" approach, which I'll reference as ""cromwell agent"" moving forward for clarity. I took a look at the limitations of the permissions issue I mentioned above, and I believe I have a workable solution. The high leve",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068
https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068:724,Usability,simpl,simple,724,"So I think I'm now clear regarding the options on this thread. One option, and the original one discussed, is a container entrypoint (""Orchestrator"" as @danbills put it). Another option, which is what I originally thought was being discussed, is an always-running sidecar. That entrypoint container would then launch our target container, similar to something like https://ohsu-comp-bio.github.io/funnel/ or https://github.com/delagoya/batch-task-runner. IMHO, the entrypoint/Orchestrator introduces some unnecessary complexity. It works for a simple case, but leaves a lot of configuration as TBD. To implement this properly we'd need to implement a standard task definition for the entrypoint (or Orchestrator) container (simple), but also pass to that container all the necessary docker parameters necessary for implementing the target container (much more difficult). There is also the question of supervision for the target container as the entrypoint/Orchestrator is hiding from batch and Cromwell the actual task's status. Also, the permissions issues I brought up in my comment above apply. I also have some concerns about the implementation of this approach within Cromwell, as I believe (but I'm not certain), this scheme would require some non-trivial changes to the StandardAsyncExecutionActor class as well as the AwsBatchAsyncExecutionActor logic (likely moreso on the later). I haven't investigated, but I also have some concerns over whether this mechanism would require significant rework of the process input/output and return code. It looks like @delagoya may have considered this in the batch-task-runner repo, but I'm not clear on this after the limited time I've had reviewing the repo. So, with that in mind, I spent a bit of time researching the ""always on sidecar"" approach, which I'll reference as ""cromwell agent"" moving forward for clarity. I took a look at the limitations of the permissions issue I mentioned above, and I believe I have a workable solution. The high leve",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068
https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068:1643,Usability,clear,clear,1643,"d need to implement a standard task definition for the entrypoint (or Orchestrator) container (simple), but also pass to that container all the necessary docker parameters necessary for implementing the target container (much more difficult). There is also the question of supervision for the target container as the entrypoint/Orchestrator is hiding from batch and Cromwell the actual task's status. Also, the permissions issues I brought up in my comment above apply. I also have some concerns about the implementation of this approach within Cromwell, as I believe (but I'm not certain), this scheme would require some non-trivial changes to the StandardAsyncExecutionActor class as well as the AwsBatchAsyncExecutionActor logic (likely moreso on the later). I haven't investigated, but I also have some concerns over whether this mechanism would require significant rework of the process input/output and return code. It looks like @delagoya may have considered this in the batch-task-runner repo, but I'm not clear on this after the limited time I've had reviewing the repo. So, with that in mind, I spent a bit of time researching the ""always on sidecar"" approach, which I'll reference as ""cromwell agent"" moving forward for clarity. I took a look at the limitations of the permissions issue I mentioned above, and I believe I have a workable solution. The high level process would work like this:. 0. Each host runs the cromwell agent container similar to the way the ecs-agent operates today; 1. The cromwell agent container listens to the system events as described above; 2. When a cromwell task is started, the cromwell agent container will pause the target container (cromwell task) immediately; 3. It can then inspect the container and use the ECS ""AWS_CONTAINER_CREDENTIALS_RELATIVE_URI"" in conjunction with the ecs-agent container credentials endpoint at 169.254.170.2 to fetch the **target container credentials**; 4. Using the target container credentials, we can localize inputs, th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068
https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068:2281,Usability,pause,pause,2281,"e StandardAsyncExecutionActor class as well as the AwsBatchAsyncExecutionActor logic (likely moreso on the later). I haven't investigated, but I also have some concerns over whether this mechanism would require significant rework of the process input/output and return code. It looks like @delagoya may have considered this in the batch-task-runner repo, but I'm not clear on this after the limited time I've had reviewing the repo. So, with that in mind, I spent a bit of time researching the ""always on sidecar"" approach, which I'll reference as ""cromwell agent"" moving forward for clarity. I took a look at the limitations of the permissions issue I mentioned above, and I believe I have a workable solution. The high level process would work like this:. 0. Each host runs the cromwell agent container similar to the way the ecs-agent operates today; 1. The cromwell agent container listens to the system events as described above; 2. When a cromwell task is started, the cromwell agent container will pause the target container (cromwell task) immediately; 3. It can then inspect the container and use the ECS ""AWS_CONTAINER_CREDENTIALS_RELATIVE_URI"" in conjunction with the ecs-agent container credentials endpoint at 169.254.170.2 to fetch the **target container credentials**; 4. Using the target container credentials, we can localize inputs, then unpause the target container; 5. Upon completion of the task (we should see this from the system events stream), we can then delocalize outputs. This process feels workable with the following advantages:. * Minimal changes to the Cromwell code base/the agent can be developed and maintained separately; * Per-task IAM roles; * No changes needed to Cromwell task definitions or containers; * Cromwell task supervision stays within AWS Batch and Cromwell; * AWS Batch Job Definition configuration does not have to pass through an intermediary. Right now I have a POC to accomplish through step 3. Unless there are objections I'm going to continue",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068
https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803:135,Availability,error,error,135,"I think the cache is unrelated, this is purely input localisation. I re-ran the job with caching disabled in the config file. The same error occurs. From this directory: /share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/37e4e046-b256-4f81-95c6-9f0c915810bf/call-SamToFastqAndBwaMem/inputs/-21323395 . There is a file 'cromwell.tmp' which seems to be a partial copy of my cromwell process' CWD. All of the logs are copied in, (cromwell.tmp/cromwell-workflow-logs/) and a single seemingly unrelated job (cromwell.tmp/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/). All of that jobs 'execution' folder, and some of its 'inputs' are copied. It is not clear if more would have been copied in or if the process was ended by the soft link error mentioned above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803
https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803:846,Availability,error,error,846,"I think the cache is unrelated, this is purely input localisation. I re-ran the job with caching disabled in the config file. The same error occurs. From this directory: /share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/37e4e046-b256-4f81-95c6-9f0c915810bf/call-SamToFastqAndBwaMem/inputs/-21323395 . There is a file 'cromwell.tmp' which seems to be a partial copy of my cromwell process' CWD. All of the logs are copied in, (cromwell.tmp/cromwell-workflow-logs/) and a single seemingly unrelated job (cromwell.tmp/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/). All of that jobs 'execution' folder, and some of its 'inputs' are copied. It is not clear if more would have been copied in or if the process was ended by the soft link error mentioned above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803
https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803:113,Modifiability,config,config,113,"I think the cache is unrelated, this is purely input localisation. I re-ran the job with caching disabled in the config file. The same error occurs. From this directory: /share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/37e4e046-b256-4f81-95c6-9f0c915810bf/call-SamToFastqAndBwaMem/inputs/-21323395 . There is a file 'cromwell.tmp' which seems to be a partial copy of my cromwell process' CWD. All of the logs are copied in, (cromwell.tmp/cromwell-workflow-logs/) and a single seemingly unrelated job (cromwell.tmp/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/). All of that jobs 'execution' folder, and some of its 'inputs' are copied. It is not clear if more would have been copied in or if the process was ended by the soft link error mentioned above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803
https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803:12,Performance,cache,cache,12,"I think the cache is unrelated, this is purely input localisation. I re-ran the job with caching disabled in the config file. The same error occurs. From this directory: /share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/37e4e046-b256-4f81-95c6-9f0c915810bf/call-SamToFastqAndBwaMem/inputs/-21323395 . There is a file 'cromwell.tmp' which seems to be a partial copy of my cromwell process' CWD. All of the logs are copied in, (cromwell.tmp/cromwell-workflow-logs/) and a single seemingly unrelated job (cromwell.tmp/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/). All of that jobs 'execution' folder, and some of its 'inputs' are copied. It is not clear if more would have been copied in or if the process was ended by the soft link error mentioned above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803
https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803:452,Testability,log,logs,452,"I think the cache is unrelated, this is purely input localisation. I re-ran the job with caching disabled in the config file. The same error occurs. From this directory: /share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/37e4e046-b256-4f81-95c6-9f0c915810bf/call-SamToFastqAndBwaMem/inputs/-21323395 . There is a file 'cromwell.tmp' which seems to be a partial copy of my cromwell process' CWD. All of the logs are copied in, (cromwell.tmp/cromwell-workflow-logs/) and a single seemingly unrelated job (cromwell.tmp/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/). All of that jobs 'execution' folder, and some of its 'inputs' are copied. It is not clear if more would have been copied in or if the process was ended by the soft link error mentioned above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803
https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803:504,Testability,log,logs,504,"I think the cache is unrelated, this is purely input localisation. I re-ran the job with caching disabled in the config file. The same error occurs. From this directory: /share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/37e4e046-b256-4f81-95c6-9f0c915810bf/call-SamToFastqAndBwaMem/inputs/-21323395 . There is a file 'cromwell.tmp' which seems to be a partial copy of my cromwell process' CWD. All of the logs are copied in, (cromwell.tmp/cromwell-workflow-logs/) and a single seemingly unrelated job (cromwell.tmp/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/). All of that jobs 'execution' folder, and some of its 'inputs' are copied. It is not clear if more would have been copied in or if the process was ended by the soft link error mentioned above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803
https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803:761,Usability,clear,clear,761,"I think the cache is unrelated, this is purely input localisation. I re-ran the job with caching disabled in the config file. The same error occurs. From this directory: /share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/37e4e046-b256-4f81-95c6-9f0c915810bf/call-SamToFastqAndBwaMem/inputs/-21323395 . There is a file 'cromwell.tmp' which seems to be a partial copy of my cromwell process' CWD. All of the logs are copied in, (cromwell.tmp/cromwell-workflow-logs/) and a single seemingly unrelated job (cromwell.tmp/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/). All of that jobs 'execution' folder, and some of its 'inputs' are copied. It is not clear if more would have been copied in or if the process was ended by the soft link error mentioned above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803
https://github.com/broadinstitute/cromwell/pull/3864#issuecomment-402756505:431,Deployability,update,update,431,"I've been seeing the configurable epilogue more as a ""hey user, here's a place for you to add stuff for your specific setup"" rather than ""you absolutely need your epilogue config to have this if you want your backend to work"", but maybe they're not that far apart after all.; I'd be ok with making it a ""required"" epilogue for cloud backends as long as we make it pretty clear in the changelog that it's required and that a config update is necessary. Also I'm not too worried that anyone has been relying on this anyway since it's a hack to support empty directories which hopefully is not wildly used.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3864#issuecomment-402756505
https://github.com/broadinstitute/cromwell/pull/3864#issuecomment-402756505:21,Modifiability,config,configurable,21,"I've been seeing the configurable epilogue more as a ""hey user, here's a place for you to add stuff for your specific setup"" rather than ""you absolutely need your epilogue config to have this if you want your backend to work"", but maybe they're not that far apart after all.; I'd be ok with making it a ""required"" epilogue for cloud backends as long as we make it pretty clear in the changelog that it's required and that a config update is necessary. Also I'm not too worried that anyone has been relying on this anyway since it's a hack to support empty directories which hopefully is not wildly used.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3864#issuecomment-402756505
https://github.com/broadinstitute/cromwell/pull/3864#issuecomment-402756505:172,Modifiability,config,config,172,"I've been seeing the configurable epilogue more as a ""hey user, here's a place for you to add stuff for your specific setup"" rather than ""you absolutely need your epilogue config to have this if you want your backend to work"", but maybe they're not that far apart after all.; I'd be ok with making it a ""required"" epilogue for cloud backends as long as we make it pretty clear in the changelog that it's required and that a config update is necessary. Also I'm not too worried that anyone has been relying on this anyway since it's a hack to support empty directories which hopefully is not wildly used.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3864#issuecomment-402756505
https://github.com/broadinstitute/cromwell/pull/3864#issuecomment-402756505:424,Modifiability,config,config,424,"I've been seeing the configurable epilogue more as a ""hey user, here's a place for you to add stuff for your specific setup"" rather than ""you absolutely need your epilogue config to have this if you want your backend to work"", but maybe they're not that far apart after all.; I'd be ok with making it a ""required"" epilogue for cloud backends as long as we make it pretty clear in the changelog that it's required and that a config update is necessary. Also I'm not too worried that anyone has been relying on this anyway since it's a hack to support empty directories which hopefully is not wildly used.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3864#issuecomment-402756505
https://github.com/broadinstitute/cromwell/pull/3864#issuecomment-402756505:371,Usability,clear,clear,371,"I've been seeing the configurable epilogue more as a ""hey user, here's a place for you to add stuff for your specific setup"" rather than ""you absolutely need your epilogue config to have this if you want your backend to work"", but maybe they're not that far apart after all.; I'd be ok with making it a ""required"" epilogue for cloud backends as long as we make it pretty clear in the changelog that it's required and that a config update is necessary. Also I'm not too worried that anyone has been relying on this anyway since it's a hack to support empty directories which hopefully is not wildly used.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3864#issuecomment-402756505
https://github.com/broadinstitute/cromwell/issues/3900#issuecomment-404939788:63,Usability,simpl,simply,63,"Hi @agraubert - one thing we discussed when we did that was to simply provide another concept altogether (because cromwell labels and google labels were 2 concepts shoehorned together) and then removed them completely as the only known users didn't need them. If we bring them back as they were before but **not** as standard cromwell labels, would this still work for you?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3900#issuecomment-404939788
https://github.com/broadinstitute/cromwell/issues/3900#issuecomment-478161147:81,Deployability,pipeline,pipeline,81,"Hey @agraubert, the best way to know that a ticket will be worked on is when the pipeline for it changes to backlog bucket. . Just as an FYI, we are focused on scale improvements and small bug fixes for the near future. Out of curiosity, are you interested in contributing this feature yourself, especially with the guidance of a Cromwell developer? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3900#issuecomment-478161147
https://github.com/broadinstitute/cromwell/issues/3900#issuecomment-478161147:316,Usability,guid,guidance,316,"Hey @agraubert, the best way to know that a ticket will be worked on is when the pipeline for it changes to backlog bucket. . Just as an FYI, we are focused on scale improvements and small bug fixes for the near future. Out of curiosity, are you interested in contributing this feature yourself, especially with the guidance of a Cromwell developer? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3900#issuecomment-478161147
https://github.com/broadinstitute/cromwell/issues/3928#issuecomment-654851201:126,Availability,echo,echo,126,"Hey,. I am trying to use TESK as a backend for a cromwell server (version 51) just running a simple task to test if it works (echo ""Hello World"" using an alpine image) and it does not work. TESK receives the input from the server with the correct syntax, however, the script files and all other files generated by cromwell are pointing to a local directory which TESK does not have (TESK is running in a kubernetes cluster). Maybe I am missing something but I this behaviour with creating local files does not work with a kubernetes cluster. Can I change it by setting the config differently or what is a possible solution? Is there anyone who is experiences with Cromwell-TESK?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3928#issuecomment-654851201
https://github.com/broadinstitute/cromwell/issues/3928#issuecomment-654851201:573,Modifiability,config,config,573,"Hey,. I am trying to use TESK as a backend for a cromwell server (version 51) just running a simple task to test if it works (echo ""Hello World"" using an alpine image) and it does not work. TESK receives the input from the server with the correct syntax, however, the script files and all other files generated by cromwell are pointing to a local directory which TESK does not have (TESK is running in a kubernetes cluster). Maybe I am missing something but I this behaviour with creating local files does not work with a kubernetes cluster. Can I change it by setting the config differently or what is a possible solution? Is there anyone who is experiences with Cromwell-TESK?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3928#issuecomment-654851201
https://github.com/broadinstitute/cromwell/issues/3928#issuecomment-654851201:108,Testability,test,test,108,"Hey,. I am trying to use TESK as a backend for a cromwell server (version 51) just running a simple task to test if it works (echo ""Hello World"" using an alpine image) and it does not work. TESK receives the input from the server with the correct syntax, however, the script files and all other files generated by cromwell are pointing to a local directory which TESK does not have (TESK is running in a kubernetes cluster). Maybe I am missing something but I this behaviour with creating local files does not work with a kubernetes cluster. Can I change it by setting the config differently or what is a possible solution? Is there anyone who is experiences with Cromwell-TESK?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3928#issuecomment-654851201
https://github.com/broadinstitute/cromwell/issues/3928#issuecomment-654851201:93,Usability,simpl,simple,93,"Hey,. I am trying to use TESK as a backend for a cromwell server (version 51) just running a simple task to test if it works (echo ""Hello World"" using an alpine image) and it does not work. TESK receives the input from the server with the correct syntax, however, the script files and all other files generated by cromwell are pointing to a local directory which TESK does not have (TESK is running in a kubernetes cluster). Maybe I am missing something but I this behaviour with creating local files does not work with a kubernetes cluster. Can I change it by setting the config differently or what is a possible solution? Is there anyone who is experiences with Cromwell-TESK?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3928#issuecomment-654851201
https://github.com/broadinstitute/cromwell/issues/3942#issuecomment-409375834:287,Availability,error,error,287,"When you make a call from an imported WDL, the call is automatically aliased to the simple task name, so in your case:. ```wdl; import ""a.wdl"" as a. workflow b {. # This task is 'a.t', but the callis given the alias 't'; call a.t {}. String x = t.s; }; ```. Similarly, this should be an error:; ```wdl; import ""a.wdl"" as a; import ""z.wdl"" as z. workflow b {. # Cannot do this because the calls would have the same alias:; call a.t {}; call z.t {}. # Which t?; String x = t.s; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3942#issuecomment-409375834
https://github.com/broadinstitute/cromwell/issues/3942#issuecomment-409375834:84,Usability,simpl,simple,84,"When you make a call from an imported WDL, the call is automatically aliased to the simple task name, so in your case:. ```wdl; import ""a.wdl"" as a. workflow b {. # This task is 'a.t', but the callis given the alias 't'; call a.t {}. String x = t.s; }; ```. Similarly, this should be an error:; ```wdl; import ""a.wdl"" as a; import ""z.wdl"" as z. workflow b {. # Cannot do this because the calls would have the same alias:; call a.t {}; call z.t {}. # Which t?; String x = t.s; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3942#issuecomment-409375834
https://github.com/broadinstitute/cromwell/issues/3955#issuecomment-409700884:92,Usability,clear,clear,92,Not sure I agree with he first poinT. I was thinking about it earlier and the answer is not clear to me.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3955#issuecomment-409700884
https://github.com/broadinstitute/cromwell/issues/3958#issuecomment-409941435:152,Usability,clear,clear,152,"Interesting. To me, `valueFrom` with a non-expression only makes sense for within the `arguments` section, but I agree that the CWl specification isn't clear on this. As a workaround I suggest using the `default` field one level up, as that appears to be the user intent here:. ``` cwl; class: CommandLineTool; cwlVersion: v1.0; id: 13CNMR; baseCommand: batchprocessNMR.py; inputs:; inputFiles:; type: File[]; inputBinding:; position: 0; frequency:; type: float?; default: 150819500.0; inputBinding:; position: 1; outputs:; output:; type: File[]; outputBinding:; glob: '*.h5'; label: 13C-NMR",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3958#issuecomment-409941435
https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-415665749:27,Deployability,update,update,27,@aednichols thanks for the update. We are currently in the process of updating the WDL spec at https://github.com/openwdl/wdl/pull/243. This will hopefully make clear what sort of regular expression should be used. After that it will be a lot easier to decide whether the regular expression evaluation is broken or not.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-415665749
https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-415665749:161,Usability,clear,clear,161,@aednichols thanks for the update. We are currently in the process of updating the WDL spec at https://github.com/openwdl/wdl/pull/243. This will hopefully make clear what sort of regular expression should be used. After that it will be a lot easier to decide whether the regular expression evaluation is broken or not.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-415665749
https://github.com/broadinstitute/cromwell/issues/4004#issuecomment-417386515:89,Safety,avoid,avoid,89,"@ruchim Yes! We eventually switched our particular workflow that motivated this issue to avoid a glob altogether. We really do expect at most one matching file, so this was simpler than pulling it out of an array later.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004#issuecomment-417386515
https://github.com/broadinstitute/cromwell/issues/4004#issuecomment-417386515:173,Usability,simpl,simpler,173,"@ruchim Yes! We eventually switched our particular workflow that motivated this issue to avoid a glob altogether. We really do expect at most one matching file, so this was simpler than pulling it out of an array later.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004#issuecomment-417386515
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:338,Availability,down,down,338,"hey @kshakir ! I'm definitely glad to help. I'm not sure a remote session would be helpful to understand yaml - it's a data structure like json, and just a way to capture a workflow. I'd be happy to answer specific questions that you might have, feel free to post them on here, and no worries about the busy-ness! I hope the :fire: calms down, at least it has been sort of not so great for CA! If it helps, I'll leave you some notes here:. - adding this circle testing doesn't interfere with your current testing; - adding the build and deploy of the docker container here is a better strategy than having travis handle everything because the two can run at the same time.; - environment variables, given docker credentials, are set on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The ma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:1740,Availability,echo,echo,1740," on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:1762,Availability,echo,echo,1762," on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:537,Deployability,deploy,deploy,537,"hey @kshakir ! I'm definitely glad to help. I'm not sure a remote session would be helpful to understand yaml - it's a data structure like json, and just a way to capture a workflow. I'd be happy to answer specific questions that you might have, feel free to post them on here, and no worries about the busy-ness! I hope the :fire: calms down, at least it has been sort of not so great for CA! If it helps, I'll leave you some notes here:. - adding this circle testing doesn't interfere with your current testing; - adding the build and deploy of the docker container here is a better strategy than having travis handle everything because the two can run at the same time.; - environment variables, given docker credentials, are set on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The ma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:973,Deployability,configurat,configuration,973,"hey @kshakir ! I'm definitely glad to help. I'm not sure a remote session would be helpful to understand yaml - it's a data structure like json, and just a way to capture a workflow. I'd be happy to answer specific questions that you might have, feel free to post them on here, and no worries about the busy-ness! I hope the :fire: calms down, at least it has been sort of not so great for CA! If it helps, I'll leave you some notes here:. - adding this circle testing doesn't interfere with your current testing; - adding the build and deploy of the docker container here is a better strategy than having travis handle everything because the two can run at the same time.; - environment variables, given docker credentials, are set on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The ma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:1877,Deployability,deploy,deploy,1877,"docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above tha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:2029,Deployability,install,install,2029,"o what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., ch",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:2113,Deployability,deploy,deploy,2113,"o what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., ch",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:3157,Deployability,configurat,configuration-reference,3157,"src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., checkout) are circle provided steps that you can read about in their docs --> https://circleci.com/docs/2.0/configuration-reference/. And importantly, in the beginning it all just looks weird. You will come back to the same file and be a little confused. This yaml business is weird, but I promise it clicks over time!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:2037,Integrability,depend,dependencies,2037,"o what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., ch",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:2711,Integrability,depend,dependency,2711,"src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., checkout) are circle provided steps that you can read about in their docs --> https://circleci.com/docs/2.0/configuration-reference/. And importantly, in the beginning it all just looks weird. You will come back to the same file and be a little confused. This yaml business is weird, but I promise it clicks over time!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:688,Modifiability,variab,variables,688,"hey @kshakir ! I'm definitely glad to help. I'm not sure a remote session would be helpful to understand yaml - it's a data structure like json, and just a way to capture a workflow. I'd be happy to answer specific questions that you might have, feel free to post them on here, and no worries about the busy-ness! I hope the :fire: calms down, at least it has been sort of not so great for CA! If it helps, I'll leave you some notes here:. - adding this circle testing doesn't interfere with your current testing; - adding the build and deploy of the docker container here is a better strategy than having travis handle everything because the two can run at the same time.; - environment variables, given docker credentials, are set on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The ma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:930,Modifiability,config,config-yaml-aka-yaml-anchors,930,"hey @kshakir ! I'm definitely glad to help. I'm not sure a remote session would be helpful to understand yaml - it's a data structure like json, and just a way to capture a workflow. I'd be happy to answer specific questions that you might have, feel free to post them on here, and no worries about the busy-ness! I hope the :fire: calms down, at least it has been sort of not so great for CA! If it helps, I'll leave you some notes here:. - adding this circle testing doesn't interfere with your current testing; - adding the build and deploy of the docker container here is a better strategy than having travis handle everything because the two can run at the same time.; - environment variables, given docker credentials, are set on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The ma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:973,Modifiability,config,configuration,973,"hey @kshakir ! I'm definitely glad to help. I'm not sure a remote session would be helpful to understand yaml - it's a data structure like json, and just a way to capture a workflow. I'd be happy to answer specific questions that you might have, feel free to post them on here, and no worries about the busy-ness! I hope the :fire: calms down, at least it has been sort of not so great for CA! If it helps, I'll leave you some notes here:. - adding this circle testing doesn't interfere with your current testing; - adding the build and deploy of the docker container here is a better strategy than having travis handle everything because the two can run at the same time.; - environment variables, given docker credentials, are set on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The ma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:1202,Modifiability,variab,variables,1202,"ppy to answer specific questions that you might have, feel free to post them on here, and no worries about the busy-ness! I hope the :fire: calms down, at least it has been sort of not so great for CA! If it helps, I'll leave you some notes here:. - adding this circle testing doesn't interfere with your current testing; - adding the build and deploy of the docker container here is a better strategy than having travis handle everything because the two can run at the same time.; - environment variables, given docker credentials, are set on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:2397,Modifiability,variab,variable,2397,"src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., checkout) are circle provided steps that you can read about in their docs --> https://circleci.com/docs/2.0/configuration-reference/. And importantly, in the beginning it all just looks weird. You will come back to the same file and be a little confused. This yaml business is weird, but I promise it clicks over time!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:3157,Modifiability,config,configuration-reference,3157,"src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., checkout) are circle provided steps that you can read about in their docs --> https://circleci.com/docs/2.0/configuration-reference/. And importantly, in the beginning it all just looks weird. You will come back to the same file and be a little confused. This yaml business is weird, but I promise it clicks over time!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:2017,Performance,load,load,2017,"o what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., ch",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:2022,Performance,cache,cache,2022,"o what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., ch",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:2103,Performance,cache,cache,2103,"o what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., ch",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:461,Testability,test,testing,461,"hey @kshakir ! I'm definitely glad to help. I'm not sure a remote session would be helpful to understand yaml - it's a data structure like json, and just a way to capture a workflow. I'd be happy to answer specific questions that you might have, feel free to post them on here, and no worries about the busy-ness! I hope the :fire: calms down, at least it has been sort of not so great for CA! If it helps, I'll leave you some notes here:. - adding this circle testing doesn't interfere with your current testing; - adding the build and deploy of the docker container here is a better strategy than having travis handle everything because the two can run at the same time.; - environment variables, given docker credentials, are set on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The ma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:505,Testability,test,testing,505,"hey @kshakir ! I'm definitely glad to help. I'm not sure a remote session would be helpful to understand yaml - it's a data structure like json, and just a way to capture a workflow. I'd be happy to answer specific questions that you might have, feel free to post them on here, and no worries about the busy-ness! I hope the :fire: calms down, at least it has been sort of not so great for CA! If it helps, I'll leave you some notes here:. - adding this circle testing doesn't interfere with your current testing; - adding the build and deploy of the docker container here is a better strategy than having travis handle everything because the two can run at the same time.; - environment variables, given docker credentials, are set on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The ma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:2079,Testability,test,test,2079,"o what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., ch",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:2330,Testability,log,logic,2330,"src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., checkout) are circle provided steps that you can read about in their docs --> https://circleci.com/docs/2.0/configuration-reference/. And importantly, in the beginning it all just looks weird. You will come back to the same file and be a little confused. This yaml business is weird, but I promise it clicks over time!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:2347,Testability,test,test,2347,"src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., checkout) are circle provided steps that you can read about in their docs --> https://circleci.com/docs/2.0/configuration-reference/. And importantly, in the beginning it all just looks weird. You will come back to the same file and be a little confused. This yaml business is weird, but I promise it clicks over time!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:1098,Usability,learn,learnings,1098,"erstand yaml - it's a data structure like json, and just a way to capture a workflow. I'd be happy to answer specific questions that you might have, feel free to post them on here, and no worries about the busy-ness! I hope the :fire: calms down, at least it has been sort of not so great for CA! If it helps, I'll leave you some notes here:. - adding this circle testing doesn't interfere with your current testing; - adding the build and deploy of the docker container here is a better strategy than having travis handle everything because the two can run at the same time.; - environment variables, given docker credentials, are set on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110:575,Availability,avail,available,575,"okay, that works for me! To answer your questions about CircleCI:. - environment variables used in the project are [encrypted](https://circleci.com/docs/2.0/security/#encryption) also using hashicorp vault! So, same thing or if not very similar deal as what you have.; - once you set them in the interface, you can't change or see them; - if the environment variables aren't set in the container with ENV or as flags with --env then they won't be saved. You would likely want to have them be [ARGS](https://vsupalov.com/docker-arg-env-variable-guide/) instead to be used and available in the container during build, but then not persisted in the container. So, as long as:; - you set secrets in the project and not the circle.yml; - you don't allow the CI to pass on secrets to other forked build requests (you would have to turn it on in settings are there are a lot of **warning don't do this!** prompts before you get there and; - you use ARGS to expose needed variables from the environment to the container for building (that don't get saved). . I think you'd be ok :) But sure, I'm definitely not a security expert. Anyway, since it's a single file, please feel free to grab the commit from here if/when you are ready.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110:296,Integrability,interface,interface,296,"okay, that works for me! To answer your questions about CircleCI:. - environment variables used in the project are [encrypted](https://circleci.com/docs/2.0/security/#encryption) also using hashicorp vault! So, same thing or if not very similar deal as what you have.; - once you set them in the interface, you can't change or see them; - if the environment variables aren't set in the container with ENV or as flags with --env then they won't be saved. You would likely want to have them be [ARGS](https://vsupalov.com/docker-arg-env-variable-guide/) instead to be used and available in the container during build, but then not persisted in the container. So, as long as:; - you set secrets in the project and not the circle.yml; - you don't allow the CI to pass on secrets to other forked build requests (you would have to turn it on in settings are there are a lot of **warning don't do this!** prompts before you get there and; - you use ARGS to expose needed variables from the environment to the container for building (that don't get saved). . I think you'd be ok :) But sure, I'm definitely not a security expert. Anyway, since it's a single file, please feel free to grab the commit from here if/when you are ready.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110:81,Modifiability,variab,variables,81,"okay, that works for me! To answer your questions about CircleCI:. - environment variables used in the project are [encrypted](https://circleci.com/docs/2.0/security/#encryption) also using hashicorp vault! So, same thing or if not very similar deal as what you have.; - once you set them in the interface, you can't change or see them; - if the environment variables aren't set in the container with ENV or as flags with --env then they won't be saved. You would likely want to have them be [ARGS](https://vsupalov.com/docker-arg-env-variable-guide/) instead to be used and available in the container during build, but then not persisted in the container. So, as long as:; - you set secrets in the project and not the circle.yml; - you don't allow the CI to pass on secrets to other forked build requests (you would have to turn it on in settings are there are a lot of **warning don't do this!** prompts before you get there and; - you use ARGS to expose needed variables from the environment to the container for building (that don't get saved). . I think you'd be ok :) But sure, I'm definitely not a security expert. Anyway, since it's a single file, please feel free to grab the commit from here if/when you are ready.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110:358,Modifiability,variab,variables,358,"okay, that works for me! To answer your questions about CircleCI:. - environment variables used in the project are [encrypted](https://circleci.com/docs/2.0/security/#encryption) also using hashicorp vault! So, same thing or if not very similar deal as what you have.; - once you set them in the interface, you can't change or see them; - if the environment variables aren't set in the container with ENV or as flags with --env then they won't be saved. You would likely want to have them be [ARGS](https://vsupalov.com/docker-arg-env-variable-guide/) instead to be used and available in the container during build, but then not persisted in the container. So, as long as:; - you set secrets in the project and not the circle.yml; - you don't allow the CI to pass on secrets to other forked build requests (you would have to turn it on in settings are there are a lot of **warning don't do this!** prompts before you get there and; - you use ARGS to expose needed variables from the environment to the container for building (that don't get saved). . I think you'd be ok :) But sure, I'm definitely not a security expert. Anyway, since it's a single file, please feel free to grab the commit from here if/when you are ready.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110:535,Modifiability,variab,variable-guide,535,"okay, that works for me! To answer your questions about CircleCI:. - environment variables used in the project are [encrypted](https://circleci.com/docs/2.0/security/#encryption) also using hashicorp vault! So, same thing or if not very similar deal as what you have.; - once you set them in the interface, you can't change or see them; - if the environment variables aren't set in the container with ENV or as flags with --env then they won't be saved. You would likely want to have them be [ARGS](https://vsupalov.com/docker-arg-env-variable-guide/) instead to be used and available in the container during build, but then not persisted in the container. So, as long as:; - you set secrets in the project and not the circle.yml; - you don't allow the CI to pass on secrets to other forked build requests (you would have to turn it on in settings are there are a lot of **warning don't do this!** prompts before you get there and; - you use ARGS to expose needed variables from the environment to the container for building (that don't get saved). . I think you'd be ok :) But sure, I'm definitely not a security expert. Anyway, since it's a single file, please feel free to grab the commit from here if/when you are ready.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110:964,Modifiability,variab,variables,964,"okay, that works for me! To answer your questions about CircleCI:. - environment variables used in the project are [encrypted](https://circleci.com/docs/2.0/security/#encryption) also using hashicorp vault! So, same thing or if not very similar deal as what you have.; - once you set them in the interface, you can't change or see them; - if the environment variables aren't set in the container with ENV or as flags with --env then they won't be saved. You would likely want to have them be [ARGS](https://vsupalov.com/docker-arg-env-variable-guide/) instead to be used and available in the container during build, but then not persisted in the container. So, as long as:; - you set secrets in the project and not the circle.yml; - you don't allow the CI to pass on secrets to other forked build requests (you would have to turn it on in settings are there are a lot of **warning don't do this!** prompts before you get there and; - you use ARGS to expose needed variables from the environment to the container for building (that don't get saved). . I think you'd be ok :) But sure, I'm definitely not a security expert. Anyway, since it's a single file, please feel free to grab the commit from here if/when you are ready.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110:116,Security,encrypt,encrypted,116,"okay, that works for me! To answer your questions about CircleCI:. - environment variables used in the project are [encrypted](https://circleci.com/docs/2.0/security/#encryption) also using hashicorp vault! So, same thing or if not very similar deal as what you have.; - once you set them in the interface, you can't change or see them; - if the environment variables aren't set in the container with ENV or as flags with --env then they won't be saved. You would likely want to have them be [ARGS](https://vsupalov.com/docker-arg-env-variable-guide/) instead to be used and available in the container during build, but then not persisted in the container. So, as long as:; - you set secrets in the project and not the circle.yml; - you don't allow the CI to pass on secrets to other forked build requests (you would have to turn it on in settings are there are a lot of **warning don't do this!** prompts before you get there and; - you use ARGS to expose needed variables from the environment to the container for building (that don't get saved). . I think you'd be ok :) But sure, I'm definitely not a security expert. Anyway, since it's a single file, please feel free to grab the commit from here if/when you are ready.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110:157,Security,secur,security,157,"okay, that works for me! To answer your questions about CircleCI:. - environment variables used in the project are [encrypted](https://circleci.com/docs/2.0/security/#encryption) also using hashicorp vault! So, same thing or if not very similar deal as what you have.; - once you set them in the interface, you can't change or see them; - if the environment variables aren't set in the container with ENV or as flags with --env then they won't be saved. You would likely want to have them be [ARGS](https://vsupalov.com/docker-arg-env-variable-guide/) instead to be used and available in the container during build, but then not persisted in the container. So, as long as:; - you set secrets in the project and not the circle.yml; - you don't allow the CI to pass on secrets to other forked build requests (you would have to turn it on in settings are there are a lot of **warning don't do this!** prompts before you get there and; - you use ARGS to expose needed variables from the environment to the container for building (that don't get saved). . I think you'd be ok :) But sure, I'm definitely not a security expert. Anyway, since it's a single file, please feel free to grab the commit from here if/when you are ready.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110:167,Security,encrypt,encryption,167,"okay, that works for me! To answer your questions about CircleCI:. - environment variables used in the project are [encrypted](https://circleci.com/docs/2.0/security/#encryption) also using hashicorp vault! So, same thing or if not very similar deal as what you have.; - once you set them in the interface, you can't change or see them; - if the environment variables aren't set in the container with ENV or as flags with --env then they won't be saved. You would likely want to have them be [ARGS](https://vsupalov.com/docker-arg-env-variable-guide/) instead to be used and available in the container during build, but then not persisted in the container. So, as long as:; - you set secrets in the project and not the circle.yml; - you don't allow the CI to pass on secrets to other forked build requests (you would have to turn it on in settings are there are a lot of **warning don't do this!** prompts before you get there and; - you use ARGS to expose needed variables from the environment to the container for building (that don't get saved). . I think you'd be ok :) But sure, I'm definitely not a security expert. Anyway, since it's a single file, please feel free to grab the commit from here if/when you are ready.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110:190,Security,hash,hashicorp,190,"okay, that works for me! To answer your questions about CircleCI:. - environment variables used in the project are [encrypted](https://circleci.com/docs/2.0/security/#encryption) also using hashicorp vault! So, same thing or if not very similar deal as what you have.; - once you set them in the interface, you can't change or see them; - if the environment variables aren't set in the container with ENV or as flags with --env then they won't be saved. You would likely want to have them be [ARGS](https://vsupalov.com/docker-arg-env-variable-guide/) instead to be used and available in the container during build, but then not persisted in the container. So, as long as:; - you set secrets in the project and not the circle.yml; - you don't allow the CI to pass on secrets to other forked build requests (you would have to turn it on in settings are there are a lot of **warning don't do this!** prompts before you get there and; - you use ARGS to expose needed variables from the environment to the container for building (that don't get saved). . I think you'd be ok :) But sure, I'm definitely not a security expert. Anyway, since it's a single file, please feel free to grab the commit from here if/when you are ready.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110:950,Security,expose,expose,950,"okay, that works for me! To answer your questions about CircleCI:. - environment variables used in the project are [encrypted](https://circleci.com/docs/2.0/security/#encryption) also using hashicorp vault! So, same thing or if not very similar deal as what you have.; - once you set them in the interface, you can't change or see them; - if the environment variables aren't set in the container with ENV or as flags with --env then they won't be saved. You would likely want to have them be [ARGS](https://vsupalov.com/docker-arg-env-variable-guide/) instead to be used and available in the container during build, but then not persisted in the container. So, as long as:; - you set secrets in the project and not the circle.yml; - you don't allow the CI to pass on secrets to other forked build requests (you would have to turn it on in settings are there are a lot of **warning don't do this!** prompts before you get there and; - you use ARGS to expose needed variables from the environment to the container for building (that don't get saved). . I think you'd be ok :) But sure, I'm definitely not a security expert. Anyway, since it's a single file, please feel free to grab the commit from here if/when you are ready.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110:1105,Security,secur,security,1105,"okay, that works for me! To answer your questions about CircleCI:. - environment variables used in the project are [encrypted](https://circleci.com/docs/2.0/security/#encryption) also using hashicorp vault! So, same thing or if not very similar deal as what you have.; - once you set them in the interface, you can't change or see them; - if the environment variables aren't set in the container with ENV or as flags with --env then they won't be saved. You would likely want to have them be [ARGS](https://vsupalov.com/docker-arg-env-variable-guide/) instead to be used and available in the container during build, but then not persisted in the container. So, as long as:; - you set secrets in the project and not the circle.yml; - you don't allow the CI to pass on secrets to other forked build requests (you would have to turn it on in settings are there are a lot of **warning don't do this!** prompts before you get there and; - you use ARGS to expose needed variables from the environment to the container for building (that don't get saved). . I think you'd be ok :) But sure, I'm definitely not a security expert. Anyway, since it's a single file, please feel free to grab the commit from here if/when you are ready.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110
https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110:544,Usability,guid,guide,544,"okay, that works for me! To answer your questions about CircleCI:. - environment variables used in the project are [encrypted](https://circleci.com/docs/2.0/security/#encryption) also using hashicorp vault! So, same thing or if not very similar deal as what you have.; - once you set them in the interface, you can't change or see them; - if the environment variables aren't set in the container with ENV or as flags with --env then they won't be saved. You would likely want to have them be [ARGS](https://vsupalov.com/docker-arg-env-variable-guide/) instead to be used and available in the container during build, but then not persisted in the container. So, as long as:; - you set secrets in the project and not the circle.yml; - you don't allow the CI to pass on secrets to other forked build requests (you would have to turn it on in settings are there are a lot of **warning don't do this!** prompts before you get there and; - you use ARGS to expose needed variables from the environment to the container for building (that don't get saved). . I think you'd be ok :) But sure, I'm definitely not a security expert. Anyway, since it's a single file, please feel free to grab the commit from here if/when you are ready.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-416275110
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:1521,Deployability,install,installing,1521,"/hello/hello.wdl \; -i centaur/src/main/resources/standardTestCases/hello/hello.inputs; ```. The accompanying `.test` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A similar script that installs the singularity binaries plus a small cluster(?) and then uses a working config file to run our Centaur test suite would be amazing for users. After it's all working, users are being pointed to docs under https://cromwell.readthedocs.io/, such as https://cromwell.readthedocs.io/en/stable/backends/TES/. A similar entry should be added for a working/tested singularity setup. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstit",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:1837,Deployability,install,installs,1837,"t` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A similar script that installs the singularity binaries plus a small cluster(?) and then uses a working config file to run our Centaur test suite would be amazing for users. After it's all working, users are being pointed to docs under https://cromwell.readthedocs.io/, such as https://cromwell.readthedocs.io/en/stable/backends/TES/. A similar entry should be added for a working/tested singularity setup. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/4039/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:2015,Deployability,install,installs,2015,"t` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A similar script that installs the singularity binaries plus a small cluster(?) and then uses a working config file to run our Centaur test suite would be amazing for users. After it's all working, users are being pointed to docs under https://cromwell.readthedocs.io/, such as https://cromwell.readthedocs.io/en/stable/backends/TES/. A similar entry should be added for a working/tested singularity setup. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/4039/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:1406,Energy Efficiency,green,greener,1406,"; java \; -Dconfig.file=my.conf \; -jar cromwell-34.jar \; run centaur/src/main/resources/standardTestCases/hello/hello.wdl \; -i centaur/src/main/resources/standardTestCases/hello/hello.inputs; ```. The accompanying `.test` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A similar script that installs the singularity binaries plus a small cluster(?) and then uses a working config file to run our Centaur test suite would be amazing for users. After it's all working, users are being pointed to docs under https://cromwell.readthedocs.io/, such as https://cromwell.readthedocs.io/en/stable/backends/TES/. A similar entry should be added for a working/tested singularity setup. [![Approved with PullApprove]",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:40,Modifiability,variab,variable,40,"> Likely we would want to add a general variable to plug in any user specific arguments (e.g., custom binds) and singularity arguments? (e.g., debug?) I don't know if this is ""how it's supposed to look"". The easiest way to verify a run is to use one (or more) of the [standard test cases](https://github.com/broadinstitute/cromwell/tree/develop/centaur/src/main/resources/standardTestCases). For example you can try to run . ```; java \; -Dconfig.file=my.conf \; -jar cromwell-34.jar \; run centaur/src/main/resources/standardTestCases/hello/hello.wdl \; -i centaur/src/main/resources/standardTestCases/hello/hello.inputs; ```. The accompanying `.test` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A simila",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:1663,Modifiability,config,configs,1663,"t` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A similar script that installs the singularity binaries plus a small cluster(?) and then uses a working config file to run our Centaur test suite would be amazing for users. After it's all working, users are being pointed to docs under https://cromwell.readthedocs.io/, such as https://cromwell.readthedocs.io/en/stable/backends/TES/. A similar entry should be added for a working/tested singularity setup. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/4039/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:2097,Modifiability,config,config,2097,"t` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A similar script that installs the singularity binaries plus a small cluster(?) and then uses a working config file to run our Centaur test suite would be amazing for users. After it's all working, users are being pointed to docs under https://cromwell.readthedocs.io/, such as https://cromwell.readthedocs.io/en/stable/backends/TES/. A similar entry should be added for a working/tested singularity setup. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/4039/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:277,Testability,test,test,277,"> Likely we would want to add a general variable to plug in any user specific arguments (e.g., custom binds) and singularity arguments? (e.g., debug?) I don't know if this is ""how it's supposed to look"". The easiest way to verify a run is to use one (or more) of the [standard test cases](https://github.com/broadinstitute/cromwell/tree/develop/centaur/src/main/resources/standardTestCases). For example you can try to run . ```; java \; -Dconfig.file=my.conf \; -jar cromwell-34.jar \; run centaur/src/main/resources/standardTestCases/hello/hello.wdl \; -i centaur/src/main/resources/standardTestCases/hello/hello.inputs; ```. The accompanying `.test` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A simila",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:647,Testability,test,test,647,"> Likely we would want to add a general variable to plug in any user specific arguments (e.g., custom binds) and singularity arguments? (e.g., debug?) I don't know if this is ""how it's supposed to look"". The easiest way to verify a run is to use one (or more) of the [standard test cases](https://github.com/broadinstitute/cromwell/tree/develop/centaur/src/main/resources/standardTestCases). For example you can try to run . ```; java \; -Dconfig.file=my.conf \; -jar cromwell-34.jar \; run centaur/src/main/resources/standardTestCases/hello/hello.wdl \; -i centaur/src/main/resources/standardTestCases/hello/hello.inputs; ```. The accompanying `.test` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A simila",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:761,Testability,test,test,761,"> Likely we would want to add a general variable to plug in any user specific arguments (e.g., custom binds) and singularity arguments? (e.g., debug?) I don't know if this is ""how it's supposed to look"". The easiest way to verify a run is to use one (or more) of the [standard test cases](https://github.com/broadinstitute/cromwell/tree/develop/centaur/src/main/resources/standardTestCases). For example you can try to run . ```; java \; -Dconfig.file=my.conf \; -jar cromwell-34.jar \; run centaur/src/main/resources/standardTestCases/hello/hello.wdl \; -i centaur/src/main/resources/standardTestCases/hello/hello.inputs; ```. The accompanying `.test` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A simila",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:1749,Testability,test,tested,1749,"t` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A similar script that installs the singularity binaries plus a small cluster(?) and then uses a working config file to run our Centaur test suite would be amazing for users. After it's all working, users are being pointed to docs under https://cromwell.readthedocs.io/, such as https://cromwell.readthedocs.io/en/stable/backends/TES/. A similar entry should be added for a working/tested singularity setup. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/4039/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:1966,Testability,test,testCentaurTes,1966,"t` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A similar script that installs the singularity binaries plus a small cluster(?) and then uses a working config file to run our Centaur test suite would be amazing for users. After it's all working, users are being pointed to docs under https://cromwell.readthedocs.io/, such as https://cromwell.readthedocs.io/en/stable/backends/TES/. A similar entry should be added for a working/tested singularity setup. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/4039/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:2128,Testability,test,test,2128,"t` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A similar script that installs the singularity binaries plus a small cluster(?) and then uses a working config file to run our Centaur test suite would be amazing for users. After it's all working, users are being pointed to docs under https://cromwell.readthedocs.io/, such as https://cromwell.readthedocs.io/en/stable/backends/TES/. A similar entry should be added for a working/tested singularity setup. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/4039/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:2374,Testability,test,tested,2374,"t` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A similar script that installs the singularity binaries plus a small cluster(?) and then uses a working config file to run our Centaur test suite would be amazing for users. After it's all working, users are being pointed to docs under https://cromwell.readthedocs.io/, such as https://cromwell.readthedocs.io/en/stable/backends/TES/. A similar entry should be added for a working/tested singularity setup. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/4039/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:1467,Usability,learn,learn,1467,"; java \; -Dconfig.file=my.conf \; -jar cromwell-34.jar \; run centaur/src/main/resources/standardTestCases/hello/hello.wdl \; -i centaur/src/main/resources/standardTestCases/hello/hello.inputs; ```. The accompanying `.test` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files … Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A similar script that installs the singularity binaries plus a small cluster(?) and then uses a working config file to run our Centaur test suite would be amazing for users. After it's all working, users are being pointed to docs under https://cromwell.readthedocs.io/, such as https://cromwell.readthedocs.io/en/stable/backends/TES/. A similar entry should be added for a working/tested singularity setup. [![Approved with PullApprove]",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416389694:18,Deployability,update,updated,18,"okay, well I just updated the linked repo to have this ""hello"" example. I've concluded that singularity makes absolutely no sense as a backend. it's more logical as just a binary that is run (Local) or with other backend (e.g., slurm). See here --> https://github.com/vsoch/wgbs-pipeline/blob/add/singularity/docs/pages/docs/tutorial/getting-started/index.md#step-4-run-test-case for the test case, and then the example wdl I'll just show you here, because it's stupid. It's just running a command, lol. ```; task dinosaur {; String singularity_container = ""shub://vsoch/hello-world""; output {; String roar = read_string(stdout()); }; command {; singularity --silent \; run \; ${singularity_container}; }; }. workflow wf_hello {; call dinosaur; output {; dinosaur.roar; }; }; ```; I think trying to use cromwell just makes it a lot **more** complicated. But I guess it's useful / helping users in other ways, so it's worth it to show how to run a singularity container as a command? /shrug. I don't totally follow what you are saying, to be honest. cromwell running singularity is just the same as cromwell running anything else, and I'm convinced (after this testing) there is no special use case. Docker has a backend and workflow runtime (because you run a docker container, the thing in a workflow) Singularity you are trying to shove into a backend or a workflow, and it has to be one or the other. It's not really making sense as a backend because it's more like an executable. I think before coming up with tools around something, it would be nice to have a clear definition of how singularity even fits in here to be special or different from any other binary.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416389694
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416389694:279,Deployability,pipeline,pipeline,279,"okay, well I just updated the linked repo to have this ""hello"" example. I've concluded that singularity makes absolutely no sense as a backend. it's more logical as just a binary that is run (Local) or with other backend (e.g., slurm). See here --> https://github.com/vsoch/wgbs-pipeline/blob/add/singularity/docs/pages/docs/tutorial/getting-started/index.md#step-4-run-test-case for the test case, and then the example wdl I'll just show you here, because it's stupid. It's just running a command, lol. ```; task dinosaur {; String singularity_container = ""shub://vsoch/hello-world""; output {; String roar = read_string(stdout()); }; command {; singularity --silent \; run \; ${singularity_container}; }; }. workflow wf_hello {; call dinosaur; output {; dinosaur.roar; }; }; ```; I think trying to use cromwell just makes it a lot **more** complicated. But I guess it's useful / helping users in other ways, so it's worth it to show how to run a singularity container as a command? /shrug. I don't totally follow what you are saying, to be honest. cromwell running singularity is just the same as cromwell running anything else, and I'm convinced (after this testing) there is no special use case. Docker has a backend and workflow runtime (because you run a docker container, the thing in a workflow) Singularity you are trying to shove into a backend or a workflow, and it has to be one or the other. It's not really making sense as a backend because it's more like an executable. I think before coming up with tools around something, it would be nice to have a clear definition of how singularity even fits in here to be special or different from any other binary.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416389694
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416389694:154,Testability,log,logical,154,"okay, well I just updated the linked repo to have this ""hello"" example. I've concluded that singularity makes absolutely no sense as a backend. it's more logical as just a binary that is run (Local) or with other backend (e.g., slurm). See here --> https://github.com/vsoch/wgbs-pipeline/blob/add/singularity/docs/pages/docs/tutorial/getting-started/index.md#step-4-run-test-case for the test case, and then the example wdl I'll just show you here, because it's stupid. It's just running a command, lol. ```; task dinosaur {; String singularity_container = ""shub://vsoch/hello-world""; output {; String roar = read_string(stdout()); }; command {; singularity --silent \; run \; ${singularity_container}; }; }. workflow wf_hello {; call dinosaur; output {; dinosaur.roar; }; }; ```; I think trying to use cromwell just makes it a lot **more** complicated. But I guess it's useful / helping users in other ways, so it's worth it to show how to run a singularity container as a command? /shrug. I don't totally follow what you are saying, to be honest. cromwell running singularity is just the same as cromwell running anything else, and I'm convinced (after this testing) there is no special use case. Docker has a backend and workflow runtime (because you run a docker container, the thing in a workflow) Singularity you are trying to shove into a backend or a workflow, and it has to be one or the other. It's not really making sense as a backend because it's more like an executable. I think before coming up with tools around something, it would be nice to have a clear definition of how singularity even fits in here to be special or different from any other binary.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416389694
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416389694:370,Testability,test,test-case,370,"okay, well I just updated the linked repo to have this ""hello"" example. I've concluded that singularity makes absolutely no sense as a backend. it's more logical as just a binary that is run (Local) or with other backend (e.g., slurm). See here --> https://github.com/vsoch/wgbs-pipeline/blob/add/singularity/docs/pages/docs/tutorial/getting-started/index.md#step-4-run-test-case for the test case, and then the example wdl I'll just show you here, because it's stupid. It's just running a command, lol. ```; task dinosaur {; String singularity_container = ""shub://vsoch/hello-world""; output {; String roar = read_string(stdout()); }; command {; singularity --silent \; run \; ${singularity_container}; }; }. workflow wf_hello {; call dinosaur; output {; dinosaur.roar; }; }; ```; I think trying to use cromwell just makes it a lot **more** complicated. But I guess it's useful / helping users in other ways, so it's worth it to show how to run a singularity container as a command? /shrug. I don't totally follow what you are saying, to be honest. cromwell running singularity is just the same as cromwell running anything else, and I'm convinced (after this testing) there is no special use case. Docker has a backend and workflow runtime (because you run a docker container, the thing in a workflow) Singularity you are trying to shove into a backend or a workflow, and it has to be one or the other. It's not really making sense as a backend because it's more like an executable. I think before coming up with tools around something, it would be nice to have a clear definition of how singularity even fits in here to be special or different from any other binary.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416389694
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416389694:388,Testability,test,test,388,"okay, well I just updated the linked repo to have this ""hello"" example. I've concluded that singularity makes absolutely no sense as a backend. it's more logical as just a binary that is run (Local) or with other backend (e.g., slurm). See here --> https://github.com/vsoch/wgbs-pipeline/blob/add/singularity/docs/pages/docs/tutorial/getting-started/index.md#step-4-run-test-case for the test case, and then the example wdl I'll just show you here, because it's stupid. It's just running a command, lol. ```; task dinosaur {; String singularity_container = ""shub://vsoch/hello-world""; output {; String roar = read_string(stdout()); }; command {; singularity --silent \; run \; ${singularity_container}; }; }. workflow wf_hello {; call dinosaur; output {; dinosaur.roar; }; }; ```; I think trying to use cromwell just makes it a lot **more** complicated. But I guess it's useful / helping users in other ways, so it's worth it to show how to run a singularity container as a command? /shrug. I don't totally follow what you are saying, to be honest. cromwell running singularity is just the same as cromwell running anything else, and I'm convinced (after this testing) there is no special use case. Docker has a backend and workflow runtime (because you run a docker container, the thing in a workflow) Singularity you are trying to shove into a backend or a workflow, and it has to be one or the other. It's not really making sense as a backend because it's more like an executable. I think before coming up with tools around something, it would be nice to have a clear definition of how singularity even fits in here to be special or different from any other binary.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416389694
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416389694:1160,Testability,test,testing,1160,"okay, well I just updated the linked repo to have this ""hello"" example. I've concluded that singularity makes absolutely no sense as a backend. it's more logical as just a binary that is run (Local) or with other backend (e.g., slurm). See here --> https://github.com/vsoch/wgbs-pipeline/blob/add/singularity/docs/pages/docs/tutorial/getting-started/index.md#step-4-run-test-case for the test case, and then the example wdl I'll just show you here, because it's stupid. It's just running a command, lol. ```; task dinosaur {; String singularity_container = ""shub://vsoch/hello-world""; output {; String roar = read_string(stdout()); }; command {; singularity --silent \; run \; ${singularity_container}; }; }. workflow wf_hello {; call dinosaur; output {; dinosaur.roar; }; }; ```; I think trying to use cromwell just makes it a lot **more** complicated. But I guess it's useful / helping users in other ways, so it's worth it to show how to run a singularity container as a command? /shrug. I don't totally follow what you are saying, to be honest. cromwell running singularity is just the same as cromwell running anything else, and I'm convinced (after this testing) there is no special use case. Docker has a backend and workflow runtime (because you run a docker container, the thing in a workflow) Singularity you are trying to shove into a backend or a workflow, and it has to be one or the other. It's not really making sense as a backend because it's more like an executable. I think before coming up with tools around something, it would be nice to have a clear definition of how singularity even fits in here to be special or different from any other binary.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416389694
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416389694:1565,Usability,clear,clear,1565,"okay, well I just updated the linked repo to have this ""hello"" example. I've concluded that singularity makes absolutely no sense as a backend. it's more logical as just a binary that is run (Local) or with other backend (e.g., slurm). See here --> https://github.com/vsoch/wgbs-pipeline/blob/add/singularity/docs/pages/docs/tutorial/getting-started/index.md#step-4-run-test-case for the test case, and then the example wdl I'll just show you here, because it's stupid. It's just running a command, lol. ```; task dinosaur {; String singularity_container = ""shub://vsoch/hello-world""; output {; String roar = read_string(stdout()); }; command {; singularity --silent \; run \; ${singularity_container}; }; }. workflow wf_hello {; call dinosaur; output {; dinosaur.roar; }; }; ```; I think trying to use cromwell just makes it a lot **more** complicated. But I guess it's useful / helping users in other ways, so it's worth it to show how to run a singularity container as a command? /shrug. I don't totally follow what you are saying, to be honest. cromwell running singularity is just the same as cromwell running anything else, and I'm convinced (after this testing) there is no special use case. Docker has a backend and workflow runtime (because you run a docker container, the thing in a workflow) Singularity you are trying to shove into a backend or a workflow, and it has to be one or the other. It's not really making sense as a backend because it's more like an executable. I think before coming up with tools around something, it would be nice to have a clear definition of how singularity even fits in here to be special or different from any other binary.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416389694
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424665338:280,Energy Efficiency,energy,energy,280,"I recognize the bind to /data, I had discussion with Seth about taking this approach! I think @leepc12 has been actively working and testing and can give quick feedback? If it works, it works, there is enough change coming to singularity wrt services that I wouldn’t put too much energy into iterating over this if it’s working.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424665338
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424665338:133,Testability,test,testing,133,"I recognize the bind to /data, I had discussion with Seth about taking this approach! I think @leepc12 has been actively working and testing and can give quick feedback? If it works, it works, there is enough change coming to singularity wrt services that I wouldn’t put too much energy into iterating over this if it’s working.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424665338
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424665338:160,Usability,feedback,feedback,160,"I recognize the bind to /data, I had discussion with Seth about taking this approach! I think @leepc12 has been actively working and testing and can give quick feedback? If it works, it works, there is enough change coming to singularity wrt services that I wouldn’t put too much energy into iterating over this if it’s working.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424665338
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053:201,Availability,echo,echo,201,"@vsoch @geoffjentry I just wanted to come back to this since singularity 3.0.1 was released a few weeks ago. The backend configuration can now be made a lot more simplistic:; ```; submit-docker = """"""; echo ' \; singularity exec --bind /run,/exports,${cwd}:${docker_cwd} docker://${docker} bash ${script}' | \; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr; """"""; ```; The bind to `/run` was neccessary on our SGE cluster to make python multiprocessing work, as in [this issue](https://github.com/sylabs/singularity/issues/455). The bind to `/exports` is also specific to our cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053:83,Deployability,release,released,83,"@vsoch @geoffjentry I just wanted to come back to this since singularity 3.0.1 was released a few weeks ago. The backend configuration can now be made a lot more simplistic:; ```; submit-docker = """"""; echo ' \; singularity exec --bind /run,/exports,${cwd}:${docker_cwd} docker://${docker} bash ${script}' | \; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr; """"""; ```; The bind to `/run` was neccessary on our SGE cluster to make python multiprocessing work, as in [this issue](https://github.com/sylabs/singularity/issues/455). The bind to `/exports` is also specific to our cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053:121,Deployability,configurat,configuration,121,"@vsoch @geoffjentry I just wanted to come back to this since singularity 3.0.1 was released a few weeks ago. The backend configuration can now be made a lot more simplistic:; ```; submit-docker = """"""; echo ' \; singularity exec --bind /run,/exports,${cwd}:${docker_cwd} docker://${docker} bash ${script}' | \; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr; """"""; ```; The bind to `/run` was neccessary on our SGE cluster to make python multiprocessing work, as in [this issue](https://github.com/sylabs/singularity/issues/455). The bind to `/exports` is also specific to our cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053:121,Modifiability,config,configuration,121,"@vsoch @geoffjentry I just wanted to come back to this since singularity 3.0.1 was released a few weeks ago. The backend configuration can now be made a lot more simplistic:; ```; submit-docker = """"""; echo ' \; singularity exec --bind /run,/exports,${cwd}:${docker_cwd} docker://${docker} bash ${script}' | \; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr; """"""; ```; The bind to `/run` was neccessary on our SGE cluster to make python multiprocessing work, as in [this issue](https://github.com/sylabs/singularity/issues/455). The bind to `/exports` is also specific to our cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053:162,Usability,simpl,simplistic,162,"@vsoch @geoffjentry I just wanted to come back to this since singularity 3.0.1 was released a few weeks ago. The backend configuration can now be made a lot more simplistic:; ```; submit-docker = """"""; echo ' \; singularity exec --bind /run,/exports,${cwd}:${docker_cwd} docker://${docker} bash ${script}' | \; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr; """"""; ```; The bind to `/run` was neccessary on our SGE cluster to make python multiprocessing work, as in [this issue](https://github.com/sylabs/singularity/issues/455). The bind to `/exports` is also specific to our cluster.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438591053
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438695873:113,Availability,ping,pinged,113,"Cool, then you are probably good to go :) Did you want feedback on something in particular? I don't know why you pinged me but I'm glad to see you have a new command!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438695873
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438695873:55,Usability,feedback,feedback,55,"Cool, then you are probably good to go :) Did you want feedback on something in particular? I don't know why you pinged me but I'm glad to see you have a new command!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438695873
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454250398:27,Modifiability,config,config,27,"If you can find an example config that would be very helpful. It should be almost as simple as tweaking the `submit-docker` flag in the config to use `udocker`, no? . I have a feeling that `udocker` will be easier to use, and will be a better recommendation for Cromwell than directly using `singularity`, because it provides a very docker-like CLI that should make the switch fairly painless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454250398
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454250398:136,Modifiability,config,config,136,"If you can find an example config that would be very helpful. It should be almost as simple as tweaking the `submit-docker` flag in the config to use `udocker`, no? . I have a feeling that `udocker` will be easier to use, and will be a better recommendation for Cromwell than directly using `singularity`, because it provides a very docker-like CLI that should make the switch fairly painless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454250398
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454250398:85,Usability,simpl,simple,85,"If you can find an example config that would be very helpful. It should be almost as simple as tweaking the `submit-docker` flag in the config to use `udocker`, no? . I have a feeling that `udocker` will be easier to use, and will be a better recommendation for Cromwell than directly using `singularity`, because it provides a very docker-like CLI that should make the switch fairly painless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454250398
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:1438,Availability,error,errors,1438,"om/engine/reference/commandline/run/#capture-container-id---cidfile) through `--dockercid expected/path/to/dockercid`, _waits_ for the docker container to finish and then manually moves the `rc` file to the expected location. ### My misunderstanding. This docker script erroneously led me to believe that Cromwell needs job identifier and it uses some mechanism to continually check the status of that job. But in fact Cromwell doesn't really poll the workload manager, but my understanding from #1499 is that it actively polls the filesystem, looking for the `rc` file within the execution directory (potentially `stdout` too if its looking for the job id). This is also logically verified by looking at the `script` file that Cromwell generates, the way it collects the return code and places it in the expected directory. - Based on my initial incorrect understanding, I then believed if I couldn't get the containerId, then how would Cromwell know that the job has actually finished. ### My additional errors. These tended to redirect me away from the actual misunderstanding. . 1. In the udocker config I [posted before](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364) (corrected with comment), I was accidentally providing the `--entry-point` param, which was causing an interactive shell to open, stopping the rest of the script from executing. 2. I failed to notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:4100,Availability,alive,alive,4100,"nfig. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${runtime_minutes} ${""-c "" + cpus} --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""## PLACE THE CORRECT CONTAINER COMMAND HERE ##""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Thanks everyone for the comments above. Edit: Correct mistype: `String queue? → String? queue`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:2862,Deployability,configurat,configuration,2862," notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:1948,Energy Efficiency,schedul,schedules,1948,"ely polls the filesystem, looking for the `rc` file within the execution directory (potentially `stdout` too if its looking for the job id). This is also logically verified by looking at the `script` file that Cromwell generates, the way it collects the return code and places it in the expected directory. - Based on my initial incorrect understanding, I then believed if I couldn't get the containerId, then how would Cromwell know that the job has actually finished. ### My additional errors. These tended to redirect me away from the actual misunderstanding. . 1. In the udocker config I [posted before](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364) (corrected with comment), I was accidentally providing the `--entry-point` param, which was causing an interactive shell to open, stopping the rest of the script from executing. 2. I failed to notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycle",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:4006,Integrability,wrap,wrap,4006,"nfig. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${runtime_minutes} ${""-c "" + cpus} --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""## PLACE THE CORRECT CONTAINER COMMAND HERE ##""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Thanks everyone for the comments above. Edit: Correct mistype: `String queue? → String? queue`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:1533,Modifiability,config,config,1533,"cid`, _waits_ for the docker container to finish and then manually moves the `rc` file to the expected location. ### My misunderstanding. This docker script erroneously led me to believe that Cromwell needs job identifier and it uses some mechanism to continually check the status of that job. But in fact Cromwell doesn't really poll the workload manager, but my understanding from #1499 is that it actively polls the filesystem, looking for the `rc` file within the execution directory (potentially `stdout` too if its looking for the job id). This is also logically verified by looking at the `script` file that Cromwell generates, the way it collects the return code and places it in the expected directory. - Based on my initial incorrect understanding, I then believed if I couldn't get the containerId, then how would Cromwell know that the job has actually finished. ### My additional errors. These tended to redirect me away from the actual misunderstanding. . 1. In the udocker config I [posted before](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364) (corrected with comment), I was accidentally providing the `--entry-point` param, which was causing an interactive shell to open, stopping the rest of the script from executing. 2. I failed to notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_us",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:2287,Modifiability,config,config,2287,"in the expected directory. - Based on my initial incorrect understanding, I then believed if I couldn't get the containerId, then how would Cromwell know that the job has actually finished. ### My additional errors. These tended to redirect me away from the actual misunderstanding. . 1. In the udocker config I [posted before](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364) (corrected with comment), I was accidentally providing the `--entry-point` param, which was causing an interactive shell to open, stopping the rest of the script from executing. 2. I failed to notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:2383,Modifiability,config,configs,2383,"ded to redirect me away from the actual misunderstanding. . 1. In the udocker config I [posted before](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364) (corrected with comment), I was accidentally providing the `--entry-point` param, which was causing an interactive shell to open, stopping the rest of the script from executing. 2. I failed to notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:2615,Modifiability,config,config,2615,"ment), I was accidentally providing the `--entry-point` param, which was causing an interactive shell to open, stopping the rest of the script from executing. 2. I failed to notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:2862,Modifiability,config,configuration,2862," notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:2921,Modifiability,config,config,2921,"k the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${run",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:2965,Modifiability,config,config,2965,"ly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${runtime_minutes} ${""-c "" + cpus} --mem-per-cpu=${requested_memory_mb_per_core} \; --w",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:3526,Modifiability,config,config,3526,"nfig. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${runtime_minutes} ${""-c "" + cpus} --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""## PLACE THE CORRECT CONTAINER COMMAND HERE ##""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Thanks everyone for the comments above. Edit: Correct mistype: `String queue? → String? queue`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:3570,Modifiability,config,config,3570,"nfig. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${runtime_minutes} ${""-c "" + cpus} --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""## PLACE THE CORRECT CONTAINER COMMAND HERE ##""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Thanks everyone for the comments above. Edit: Correct mistype: `String queue? → String? queue`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:3696,Performance,queue,queue,3696,"nfig. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${runtime_minutes} ${""-c "" + cpus} --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""## PLACE THE CORRECT CONTAINER COMMAND HERE ##""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Thanks everyone for the comments above. Edit: Correct mistype: `String queue? → String? queue`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:3907,Performance,queue,queue,3907,"nfig. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${runtime_minutes} ${""-c "" + cpus} --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""## PLACE THE CORRECT CONTAINER COMMAND HERE ##""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Thanks everyone for the comments above. Edit: Correct mistype: `String queue? → String? queue`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:4266,Performance,queue,queue,4266,"nfig. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${runtime_minutes} ${""-c "" + cpus} --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""## PLACE THE CORRECT CONTAINER COMMAND HERE ##""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Thanks everyone for the comments above. Edit: Correct mistype: `String queue? → String? queue`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:4283,Performance,queue,queue,4283,"nfig. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${runtime_minutes} ${""-c "" + cpus} --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""## PLACE THE CORRECT CONTAINER COMMAND HERE ##""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Thanks everyone for the comments above. Edit: Correct mistype: `String queue? → String? queue`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:2750,Security,hash,hash-lookup,2750,"to open, stopping the rest of the script from executing. 2. I failed to notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you shoul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:3404,Security,hash,hash-lookup,3404,"nfig. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """"""; # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; ## PLACE THE CORRECT CONTAINER COMMAND HERE ##; """"""; }; }; }; }; ```. And applied for something like SLURM:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: SLURM; providers: {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? queue; String? docker; String? docker_user; """"""; # you should have a submit script as well, ; submit-docker = """"""; sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr ${""-p "" + queue} \; -t ${runtime_minutes} ${""-c "" + cpus} --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""## PLACE THE CORRECT CONTAINER COMMAND HERE ##""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Thanks everyone for the comments above. Edit: Correct mistype: `String queue? → String? queue`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:1104,Testability,log,logically,1104,"r. **Goal**: Find a suitable replacement (preferably drop in) to Docker. The two main contenders were Singularity and udocker. When you run a docker container through Cromwell, it runs a special submit script that gets the [container id](https://docs.docker.com/engine/reference/commandline/run/#capture-container-id---cidfile) through `--dockercid expected/path/to/dockercid`, _waits_ for the docker container to finish and then manually moves the `rc` file to the expected location. ### My misunderstanding. This docker script erroneously led me to believe that Cromwell needs job identifier and it uses some mechanism to continually check the status of that job. But in fact Cromwell doesn't really poll the workload manager, but my understanding from #1499 is that it actively polls the filesystem, looking for the `rc` file within the execution directory (potentially `stdout` too if its looking for the job id). This is also logically verified by looking at the `script` file that Cromwell generates, the way it collects the return code and places it in the expected directory. - Based on my initial incorrect understanding, I then believed if I couldn't get the containerId, then how would Cromwell know that the job has actually finished. ### My additional errors. These tended to redirect me away from the actual misunderstanding. . 1. In the udocker config I [posted before](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364) (corrected with comment), I was accidentally providing the `--entry-point` param, which was causing an interactive shell to open, stopping the rest of the script from executing. 2. I failed to notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An ea",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:103,Usability,learn,learn,103,"I had a bit of troubles with getting this working, but wanted to report back and hopefully someone can learn from my misunderstandings. Just skip to the results for the tl;dr. **Goal**: Find a suitable replacement (preferably drop in) to Docker. The two main contenders were Singularity and udocker. When you run a docker container through Cromwell, it runs a special submit script that gets the [container id](https://docs.docker.com/engine/reference/commandline/run/#capture-container-id---cidfile) through `--dockercid expected/path/to/dockercid`, _waits_ for the docker container to finish and then manually moves the `rc` file to the expected location. ### My misunderstanding. This docker script erroneously led me to believe that Cromwell needs job identifier and it uses some mechanism to continually check the status of that job. But in fact Cromwell doesn't really poll the workload manager, but my understanding from #1499 is that it actively polls the filesystem, looking for the `rc` file within the execution directory (potentially `stdout` too if its looking for the job id). This is also logically verified by looking at the `script` file that Cromwell generates, the way it collects the return code and places it in the expected directory. - Based on my initial incorrect understanding, I then believed if I couldn't get the containerId, then how would Cromwell know that the job has actually finished. ### My additional errors. These tended to redirect me away from the actual misunderstanding. . 1. In the udocker config I [posted before](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364) (corrected with comment), I was accidentally providing the `--entry-point` param, which was causing an interactive shell to open, stopping the rest of the script from executing. 2. I failed to notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461708333:527,Modifiability,sandbox,sandbox,527,"Would you mind explaining the difference between `pull` and `build`? The reason I did build is because I needed to know where the output image ended up, so I could run it directly. If I `singularity pull docker://ubuntu`'d the image, and then `singularity run docker://ubuntu` from the worker, it would still try to pull the image a second time, and then hang forever because it didn't have network access. . Also, isn't the ability to build a binary image something that `build` can do, not `pull`?. The only reason I built a sandbox instead is simply because my admins wouldn't set the `setuid` bit, so it wouldn't work. Am I missing something here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461708333
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461708333:399,Security,access,access,399,"Would you mind explaining the difference between `pull` and `build`? The reason I did build is because I needed to know where the output image ended up, so I could run it directly. If I `singularity pull docker://ubuntu`'d the image, and then `singularity run docker://ubuntu` from the worker, it would still try to pull the image a second time, and then hang forever because it didn't have network access. . Also, isn't the ability to build a binary image something that `build` can do, not `pull`?. The only reason I built a sandbox instead is simply because my admins wouldn't set the `setuid` bit, so it wouldn't work. Am I missing something here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461708333
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461708333:527,Testability,sandbox,sandbox,527,"Would you mind explaining the difference between `pull` and `build`? The reason I did build is because I needed to know where the output image ended up, so I could run it directly. If I `singularity pull docker://ubuntu`'d the image, and then `singularity run docker://ubuntu` from the worker, it would still try to pull the image a second time, and then hang forever because it didn't have network access. . Also, isn't the ability to build a binary image something that `build` can do, not `pull`?. The only reason I built a sandbox instead is simply because my admins wouldn't set the `setuid` bit, so it wouldn't work. Am I missing something here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461708333
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461708333:546,Usability,simpl,simply,546,"Would you mind explaining the difference between `pull` and `build`? The reason I did build is because I needed to know where the output image ended up, so I could run it directly. If I `singularity pull docker://ubuntu`'d the image, and then `singularity run docker://ubuntu` from the worker, it would still try to pull the image a second time, and then hang forever because it didn't have network access. . Also, isn't the ability to build a binary image something that `build` can do, not `pull`?. The only reason I built a sandbox instead is simply because my admins wouldn't set the `setuid` bit, so it wouldn't work. Am I missing something here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461708333
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-462488066:144,Energy Efficiency,schedul,schedulers,144,"Why not “Using containers with Cromwell”? Have a section for singularity (and undocker), and have a subsection of that for singularity with job schedulers?. There are other Cromwell github threads that have the user goal as practically: I can’t use docker, what do I do? That’s definitely a part of my use case, and reproducibility is a big point as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-462488066
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-462488066:78,Usability,undo,undocker,78,"Why not “Using containers with Cromwell”? Have a section for singularity (and undocker), and have a subsection of that for singularity with job schedulers?. There are other Cromwell github threads that have the user goal as practically: I can’t use docker, what do I do? That’s definitely a part of my use case, and reproducibility is a big point as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-462488066
https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468970428:116,Usability,feedback,feedback,116,Everyone in this conversation please see -> https://github.com/broadinstitute/cromwell/pull/4697/files We will need feedback / review of the various files to make sure they are good.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-468970428
https://github.com/broadinstitute/cromwell/issues/4049#issuecomment-417482228:562,Usability,intuit,intuition,562,"You can represent `0.00005` as a float, and I'd expect you to print it as a decimal. `0.000050000000000001` isn't representable, but I don't mind losing the tail end of that. Similarly, you can represent `10000000` as a float, and I'd expect you to print it as a decimal (but it prints as 1E7). `10000000.00000001` isnt representable and I don't care. At some point you can write 1 with N 0s after it for a value of N that makes it not exactly representable in a float. At that point I'm okay with switching to exponential. This is an argument based entirely on intuition, mind you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4049#issuecomment-417482228
https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019:499,Performance,concurren,concurrent,499,"I think we might be having a similar issue:; ```; Bad output 'pindel.deletions': Futures timed out after [60 seconds]; Bad output 'pindel.insertions': Futures timed out after [60 seconds]; Bad output 'pindel.long_insertions': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. If we don't want to change our caching strategy, can we simply increase the timeout? Can that be done with `akka.http.server.request-timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019
https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019:560,Performance,concurren,concurrent,560,"I think we might be having a similar issue:; ```; Bad output 'pindel.deletions': Futures timed out after [60 seconds]; Bad output 'pindel.insertions': Futures timed out after [60 seconds]; Bad output 'pindel.long_insertions': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. If we don't want to change our caching strategy, can we simply increase the timeout? Can that be done with `akka.http.server.request-timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019
https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019:626,Performance,concurren,concurrent,626,"I think we might be having a similar issue:; ```; Bad output 'pindel.deletions': Futures timed out after [60 seconds]; Bad output 'pindel.insertions': Futures timed out after [60 seconds]; Bad output 'pindel.long_insertions': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. If we don't want to change our caching strategy, can we simply increase the timeout? Can that be done with `akka.http.server.request-timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019
https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019:699,Performance,concurren,concurrent,699,"I think we might be having a similar issue:; ```; Bad output 'pindel.deletions': Futures timed out after [60 seconds]; Bad output 'pindel.insertions': Futures timed out after [60 seconds]; Bad output 'pindel.long_insertions': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. If we don't want to change our caching strategy, can we simply increase the timeout? Can that be done with `akka.http.server.request-timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019
https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019:1020,Performance,concurren,concurrent,1020,"I think we might be having a similar issue:; ```; Bad output 'pindel.deletions': Futures timed out after [60 seconds]; Bad output 'pindel.insertions': Futures timed out after [60 seconds]; Bad output 'pindel.long_insertions': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. If we don't want to change our caching strategy, can we simply increase the timeout? Can that be done with `akka.http.server.request-timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019
https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019:1731,Safety,timeout,timeout,1731,"I think we might be having a similar issue:; ```; Bad output 'pindel.deletions': Futures timed out after [60 seconds]; Bad output 'pindel.insertions': Futures timed out after [60 seconds]; Bad output 'pindel.long_insertions': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. If we don't want to change our caching strategy, can we simply increase the timeout? Can that be done with `akka.http.server.request-timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019
https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019:1788,Safety,timeout,timeout,1788,"I think we might be having a similar issue:; ```; Bad output 'pindel.deletions': Futures timed out after [60 seconds]; Bad output 'pindel.insertions': Futures timed out after [60 seconds]; Bad output 'pindel.long_insertions': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. If we don't want to change our caching strategy, can we simply increase the timeout? Can that be done with `akka.http.server.request-timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019
https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019:1711,Usability,simpl,simply,1711,"I think we might be having a similar issue:; ```; Bad output 'pindel.deletions': Futures timed out after [60 seconds]; Bad output 'pindel.insertions': Futures timed out after [60 seconds]; Bad output 'pindel.long_insertions': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. If we don't want to change our caching strategy, can we simply increase the timeout? Can that be done with `akka.http.server.request-timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-444687019
https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912:90,Availability,error,errors,90,"We recently adjusted this timeout, I would try again with the latest `develop` build. The errors are likely happening because Cromwell is putting work into its IO queue faster than it can finish it. There's a backpressure system that aims to prevent this by stopping Cromwell picking up new work when IO operations aren't getting finished fast enough, but in this case it isn't responsive enough to keep you out of trouble. If you continue seeing these errors, you can tune down `system.io.command-backpressure-staleness` to make the engine more sensitive to long IO queues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912
https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912:453,Availability,error,errors,453,"We recently adjusted this timeout, I would try again with the latest `develop` build. The errors are likely happening because Cromwell is putting work into its IO queue faster than it can finish it. There's a backpressure system that aims to prevent this by stopping Cromwell picking up new work when IO operations aren't getting finished fast enough, but in this case it isn't responsive enough to keep you out of trouble. If you continue seeing these errors, you can tune down `system.io.command-backpressure-staleness` to make the engine more sensitive to long IO queues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912
https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912:474,Availability,down,down,474,"We recently adjusted this timeout, I would try again with the latest `develop` build. The errors are likely happening because Cromwell is putting work into its IO queue faster than it can finish it. There's a backpressure system that aims to prevent this by stopping Cromwell picking up new work when IO operations aren't getting finished fast enough, but in this case it isn't responsive enough to keep you out of trouble. If you continue seeing these errors, you can tune down `system.io.command-backpressure-staleness` to make the engine more sensitive to long IO queues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912
https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912:163,Performance,queue,queue,163,"We recently adjusted this timeout, I would try again with the latest `develop` build. The errors are likely happening because Cromwell is putting work into its IO queue faster than it can finish it. There's a backpressure system that aims to prevent this by stopping Cromwell picking up new work when IO operations aren't getting finished fast enough, but in this case it isn't responsive enough to keep you out of trouble. If you continue seeing these errors, you can tune down `system.io.command-backpressure-staleness` to make the engine more sensitive to long IO queues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912
https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912:469,Performance,tune,tune,469,"We recently adjusted this timeout, I would try again with the latest `develop` build. The errors are likely happening because Cromwell is putting work into its IO queue faster than it can finish it. There's a backpressure system that aims to prevent this by stopping Cromwell picking up new work when IO operations aren't getting finished fast enough, but in this case it isn't responsive enough to keep you out of trouble. If you continue seeing these errors, you can tune down `system.io.command-backpressure-staleness` to make the engine more sensitive to long IO queues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912
https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912:567,Performance,queue,queues,567,"We recently adjusted this timeout, I would try again with the latest `develop` build. The errors are likely happening because Cromwell is putting work into its IO queue faster than it can finish it. There's a backpressure system that aims to prevent this by stopping Cromwell picking up new work when IO operations aren't getting finished fast enough, but in this case it isn't responsive enough to keep you out of trouble. If you continue seeing these errors, you can tune down `system.io.command-backpressure-staleness` to make the engine more sensitive to long IO queues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912
https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912:26,Safety,timeout,timeout,26,"We recently adjusted this timeout, I would try again with the latest `develop` build. The errors are likely happening because Cromwell is putting work into its IO queue faster than it can finish it. There's a backpressure system that aims to prevent this by stopping Cromwell picking up new work when IO operations aren't getting finished fast enough, but in this case it isn't responsive enough to keep you out of trouble. If you continue seeing these errors, you can tune down `system.io.command-backpressure-staleness` to make the engine more sensitive to long IO queues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912
https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912:378,Usability,responsiv,responsive,378,"We recently adjusted this timeout, I would try again with the latest `develop` build. The errors are likely happening because Cromwell is putting work into its IO queue faster than it can finish it. There's a backpressure system that aims to prevent this by stopping Cromwell picking up new work when IO operations aren't getting finished fast enough, but in this case it isn't responsive enough to keep you out of trouble. If you continue seeing these errors, you can tune down `system.io.command-backpressure-staleness` to make the engine more sensitive to long IO queues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912
https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081:1105,Deployability,configurat,configuration,1105,"> I don't have a big problem with this change except that it only half-solves the problem as far as I can see - because one image might have a different root requirement than another image, and restarting Cromwell between running those two tasks is not going to be a viable answer. @cjllanwarne I do agree that different containers might have different requirements. You are completely right. However I do not think this will be a big problem in practice. . - On docker this does not matter. The docker runtime will simply create the required root folder. So there never was a problem here. - For singularity it does. But luckily there is [biocontainers](https://github.com/BioContainers/containers). These all have a `/data` folder. So there this problem is also not applicable. - In case there are people who prefer to invent their own containers instead of using those from biocontainers, they will probably invent their own standard. And if they use some other solution and build on top of that, they will probably adhere to that standard. Furthermore, implementing a solution that enables a per task configuration would:; * Drastically increase the configuration time needed to get a cromwell workflow running with singularity. By orders of magnitude. Since setting a per-task configuration is not going to be fast by any measure.; * Require more complex code to fix.; * Require complex test code to cover all use cases. I think the cost/benefit ratio is rather bad in this case. I do not think there are much use cases for fine-grained control as I outlinded above, and the code requirements are rather high. The fix in the pull request solves the problem that we have (and probably other singularity users have) in the simplest way possible. I think it covers most singularity use cases. And if some people need this per task configuration, those people can also make their own pull request :wink:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081
https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081:1154,Deployability,configurat,configuration,1154,"> I don't have a big problem with this change except that it only half-solves the problem as far as I can see - because one image might have a different root requirement than another image, and restarting Cromwell between running those two tasks is not going to be a viable answer. @cjllanwarne I do agree that different containers might have different requirements. You are completely right. However I do not think this will be a big problem in practice. . - On docker this does not matter. The docker runtime will simply create the required root folder. So there never was a problem here. - For singularity it does. But luckily there is [biocontainers](https://github.com/BioContainers/containers). These all have a `/data` folder. So there this problem is also not applicable. - In case there are people who prefer to invent their own containers instead of using those from biocontainers, they will probably invent their own standard. And if they use some other solution and build on top of that, they will probably adhere to that standard. Furthermore, implementing a solution that enables a per task configuration would:; * Drastically increase the configuration time needed to get a cromwell workflow running with singularity. By orders of magnitude. Since setting a per-task configuration is not going to be fast by any measure.; * Require more complex code to fix.; * Require complex test code to cover all use cases. I think the cost/benefit ratio is rather bad in this case. I do not think there are much use cases for fine-grained control as I outlinded above, and the code requirements are rather high. The fix in the pull request solves the problem that we have (and probably other singularity users have) in the simplest way possible. I think it covers most singularity use cases. And if some people need this per task configuration, those people can also make their own pull request :wink:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081
https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081:1282,Deployability,configurat,configuration,1282,"> I don't have a big problem with this change except that it only half-solves the problem as far as I can see - because one image might have a different root requirement than another image, and restarting Cromwell between running those two tasks is not going to be a viable answer. @cjllanwarne I do agree that different containers might have different requirements. You are completely right. However I do not think this will be a big problem in practice. . - On docker this does not matter. The docker runtime will simply create the required root folder. So there never was a problem here. - For singularity it does. But luckily there is [biocontainers](https://github.com/BioContainers/containers). These all have a `/data` folder. So there this problem is also not applicable. - In case there are people who prefer to invent their own containers instead of using those from biocontainers, they will probably invent their own standard. And if they use some other solution and build on top of that, they will probably adhere to that standard. Furthermore, implementing a solution that enables a per task configuration would:; * Drastically increase the configuration time needed to get a cromwell workflow running with singularity. By orders of magnitude. Since setting a per-task configuration is not going to be fast by any measure.; * Require more complex code to fix.; * Require complex test code to cover all use cases. I think the cost/benefit ratio is rather bad in this case. I do not think there are much use cases for fine-grained control as I outlinded above, and the code requirements are rather high. The fix in the pull request solves the problem that we have (and probably other singularity users have) in the simplest way possible. I think it covers most singularity use cases. And if some people need this per task configuration, those people can also make their own pull request :wink:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081
https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081:1833,Deployability,configurat,configuration,1833,"> I don't have a big problem with this change except that it only half-solves the problem as far as I can see - because one image might have a different root requirement than another image, and restarting Cromwell between running those two tasks is not going to be a viable answer. @cjllanwarne I do agree that different containers might have different requirements. You are completely right. However I do not think this will be a big problem in practice. . - On docker this does not matter. The docker runtime will simply create the required root folder. So there never was a problem here. - For singularity it does. But luckily there is [biocontainers](https://github.com/BioContainers/containers). These all have a `/data` folder. So there this problem is also not applicable. - In case there are people who prefer to invent their own containers instead of using those from biocontainers, they will probably invent their own standard. And if they use some other solution and build on top of that, they will probably adhere to that standard. Furthermore, implementing a solution that enables a per task configuration would:; * Drastically increase the configuration time needed to get a cromwell workflow running with singularity. By orders of magnitude. Since setting a per-task configuration is not going to be fast by any measure.; * Require more complex code to fix.; * Require complex test code to cover all use cases. I think the cost/benefit ratio is rather bad in this case. I do not think there are much use cases for fine-grained control as I outlinded above, and the code requirements are rather high. The fix in the pull request solves the problem that we have (and probably other singularity users have) in the simplest way possible. I think it covers most singularity use cases. And if some people need this per task configuration, those people can also make their own pull request :wink:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081
https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081:1105,Modifiability,config,configuration,1105,"> I don't have a big problem with this change except that it only half-solves the problem as far as I can see - because one image might have a different root requirement than another image, and restarting Cromwell between running those two tasks is not going to be a viable answer. @cjllanwarne I do agree that different containers might have different requirements. You are completely right. However I do not think this will be a big problem in practice. . - On docker this does not matter. The docker runtime will simply create the required root folder. So there never was a problem here. - For singularity it does. But luckily there is [biocontainers](https://github.com/BioContainers/containers). These all have a `/data` folder. So there this problem is also not applicable. - In case there are people who prefer to invent their own containers instead of using those from biocontainers, they will probably invent their own standard. And if they use some other solution and build on top of that, they will probably adhere to that standard. Furthermore, implementing a solution that enables a per task configuration would:; * Drastically increase the configuration time needed to get a cromwell workflow running with singularity. By orders of magnitude. Since setting a per-task configuration is not going to be fast by any measure.; * Require more complex code to fix.; * Require complex test code to cover all use cases. I think the cost/benefit ratio is rather bad in this case. I do not think there are much use cases for fine-grained control as I outlinded above, and the code requirements are rather high. The fix in the pull request solves the problem that we have (and probably other singularity users have) in the simplest way possible. I think it covers most singularity use cases. And if some people need this per task configuration, those people can also make their own pull request :wink:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081
https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081:1154,Modifiability,config,configuration,1154,"> I don't have a big problem with this change except that it only half-solves the problem as far as I can see - because one image might have a different root requirement than another image, and restarting Cromwell between running those two tasks is not going to be a viable answer. @cjllanwarne I do agree that different containers might have different requirements. You are completely right. However I do not think this will be a big problem in practice. . - On docker this does not matter. The docker runtime will simply create the required root folder. So there never was a problem here. - For singularity it does. But luckily there is [biocontainers](https://github.com/BioContainers/containers). These all have a `/data` folder. So there this problem is also not applicable. - In case there are people who prefer to invent their own containers instead of using those from biocontainers, they will probably invent their own standard. And if they use some other solution and build on top of that, they will probably adhere to that standard. Furthermore, implementing a solution that enables a per task configuration would:; * Drastically increase the configuration time needed to get a cromwell workflow running with singularity. By orders of magnitude. Since setting a per-task configuration is not going to be fast by any measure.; * Require more complex code to fix.; * Require complex test code to cover all use cases. I think the cost/benefit ratio is rather bad in this case. I do not think there are much use cases for fine-grained control as I outlinded above, and the code requirements are rather high. The fix in the pull request solves the problem that we have (and probably other singularity users have) in the simplest way possible. I think it covers most singularity use cases. And if some people need this per task configuration, those people can also make their own pull request :wink:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081
https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081:1282,Modifiability,config,configuration,1282,"> I don't have a big problem with this change except that it only half-solves the problem as far as I can see - because one image might have a different root requirement than another image, and restarting Cromwell between running those two tasks is not going to be a viable answer. @cjllanwarne I do agree that different containers might have different requirements. You are completely right. However I do not think this will be a big problem in practice. . - On docker this does not matter. The docker runtime will simply create the required root folder. So there never was a problem here. - For singularity it does. But luckily there is [biocontainers](https://github.com/BioContainers/containers). These all have a `/data` folder. So there this problem is also not applicable. - In case there are people who prefer to invent their own containers instead of using those from biocontainers, they will probably invent their own standard. And if they use some other solution and build on top of that, they will probably adhere to that standard. Furthermore, implementing a solution that enables a per task configuration would:; * Drastically increase the configuration time needed to get a cromwell workflow running with singularity. By orders of magnitude. Since setting a per-task configuration is not going to be fast by any measure.; * Require more complex code to fix.; * Require complex test code to cover all use cases. I think the cost/benefit ratio is rather bad in this case. I do not think there are much use cases for fine-grained control as I outlinded above, and the code requirements are rather high. The fix in the pull request solves the problem that we have (and probably other singularity users have) in the simplest way possible. I think it covers most singularity use cases. And if some people need this per task configuration, those people can also make their own pull request :wink:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081
https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081:1833,Modifiability,config,configuration,1833,"> I don't have a big problem with this change except that it only half-solves the problem as far as I can see - because one image might have a different root requirement than another image, and restarting Cromwell between running those two tasks is not going to be a viable answer. @cjllanwarne I do agree that different containers might have different requirements. You are completely right. However I do not think this will be a big problem in practice. . - On docker this does not matter. The docker runtime will simply create the required root folder. So there never was a problem here. - For singularity it does. But luckily there is [biocontainers](https://github.com/BioContainers/containers). These all have a `/data` folder. So there this problem is also not applicable. - In case there are people who prefer to invent their own containers instead of using those from biocontainers, they will probably invent their own standard. And if they use some other solution and build on top of that, they will probably adhere to that standard. Furthermore, implementing a solution that enables a per task configuration would:; * Drastically increase the configuration time needed to get a cromwell workflow running with singularity. By orders of magnitude. Since setting a per-task configuration is not going to be fast by any measure.; * Require more complex code to fix.; * Require complex test code to cover all use cases. I think the cost/benefit ratio is rather bad in this case. I do not think there are much use cases for fine-grained control as I outlinded above, and the code requirements are rather high. The fix in the pull request solves the problem that we have (and probably other singularity users have) in the simplest way possible. I think it covers most singularity use cases. And if some people need this per task configuration, those people can also make their own pull request :wink:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081
https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081:1392,Testability,test,test,1392,"> I don't have a big problem with this change except that it only half-solves the problem as far as I can see - because one image might have a different root requirement than another image, and restarting Cromwell between running those two tasks is not going to be a viable answer. @cjllanwarne I do agree that different containers might have different requirements. You are completely right. However I do not think this will be a big problem in practice. . - On docker this does not matter. The docker runtime will simply create the required root folder. So there never was a problem here. - For singularity it does. But luckily there is [biocontainers](https://github.com/BioContainers/containers). These all have a `/data` folder. So there this problem is also not applicable. - In case there are people who prefer to invent their own containers instead of using those from biocontainers, they will probably invent their own standard. And if they use some other solution and build on top of that, they will probably adhere to that standard. Furthermore, implementing a solution that enables a per task configuration would:; * Drastically increase the configuration time needed to get a cromwell workflow running with singularity. By orders of magnitude. Since setting a per-task configuration is not going to be fast by any measure.; * Require more complex code to fix.; * Require complex test code to cover all use cases. I think the cost/benefit ratio is rather bad in this case. I do not think there are much use cases for fine-grained control as I outlinded above, and the code requirements are rather high. The fix in the pull request solves the problem that we have (and probably other singularity users have) in the simplest way possible. I think it covers most singularity use cases. And if some people need this per task configuration, those people can also make their own pull request :wink:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081
https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081:516,Usability,simpl,simply,516,"> I don't have a big problem with this change except that it only half-solves the problem as far as I can see - because one image might have a different root requirement than another image, and restarting Cromwell between running those two tasks is not going to be a viable answer. @cjllanwarne I do agree that different containers might have different requirements. You are completely right. However I do not think this will be a big problem in practice. . - On docker this does not matter. The docker runtime will simply create the required root folder. So there never was a problem here. - For singularity it does. But luckily there is [biocontainers](https://github.com/BioContainers/containers). These all have a `/data` folder. So there this problem is also not applicable. - In case there are people who prefer to invent their own containers instead of using those from biocontainers, they will probably invent their own standard. And if they use some other solution and build on top of that, they will probably adhere to that standard. Furthermore, implementing a solution that enables a per task configuration would:; * Drastically increase the configuration time needed to get a cromwell workflow running with singularity. By orders of magnitude. Since setting a per-task configuration is not going to be fast by any measure.; * Require more complex code to fix.; * Require complex test code to cover all use cases. I think the cost/benefit ratio is rather bad in this case. I do not think there are much use cases for fine-grained control as I outlinded above, and the code requirements are rather high. The fix in the pull request solves the problem that we have (and probably other singularity users have) in the simplest way possible. I think it covers most singularity use cases. And if some people need this per task configuration, those people can also make their own pull request :wink:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081
https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081:1726,Usability,simpl,simplest,1726,"> I don't have a big problem with this change except that it only half-solves the problem as far as I can see - because one image might have a different root requirement than another image, and restarting Cromwell between running those two tasks is not going to be a viable answer. @cjllanwarne I do agree that different containers might have different requirements. You are completely right. However I do not think this will be a big problem in practice. . - On docker this does not matter. The docker runtime will simply create the required root folder. So there never was a problem here. - For singularity it does. But luckily there is [biocontainers](https://github.com/BioContainers/containers). These all have a `/data` folder. So there this problem is also not applicable. - In case there are people who prefer to invent their own containers instead of using those from biocontainers, they will probably invent their own standard. And if they use some other solution and build on top of that, they will probably adhere to that standard. Furthermore, implementing a solution that enables a per task configuration would:; * Drastically increase the configuration time needed to get a cromwell workflow running with singularity. By orders of magnitude. Since setting a per-task configuration is not going to be fast by any measure.; * Require more complex code to fix.; * Require complex test code to cover all use cases. I think the cost/benefit ratio is rather bad in this case. I do not think there are much use cases for fine-grained control as I outlinded above, and the code requirements are rather high. The fix in the pull request solves the problem that we have (and probably other singularity users have) in the simplest way possible. I think it covers most singularity use cases. And if some people need this per task configuration, those people can also make their own pull request :wink:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420906081
https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-421985663:96,Safety,avoid,avoid,96,@cjllanwarne thanks for the feedback. I added the requested code. In the testing I took care to avoid code duplication. Is the code up to cromwell standards?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-421985663
https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-421985663:73,Testability,test,testing,73,@cjllanwarne thanks for the feedback. I added the requested code. In the testing I took care to avoid code duplication. Is the code up to cromwell standards?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-421985663
https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-421985663:28,Usability,feedback,feedback,28,@cjllanwarne thanks for the feedback. I added the requested code. In the testing I took care to avoid code duplication. Is the code up to cromwell standards?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-421985663
https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-422012622:22,Testability,test,test,22,"Hmm the dockerScripts test does fail. But I do not see a clear reason why this is, and how it is related to the pull request.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-422012622
https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-422012622:57,Usability,clear,clear,57,"Hmm the dockerScripts test does fail. But I do not see a clear reason why this is, and how it is related to the pull request.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-422012622
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:2543,Availability,heartbeat,heartbeat,2543,"CWL --type-version v1.0 --workflow-root main; [2018-10-23 17:48:48,28] [info] Running with database db.url = jdbc:hsqldb:mem:3bd78058-b880-451a-b3ef-71a48a2a17ce;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,34] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-23 17:48:55,36] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-23 17:48:55,49] [info] Running with database db.url = jdbc:hsqldb:mem:35603602-72c4-4c47-8662-7fdf49e59cf1;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,95] [info] Slf4jLogger started; [2018-10-23 17:48:56,03] [info] Pre Processing Workflow...; [2018-10-23 17:48:56,20] [info] Pre-Processing /home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl; [2018-10-23 17:49:21,60] [info] Pre Processing Inputs...; [2018-10-23 17:49:21,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-5deb9cb"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-23 17:49:21,93] [info] Metadata summary refreshing every 2 seconds.; [2018-10-23 17:49:22,12] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:2607,Availability,heartbeat,heartbeatInterval,2607,"CWL --type-version v1.0 --workflow-root main; [2018-10-23 17:48:48,28] [info] Running with database db.url = jdbc:hsqldb:mem:3bd78058-b880-451a-b3ef-71a48a2a17ce;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,34] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-23 17:48:55,36] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-23 17:48:55,49] [info] Running with database db.url = jdbc:hsqldb:mem:35603602-72c4-4c47-8662-7fdf49e59cf1;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,95] [info] Slf4jLogger started; [2018-10-23 17:48:56,03] [info] Pre Processing Workflow...; [2018-10-23 17:48:56,20] [info] Pre-Processing /home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl; [2018-10-23 17:49:21,60] [info] Pre Processing Inputs...; [2018-10-23 17:49:21,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-5deb9cb"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-23 17:49:21,93] [info] Metadata summary refreshing every 2 seconds.; [2018-10-23 17:49:22,12] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:4398,Availability,error,error,4398," (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,80] [info] 1 new workflows fetched; [2018-10-23 17:49:23,81] [info] WorkflowManagerActor Starting workflow d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,84] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-10-23 17:49:23,84] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-10-23 17:49:23,88] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-23 17:49:23,97] [info] MaterializeWorkflowDescriptorActor [d186ca94]: Parsing workflow as CWL v1.0; [2018-10-23 17:49:24,53] [error] WorkflowManagerActor Workflow d186ca94-b85b-4729-befc-8ad28a05976c failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Custom type file:///home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl#capture_kit.yml/capture_kit was referred to but not found in schema def SchemaDefRequirement([Lshapeless.$colon$plus$colon;@52b558ea,SchemaDefRequirement).; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:214); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:184); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowD",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:7423,Availability,down,down,7423,"scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2018-10-23 17:49:24,53] [info] WorkflowManagerActor WorkflowActor-d186ca94-b85b-4729-befc-8ad28a05976c is in a terminal state: WorkflowFailedState; [2018-10-23 17:49:27,64] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2018-10-23 17:49:32,16] [info] Workflow polling stopped; [2018-10-23 17:49:32,17] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-10-23 17:49:32,18] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-10-23 17:49:32,18] [info] Aborting all running workflows.; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:7511,Availability,down,down,7511,"scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2018-10-23 17:49:24,53] [info] WorkflowManagerActor WorkflowActor-d186ca94-b85b-4729-befc-8ad28a05976c is in a terminal state: WorkflowFailedState; [2018-10-23 17:49:27,64] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2018-10-23 17:49:32,16] [info] Workflow polling stopped; [2018-10-23 17:49:32,17] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-10-23 17:49:32,18] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-10-23 17:49:32,18] [info] Aborting all running workflows.; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:7667,Availability,down,down,7667,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:7954,Availability,down,down,7954,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:8137,Availability,down,down,8137,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:8246,Availability,down,down,8246,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:8340,Availability,down,down,8340,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:8489,Availability,down,down,8489,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:8601,Availability,down,down,8601,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:8733,Availability,down,down,8733,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:8826,Availability,down,down,8826,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:8933,Availability,down,down,8933,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:9023,Availability,down,down,9023,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:9100,Availability,down,down,9100,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:9474,Availability,down,down,9474,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:2553,Deployability,configurat,configuration,2553,"CWL --type-version v1.0 --workflow-root main; [2018-10-23 17:48:48,28] [info] Running with database db.url = jdbc:hsqldb:mem:3bd78058-b880-451a-b3ef-71a48a2a17ce;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,34] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-23 17:48:55,36] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-23 17:48:55,49] [info] Running with database db.url = jdbc:hsqldb:mem:35603602-72c4-4c47-8662-7fdf49e59cf1;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,95] [info] Slf4jLogger started; [2018-10-23 17:48:56,03] [info] Pre Processing Workflow...; [2018-10-23 17:48:56,20] [info] Pre-Processing /home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl; [2018-10-23 17:49:21,60] [info] Pre Processing Inputs...; [2018-10-23 17:49:21,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-5deb9cb"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-23 17:49:21,93] [info] Metadata summary refreshing every 2 seconds.; [2018-10-23 17:49:22,12] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:4087,Integrability,message,message,4087,"lush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,80] [info] 1 new workflows fetched; [2018-10-23 17:49:23,81] [info] WorkflowManagerActor Starting workflow d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,84] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-10-23 17:49:23,84] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-10-23 17:49:23,88] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-23 17:49:23,97] [info] MaterializeWorkflowDescriptorActor [d186ca94]: Parsing workflow as CWL v1.0; [2018-10-23 17:49:24,53] [error] WorkflowManagerActor Workflow d186ca94-b85b-4729-befc-8ad28a05976c failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Custom type file:///home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl#capture_kit.yml/capture_kit was referred to but not found in schema def SchemaDefRequirement([Lshapeless.$colo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:8616,Integrability,message,messages,8616,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:8948,Integrability,message,messages,8948,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:9038,Integrability,message,messages,9038,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:2553,Modifiability,config,configuration,2553,"CWL --type-version v1.0 --workflow-root main; [2018-10-23 17:48:48,28] [info] Running with database db.url = jdbc:hsqldb:mem:3bd78058-b880-451a-b3ef-71a48a2a17ce;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,34] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-23 17:48:55,36] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-23 17:48:55,49] [info] Running with database db.url = jdbc:hsqldb:mem:35603602-72c4-4c47-8662-7fdf49e59cf1;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,95] [info] Slf4jLogger started; [2018-10-23 17:48:56,03] [info] Pre Processing Workflow...; [2018-10-23 17:48:56,20] [info] Pre-Processing /home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl; [2018-10-23 17:49:21,60] [info] Pre Processing Inputs...; [2018-10-23 17:49:21,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-5deb9cb"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-23 17:49:21,93] [info] Metadata summary refreshing every 2 seconds.; [2018-10-23 17:49:22,12] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:2844,Modifiability,config,configured,2844,"7:48:55,34] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-23 17:48:55,36] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-23 17:48:55,49] [info] Running with database db.url = jdbc:hsqldb:mem:35603602-72c4-4c47-8662-7fdf49e59cf1;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,95] [info] Slf4jLogger started; [2018-10-23 17:48:56,03] [info] Pre Processing Workflow...; [2018-10-23 17:48:56,20] [info] Pre-Processing /home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl; [2018-10-23 17:49:21,60] [info] Pre Processing Inputs...; [2018-10-23 17:49:21,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-5deb9cb"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-23 17:49:21,93] [info] Metadata summary refreshing every 2 seconds.; [2018-10-23 17:49:22,12] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,80] [info] 1 new workflows fetched; [2018-10-23 17:49:23,81] [info] WorkflowManagerActor Starting workflow d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,83] [info] WorkflowManagerActor Su",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:2964,Modifiability,config,configured,2964,"ch size of 100000; [2018-10-23 17:48:55,36] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-23 17:48:55,49] [info] Running with database db.url = jdbc:hsqldb:mem:35603602-72c4-4c47-8662-7fdf49e59cf1;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,95] [info] Slf4jLogger started; [2018-10-23 17:48:56,03] [info] Pre Processing Workflow...; [2018-10-23 17:48:56,20] [info] Pre-Processing /home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl; [2018-10-23 17:49:21,60] [info] Pre Processing Inputs...; [2018-10-23 17:49:21,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-5deb9cb"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-23 17:49:21,93] [info] Metadata summary refreshing every 2 seconds.; [2018-10-23 17:49:22,12] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,80] [info] 1 new workflows fetched; [2018-10-23 17:49:23,81] [info] WorkflowManagerActor Starting workflow d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,84] [info] Retrieved 1 wo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:3085,Modifiability,config,configured,3085,"o] Running with database db.url = jdbc:hsqldb:mem:35603602-72c4-4c47-8662-7fdf49e59cf1;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,95] [info] Slf4jLogger started; [2018-10-23 17:48:56,03] [info] Pre Processing Workflow...; [2018-10-23 17:48:56,20] [info] Pre-Processing /home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl; [2018-10-23 17:49:21,60] [info] Pre Processing Inputs...; [2018-10-23 17:49:21,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-5deb9cb"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-23 17:49:21,93] [info] Metadata summary refreshing every 2 seconds.; [2018-10-23 17:49:22,12] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,80] [info] 1 new workflows fetched; [2018-10-23 17:49:23,81] [info] WorkflowManagerActor Starting workflow d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,84] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-10-23 17:49:23,84] [warn] SingleWorkflowRunnerActor: received unexpected messa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:4192,Modifiability,config,configured,4192,"lush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-23 17:49:22,22] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-23 17:49:23,62] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-23 17:49:23,67] [info] SingleWorkflowRunnerActor: Version 37-634ac5b-SNAP; [2018-10-23 17:49:23,68] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-23 17:49:23,76] [info] CWL (v1.0) workflow d186ca94-b85b-4729-befc-8ad28a05976c submitted; [2018-10-23 17:49:23,80] [info] SingleWorkflowRunnerActor: Workflow submitted d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,80] [info] 1 new workflows fetched; [2018-10-23 17:49:23,81] [info] WorkflowManagerActor Starting workflow d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-d186ca94-b85b-4729-befc-8ad28a05976c; [2018-10-23 17:49:23,84] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-10-23 17:49:23,84] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-10-23 17:49:23,88] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-23 17:49:23,97] [info] MaterializeWorkflowDescriptorActor [d186ca94]: Parsing workflow as CWL v1.0; [2018-10-23 17:49:24,53] [error] WorkflowManagerActor Workflow d186ca94-b85b-4729-befc-8ad28a05976c failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Custom type file:///home/jeremiah/code/gdc-dnaseq-cwl/workflows/bamfastq_align/test_pack.cwl#capture_kit.yml/capture_kit was referred to but not found in schema def SchemaDefRequirement([Lshapeless.$colo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:8609,Performance,queue,queued,8609,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:8941,Performance,queue,queued,8941,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:9031,Performance,queue,queued,9031,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:1377,Testability,test,test,1377,"am; type: File; outputSource: touch_bam/output. steps:; - id: touch_bam; run: touch.cwl; in:; - id: input; source: bam; out:; - id: output; ```; **test_wf.json**; ```; {; ""bam"": ""a.bam"",; ""capture_kit"": {; ""bait"": ""abait""; }; }; ```; **capture_kit.yml**; ```; - name: capture_kit; type: record; fields:; - name: bait; type: string; ```; **touch.cwl**; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0. requirements:; - class: DockerRequirement; dockerPull: ubuntu:bionic-20180426. class: CommandLineTool. inputs:; - id: input; type: string; inputBinding:; position: 0. outputs:; - id: output; type: File; outputBinding:; glob: $(inputs.input); ; baseCommand: [touch]; ```. this works:; ```; (cwl) [jeremiah@sasha bamfastq_align]$ java -Dconfig.file=/home/jeremiah/code/cromwell/cromwell.examples.conf -jar /home/jeremiah/code/cromwell/server/target/scala-2.12/cromwell-37-634ac5b-SNAP.jar run test_wf.cwl --inputs test_wf.json --type CWL --type-version v1.0; ```; but this fails:; ```; (cwl) [jeremiah@sasha bamfastq_align]$ cwltool --pack test.cwl > test_pack.cwl; (cwl) [jeremiah@sasha bamfastq_align]$ java -Dconfig.file=/home/jeremiah/code/cromwell/cromwell.examples.conf -jar /home/jeremiah/code/cromwell/server/target/scala-2.12/cromwell-37-634ac5b-SNAP.jar run test_pack.cwl --inputs test_wf.json --type CWL --type-version v1.0 --workflow-root main; [2018-10-23 17:48:48,28] [info] Running with database db.url = jdbc:hsqldb:mem:3bd78058-b880-451a-b3ef-71a48a2a17ce;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,34] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-23 17:48:55,36] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-23 17:48:55,49] [info] Running with database db.url = jdbc:hsqldb:mem:35603602-72c4-4c47-8662-7fdf49e59cf1;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,95] [info] Slf4jLogger started; [2018-10-23 17:48:56,03] [info] Pre Processing Workflow...; [2018-10-23 17:4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:17,Usability,simpl,simplified,17,"I've generated a simplified case to reproduce this issue:. **test_wf.cwl**; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0. class: Workflow. requirements:; - class: SchemaDefRequirement; types:; - $import: capture_kit.yml. inputs:; - id: bam; type: string; - id: capture_kit; type: capture_kit.yml#capture_kit. outputs:; - id: output_bam; type: File; outputSource: touch_bam/output. steps:; - id: touch_bam; run: touch.cwl; in:; - id: input; source: bam; out:; - id: output; ```; **test_wf.json**; ```; {; ""bam"": ""a.bam"",; ""capture_kit"": {; ""bait"": ""abait""; }; }; ```; **capture_kit.yml**; ```; - name: capture_kit; type: record; fields:; - name: bait; type: string; ```; **touch.cwl**; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0. requirements:; - class: DockerRequirement; dockerPull: ubuntu:bionic-20180426. class: CommandLineTool. inputs:; - id: input; type: string; inputBinding:; position: 0. outputs:; - id: output; type: File; outputBinding:; glob: $(inputs.input); ; baseCommand: [touch]; ```. this works:; ```; (cwl) [jeremiah@sasha bamfastq_align]$ java -Dconfig.file=/home/jeremiah/code/cromwell/cromwell.examples.conf -jar /home/jeremiah/code/cromwell/server/target/scala-2.12/cromwell-37-634ac5b-SNAP.jar run test_wf.cwl --inputs test_wf.json --type CWL --type-version v1.0; ```; but this fails:; ```; (cwl) [jeremiah@sasha bamfastq_align]$ cwltool --pack test.cwl > test_pack.cwl; (cwl) [jeremiah@sasha bamfastq_align]$ java -Dconfig.file=/home/jeremiah/code/cromwell/cromwell.examples.conf -jar /home/jeremiah/code/cromwell/server/target/scala-2.12/cromwell-37-634ac5b-SNAP.jar run test_pack.cwl --inputs test_wf.json --type CWL --type-version v1.0 --workflow-root main; [2018-10-23 17:48:48,28] [info] Running with database db.url = jdbc:hsqldb:mem:3bd78058-b880-451a-b3ef-71a48a2a17ce;shutdown=false;hsqldb.tx=mvcc; [2018-10-23 17:48:55,34] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-23",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856
https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424758349:89,Deployability,update,updated,89,"@geoffjentry, thanks for clearing that up. I guess I know more if the documentation gets updated. @ffinfo, thanks for picking this up!. We do have a very well trained HPC admin team that will bash (hehe) anyone abusing the SGE queque master too much.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424758349
https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424758349:25,Usability,clear,clearing,25,"@geoffjentry, thanks for clearing that up. I guess I know more if the documentation gets updated. @ffinfo, thanks for picking this up!. We do have a very well trained HPC admin team that will bash (hehe) anyone abusing the SGE queque master too much.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424758349
https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-442192218:225,Energy Efficiency,schedul,schedule,225,"We've merged a few improvements based on the investigation (due in Cromwell 37), but we cannot guarantee this specific problem is fixed. The root cause is proving elusive and we find ourselves needing to pause for now due to schedule pressure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-442192218
https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-442192218:204,Usability,pause,pause,204,"We've merged a few improvements based on the investigation (due in Cromwell 37), but we cannot guarantee this specific problem is fixed. The root cause is proving elusive and we find ourselves needing to pause for now due to schedule pressure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-442192218
https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-457242206:190,Usability,simpl,simply,190,"I'd also like to note that we've now seen this at least 3 times in production in the past month and a bit. Once on Dec 21, once on Jan 10, and again today. So far the temporary fix has been simply restarting Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-457242206
https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-423199632:68,Usability,simpl,simple,68,"As I thought about it more my use cases were all different than the simple ""make a query and stream the results"" which is likely what we'd want here so my skepticism is probably unfounded :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124#issuecomment-423199632
https://github.com/broadinstitute/cromwell/issues/4136#issuecomment-423669377:140,Availability,failure,failure,140,"It seems that the input to the missing-in-action scatter, the indexed array that it scatters on, is empty and that is the root cause of the failure....; IMO it would help to see in the log/report that the scatter has actually been observed but it simply had 0 elements which quite often may indicate that something went wrong (perhaps it should have a warning icon next to it in the report (e.g. a yellow ! triangle) .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136#issuecomment-423669377
https://github.com/broadinstitute/cromwell/issues/4136#issuecomment-423669377:185,Testability,log,log,185,"It seems that the input to the missing-in-action scatter, the indexed array that it scatters on, is empty and that is the root cause of the failure....; IMO it would help to see in the log/report that the scatter has actually been observed but it simply had 0 elements which quite often may indicate that something went wrong (perhaps it should have a warning icon next to it in the report (e.g. a yellow ! triangle) .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136#issuecomment-423669377
https://github.com/broadinstitute/cromwell/issues/4136#issuecomment-423669377:247,Usability,simpl,simply,247,"It seems that the input to the missing-in-action scatter, the indexed array that it scatters on, is empty and that is the root cause of the failure....; IMO it would help to see in the log/report that the scatter has actually been observed but it simply had 0 elements which quite often may indicate that something went wrong (perhaps it should have a warning icon next to it in the report (e.g. a yellow ! triangle) .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136#issuecomment-423669377
https://github.com/broadinstitute/cromwell/issues/4160#issuecomment-473364450:125,Testability,log,logs,125,We've improved this entirely as instead of reporting bad return codes at all -- we've simply given the user the links to the logs in v2 to debug -- as that's the true source for what went wrong. Closed by https://github.com/broadinstitute/cromwell/pull/4718,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4160#issuecomment-473364450
https://github.com/broadinstitute/cromwell/issues/4160#issuecomment-473364450:86,Usability,simpl,simply,86,We've improved this entirely as instead of reporting bad return codes at all -- we've simply given the user the links to the logs in v2 to debug -- as that's the true source for what went wrong. Closed by https://github.com/broadinstitute/cromwell/pull/4718,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4160#issuecomment-473364450
https://github.com/broadinstitute/cromwell/issues/4200#issuecomment-521455671:389,Availability,error,error,389,"Yes, would be great to see a fix for this. Attached is a simple WDL repro using an expression over an http input that fails with the following. It appears the filename prefix is being incorrectly remapped using the current local directory instead of maintaining the http:// prefix. . java -jar cromwell.jar run http_inputs.wdl.txt --inputs http_inputs.json.txt. `[2019-08-14 22:23:39,50] [error] WorkflowManagerActor Workflow 09c208ef-58b5-4571-9587-e3e2e58a0831 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'disk_space' (reason 1 of 1): Evaluating ceil((size(jamie2, ""GB"") * 2.25)) failed: [Attempted 1 time(s)] - NoSuchFileException: /home/cromwellbuild/cromwell/centaur/src/main/resources/standardTestCases/http_inputs/http:/raw.githubusercontent.com/broadinstitute/cromwell/develop/docs/jamie_the_cromwell_pig.png`. [http_inputs.json.txt](https://github.com/broadinstitute/cromwell/files/3503691/http_inputs.json.txt); [http_inputs.wdl.txt](https://github.com/broadinstitute/cromwell/files/3503693/http_inputs.wdl.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4200#issuecomment-521455671
https://github.com/broadinstitute/cromwell/issues/4200#issuecomment-521455671:57,Usability,simpl,simple,57,"Yes, would be great to see a fix for this. Attached is a simple WDL repro using an expression over an http input that fails with the following. It appears the filename prefix is being incorrectly remapped using the current local directory instead of maintaining the http:// prefix. . java -jar cromwell.jar run http_inputs.wdl.txt --inputs http_inputs.json.txt. `[2019-08-14 22:23:39,50] [error] WorkflowManagerActor Workflow 09c208ef-58b5-4571-9587-e3e2e58a0831 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'disk_space' (reason 1 of 1): Evaluating ceil((size(jamie2, ""GB"") * 2.25)) failed: [Attempted 1 time(s)] - NoSuchFileException: /home/cromwellbuild/cromwell/centaur/src/main/resources/standardTestCases/http_inputs/http:/raw.githubusercontent.com/broadinstitute/cromwell/develop/docs/jamie_the_cromwell_pig.png`. [http_inputs.json.txt](https://github.com/broadinstitute/cromwell/files/3503691/http_inputs.json.txt); [http_inputs.wdl.txt](https://github.com/broadinstitute/cromwell/files/3503693/http_inputs.wdl.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4200#issuecomment-521455671
https://github.com/broadinstitute/cromwell/issues/4212#issuecomment-468069127:402,Usability,feedback,feedback,402,"We just merged a little bit of [cleanup](https://github.com/broadinstitute/cromwell/pull/4666/files#diff-a0a39c998215634e799c30ace2dfd58b) around the issue above, including editing [the docs](https://cromwell.readthedocs.io/en/develop/backends/SGE/) and adding a new `docker_out`, `docker_err`, and `docker_script` that should behave closer to expectations. Please check it out and file any additional feedback as a new issue or PR? Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4212#issuecomment-468069127
https://github.com/broadinstitute/cromwell/issues/4226#issuecomment-429104968:121,Safety,timeout,timeout,121,"One case for caching the results for a few seconds would be the possibility of just missing the results due to requester timeout, and then when the retry happens having to recalculate the entire thing again. ---. Alternatively (getting into alternative metadata schemes) we do *permanent* caching - ie replacing all those simpletons with the pre-processed metadata response for completed workflows (maybe triggered by a metadata request for the workflow), so that we don't need to continually rebuild them even months after a workflow completes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4226#issuecomment-429104968
https://github.com/broadinstitute/cromwell/issues/4226#issuecomment-429104968:322,Usability,simpl,simpletons,322,"One case for caching the results for a few seconds would be the possibility of just missing the results due to requester timeout, and then when the retry happens having to recalculate the entire thing again. ---. Alternatively (getting into alternative metadata schemes) we do *permanent* caching - ie replacing all those simpletons with the pre-processed metadata response for completed workflows (maybe triggered by a metadata request for the workflow), so that we don't need to continually rebuild them even months after a workflow completes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4226#issuecomment-429104968
https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561407757:260,Deployability,pipeline,pipeline,260,"Note for the others (since @aednichols and I are discussing this offline) -- `womgraph` includes inputs/outputs in the graph, which `graph` does not. My $0.02 is that this can be either a very good thing or a very bad thing depending on the complexity of your pipeline: for a simple one it' really nice, but for a very complex one it makes it really hard to read.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561407757
https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561407757:224,Integrability,depend,depending,224,"Note for the others (since @aednichols and I are discussing this offline) -- `womgraph` includes inputs/outputs in the graph, which `graph` does not. My $0.02 is that this can be either a very good thing or a very bad thing depending on the complexity of your pipeline: for a simple one it' really nice, but for a very complex one it makes it really hard to read.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561407757
https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561407757:276,Usability,simpl,simple,276,"Note for the others (since @aednichols and I are discussing this offline) -- `womgraph` includes inputs/outputs in the graph, which `graph` does not. My $0.02 is that this can be either a very good thing or a very bad thing depending on the complexity of your pipeline: for a simple one it' really nice, but for a very complex one it makes it really hard to read.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561407757
https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561411295:71,Deployability,pipeline,pipeline,71,"`womgraph` produced vastly too much output for my (apparently) complex pipeline. `dot` couldn't render it in png at all and produced a corrupted pdf file. I think that an option that produces ""simple"" output that non-developers can look at and mostly comprehend, like the old `graph` option, is still very desirable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561411295
https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561411295:193,Usability,simpl,simple,193,"`womgraph` produced vastly too much output for my (apparently) complex pipeline. `dot` couldn't render it in png at all and produced a corrupted pdf file. I think that an option that produces ""simple"" output that non-developers can look at and mostly comprehend, like the old `graph` option, is still very desirable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561411295
https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-562666348:390,Deployability,pipeline,pipeline,390,"> Have y'all tried womtool womgraph?. As I said above:. > The womgraph command still works, but the output from that command is so verbose it's unusable for viewing our workflows. `womgraph` may be helpful for simple workflows or debugging womtool but it's not a substitute for `graph` when viewing more complex workflows. . For example, here's the output for the GATK best practices exome pipeline. [exome.pdf](https://github.com/broadinstitute/cromwell/files/3933299/exome.pdf)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-562666348
https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-562666348:210,Usability,simpl,simple,210,"> Have y'all tried womtool womgraph?. As I said above:. > The womgraph command still works, but the output from that command is so verbose it's unusable for viewing our workflows. `womgraph` may be helpful for simple workflows or debugging womtool but it's not a substitute for `graph` when viewing more complex workflows. . For example, here's the output for the GATK best practices exome pipeline. [exome.pdf](https://github.com/broadinstitute/cromwell/files/3933299/exome.pdf)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-562666348
https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-445825939:58,Availability,failure,failure,58,"Don't know if this is related but I'm seeing this type of failure submitting [this workflow](https://github.com/bcbio/test_bcbio_cwl/blob/master/prealign/prealign-workflow/main-prealign.cwl) by url:. ```; MaterializeWorkflowDescriptorActor [UUID(dfefc8c0)]: Parsing workflow as CWL v1.0; 2018-12-10 13:27:22,372 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/main-prealign.cwl; 2018-12-10 13:31:56,222 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/organize_noalign.cwl; 2018-12-10 13:32:14,196 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/prep_samples_to_rec.cwl; 2018-12-10 13:32:32,071 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/prep_samples.cwl; 2018-12-10 13:32:49,793 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/postprocess_alignment_to_rec.cwl; 2018-12-10 13:33:07,284 cromwell-system-akka.dispatchers.engine-dispatcher-34 ERROR - WorkflowManagerActor Workflow dfefc8c0-c3a1-449c-a747-13147bf8b980 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to run cwltool on file https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/postprocess_alignment_to_rec.cwl (reason 1 of 1): Traceback (most recent call last):; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 11, in cwltool_salad; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/cwltool/load_tool.py"", line 113, in fetch_document; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 933",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-445825939
https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-445825939:2283,Performance,load,load,2283,"escriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to run cwltool on file https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/postprocess_alignment_to_rec.cwl (reason 1 of 1): Traceback (most recent call last):; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 11, in cwltool_salad; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/cwltool/load_tool.py"", line 113, in fetch_document; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 933, in fetch; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 933, in fetch; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/main.py"", line 948, in round_trip_load; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/main.py"", line 899, in load; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/constructor.py"", line 104, in get_single_data; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 79, in get_single_node; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 102, in compose_document; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 139, in compose_node; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 218, in compose_mapping_node; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 137, in compose_node; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 180, in compose_sequence_node; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 139, in compose_node; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 211, in compose_mapping_node; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/parser.py"", line 141, in check_event; Fi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-445825939
https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-445825939:3856,Usability,simpl,simple,3856,"oser.py"", line 137, in compose_node; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 180, in compose_sequence_node; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 139, in compose_node; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/composer.py"", line 211, in compose_mapping_node; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/parser.py"", line 141, in check_event; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/parser.py"", line 575, in parse_block_mapping_key; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/scanner.py"", line 1723, in peek_token; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/scanner.py"", line 222, in fetch_more_tokens; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/scanner.py"", line 444, in fetch_stream_end; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/scanner.py"", line 383, in remove_possible_simple_key; ruamel.yaml.scanner.ScannerError: while scanning a simple key; in ""https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/postprocess_alignment_to_rec.cwl"", line 230, column 1; could not find expected ':'; in ""https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/postprocess_alignment_to_rec.cwl"", line 230, column 3. 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-445825939
https://github.com/broadinstitute/cromwell/issues/4242#issuecomment-442179028:46,Availability,heartbeat,heartbeats,46,"Cromwell should already [clear workflow store heartbeats](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/server/CromwellShutdown.scala#L165) on graceful shutdown. If Cromwell is being shut down gracefully and heartbeats aren't being cleared then there's a bug, but if Cromwell is not being shut down gracefully then delayed workflow pickup for the duration of the heartbeat TTL would be the expected behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4242#issuecomment-442179028
https://github.com/broadinstitute/cromwell/issues/4242#issuecomment-442179028:229,Availability,down,down,229,"Cromwell should already [clear workflow store heartbeats](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/server/CromwellShutdown.scala#L165) on graceful shutdown. If Cromwell is being shut down gracefully and heartbeats aren't being cleared then there's a bug, but if Cromwell is not being shut down gracefully then delayed workflow pickup for the duration of the heartbeat TTL would be the expected behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4242#issuecomment-442179028
https://github.com/broadinstitute/cromwell/issues/4242#issuecomment-442179028:249,Availability,heartbeat,heartbeats,249,"Cromwell should already [clear workflow store heartbeats](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/server/CromwellShutdown.scala#L165) on graceful shutdown. If Cromwell is being shut down gracefully and heartbeats aren't being cleared then there's a bug, but if Cromwell is not being shut down gracefully then delayed workflow pickup for the duration of the heartbeat TTL would be the expected behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4242#issuecomment-442179028
https://github.com/broadinstitute/cromwell/issues/4242#issuecomment-442179028:335,Availability,down,down,335,"Cromwell should already [clear workflow store heartbeats](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/server/CromwellShutdown.scala#L165) on graceful shutdown. If Cromwell is being shut down gracefully and heartbeats aren't being cleared then there's a bug, but if Cromwell is not being shut down gracefully then delayed workflow pickup for the duration of the heartbeat TTL would be the expected behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4242#issuecomment-442179028
https://github.com/broadinstitute/cromwell/issues/4242#issuecomment-442179028:404,Availability,heartbeat,heartbeat,404,"Cromwell should already [clear workflow store heartbeats](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/server/CromwellShutdown.scala#L165) on graceful shutdown. If Cromwell is being shut down gracefully and heartbeats aren't being cleared then there's a bug, but if Cromwell is not being shut down gracefully then delayed workflow pickup for the duration of the heartbeat TTL would be the expected behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4242#issuecomment-442179028
https://github.com/broadinstitute/cromwell/issues/4242#issuecomment-442179028:25,Usability,clear,clear,25,"Cromwell should already [clear workflow store heartbeats](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/server/CromwellShutdown.scala#L165) on graceful shutdown. If Cromwell is being shut down gracefully and heartbeats aren't being cleared then there's a bug, but if Cromwell is not being shut down gracefully then delayed workflow pickup for the duration of the heartbeat TTL would be the expected behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4242#issuecomment-442179028
https://github.com/broadinstitute/cromwell/issues/4242#issuecomment-442179028:273,Usability,clear,cleared,273,"Cromwell should already [clear workflow store heartbeats](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/server/CromwellShutdown.scala#L165) on graceful shutdown. If Cromwell is being shut down gracefully and heartbeats aren't being cleared then there's a bug, but if Cromwell is not being shut down gracefully then delayed workflow pickup for the duration of the heartbeat TTL would be the expected behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4242#issuecomment-442179028
https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-429893248:189,Testability,log,logging,189,"@mcovarr I remember you mentioned a way to pause & examine the Slick queries in the debugger - what's a good code location to pause at?. The last time I looked at this I just enabled MySQL logging, which was a ton to dig through",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-429893248
https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-429893248:43,Usability,pause,pause,43,"@mcovarr I remember you mentioned a way to pause & examine the Slick queries in the debugger - what's a good code location to pause at?. The last time I looked at this I just enabled MySQL logging, which was a ton to dig through",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-429893248
https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-429893248:126,Usability,pause,pause,126,"@mcovarr I remember you mentioned a way to pause & examine the Slick queries in the debugger - what's a good code location to pause at?. The last time I looked at this I just enabled MySQL logging, which was a ton to dig through",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4249#issuecomment-429893248
https://github.com/broadinstitute/cromwell/issues/4274#issuecomment-430881926:505,Usability,simpl,simply,505,"I've thought some more about this, and my guess is that this was never implemented because AWS supports auto-expanding compute disks, as explained here: https://docs.opendata.aws/genomics-workflows/aws-batch/create-custom-ami/. This is of course a great feature, but unfortunately it means that WDL written for Google Cloud will fail when run on AWS because it doesn't understand more complex disk specs. To fix this, I suggest that we expand the `LocalDiskPattern` to include the disk size and type, but simply ignore it. This will mean that disk specs such as `local-disk 100 HDD` will be interpreted to have the same meaning as `local-disk`, but will add greater compatibility to the AWS executor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4274#issuecomment-430881926
https://github.com/broadinstitute/cromwell/issues/4277#issuecomment-437019276:5,Usability,learn,learned,5,I've learned that this will never work given how the timing diagram works today. A fix for this behavior is what Chris seems to be describing here: https://github.com/broadinstitute/cromwell/issues/4309 @tjeandet does that sound right?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4277#issuecomment-437019276
https://github.com/broadinstitute/cromwell/issues/4277#issuecomment-437404208:84,Usability,simpl,simply,84,I think that would definitely help but we might still have this issue if JobManager simply links out to CromIAM without providing the auth headers for the timing endpoint itself.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4277#issuecomment-437404208
https://github.com/broadinstitute/cromwell/pull/4285#issuecomment-431869212:176,Testability,test,tests,176,"> This seems too simple to be correct. My usual response when I start thinking like this is ""do I have any lingering doubts that can be documented?"" And if so - ""can I add any tests for to make myself feel more confident?""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4285#issuecomment-431869212
https://github.com/broadinstitute/cromwell/pull/4285#issuecomment-431869212:17,Usability,simpl,simple,17,"> This seems too simple to be correct. My usual response when I start thinking like this is ""do I have any lingering doubts that can be documented?"" And if so - ""can I add any tests for to make myself feel more confident?""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4285#issuecomment-431869212
https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443405463:46,Testability,test,test,46,"Thanks, @wleepang, for the numbers and simple test code. @TimurIs, nice catch and thanks for sharing!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443405463
https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443405463:39,Usability,simpl,simple,39,"Thanks, @wleepang, for the numbers and simple test code. @TimurIs, nice catch and thanks for sharing!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-443405463
https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444669480:374,Availability,avail,available,374,"> @alartin You're spot on in regards to array jobs. Unfortunately the internal design of Cromwell makes it difficult to do this, it's come up before for HPC backends as well. Something we should address some day but it'd be a fairly major undertaking. Hi @geoffjentry I wonder if there is any guide for developer, especially for the backend impl. Or is there some doc/slide available for that? I am willing to implement backend for other public cloud vendor and curious about how to get it started quickly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444669480
https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444669480:293,Usability,guid,guide,293,"> @alartin You're spot on in regards to array jobs. Unfortunately the internal design of Cromwell makes it difficult to do this, it's come up before for HPC backends as well. Something we should address some day but it'd be a fairly major undertaking. Hi @geoffjentry I wonder if there is any guide for developer, especially for the backend impl. Or is there some doc/slide available for that? I am willing to implement backend for other public cloud vendor and curious about how to get it started quickly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444669480
https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685185759:88,Testability,log,logs,88,"To make it less of a blind hunt, it's also possible to look into your Stackdriver Audit logs - they should list all GCP API calls in your project that failed with 403. This way you can get a better sense of which ones Cromwell is actually using. I've been meaning to write a tool to simplify this kind of analysis, but you can do it with the logs even now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685185759
https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685185759:342,Testability,log,logs,342,"To make it less of a blind hunt, it's also possible to look into your Stackdriver Audit logs - they should list all GCP API calls in your project that failed with 403. This way you can get a better sense of which ones Cromwell is actually using. I've been meaning to write a tool to simplify this kind of analysis, but you can do it with the logs even now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685185759
https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685185759:283,Usability,simpl,simplify,283,"To make it less of a blind hunt, it's also possible to look into your Stackdriver Audit logs - they should list all GCP API calls in your project that failed with 403. This way you can get a better sense of which ones Cromwell is actually using. I've been meaning to write a tool to simplify this kind of analysis, but you can do it with the logs even now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685185759
https://github.com/broadinstitute/cromwell/issues/4333#issuecomment-434045778:6,Usability,clear,clear,6,"To be clear, this would be Phase I i.e. a new service within Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4333#issuecomment-434045778
https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-922157553:204,Usability,simpl,simpler,204,I think the home of all these docs moved. I guess here is where it now lives: https://docs.opendata.aws/genomics-workflows/quick-start.html. Although I get the impression that this stack will become much simpler to setup with the upcoming [Genomics CLI](https://aws.amazon.com/blogs/industries/announcing-amazon-genomics-cli-preview/) (which you can apply to preview).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-922157553
https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-922527156:997,Deployability,deploy,deployment,997,"Hi @multimeric ,. Thank you so much for providing the current location of those links. . I still have a question regarding the solution proposed here. In the current documentation website, I could not find the guidance or CloudFormation for creating a custom AMI, which is described above. And moreover, I also could not find the description saying that ""The AMI type needs to be specified as 'cromwell' and the Scratch mount point needs to be specified as \cromwell_mount"", which was mentioned above as the solution. . Is it because the whole creation procedure has been changed since then? In the current documentation, I only found [this link](https://docs.opendata.aws/genomics-workflows/core-env/create-custom-compute-resources.html#custom-amis) which only briefly talked about creating a custom AMI but not gave any CloudFormation link. Do you know where I should look for this information? Thanks!. Also, thank you for letting me know about the Genomics CLI. I have to say that the current deployment procedure on AWS is way more complicated than GCP, especially given that my company's cloud team puts more restrictions which complicates the standard procedure that used to work in my personal AWS account. I'm looking forward to hearing from the development of Genomics CLI. Sincerely,; Yiming",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-922527156
https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-922527156:210,Usability,guid,guidance,210,"Hi @multimeric ,. Thank you so much for providing the current location of those links. . I still have a question regarding the solution proposed here. In the current documentation website, I could not find the guidance or CloudFormation for creating a custom AMI, which is described above. And moreover, I also could not find the description saying that ""The AMI type needs to be specified as 'cromwell' and the Scratch mount point needs to be specified as \cromwell_mount"", which was mentioned above as the solution. . Is it because the whole creation procedure has been changed since then? In the current documentation, I only found [this link](https://docs.opendata.aws/genomics-workflows/core-env/create-custom-compute-resources.html#custom-amis) which only briefly talked about creating a custom AMI but not gave any CloudFormation link. Do you know where I should look for this information? Thanks!. Also, thank you for letting me know about the Genomics CLI. I have to say that the current deployment procedure on AWS is way more complicated than GCP, especially given that my company's cloud team puts more restrictions which complicates the standard procedure that used to work in my personal AWS account. I'm looking forward to hearing from the development of Genomics CLI. Sincerely,; Yiming",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-922527156
https://github.com/broadinstitute/cromwell/issues/4352#issuecomment-435486154:45,Usability,feedback,feedback,45,I found [this](https://github.com/docker/hub-feedback/issues/331). We should still do the experiment Thibault proposes but it looks like there might be some reasonable scale factor to apply here,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4352#issuecomment-435486154
https://github.com/broadinstitute/cromwell/issues/4361#issuecomment-759827004:117,Usability,simpl,simply,117,"Why do ""inputs"" directories have multiple sub-directories? Why do they have ANY sub-directories? Why doesn't Comwell simply put all the input files in the inputs directory?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4361#issuecomment-759827004
https://github.com/broadinstitute/cromwell/issues/4381#issuecomment-438110931:128,Safety,avoid,avoiding,128,"I just mentioned this in another issue @TMiguelT and I know it doesn't help your immediate problem but I'd personally recommend avoiding docker containers which use `ENTRYPOINT` for reproducible workflows. . For instance, `ENTRYPOINT` is slated to be [permanently overridden in CWL 2.0](https://github.com/common-workflow-language/common-workflow-language/issues/522), and it's clear that both WDL and CWL were designed without `ENTRYPOINT` in mind.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381#issuecomment-438110931
https://github.com/broadinstitute/cromwell/issues/4381#issuecomment-438110931:378,Usability,clear,clear,378,"I just mentioned this in another issue @TMiguelT and I know it doesn't help your immediate problem but I'd personally recommend avoiding docker containers which use `ENTRYPOINT` for reproducible workflows. . For instance, `ENTRYPOINT` is slated to be [permanently overridden in CWL 2.0](https://github.com/common-workflow-language/common-workflow-language/issues/522), and it's clear that both WDL and CWL were designed without `ENTRYPOINT` in mind.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381#issuecomment-438110931
https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230:443,Availability,recover,recover,443,"I asked your question to PAPI and here is the response:. > This detail is not something that should be counted on in a containerized environment.; That said: the /dev/disk/by-id/* system is simply a convenient alias. The underlying block storage doesn't change (eg, /dev/disk/by-id/google-local-disk is a symlink to a block device, in this case, /dev/sdb). So they should be able to continue monitoring if they want, it will just be harder to recover the mapping.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230
https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230:392,Energy Efficiency,monitor,monitoring,392,"I asked your question to PAPI and here is the response:. > This detail is not something that should be counted on in a containerized environment.; That said: the /dev/disk/by-id/* system is simply a convenient alias. The underlying block storage doesn't change (eg, /dev/disk/by-id/google-local-disk is a symlink to a block device, in this case, /dev/sdb). So they should be able to continue monitoring if they want, it will just be harder to recover the mapping.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230
https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230:443,Safety,recover,recover,443,"I asked your question to PAPI and here is the response:. > This detail is not something that should be counted on in a containerized environment.; That said: the /dev/disk/by-id/* system is simply a convenient alias. The underlying block storage doesn't change (eg, /dev/disk/by-id/google-local-disk is a symlink to a block device, in this case, /dev/sdb). So they should be able to continue monitoring if they want, it will just be harder to recover the mapping.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230
https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230:190,Usability,simpl,simply,190,"I asked your question to PAPI and here is the response:. > This detail is not something that should be counted on in a containerized environment.; That said: the /dev/disk/by-id/* system is simply a convenient alias. The underlying block storage doesn't change (eg, /dev/disk/by-id/google-local-disk is a symlink to a block device, in this case, /dev/sdb). So they should be able to continue monitoring if they want, it will just be harder to recover the mapping.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230
https://github.com/broadinstitute/cromwell/issues/4404#issuecomment-461572753:17,Usability,clear,clear,17,"@abdulrauf To be clear - are you saying that you originally were using `/g/cromwell/cromwell-executions` but then changed to `/fast/gdr/uat/cromwell-executions`? If so, is it the case that the workflows showing up as `/g/cromwell/cromwell-executions` are workflows which were run prior to that change? Cromwell will not migrate these directories",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4404#issuecomment-461572753
https://github.com/broadinstitute/cromwell/issues/4406#issuecomment-457345628:398,Usability,clear,clears,398,"We've got an ongoing situation with Green where their Cromwell halts workflow processing with zips after a certain amount of time https://github.com/broadinstitute/cromwell/issues/4117. . Our current hypothesis is that they are running out of disk space for unzipped imports. Their Cromwell already had hundreds of MBs worth within a few hours after restart (which, because of the way they Docker, clears the disk). @ApChagi @tbl3rd @hjfbynara @tlangs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4406#issuecomment-457345628
https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-479192270:29,Availability,error,error,29,"I'm consistently seeing this error, not the one in #4563 (which could be as simple as ""the files in question are very small in my case""). It appears to be looking for a file called `foo.log` where `foo` is the name of the task. So for instance `heightProduct-stderr.log` exists but `heightProduct.log` does not. I'm not certain where that's coming from",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-479192270
https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-479192270:186,Testability,log,log,186,"I'm consistently seeing this error, not the one in #4563 (which could be as simple as ""the files in question are very small in my case""). It appears to be looking for a file called `foo.log` where `foo` is the name of the task. So for instance `heightProduct-stderr.log` exists but `heightProduct.log` does not. I'm not certain where that's coming from",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-479192270
https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-479192270:266,Testability,log,log,266,"I'm consistently seeing this error, not the one in #4563 (which could be as simple as ""the files in question are very small in my case""). It appears to be looking for a file called `foo.log` where `foo` is the name of the task. So for instance `heightProduct-stderr.log` exists but `heightProduct.log` does not. I'm not certain where that's coming from",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-479192270
https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-479192270:297,Testability,log,log,297,"I'm consistently seeing this error, not the one in #4563 (which could be as simple as ""the files in question are very small in my case""). It appears to be looking for a file called `foo.log` where `foo` is the name of the task. So for instance `heightProduct-stderr.log` exists but `heightProduct.log` does not. I'm not certain where that's coming from",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-479192270
https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-479192270:76,Usability,simpl,simple,76,"I'm consistently seeing this error, not the one in #4563 (which could be as simple as ""the files in question are very small in my case""). It appears to be looking for a file called `foo.log` where `foo` is the name of the task. So for instance `heightProduct-stderr.log` exists but `heightProduct.log` does not. I'm not certain where that's coming from",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-479192270
https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1303286500:251,Availability,down,down-featured,251,"> The docs are correct, the local docker backend does not recognize CPU and memory attributes, because it's impossible to implement with the Docker Desktop API. And even if it was, it would probably not ship because the local backend is intended as a down-featured sandbox environment. @aednichols Are you talking specifically about macOS? You can limit `cpu` and `memory` by running docker on linux though.; I've gotten `cpu` (cores actually) limit working with the following code in the conf file:; ```; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; Int cpu = 1; """""". # Submit string when there is no ""docker"" runtime attribute.; submit = ""/usr/bin/env bash ${script}"". # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; ${""--cpus="" + cpu} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd} \; ${docker} ${docker_script}; """"""; ```. A task that needs more cpu cores would simply request it with the runtime block:; ```; runtime {; docker: ""...""; cpu: 3; }; ```. I've gotten the idea from @ruchim post. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1303286500
https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1303286500:265,Modifiability,sandbox,sandbox,265,"> The docs are correct, the local docker backend does not recognize CPU and memory attributes, because it's impossible to implement with the Docker Desktop API. And even if it was, it would probably not ship because the local backend is intended as a down-featured sandbox environment. @aednichols Are you talking specifically about macOS? You can limit `cpu` and `memory` by running docker on linux though.; I've gotten `cpu` (cores actually) limit working with the following code in the conf file:; ```; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; Int cpu = 1; """""". # Submit string when there is no ""docker"" runtime attribute.; submit = ""/usr/bin/env bash ${script}"". # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; ${""--cpus="" + cpu} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd} \; ${docker} ${docker_script}; """"""; ```. A task that needs more cpu cores would simply request it with the runtime block:; ```; runtime {; docker: ""...""; cpu: 3; }; ```. I've gotten the idea from @ruchim post. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1303286500
https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1303286500:265,Testability,sandbox,sandbox,265,"> The docs are correct, the local docker backend does not recognize CPU and memory attributes, because it's impossible to implement with the Docker Desktop API. And even if it was, it would probably not ship because the local backend is intended as a down-featured sandbox environment. @aednichols Are you talking specifically about macOS? You can limit `cpu` and `memory` by running docker on linux though.; I've gotten `cpu` (cores actually) limit working with the following code in the conf file:; ```; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; Int cpu = 1; """""". # Submit string when there is no ""docker"" runtime attribute.; submit = ""/usr/bin/env bash ${script}"". # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; ${""--cpus="" + cpu} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd} \; ${docker} ${docker_script}; """"""; ```. A task that needs more cpu cores would simply request it with the runtime block:; ```; runtime {; docker: ""...""; cpu: 3; }; ```. I've gotten the idea from @ruchim post. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1303286500
https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1303286500:1032,Usability,simpl,simply,1032,"> The docs are correct, the local docker backend does not recognize CPU and memory attributes, because it's impossible to implement with the Docker Desktop API. And even if it was, it would probably not ship because the local backend is intended as a down-featured sandbox environment. @aednichols Are you talking specifically about macOS? You can limit `cpu` and `memory` by running docker on linux though.; I've gotten `cpu` (cores actually) limit working with the following code in the conf file:; ```; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; Int cpu = 1; """""". # Submit string when there is no ""docker"" runtime attribute.; submit = ""/usr/bin/env bash ${script}"". # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; docker run \; --rm -i \; ${""--user "" + docker_user} \; ${""--cpus="" + cpu} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd} \; ${docker} ${docker_script}; """"""; ```. A task that needs more cpu cores would simply request it with the runtime block:; ```; runtime {; docker: ""...""; cpu: 3; }; ```. I've gotten the idea from @ruchim post. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1303286500
https://github.com/broadinstitute/cromwell/pull/4432#issuecomment-443780949:232,Usability,simpl,simply,232,"is the `Optional` type really necessary?. Here's a description of a required String input:; ```; {; ""name"": ""my_wf.string_input"",; ""valueType"": {; ""typeName"": ""String""; },; ""optional"": false; },; ```. could an optional String input simply be:; ```; {; ""name"": ""my_wf.optional_input"",; ""valueType"": {; ""typeName"": ""String""; },; ""optional"": true,; ""default"": ""hello""; },; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4432#issuecomment-443780949
https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-464201945:92,Deployability,configurat,configuration,92,That is quite some digging you've done! I hope you were able to reach an acceptable working configuration. I am going to go ahead and close this issue because there is not a clear Cromwell bug here as far as I can see.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-464201945
https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-464201945:92,Modifiability,config,configuration,92,That is quite some digging you've done! I hope you were able to reach an acceptable working configuration. I am going to go ahead and close this issue because there is not a clear Cromwell bug here as far as I can see.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-464201945
https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-464201945:174,Usability,clear,clear,174,That is quite some digging you've done! I hope you were able to reach an acceptable working configuration. I am going to go ahead and close this issue because there is not a clear Cromwell bug here as far as I can see.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-464201945
https://github.com/broadinstitute/cromwell/issues/4475#issuecomment-456488628:218,Usability,simpl,simpler,218,This is a requirement for FC prod. I hadn't considered removing `collectionName` but it's an interesting point. It'd certainly remove the wonkiness of having a subtly different API and might make the swagger headaches simpler,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4475#issuecomment-456488628
https://github.com/broadinstitute/cromwell/pull/4493#issuecomment-450958692:33,Usability,learn,learn,33,Closing until I have a chance to learn me some jq for great good,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4493#issuecomment-450958692
https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480:49,Availability,error,error,49,"The Rawls behavior is not a simple ""retry on any error"". Here's what I'm seeing:; * User aborts a submission in FireCloud.; * Rawls marks submission as `Aborting`; * In a background actor, Rawls finds all workflows inside `Aborting` submissions and sends an abort request to Cromwell.; * In a background actor, Rawls periodically queries Cromwell for the status of each active workflow, then updates its db based on Cromwell's response; * if all workflows in a submission are complete (failed, succeeded, aborted), the submission is marked as complete. In the aberrant workflow cases I checked, Cromwell is returning a status of `Running` or `Submitted` from its workflow-status endpoint, but returns a 404 from its abort endpoint. I suspect that 1) the abort request will never succeed; 2) the workflow status endpoint's response will never change, and therefore 3) Rawls will never update its db and will be stuck trying to abort the workflow forever.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480
https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480:392,Deployability,update,updates,392,"The Rawls behavior is not a simple ""retry on any error"". Here's what I'm seeing:; * User aborts a submission in FireCloud.; * Rawls marks submission as `Aborting`; * In a background actor, Rawls finds all workflows inside `Aborting` submissions and sends an abort request to Cromwell.; * In a background actor, Rawls periodically queries Cromwell for the status of each active workflow, then updates its db based on Cromwell's response; * if all workflows in a submission are complete (failed, succeeded, aborted), the submission is marked as complete. In the aberrant workflow cases I checked, Cromwell is returning a status of `Running` or `Submitted` from its workflow-status endpoint, but returns a 404 from its abort endpoint. I suspect that 1) the abort request will never succeed; 2) the workflow status endpoint's response will never change, and therefore 3) Rawls will never update its db and will be stuck trying to abort the workflow forever.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480
https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480:884,Deployability,update,update,884,"The Rawls behavior is not a simple ""retry on any error"". Here's what I'm seeing:; * User aborts a submission in FireCloud.; * Rawls marks submission as `Aborting`; * In a background actor, Rawls finds all workflows inside `Aborting` submissions and sends an abort request to Cromwell.; * In a background actor, Rawls periodically queries Cromwell for the status of each active workflow, then updates its db based on Cromwell's response; * if all workflows in a submission are complete (failed, succeeded, aborted), the submission is marked as complete. In the aberrant workflow cases I checked, Cromwell is returning a status of `Running` or `Submitted` from its workflow-status endpoint, but returns a 404 from its abort endpoint. I suspect that 1) the abort request will never succeed; 2) the workflow status endpoint's response will never change, and therefore 3) Rawls will never update its db and will be stuck trying to abort the workflow forever.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480
https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480:89,Safety,abort,aborts,89,"The Rawls behavior is not a simple ""retry on any error"". Here's what I'm seeing:; * User aborts a submission in FireCloud.; * Rawls marks submission as `Aborting`; * In a background actor, Rawls finds all workflows inside `Aborting` submissions and sends an abort request to Cromwell.; * In a background actor, Rawls periodically queries Cromwell for the status of each active workflow, then updates its db based on Cromwell's response; * if all workflows in a submission are complete (failed, succeeded, aborted), the submission is marked as complete. In the aberrant workflow cases I checked, Cromwell is returning a status of `Running` or `Submitted` from its workflow-status endpoint, but returns a 404 from its abort endpoint. I suspect that 1) the abort request will never succeed; 2) the workflow status endpoint's response will never change, and therefore 3) Rawls will never update its db and will be stuck trying to abort the workflow forever.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480
https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480:258,Safety,abort,abort,258,"The Rawls behavior is not a simple ""retry on any error"". Here's what I'm seeing:; * User aborts a submission in FireCloud.; * Rawls marks submission as `Aborting`; * In a background actor, Rawls finds all workflows inside `Aborting` submissions and sends an abort request to Cromwell.; * In a background actor, Rawls periodically queries Cromwell for the status of each active workflow, then updates its db based on Cromwell's response; * if all workflows in a submission are complete (failed, succeeded, aborted), the submission is marked as complete. In the aberrant workflow cases I checked, Cromwell is returning a status of `Running` or `Submitted` from its workflow-status endpoint, but returns a 404 from its abort endpoint. I suspect that 1) the abort request will never succeed; 2) the workflow status endpoint's response will never change, and therefore 3) Rawls will never update its db and will be stuck trying to abort the workflow forever.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480
https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480:505,Safety,abort,aborted,505,"The Rawls behavior is not a simple ""retry on any error"". Here's what I'm seeing:; * User aborts a submission in FireCloud.; * Rawls marks submission as `Aborting`; * In a background actor, Rawls finds all workflows inside `Aborting` submissions and sends an abort request to Cromwell.; * In a background actor, Rawls periodically queries Cromwell for the status of each active workflow, then updates its db based on Cromwell's response; * if all workflows in a submission are complete (failed, succeeded, aborted), the submission is marked as complete. In the aberrant workflow cases I checked, Cromwell is returning a status of `Running` or `Submitted` from its workflow-status endpoint, but returns a 404 from its abort endpoint. I suspect that 1) the abort request will never succeed; 2) the workflow status endpoint's response will never change, and therefore 3) Rawls will never update its db and will be stuck trying to abort the workflow forever.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480
https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480:716,Safety,abort,abort,716,"The Rawls behavior is not a simple ""retry on any error"". Here's what I'm seeing:; * User aborts a submission in FireCloud.; * Rawls marks submission as `Aborting`; * In a background actor, Rawls finds all workflows inside `Aborting` submissions and sends an abort request to Cromwell.; * In a background actor, Rawls periodically queries Cromwell for the status of each active workflow, then updates its db based on Cromwell's response; * if all workflows in a submission are complete (failed, succeeded, aborted), the submission is marked as complete. In the aberrant workflow cases I checked, Cromwell is returning a status of `Running` or `Submitted` from its workflow-status endpoint, but returns a 404 from its abort endpoint. I suspect that 1) the abort request will never succeed; 2) the workflow status endpoint's response will never change, and therefore 3) Rawls will never update its db and will be stuck trying to abort the workflow forever.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480
https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480:754,Safety,abort,abort,754,"The Rawls behavior is not a simple ""retry on any error"". Here's what I'm seeing:; * User aborts a submission in FireCloud.; * Rawls marks submission as `Aborting`; * In a background actor, Rawls finds all workflows inside `Aborting` submissions and sends an abort request to Cromwell.; * In a background actor, Rawls periodically queries Cromwell for the status of each active workflow, then updates its db based on Cromwell's response; * if all workflows in a submission are complete (failed, succeeded, aborted), the submission is marked as complete. In the aberrant workflow cases I checked, Cromwell is returning a status of `Running` or `Submitted` from its workflow-status endpoint, but returns a 404 from its abort endpoint. I suspect that 1) the abort request will never succeed; 2) the workflow status endpoint's response will never change, and therefore 3) Rawls will never update its db and will be stuck trying to abort the workflow forever.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480
https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480:926,Safety,abort,abort,926,"The Rawls behavior is not a simple ""retry on any error"". Here's what I'm seeing:; * User aborts a submission in FireCloud.; * Rawls marks submission as `Aborting`; * In a background actor, Rawls finds all workflows inside `Aborting` submissions and sends an abort request to Cromwell.; * In a background actor, Rawls periodically queries Cromwell for the status of each active workflow, then updates its db based on Cromwell's response; * if all workflows in a submission are complete (failed, succeeded, aborted), the submission is marked as complete. In the aberrant workflow cases I checked, Cromwell is returning a status of `Running` or `Submitted` from its workflow-status endpoint, but returns a 404 from its abort endpoint. I suspect that 1) the abort request will never succeed; 2) the workflow status endpoint's response will never change, and therefore 3) Rawls will never update its db and will be stuck trying to abort the workflow forever.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480
https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480:28,Usability,simpl,simple,28,"The Rawls behavior is not a simple ""retry on any error"". Here's what I'm seeing:; * User aborts a submission in FireCloud.; * Rawls marks submission as `Aborting`; * In a background actor, Rawls finds all workflows inside `Aborting` submissions and sends an abort request to Cromwell.; * In a background actor, Rawls periodically queries Cromwell for the status of each active workflow, then updates its db based on Cromwell's response; * if all workflows in a submission are complete (failed, succeeded, aborted), the submission is marked as complete. In the aberrant workflow cases I checked, Cromwell is returning a status of `Running` or `Submitted` from its workflow-status endpoint, but returns a 404 from its abort endpoint. I suspect that 1) the abort request will never succeed; 2) the workflow status endpoint's response will never change, and therefore 3) Rawls will never update its db and will be stuck trying to abort the workflow forever.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480
https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693:186,Deployability,configurat,configuration,186,"@geoffjentry That makes sense, thanks. Given the current code structure it's not at all clear to me how Docker-dependent branching would fit in - maybe this would be easier as a boolean configuration option adjacent to `workflow-log-dir`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693
https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693:111,Integrability,depend,dependent,111,"@geoffjentry That makes sense, thanks. Given the current code structure it's not at all clear to me how Docker-dependent branching would fit in - maybe this would be easier as a boolean configuration option adjacent to `workflow-log-dir`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693
https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693:186,Modifiability,config,configuration,186,"@geoffjentry That makes sense, thanks. Given the current code structure it's not at all clear to me how Docker-dependent branching would fit in - maybe this would be easier as a boolean configuration option adjacent to `workflow-log-dir`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693
https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693:229,Testability,log,log-dir,229,"@geoffjentry That makes sense, thanks. Given the current code structure it's not at all clear to me how Docker-dependent branching would fit in - maybe this would be easier as a boolean configuration option adjacent to `workflow-log-dir`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693
https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693:88,Usability,clear,clear,88,"@geoffjentry That makes sense, thanks. Given the current code structure it's not at all clear to me how Docker-dependent branching would fit in - maybe this would be easier as a boolean configuration option adjacent to `workflow-log-dir`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693
https://github.com/broadinstitute/cromwell/issues/4501#issuecomment-477711210:184,Deployability,update,update,184,"When I first looked at this I thought it was not in our power to change because the standard said something different, but I believe the standard allows for `{}` and we simply need to update the parser in OpenWDL (and then Cromwell).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4501#issuecomment-477711210
https://github.com/broadinstitute/cromwell/issues/4501#issuecomment-477711210:56,Energy Efficiency,power,power,56,"When I first looked at this I thought it was not in our power to change because the standard said something different, but I believe the standard allows for `{}` and we simply need to update the parser in OpenWDL (and then Cromwell).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4501#issuecomment-477711210
https://github.com/broadinstitute/cromwell/issues/4501#issuecomment-477711210:169,Usability,simpl,simply,169,"When I first looked at this I thought it was not in our power to change because the standard said something different, but I believe the standard allows for `{}` and we simply need to update the parser in OpenWDL (and then Cromwell).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4501#issuecomment-477711210
https://github.com/broadinstitute/cromwell/pull/4506#issuecomment-451268847:98,Usability,clear,cleared,98,"I think we should close this as it is under heavy construction, and re-reviewed once the dust has cleared",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4506#issuecomment-451268847
https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451494152:217,Deployability,pipeline,pipelines,217,"I'm concerned that the awesomeness of this will be lost if there's not a dead-simple way to get the graph. But I can accept that as a feature request on this already nice PR. Put the code in `supportedBackends/google/pipelines/v2alpha1/src/main/resources/cromwell-monitor/`, please and thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451494152
https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451494152:264,Energy Efficiency,monitor,monitor,264,"I'm concerned that the awesomeness of this will be lost if there's not a dead-simple way to get the graph. But I can accept that as a feature request on this already nice PR. Put the code in `supportedBackends/google/pipelines/v2alpha1/src/main/resources/cromwell-monitor/`, please and thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451494152
https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451494152:78,Usability,simpl,simple,78,"I'm concerned that the awesomeness of this will be lost if there's not a dead-simple way to get the graph. But I can accept that as a feature request on this already nice PR. Put the code in `supportedBackends/google/pipelines/v2alpha1/src/main/resources/cromwell-monitor/`, please and thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451494152
https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451516509:130,Availability,avail,available,130,"that's a good question. I'd say to wire it in the same way as the current monitoring script option (don't have that answer easily available to me atm). . re why default off, i've learned to be conservative w/ these sorts of things",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451516509
https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451516509:74,Energy Efficiency,monitor,monitoring,74,"that's a good question. I'd say to wire it in the same way as the current monitoring script option (don't have that answer easily available to me atm). . re why default off, i've learned to be conservative w/ these sorts of things",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451516509
https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451516509:179,Usability,learn,learned,179,"that's a good question. I'd say to wire it in the same way as the current monitoring script option (don't have that answer easily available to me atm). . re why default off, i've learned to be conservative w/ these sorts of things",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451516509
https://github.com/broadinstitute/cromwell/pull/4516#issuecomment-452047854:110,Usability,feedback,feedback,110,@rebrown1395 unassigning myself as there's currently nothing for me to do here: there are no reviewers and no feedback.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4516#issuecomment-452047854
https://github.com/broadinstitute/cromwell/pull/4522#issuecomment-469444875:79,Usability,guid,guide,79,"I'm going to close this PR as clutter, but I will leave the branch intact as a guide for if (hopefully when!) we circle back to this functionality",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4522#issuecomment-469444875
https://github.com/broadinstitute/cromwell/issues/4536#issuecomment-453705680:171,Testability,test,testing,171,"I think it's reasonable to assume the Bash shell, since's it's mostly ubiquitous now (or maybe it could be rewritten to work with `sh`?). Adding Alpine and Busybox to the testing suite sounds like an excellent idea. Is this as simple as just reducing the use of newer flags/utilities and ensuring those tests pass?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4536#issuecomment-453705680
https://github.com/broadinstitute/cromwell/issues/4536#issuecomment-453705680:303,Testability,test,tests,303,"I think it's reasonable to assume the Bash shell, since's it's mostly ubiquitous now (or maybe it could be rewritten to work with `sh`?). Adding Alpine and Busybox to the testing suite sounds like an excellent idea. Is this as simple as just reducing the use of newer flags/utilities and ensuring those tests pass?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4536#issuecomment-453705680
https://github.com/broadinstitute/cromwell/issues/4536#issuecomment-453705680:227,Usability,simpl,simple,227,"I think it's reasonable to assume the Bash shell, since's it's mostly ubiquitous now (or maybe it could be rewritten to work with `sh`?). Adding Alpine and Busybox to the testing suite sounds like an excellent idea. Is this as simple as just reducing the use of newer flags/utilities and ensuring those tests pass?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4536#issuecomment-453705680
https://github.com/broadinstitute/cromwell/issues/4536#issuecomment-461988230:23,Usability,clear,clear,23,"Yes, sorry that wasn't clear. This should be fixed in Cromwell 37 thanks to @DavyCats contribution in #4597.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4536#issuecomment-461988230
https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894:355,Availability,down,down,355,"Hi @dtenenba , we fully appreciate the importance of call caching and are looking into this. can I confirm a few things:. * that this is occurring on different files each run?; * you are seeing it every run of non-trivial size; * You have experienced at least one call-cache success run of any workflow (including a trivial one). This will help me narrow down what is going on. . To be clear, this should be working and we are aware that hashing is not a manual process but a simple value lookup.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894
https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894:269,Performance,cache,cache,269,"Hi @dtenenba , we fully appreciate the importance of call caching and are looking into this. can I confirm a few things:. * that this is occurring on different files each run?; * you are seeing it every run of non-trivial size; * You have experienced at least one call-cache success run of any workflow (including a trivial one). This will help me narrow down what is going on. . To be clear, this should be working and we are aware that hashing is not a manual process but a simple value lookup.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894
https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894:438,Security,hash,hashing,438,"Hi @dtenenba , we fully appreciate the importance of call caching and are looking into this. can I confirm a few things:. * that this is occurring on different files each run?; * you are seeing it every run of non-trivial size; * You have experienced at least one call-cache success run of any workflow (including a trivial one). This will help me narrow down what is going on. . To be clear, this should be working and we are aware that hashing is not a manual process but a simple value lookup.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894
https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894:386,Usability,clear,clear,386,"Hi @dtenenba , we fully appreciate the importance of call caching and are looking into this. can I confirm a few things:. * that this is occurring on different files each run?; * you are seeing it every run of non-trivial size; * You have experienced at least one call-cache success run of any workflow (including a trivial one). This will help me narrow down what is going on. . To be clear, this should be working and we are aware that hashing is not a manual process but a simple value lookup.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894
https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894:476,Usability,simpl,simple,476,"Hi @dtenenba , we fully appreciate the importance of call caching and are looking into this. can I confirm a few things:. * that this is occurring on different files each run?; * you are seeing it every run of non-trivial size; * You have experienced at least one call-cache success run of any workflow (including a trivial one). This will help me narrow down what is going on. . To be clear, this should be working and we are aware that hashing is not a manual process but a simple value lookup.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894
https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-467537877:253,Usability,guid,guidance,253,"This is prohibitive for us to make progress using AWS. Between call caching not working AND the inability of Cromwell to stage input and output data from S3, this breaks it all. @wleepang is this data staging issue partly something AWS can provide some guidance on? It seems specific to AWS as the backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-467537877
https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468964262:338,Deployability,update,updated,338,"So it appears that what I did (creating a new AMI with the Scratch Mount Point set to `/cromwell_root` instead of the default `/scratch`) cleared up this particular issue. However, I don't think we should close the issue yet because it doesn't appear to be documented anywhere that this is what you need to do. Until the documentation is updated I'd like to see the issue remain open. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468964262
https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468964262:138,Usability,clear,cleared,138,"So it appears that what I did (creating a new AMI with the Scratch Mount Point set to `/cromwell_root` instead of the default `/scratch`) cleared up this particular issue. However, I don't think we should close the issue yet because it doesn't appear to be documented anywhere that this is what you need to do. Until the documentation is updated I'd like to see the issue remain open. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468964262
https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-457733172:225,Usability,learn,learned,225,"@chapmanb Definitely. In particular I expect your WFs to have a rough go of things in AWS just because you do lean so heavily on those sorts of constructs, as we discovered w/ the Google backend. The hope is that the lessons learned over there make it a much easier path in AWS but it'l still wind up as work needing to be done.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-457733172
https://github.com/broadinstitute/cromwell/issues/4613#issuecomment-461156936:65,Testability,test,tests,65,"To be clear @chapmanb, we never think of it as ""you breaking our tests"", but rather ""we should test differently so we don't have to bug Brad as much when things change""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4613#issuecomment-461156936
https://github.com/broadinstitute/cromwell/issues/4613#issuecomment-461156936:95,Testability,test,test,95,"To be clear @chapmanb, we never think of it as ""you breaking our tests"", but rather ""we should test differently so we don't have to bug Brad as much when things change""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4613#issuecomment-461156936
https://github.com/broadinstitute/cromwell/issues/4613#issuecomment-461156936:6,Usability,clear,clear,6,"To be clear @chapmanb, we never think of it as ""you breaking our tests"", but rather ""we should test differently so we don't have to bug Brad as much when things change""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4613#issuecomment-461156936
https://github.com/broadinstitute/cromwell/issues/4625#issuecomment-465393316:23,Usability,clear,clear,23,"@tmm211 And just to be clear, not asking you to solve the original issue, but rather the problem @pb-cdunn is having w/ the forum",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4625#issuecomment-465393316
https://github.com/broadinstitute/cromwell/pull/4632#issuecomment-469795137:150,Usability,usab,usable,150,@cjllanwarne [rebasing on this PR](https://github.com/broadinstitute/cromwell/pull/4631/files) so the `NoopHealthMonitorServiceActor` actually became usable.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4632#issuecomment-469795137
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232:352,Deployability,install,install,352,"Hey @TMiguelT, I made a few small changes to udocker, added the notes we discussed and a next steps section to follow the general template of the other tutorials. I wanted to add a small section about the caching of udocker images but don't know udocker well enough to really assert this:. > #### Caching in udocker; > udocker caches images within the install or user directory, thus reducing the need to pull and build the docker containers at every stage. Clarification is required on whether udocker will concurrently write to the same cache directory for largely scattered workflows. So I've just left it out. I also think it might be worth saying more explicitly that Singularity is technically user-installable (just without `setuid`, as I didn't realise until our conversation. If you're happy with what's there now, I'll remove the WIP and put it up for review again. If there's anyone out there reading, we'd love to get your feedback or clarification on any points.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232:705,Deployability,install,installable,705,"Hey @TMiguelT, I made a few small changes to udocker, added the notes we discussed and a next steps section to follow the general template of the other tutorials. I wanted to add a small section about the caching of udocker images but don't know udocker well enough to really assert this:. > #### Caching in udocker; > udocker caches images within the install or user directory, thus reducing the need to pull and build the docker containers at every stage. Clarification is required on whether udocker will concurrently write to the same cache directory for largely scattered workflows. So I've just left it out. I also think it might be worth saying more explicitly that Singularity is technically user-installable (just without `setuid`, as I didn't realise until our conversation. If you're happy with what's there now, I'll remove the WIP and put it up for review again. If there's anyone out there reading, we'd love to get your feedback or clarification on any points.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232:327,Performance,cache,caches,327,"Hey @TMiguelT, I made a few small changes to udocker, added the notes we discussed and a next steps section to follow the general template of the other tutorials. I wanted to add a small section about the caching of udocker images but don't know udocker well enough to really assert this:. > #### Caching in udocker; > udocker caches images within the install or user directory, thus reducing the need to pull and build the docker containers at every stage. Clarification is required on whether udocker will concurrently write to the same cache directory for largely scattered workflows. So I've just left it out. I also think it might be worth saying more explicitly that Singularity is technically user-installable (just without `setuid`, as I didn't realise until our conversation. If you're happy with what's there now, I'll remove the WIP and put it up for review again. If there's anyone out there reading, we'd love to get your feedback or clarification on any points.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232:508,Performance,concurren,concurrently,508,"Hey @TMiguelT, I made a few small changes to udocker, added the notes we discussed and a next steps section to follow the general template of the other tutorials. I wanted to add a small section about the caching of udocker images but don't know udocker well enough to really assert this:. > #### Caching in udocker; > udocker caches images within the install or user directory, thus reducing the need to pull and build the docker containers at every stage. Clarification is required on whether udocker will concurrently write to the same cache directory for largely scattered workflows. So I've just left it out. I also think it might be worth saying more explicitly that Singularity is technically user-installable (just without `setuid`, as I didn't realise until our conversation. If you're happy with what's there now, I'll remove the WIP and put it up for review again. If there's anyone out there reading, we'd love to get your feedback or clarification on any points.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232:539,Performance,cache,cache,539,"Hey @TMiguelT, I made a few small changes to udocker, added the notes we discussed and a next steps section to follow the general template of the other tutorials. I wanted to add a small section about the caching of udocker images but don't know udocker well enough to really assert this:. > #### Caching in udocker; > udocker caches images within the install or user directory, thus reducing the need to pull and build the docker containers at every stage. Clarification is required on whether udocker will concurrently write to the same cache directory for largely scattered workflows. So I've just left it out. I also think it might be worth saying more explicitly that Singularity is technically user-installable (just without `setuid`, as I didn't realise until our conversation. If you're happy with what's there now, I'll remove the WIP and put it up for review again. If there's anyone out there reading, we'd love to get your feedback or clarification on any points.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232:276,Testability,assert,assert,276,"Hey @TMiguelT, I made a few small changes to udocker, added the notes we discussed and a next steps section to follow the general template of the other tutorials. I wanted to add a small section about the caching of udocker images but don't know udocker well enough to really assert this:. > #### Caching in udocker; > udocker caches images within the install or user directory, thus reducing the need to pull and build the docker containers at every stage. Clarification is required on whether udocker will concurrently write to the same cache directory for largely scattered workflows. So I've just left it out. I also think it might be worth saying more explicitly that Singularity is technically user-installable (just without `setuid`, as I didn't realise until our conversation. If you're happy with what's there now, I'll remove the WIP and put it up for review again. If there's anyone out there reading, we'd love to get your feedback or clarification on any points.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232:935,Usability,feedback,feedback,935,"Hey @TMiguelT, I made a few small changes to udocker, added the notes we discussed and a next steps section to follow the general template of the other tutorials. I wanted to add a small section about the caching of udocker images but don't know udocker well enough to really assert this:. > #### Caching in udocker; > udocker caches images within the install or user directory, thus reducing the need to pull and build the docker containers at every stage. Clarification is required on whether udocker will concurrently write to the same cache directory for largely scattered workflows. So I've just left it out. I also think it might be worth saying more explicitly that Singularity is technically user-installable (just without `setuid`, as I didn't realise until our conversation. If you're happy with what's there now, I'll remove the WIP and put it up for review again. If there's anyone out there reading, we'd love to get your feedback or clarification on any points.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-464519232
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702:499,Availability,error,error,499,"Hey @TMiguelT @vsoch, we noticed that on a system without `mksquashfs` in its path, the `singularity exec` from Dockerhub fails. This seems to be backed up here: http://singularity.lbl.gov/install-linux. > Note that when you configure, squashfs-tools is not required, however it is required for full functionality. You will see this message after the configuration:; > `mksquashfs from squash-tools is required for full functionality`; > If you choose not to install squashfs-tools, you will hit an error when you try a pull from Docker Hub, for example. I get slightly conflicting information from the Singularity 3 docs which just says: ; > Note that squashfs-tools is an image build dependency only and is not required for Singularity build and run commands.; (https://www.sylabs.io/guides/3.0/user-guide/quick_start.html?highlight=squashfs). We did install `squashfs` and it's in our `$PATH`, but it seems Singularity is only looking at:; - `/bin/mksquashfs`; - `/usr/bin/mksquashfs`; - `/sbin/mksquashfs`; - `/usr/sbin/mksquashfs`; - `/usr/local/bin/mksquashfs`; - `/usr/local/sbin/mksquashfs`. Any thoughts here, as you are almost always required to pull from docker hub (it's kind of the default). ___. I also noticed with some testing that in the udocker submit, if you exclude the `--rm` it will run a bit quicker. @danbills, am I able to make changes since the review?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702:189,Deployability,install,install-linux,189,"Hey @TMiguelT @vsoch, we noticed that on a system without `mksquashfs` in its path, the `singularity exec` from Dockerhub fails. This seems to be backed up here: http://singularity.lbl.gov/install-linux. > Note that when you configure, squashfs-tools is not required, however it is required for full functionality. You will see this message after the configuration:; > `mksquashfs from squash-tools is required for full functionality`; > If you choose not to install squashfs-tools, you will hit an error when you try a pull from Docker Hub, for example. I get slightly conflicting information from the Singularity 3 docs which just says: ; > Note that squashfs-tools is an image build dependency only and is not required for Singularity build and run commands.; (https://www.sylabs.io/guides/3.0/user-guide/quick_start.html?highlight=squashfs). We did install `squashfs` and it's in our `$PATH`, but it seems Singularity is only looking at:; - `/bin/mksquashfs`; - `/usr/bin/mksquashfs`; - `/sbin/mksquashfs`; - `/usr/sbin/mksquashfs`; - `/usr/local/bin/mksquashfs`; - `/usr/local/sbin/mksquashfs`. Any thoughts here, as you are almost always required to pull from docker hub (it's kind of the default). ___. I also noticed with some testing that in the udocker submit, if you exclude the `--rm` it will run a bit quicker. @danbills, am I able to make changes since the review?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702:351,Deployability,configurat,configuration,351,"Hey @TMiguelT @vsoch, we noticed that on a system without `mksquashfs` in its path, the `singularity exec` from Dockerhub fails. This seems to be backed up here: http://singularity.lbl.gov/install-linux. > Note that when you configure, squashfs-tools is not required, however it is required for full functionality. You will see this message after the configuration:; > `mksquashfs from squash-tools is required for full functionality`; > If you choose not to install squashfs-tools, you will hit an error when you try a pull from Docker Hub, for example. I get slightly conflicting information from the Singularity 3 docs which just says: ; > Note that squashfs-tools is an image build dependency only and is not required for Singularity build and run commands.; (https://www.sylabs.io/guides/3.0/user-guide/quick_start.html?highlight=squashfs). We did install `squashfs` and it's in our `$PATH`, but it seems Singularity is only looking at:; - `/bin/mksquashfs`; - `/usr/bin/mksquashfs`; - `/sbin/mksquashfs`; - `/usr/sbin/mksquashfs`; - `/usr/local/bin/mksquashfs`; - `/usr/local/sbin/mksquashfs`. Any thoughts here, as you are almost always required to pull from docker hub (it's kind of the default). ___. I also noticed with some testing that in the udocker submit, if you exclude the `--rm` it will run a bit quicker. @danbills, am I able to make changes since the review?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702:459,Deployability,install,install,459,"Hey @TMiguelT @vsoch, we noticed that on a system without `mksquashfs` in its path, the `singularity exec` from Dockerhub fails. This seems to be backed up here: http://singularity.lbl.gov/install-linux. > Note that when you configure, squashfs-tools is not required, however it is required for full functionality. You will see this message after the configuration:; > `mksquashfs from squash-tools is required for full functionality`; > If you choose not to install squashfs-tools, you will hit an error when you try a pull from Docker Hub, for example. I get slightly conflicting information from the Singularity 3 docs which just says: ; > Note that squashfs-tools is an image build dependency only and is not required for Singularity build and run commands.; (https://www.sylabs.io/guides/3.0/user-guide/quick_start.html?highlight=squashfs). We did install `squashfs` and it's in our `$PATH`, but it seems Singularity is only looking at:; - `/bin/mksquashfs`; - `/usr/bin/mksquashfs`; - `/sbin/mksquashfs`; - `/usr/sbin/mksquashfs`; - `/usr/local/bin/mksquashfs`; - `/usr/local/sbin/mksquashfs`. Any thoughts here, as you are almost always required to pull from docker hub (it's kind of the default). ___. I also noticed with some testing that in the udocker submit, if you exclude the `--rm` it will run a bit quicker. @danbills, am I able to make changes since the review?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702:853,Deployability,install,install,853,"Hey @TMiguelT @vsoch, we noticed that on a system without `mksquashfs` in its path, the `singularity exec` from Dockerhub fails. This seems to be backed up here: http://singularity.lbl.gov/install-linux. > Note that when you configure, squashfs-tools is not required, however it is required for full functionality. You will see this message after the configuration:; > `mksquashfs from squash-tools is required for full functionality`; > If you choose not to install squashfs-tools, you will hit an error when you try a pull from Docker Hub, for example. I get slightly conflicting information from the Singularity 3 docs which just says: ; > Note that squashfs-tools is an image build dependency only and is not required for Singularity build and run commands.; (https://www.sylabs.io/guides/3.0/user-guide/quick_start.html?highlight=squashfs). We did install `squashfs` and it's in our `$PATH`, but it seems Singularity is only looking at:; - `/bin/mksquashfs`; - `/usr/bin/mksquashfs`; - `/sbin/mksquashfs`; - `/usr/sbin/mksquashfs`; - `/usr/local/bin/mksquashfs`; - `/usr/local/sbin/mksquashfs`. Any thoughts here, as you are almost always required to pull from docker hub (it's kind of the default). ___. I also noticed with some testing that in the udocker submit, if you exclude the `--rm` it will run a bit quicker. @danbills, am I able to make changes since the review?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702:333,Integrability,message,message,333,"Hey @TMiguelT @vsoch, we noticed that on a system without `mksquashfs` in its path, the `singularity exec` from Dockerhub fails. This seems to be backed up here: http://singularity.lbl.gov/install-linux. > Note that when you configure, squashfs-tools is not required, however it is required for full functionality. You will see this message after the configuration:; > `mksquashfs from squash-tools is required for full functionality`; > If you choose not to install squashfs-tools, you will hit an error when you try a pull from Docker Hub, for example. I get slightly conflicting information from the Singularity 3 docs which just says: ; > Note that squashfs-tools is an image build dependency only and is not required for Singularity build and run commands.; (https://www.sylabs.io/guides/3.0/user-guide/quick_start.html?highlight=squashfs). We did install `squashfs` and it's in our `$PATH`, but it seems Singularity is only looking at:; - `/bin/mksquashfs`; - `/usr/bin/mksquashfs`; - `/sbin/mksquashfs`; - `/usr/sbin/mksquashfs`; - `/usr/local/bin/mksquashfs`; - `/usr/local/sbin/mksquashfs`. Any thoughts here, as you are almost always required to pull from docker hub (it's kind of the default). ___. I also noticed with some testing that in the udocker submit, if you exclude the `--rm` it will run a bit quicker. @danbills, am I able to make changes since the review?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702:686,Integrability,depend,dependency,686,"Hey @TMiguelT @vsoch, we noticed that on a system without `mksquashfs` in its path, the `singularity exec` from Dockerhub fails. This seems to be backed up here: http://singularity.lbl.gov/install-linux. > Note that when you configure, squashfs-tools is not required, however it is required for full functionality. You will see this message after the configuration:; > `mksquashfs from squash-tools is required for full functionality`; > If you choose not to install squashfs-tools, you will hit an error when you try a pull from Docker Hub, for example. I get slightly conflicting information from the Singularity 3 docs which just says: ; > Note that squashfs-tools is an image build dependency only and is not required for Singularity build and run commands.; (https://www.sylabs.io/guides/3.0/user-guide/quick_start.html?highlight=squashfs). We did install `squashfs` and it's in our `$PATH`, but it seems Singularity is only looking at:; - `/bin/mksquashfs`; - `/usr/bin/mksquashfs`; - `/sbin/mksquashfs`; - `/usr/sbin/mksquashfs`; - `/usr/local/bin/mksquashfs`; - `/usr/local/sbin/mksquashfs`. Any thoughts here, as you are almost always required to pull from docker hub (it's kind of the default). ___. I also noticed with some testing that in the udocker submit, if you exclude the `--rm` it will run a bit quicker. @danbills, am I able to make changes since the review?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702:225,Modifiability,config,configure,225,"Hey @TMiguelT @vsoch, we noticed that on a system without `mksquashfs` in its path, the `singularity exec` from Dockerhub fails. This seems to be backed up here: http://singularity.lbl.gov/install-linux. > Note that when you configure, squashfs-tools is not required, however it is required for full functionality. You will see this message after the configuration:; > `mksquashfs from squash-tools is required for full functionality`; > If you choose not to install squashfs-tools, you will hit an error when you try a pull from Docker Hub, for example. I get slightly conflicting information from the Singularity 3 docs which just says: ; > Note that squashfs-tools is an image build dependency only and is not required for Singularity build and run commands.; (https://www.sylabs.io/guides/3.0/user-guide/quick_start.html?highlight=squashfs). We did install `squashfs` and it's in our `$PATH`, but it seems Singularity is only looking at:; - `/bin/mksquashfs`; - `/usr/bin/mksquashfs`; - `/sbin/mksquashfs`; - `/usr/sbin/mksquashfs`; - `/usr/local/bin/mksquashfs`; - `/usr/local/sbin/mksquashfs`. Any thoughts here, as you are almost always required to pull from docker hub (it's kind of the default). ___. I also noticed with some testing that in the udocker submit, if you exclude the `--rm` it will run a bit quicker. @danbills, am I able to make changes since the review?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702:351,Modifiability,config,configuration,351,"Hey @TMiguelT @vsoch, we noticed that on a system without `mksquashfs` in its path, the `singularity exec` from Dockerhub fails. This seems to be backed up here: http://singularity.lbl.gov/install-linux. > Note that when you configure, squashfs-tools is not required, however it is required for full functionality. You will see this message after the configuration:; > `mksquashfs from squash-tools is required for full functionality`; > If you choose not to install squashfs-tools, you will hit an error when you try a pull from Docker Hub, for example. I get slightly conflicting information from the Singularity 3 docs which just says: ; > Note that squashfs-tools is an image build dependency only and is not required for Singularity build and run commands.; (https://www.sylabs.io/guides/3.0/user-guide/quick_start.html?highlight=squashfs). We did install `squashfs` and it's in our `$PATH`, but it seems Singularity is only looking at:; - `/bin/mksquashfs`; - `/usr/bin/mksquashfs`; - `/sbin/mksquashfs`; - `/usr/sbin/mksquashfs`; - `/usr/local/bin/mksquashfs`; - `/usr/local/sbin/mksquashfs`. Any thoughts here, as you are almost always required to pull from docker hub (it's kind of the default). ___. I also noticed with some testing that in the udocker submit, if you exclude the `--rm` it will run a bit quicker. @danbills, am I able to make changes since the review?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702:1235,Testability,test,testing,1235,"Hey @TMiguelT @vsoch, we noticed that on a system without `mksquashfs` in its path, the `singularity exec` from Dockerhub fails. This seems to be backed up here: http://singularity.lbl.gov/install-linux. > Note that when you configure, squashfs-tools is not required, however it is required for full functionality. You will see this message after the configuration:; > `mksquashfs from squash-tools is required for full functionality`; > If you choose not to install squashfs-tools, you will hit an error when you try a pull from Docker Hub, for example. I get slightly conflicting information from the Singularity 3 docs which just says: ; > Note that squashfs-tools is an image build dependency only and is not required for Singularity build and run commands.; (https://www.sylabs.io/guides/3.0/user-guide/quick_start.html?highlight=squashfs). We did install `squashfs` and it's in our `$PATH`, but it seems Singularity is only looking at:; - `/bin/mksquashfs`; - `/usr/bin/mksquashfs`; - `/sbin/mksquashfs`; - `/usr/sbin/mksquashfs`; - `/usr/local/bin/mksquashfs`; - `/usr/local/sbin/mksquashfs`. Any thoughts here, as you are almost always required to pull from docker hub (it's kind of the default). ___. I also noticed with some testing that in the udocker submit, if you exclude the `--rm` it will run a bit quicker. @danbills, am I able to make changes since the review?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702:786,Usability,guid,guides,786,"Hey @TMiguelT @vsoch, we noticed that on a system without `mksquashfs` in its path, the `singularity exec` from Dockerhub fails. This seems to be backed up here: http://singularity.lbl.gov/install-linux. > Note that when you configure, squashfs-tools is not required, however it is required for full functionality. You will see this message after the configuration:; > `mksquashfs from squash-tools is required for full functionality`; > If you choose not to install squashfs-tools, you will hit an error when you try a pull from Docker Hub, for example. I get slightly conflicting information from the Singularity 3 docs which just says: ; > Note that squashfs-tools is an image build dependency only and is not required for Singularity build and run commands.; (https://www.sylabs.io/guides/3.0/user-guide/quick_start.html?highlight=squashfs). We did install `squashfs` and it's in our `$PATH`, but it seems Singularity is only looking at:; - `/bin/mksquashfs`; - `/usr/bin/mksquashfs`; - `/sbin/mksquashfs`; - `/usr/sbin/mksquashfs`; - `/usr/local/bin/mksquashfs`; - `/usr/local/sbin/mksquashfs`. Any thoughts here, as you are almost always required to pull from docker hub (it's kind of the default). ___. I also noticed with some testing that in the udocker submit, if you exclude the `--rm` it will run a bit quicker. @danbills, am I able to make changes since the review?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702
https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702:802,Usability,guid,guide,802,"Hey @TMiguelT @vsoch, we noticed that on a system without `mksquashfs` in its path, the `singularity exec` from Dockerhub fails. This seems to be backed up here: http://singularity.lbl.gov/install-linux. > Note that when you configure, squashfs-tools is not required, however it is required for full functionality. You will see this message after the configuration:; > `mksquashfs from squash-tools is required for full functionality`; > If you choose not to install squashfs-tools, you will hit an error when you try a pull from Docker Hub, for example. I get slightly conflicting information from the Singularity 3 docs which just says: ; > Note that squashfs-tools is an image build dependency only and is not required for Singularity build and run commands.; (https://www.sylabs.io/guides/3.0/user-guide/quick_start.html?highlight=squashfs). We did install `squashfs` and it's in our `$PATH`, but it seems Singularity is only looking at:; - `/bin/mksquashfs`; - `/usr/bin/mksquashfs`; - `/sbin/mksquashfs`; - `/usr/sbin/mksquashfs`; - `/usr/local/bin/mksquashfs`; - `/usr/local/sbin/mksquashfs`. Any thoughts here, as you are almost always required to pull from docker hub (it's kind of the default). ___. I also noticed with some testing that in the udocker submit, if you exclude the `--rm` it will run a bit quicker. @danbills, am I able to make changes since the review?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702
https://github.com/broadinstitute/cromwell/issues/4638#issuecomment-468419113:207,Performance,scalab,scalable,207,"To give some context, as we work on the NHGRI AnVIL, it is of interest to produce workflows that utilize the PanCancer atlas. The representation of the atlas in BigQuery is excellent for learning both about scalable solutions and about cancer biology. With Cromwell+R we can protect the investigators from working with SQL directly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4638#issuecomment-468419113
https://github.com/broadinstitute/cromwell/issues/4638#issuecomment-468419113:187,Usability,learn,learning,187,"To give some context, as we work on the NHGRI AnVIL, it is of interest to produce workflows that utilize the PanCancer atlas. The representation of the atlas in BigQuery is excellent for learning both about scalable solutions and about cancer biology. With Cromwell+R we can protect the investigators from working with SQL directly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4638#issuecomment-468419113
https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-464255682:94,Usability,simpl,simplier,94,@aednichols kind-of: I did not have permission to reopen #4555 and I also decided to create a simplier workflow that shows the problem,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-464255682
https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-1017704304:758,Availability,echo,echo,758,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-1017704304
https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-1017704304:991,Availability,error,error,991,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-1017704304
https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-1017704304:216,Testability,test,teste,216,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-1017704304
https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-1017704304:1022,Usability,simpl,simple,1022,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-1017704304
https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-1017704304:1209,Usability,simpl,simply,1209,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-1017704304
https://github.com/broadinstitute/cromwell/issues/4668#issuecomment-491862476:158,Deployability,release,release,158,"I included some changes in #4961 that might work for this enhancement request. The changes in that PR publish ""workflowProcessingEvents"" for workflow pickup, release, and completion. The first two events types can be multi-valued since Cromwell can be restarted and possibly upgraded during the execution of a workflow. Sample metadata from a simple run:. ```; {; ""workflowName"": ""wf_hello"",; ""workflowProcessingEvents"": [; {; ""cromwellId"": ""cromid-4db4123"",; ""timestamp"": ""2019-05-13T15:00:22.152Z"",; ""cromwellVersion"": ""41-07606c8-SNAP"",; ""description"": ""Finished""; },; {; ""cromwellId"": ""cromid-4db4123"",; ""description"": ""PickedUp"",; ""timestamp"": ""2019-05-13T15:00:10.879Z"",; ""cromwellVersion"": ""41-07606c8-SNAP""; }; ],; ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4668#issuecomment-491862476
https://github.com/broadinstitute/cromwell/issues/4668#issuecomment-491862476:275,Deployability,upgrade,upgraded,275,"I included some changes in #4961 that might work for this enhancement request. The changes in that PR publish ""workflowProcessingEvents"" for workflow pickup, release, and completion. The first two events types can be multi-valued since Cromwell can be restarted and possibly upgraded during the execution of a workflow. Sample metadata from a simple run:. ```; {; ""workflowName"": ""wf_hello"",; ""workflowProcessingEvents"": [; {; ""cromwellId"": ""cromid-4db4123"",; ""timestamp"": ""2019-05-13T15:00:22.152Z"",; ""cromwellVersion"": ""41-07606c8-SNAP"",; ""description"": ""Finished""; },; {; ""cromwellId"": ""cromid-4db4123"",; ""description"": ""PickedUp"",; ""timestamp"": ""2019-05-13T15:00:10.879Z"",; ""cromwellVersion"": ""41-07606c8-SNAP""; }; ],; ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4668#issuecomment-491862476
https://github.com/broadinstitute/cromwell/issues/4668#issuecomment-491862476:58,Modifiability,enhance,enhancement,58,"I included some changes in #4961 that might work for this enhancement request. The changes in that PR publish ""workflowProcessingEvents"" for workflow pickup, release, and completion. The first two events types can be multi-valued since Cromwell can be restarted and possibly upgraded during the execution of a workflow. Sample metadata from a simple run:. ```; {; ""workflowName"": ""wf_hello"",; ""workflowProcessingEvents"": [; {; ""cromwellId"": ""cromid-4db4123"",; ""timestamp"": ""2019-05-13T15:00:22.152Z"",; ""cromwellVersion"": ""41-07606c8-SNAP"",; ""description"": ""Finished""; },; {; ""cromwellId"": ""cromid-4db4123"",; ""description"": ""PickedUp"",; ""timestamp"": ""2019-05-13T15:00:10.879Z"",; ""cromwellVersion"": ""41-07606c8-SNAP""; }; ],; ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4668#issuecomment-491862476
https://github.com/broadinstitute/cromwell/issues/4668#issuecomment-491862476:343,Usability,simpl,simple,343,"I included some changes in #4961 that might work for this enhancement request. The changes in that PR publish ""workflowProcessingEvents"" for workflow pickup, release, and completion. The first two events types can be multi-valued since Cromwell can be restarted and possibly upgraded during the execution of a workflow. Sample metadata from a simple run:. ```; {; ""workflowName"": ""wf_hello"",; ""workflowProcessingEvents"": [; {; ""cromwellId"": ""cromid-4db4123"",; ""timestamp"": ""2019-05-13T15:00:22.152Z"",; ""cromwellVersion"": ""41-07606c8-SNAP"",; ""description"": ""Finished""; },; {; ""cromwellId"": ""cromid-4db4123"",; ""description"": ""PickedUp"",; ""timestamp"": ""2019-05-13T15:00:10.879Z"",; ""cromwellVersion"": ""41-07606c8-SNAP""; }; ],; ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4668#issuecomment-491862476
https://github.com/broadinstitute/cromwell/issues/4682#issuecomment-471224124:303,Integrability,protocol,protocol,303,"@salonishah11 for example, I'm running a cromwell container in server mode, bound to port 8000: ; ```; docker run -p 8000:8000 cromwell server; ```; but when I try to ; ```; docker run cromwell submit --host 0.0.0.0:8000 ...; ```; I get: ; ```; Error: Option --host failed when given '0.0.0.0:8000'. no protocol: 0.0.0.0:8000; ```. Some simple docs would help here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4682#issuecomment-471224124
https://github.com/broadinstitute/cromwell/issues/4682#issuecomment-471224124:337,Usability,simpl,simple,337,"@salonishah11 for example, I'm running a cromwell container in server mode, bound to port 8000: ; ```; docker run -p 8000:8000 cromwell server; ```; but when I try to ; ```; docker run cromwell submit --host 0.0.0.0:8000 ...; ```; I get: ; ```; Error: Option --host failed when given '0.0.0.0:8000'. no protocol: 0.0.0.0:8000; ```. Some simple docs would help here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4682#issuecomment-471224124
https://github.com/broadinstitute/cromwell/pull/4697#issuecomment-470293660:561,Usability,clear,clearly,561,"> As for HPC and docker, there are some renegades out there running nodes with docker. The very few I've come across are not on-prem, spin up private larger machines on cloud resources, add HPC+docker, and then run whatever scripts on top of that. This includes our CI... until we get one of the non-root LXC implementations going. Interesting! If it's a cloud setup, I don't know if we would call that HPC. When I say HPC I typically mean SLURM, SGE, etc., where there are many thousands of untrusted users. Putting Docker on those systems - no admin thinking clearly would do it. For a really small number of users that you absolutely trust? That could be a thing, haha. :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4697#issuecomment-470293660
https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469889241:19,Deployability,update,updated,19,@mcovarr @kcibul I updated the examples above to reflect the SQL queries generated post PR feedback. Main changes:; - No `EXISTS` for the types of query that Job Manager 0.5.9 produces (labelsAnd and labelsOr).; - Because we now use `JOIN`s for both `LabelAND` and `LabelOR` queries.; - Using `1` instead of `*` for those sub-selects. Would appreciate any other thoughts,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469889241
https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469889241:91,Usability,feedback,feedback,91,@mcovarr @kcibul I updated the examples above to reflect the SQL queries generated post PR feedback. Main changes:; - No `EXISTS` for the types of query that Job Manager 0.5.9 produces (labelsAnd and labelsOr).; - Because we now use `JOIN`s for both `LabelAND` and `LabelOR` queries.; - Using `1` instead of `*` for those sub-selects. Would appreciate any other thoughts,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469889241
https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469900089:57,Performance,perform,performance,57,Looks good delta some final feedback incorporation and a performance test on alpha.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469900089
https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469900089:69,Testability,test,test,69,Looks good delta some final feedback incorporation and a performance test on alpha.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469900089
https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469900089:28,Usability,feedback,feedback,28,Looks good delta some final feedback incorporation and a performance test on alpha.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4702#issuecomment-469900089
https://github.com/broadinstitute/cromwell/issues/4731#issuecomment-484529709:21,Usability,learn,learned,21,"Brain dumping what I learned from Emil:; This localization code is in the [proxy](https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/aws/src/main/resources/ecs-proxy/proxy).; Probably needs to use a force global flag as in the Java SDK. Looking forward, I also see an issue w/ call caching to files outside the compute region, as the [filesystem copy](https://github.com/broadinstitute/cromwell/blob/develop/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java) is not using the force global flag.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731#issuecomment-484529709
https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165:264,Deployability,configurat,configuration,264,"> Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed?. Using ""default credentials"" means that any one of a very long list of configuration setups are auto-detected by the AWS Java SDK. In each SDK-configuration case, the credentials are **not** read from the cromwell-config file and then the values passed on to the AWS SDK. Instead the SDK ""discovers"" them. https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default. This ticket is to instead wire in credentials to the SDK via [Explicitly Specifying Credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-explicit). In the tests:; - Each of the ""[Default Credential Provider](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default)"" options would not be configured on the system; - The `java -Dconfig.file=..."" would contain the AWS key, secret and probably region; - When the various AWS SDK functions run, they each use the passed in credentials to run the commands for S3, Batch, etc.; - The jobs should still run to completion",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165
https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165:336,Deployability,configurat,configuration,336,"> Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed?. Using ""default credentials"" means that any one of a very long list of configuration setups are auto-detected by the AWS Java SDK. In each SDK-configuration case, the credentials are **not** read from the cromwell-config file and then the values passed on to the AWS SDK. Instead the SDK ""discovers"" them. https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default. This ticket is to instead wire in credentials to the SDK via [Explicitly Specifying Credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-explicit). In the tests:; - Each of the ""[Default Credential Provider](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default)"" options would not be configured on the system; - The `java -Dconfig.file=..."" would contain the AWS key, secret and probably region; - When the various AWS SDK functions run, they each use the passed in credentials to run the commands for S3, Batch, etc.; - The jobs should still run to completion",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165
https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165:264,Modifiability,config,configuration,264,"> Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed?. Using ""default credentials"" means that any one of a very long list of configuration setups are auto-detected by the AWS Java SDK. In each SDK-configuration case, the credentials are **not** read from the cromwell-config file and then the values passed on to the AWS SDK. Instead the SDK ""discovers"" them. https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default. This ticket is to instead wire in credentials to the SDK via [Explicitly Specifying Credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-explicit). In the tests:; - Each of the ""[Default Credential Provider](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default)"" options would not be configured on the system; - The `java -Dconfig.file=..."" would contain the AWS key, secret and probably region; - When the various AWS SDK functions run, they each use the passed in credentials to run the commands for S3, Batch, etc.; - The jobs should still run to completion",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165
https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165:336,Modifiability,config,configuration,336,"> Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed?. Using ""default credentials"" means that any one of a very long list of configuration setups are auto-detected by the AWS Java SDK. In each SDK-configuration case, the credentials are **not** read from the cromwell-config file and then the values passed on to the AWS SDK. Instead the SDK ""discovers"" them. https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default. This ticket is to instead wire in credentials to the SDK via [Explicitly Specifying Credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-explicit). In the tests:; - Each of the ""[Default Credential Provider](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default)"" options would not be configured on the system; - The `java -Dconfig.file=..."" would contain the AWS key, secret and probably region; - When the various AWS SDK functions run, they each use the passed in credentials to run the commands for S3, Batch, etc.; - The jobs should still run to completion",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165
https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165:407,Modifiability,config,config,407,"> Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed?. Using ""default credentials"" means that any one of a very long list of configuration setups are auto-detected by the AWS Java SDK. In each SDK-configuration case, the credentials are **not** read from the cromwell-config file and then the values passed on to the AWS SDK. Instead the SDK ""discovers"" them. https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default. This ticket is to instead wire in credentials to the SDK via [Explicitly Specifying Credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-explicit). In the tests:; - Each of the ""[Default Credential Provider](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default)"" options would not be configured on the system; - The `java -Dconfig.file=..."" would contain the AWS key, secret and probably region; - When the various AWS SDK functions run, they each use the passed in credentials to run the commands for S3, Batch, etc.; - The jobs should still run to completion",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165
https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165:974,Modifiability,config,configured,974,"> Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed?. Using ""default credentials"" means that any one of a very long list of configuration setups are auto-detected by the AWS Java SDK. In each SDK-configuration case, the credentials are **not** read from the cromwell-config file and then the values passed on to the AWS SDK. Instead the SDK ""discovers"" them. https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default. This ticket is to instead wire in credentials to the SDK via [Explicitly Specifying Credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-explicit). In the tests:; - Each of the ""[Default Credential Provider](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default)"" options would not be configured on the system; - The `java -Dconfig.file=..."" would contain the AWS key, secret and probably region; - When the various AWS SDK functions run, they each use the passed in credentials to run the commands for S3, Batch, etc.; - The jobs should still run to completion",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165
https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165:294,Safety,detect,detected,294,"> Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed?. Using ""default credentials"" means that any one of a very long list of configuration setups are auto-detected by the AWS Java SDK. In each SDK-configuration case, the credentials are **not** read from the cromwell-config file and then the values passed on to the AWS SDK. Instead the SDK ""discovers"" them. https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default. This ticket is to instead wire in credentials to the SDK via [Explicitly Specifying Credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-explicit). In the tests:; - Each of the ""[Default Credential Provider](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default)"" options would not be configured on the system; - The `java -Dconfig.file=..."" would contain the AWS key, secret and probably region; - When the various AWS SDK functions run, they each use the passed in credentials to run the commands for S3, Batch, etc.; - The jobs should still run to completion",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165
https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165:161,Testability,test,tests,161,"> Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed?. Using ""default credentials"" means that any one of a very long list of configuration setups are auto-detected by the AWS Java SDK. In each SDK-configuration case, the credentials are **not** read from the cromwell-config file and then the values passed on to the AWS SDK. Instead the SDK ""discovers"" them. https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default. This ticket is to instead wire in credentials to the SDK via [Explicitly Specifying Credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-explicit). In the tests:; - Each of the ""[Default Credential Provider](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default)"" options would not be configured on the system; - The `java -Dconfig.file=..."" would contain the AWS key, secret and probably region; - When the various AWS SDK functions run, they each use the passed in credentials to run the commands for S3, Batch, etc.; - The jobs should still run to completion",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165
https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165:801,Testability,test,tests,801,"> Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed?. Using ""default credentials"" means that any one of a very long list of configuration setups are auto-detected by the AWS Java SDK. In each SDK-configuration case, the credentials are **not** read from the cromwell-config file and then the values passed on to the AWS SDK. Instead the SDK ""discovers"" them. https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default. This ticket is to instead wire in credentials to the SDK via [Explicitly Specifying Credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-explicit). In the tests:; - Each of the ""[Default Credential Provider](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default)"" options would not be configured on the system; - The `java -Dconfig.file=..."" would contain the AWS key, secret and probably region; - When the various AWS SDK functions run, they each use the passed in credentials to run the commands for S3, Batch, etc.; - The jobs should still run to completion",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165
https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165:553,Usability,guid,guide,553,"> Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed?. Using ""default credentials"" means that any one of a very long list of configuration setups are auto-detected by the AWS Java SDK. In each SDK-configuration case, the credentials are **not** read from the cromwell-config file and then the values passed on to the AWS SDK. Instead the SDK ""discovers"" them. https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default. This ticket is to instead wire in credentials to the SDK via [Explicitly Specifying Credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-explicit). In the tests:; - Each of the ""[Default Credential Provider](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default)"" options would not be configured on the system; - The `java -Dconfig.file=..."" would contain the AWS key, secret and probably region; - When the various AWS SDK functions run, they each use the passed in credentials to run the commands for S3, Batch, etc.; - The jobs should still run to completion",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165
https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165:748,Usability,guid,guide,748,"> Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed?. Using ""default credentials"" means that any one of a very long list of configuration setups are auto-detected by the AWS Java SDK. In each SDK-configuration case, the credentials are **not** read from the cromwell-config file and then the values passed on to the AWS SDK. Instead the SDK ""discovers"" them. https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default. This ticket is to instead wire in credentials to the SDK via [Explicitly Specifying Credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-explicit). In the tests:; - Each of the ""[Default Credential Provider](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default)"" options would not be configured on the system; - The `java -Dconfig.file=..."" would contain the AWS key, secret and probably region; - When the various AWS SDK functions run, they each use the passed in credentials to run the commands for S3, Batch, etc.; - The jobs should still run to completion",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165
https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165:908,Usability,guid,guide,908,"> Does it mean that the credentials, which Travis and Jenkins use, are default credentials and the purpose of this task is to set other ones and check that four tests were successfully passed?. Using ""default credentials"" means that any one of a very long list of configuration setups are auto-detected by the AWS Java SDK. In each SDK-configuration case, the credentials are **not** read from the cromwell-config file and then the values passed on to the AWS SDK. Instead the SDK ""discovers"" them. https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default. This ticket is to instead wire in credentials to the SDK via [Explicitly Specifying Credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-explicit). In the tests:; - Each of the ""[Default Credential Provider](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default)"" options would not be configured on the system; - The `java -Dconfig.file=..."" would contain the AWS key, secret and probably region; - When the various AWS SDK functions run, they each use the passed in credentials to run the commands for S3, Batch, etc.; - The jobs should still run to completion",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740#issuecomment-505233165
https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473341426:104,Modifiability,config,config,104,@geoffjentry I ran into the same problem with `long_cmd` when I made the same mistake of using a non-CI config during horicromtal testing. Some lessons learned [here](https://github.com/broadinstitute/cromwell/pull/4748/files).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473341426
https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473341426:130,Testability,test,testing,130,@geoffjentry I ran into the same problem with `long_cmd` when I made the same mistake of using a non-CI config during horicromtal testing. Some lessons learned [here](https://github.com/broadinstitute/cromwell/pull/4748/files).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473341426
https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473341426:152,Usability,learn,learned,152,@geoffjentry I ran into the same problem with `long_cmd` when I made the same mistake of using a non-CI config during horicromtal testing. Some lessons learned [here](https://github.com/broadinstitute/cromwell/pull/4748/files).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4747#issuecomment-473341426
https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247:568,Availability,error,error,568,"Running the workflow; ```; version 1.0. workflow test {; input {; Array[File]? y = [""some/file/path.txt""]; }. output {; Array[File] x = select_first([y, []]); }; }; ```; on latest `develop` seems to work fine, producing outputs; ```; {; ""test.x"": [""some/file/path.txt""]; }; ```. Is this an accurate simplification of your problem case?. There is a good chance this bug is fixed in 37 onward as a result of https://github.com/broadinstitute/cromwell/pull/4324; >Fixed a regression in Cromwell 36 that could cause operations on empty arrays to fail with a spurious type error. I suspect your workflow got stuck after failing because the `WomArray` code [throws an exception](https://github.com/broadinstitute/cromwell/blob/develop/wom/src/main/scala/wom/values/WomArray.scala#L37) that screws up control flow. I believe this is a ""this should never happen"" case so we did not bother upgrading it to our fancier error handling that encodes failures in the type system to achieve predictable behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247
https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247:909,Availability,error,error,909,"Running the workflow; ```; version 1.0. workflow test {; input {; Array[File]? y = [""some/file/path.txt""]; }. output {; Array[File] x = select_first([y, []]); }; }; ```; on latest `develop` seems to work fine, producing outputs; ```; {; ""test.x"": [""some/file/path.txt""]; }; ```. Is this an accurate simplification of your problem case?. There is a good chance this bug is fixed in 37 onward as a result of https://github.com/broadinstitute/cromwell/pull/4324; >Fixed a regression in Cromwell 36 that could cause operations on empty arrays to fail with a spurious type error. I suspect your workflow got stuck after failing because the `WomArray` code [throws an exception](https://github.com/broadinstitute/cromwell/blob/develop/wom/src/main/scala/wom/values/WomArray.scala#L37) that screws up control flow. I believe this is a ""this should never happen"" case so we did not bother upgrading it to our fancier error handling that encodes failures in the type system to achieve predictable behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247
https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247:937,Availability,failure,failures,937,"Running the workflow; ```; version 1.0. workflow test {; input {; Array[File]? y = [""some/file/path.txt""]; }. output {; Array[File] x = select_first([y, []]); }; }; ```; on latest `develop` seems to work fine, producing outputs; ```; {; ""test.x"": [""some/file/path.txt""]; }; ```. Is this an accurate simplification of your problem case?. There is a good chance this bug is fixed in 37 onward as a result of https://github.com/broadinstitute/cromwell/pull/4324; >Fixed a regression in Cromwell 36 that could cause operations on empty arrays to fail with a spurious type error. I suspect your workflow got stuck after failing because the `WomArray` code [throws an exception](https://github.com/broadinstitute/cromwell/blob/develop/wom/src/main/scala/wom/values/WomArray.scala#L37) that screws up control flow. I believe this is a ""this should never happen"" case so we did not bother upgrading it to our fancier error handling that encodes failures in the type system to achieve predictable behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247
https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247:976,Safety,predict,predictable,976,"Running the workflow; ```; version 1.0. workflow test {; input {; Array[File]? y = [""some/file/path.txt""]; }. output {; Array[File] x = select_first([y, []]); }; }; ```; on latest `develop` seems to work fine, producing outputs; ```; {; ""test.x"": [""some/file/path.txt""]; }; ```. Is this an accurate simplification of your problem case?. There is a good chance this bug is fixed in 37 onward as a result of https://github.com/broadinstitute/cromwell/pull/4324; >Fixed a regression in Cromwell 36 that could cause operations on empty arrays to fail with a spurious type error. I suspect your workflow got stuck after failing because the `WomArray` code [throws an exception](https://github.com/broadinstitute/cromwell/blob/develop/wom/src/main/scala/wom/values/WomArray.scala#L37) that screws up control flow. I believe this is a ""this should never happen"" case so we did not bother upgrading it to our fancier error handling that encodes failures in the type system to achieve predictable behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247
https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247:49,Testability,test,test,49,"Running the workflow; ```; version 1.0. workflow test {; input {; Array[File]? y = [""some/file/path.txt""]; }. output {; Array[File] x = select_first([y, []]); }; }; ```; on latest `develop` seems to work fine, producing outputs; ```; {; ""test.x"": [""some/file/path.txt""]; }; ```. Is this an accurate simplification of your problem case?. There is a good chance this bug is fixed in 37 onward as a result of https://github.com/broadinstitute/cromwell/pull/4324; >Fixed a regression in Cromwell 36 that could cause operations on empty arrays to fail with a spurious type error. I suspect your workflow got stuck after failing because the `WomArray` code [throws an exception](https://github.com/broadinstitute/cromwell/blob/develop/wom/src/main/scala/wom/values/WomArray.scala#L37) that screws up control flow. I believe this is a ""this should never happen"" case so we did not bother upgrading it to our fancier error handling that encodes failures in the type system to achieve predictable behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247
https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247:238,Testability,test,test,238,"Running the workflow; ```; version 1.0. workflow test {; input {; Array[File]? y = [""some/file/path.txt""]; }. output {; Array[File] x = select_first([y, []]); }; }; ```; on latest `develop` seems to work fine, producing outputs; ```; {; ""test.x"": [""some/file/path.txt""]; }; ```. Is this an accurate simplification of your problem case?. There is a good chance this bug is fixed in 37 onward as a result of https://github.com/broadinstitute/cromwell/pull/4324; >Fixed a regression in Cromwell 36 that could cause operations on empty arrays to fail with a spurious type error. I suspect your workflow got stuck after failing because the `WomArray` code [throws an exception](https://github.com/broadinstitute/cromwell/blob/develop/wom/src/main/scala/wom/values/WomArray.scala#L37) that screws up control flow. I believe this is a ""this should never happen"" case so we did not bother upgrading it to our fancier error handling that encodes failures in the type system to achieve predictable behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247
https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247:299,Usability,simpl,simplification,299,"Running the workflow; ```; version 1.0. workflow test {; input {; Array[File]? y = [""some/file/path.txt""]; }. output {; Array[File] x = select_first([y, []]); }; }; ```; on latest `develop` seems to work fine, producing outputs; ```; {; ""test.x"": [""some/file/path.txt""]; }; ```. Is this an accurate simplification of your problem case?. There is a good chance this bug is fixed in 37 onward as a result of https://github.com/broadinstitute/cromwell/pull/4324; >Fixed a regression in Cromwell 36 that could cause operations on empty arrays to fail with a spurious type error. I suspect your workflow got stuck after failing because the `WomArray` code [throws an exception](https://github.com/broadinstitute/cromwell/blob/develop/wom/src/main/scala/wom/values/WomArray.scala#L37) that screws up control flow. I believe this is a ""this should never happen"" case so we did not bother upgrading it to our fancier error handling that encodes failures in the type system to achieve predictable behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247
https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402:606,Availability,down,downstream,606,"Since it's been a month I thought I'd post an update. Main items:; - In general I'm having to make a lot more changes to the Scala code than I expected due to queries being written in a way that Postgres doesn't like. (This isn't a criticism, more of a heads-up.) Nothing functional, just refactoring.; - The way `Blob` is handled in Slick+Postgres turns out to be a massive pain. I'm not sure if Slick is lazy-loading these fields or I just don't understand how it works under the hood, but the workaround is that the blobs need to be accessed as part of a transaction, which involved some refactoring of downstream processing.; - Semi-related question: is there a reason why the entire contents of the `importsZip` need to be stored in the database? This quickly leads to an enormous METADATA_ENTRY table - possibly because I have call caching turned on, I haven't checked whether this is the cause yet.; - The auto-incremented fields that are `Option[Long]` in the data model can't be handled the same way in Postgres; I haven't decided whether this is simply different database behavior or a bug somewhere. Anyway I found a workaround for that too.; - I may have messed up and branched from `master` in my fork by mistake, and in any case I'm definitely out of sync with your `develop`. Do you have a preferred workflow to bring my branch up to date, i.e. to minimize the mess in the Git history? (Despite using Git daily I'm still not totally sure what ""best practice"" is.). At this point I can at least run a workflow using Postgres, minus call caching. I'm going to be focusing on completing and testing this in the next couple of weeks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402
https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402:46,Deployability,update,update,46,"Since it's been a month I thought I'd post an update. Main items:; - In general I'm having to make a lot more changes to the Scala code than I expected due to queries being written in a way that Postgres doesn't like. (This isn't a criticism, more of a heads-up.) Nothing functional, just refactoring.; - The way `Blob` is handled in Slick+Postgres turns out to be a massive pain. I'm not sure if Slick is lazy-loading these fields or I just don't understand how it works under the hood, but the workaround is that the blobs need to be accessed as part of a transaction, which involved some refactoring of downstream processing.; - Semi-related question: is there a reason why the entire contents of the `importsZip` need to be stored in the database? This quickly leads to an enormous METADATA_ENTRY table - possibly because I have call caching turned on, I haven't checked whether this is the cause yet.; - The auto-incremented fields that are `Option[Long]` in the data model can't be handled the same way in Postgres; I haven't decided whether this is simply different database behavior or a bug somewhere. Anyway I found a workaround for that too.; - I may have messed up and branched from `master` in my fork by mistake, and in any case I'm definitely out of sync with your `develop`. Do you have a preferred workflow to bring my branch up to date, i.e. to minimize the mess in the Git history? (Despite using Git daily I'm still not totally sure what ""best practice"" is.). At this point I can at least run a workflow using Postgres, minus call caching. I'm going to be focusing on completing and testing this in the next couple of weeks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402
https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402:289,Modifiability,refactor,refactoring,289,"Since it's been a month I thought I'd post an update. Main items:; - In general I'm having to make a lot more changes to the Scala code than I expected due to queries being written in a way that Postgres doesn't like. (This isn't a criticism, more of a heads-up.) Nothing functional, just refactoring.; - The way `Blob` is handled in Slick+Postgres turns out to be a massive pain. I'm not sure if Slick is lazy-loading these fields or I just don't understand how it works under the hood, but the workaround is that the blobs need to be accessed as part of a transaction, which involved some refactoring of downstream processing.; - Semi-related question: is there a reason why the entire contents of the `importsZip` need to be stored in the database? This quickly leads to an enormous METADATA_ENTRY table - possibly because I have call caching turned on, I haven't checked whether this is the cause yet.; - The auto-incremented fields that are `Option[Long]` in the data model can't be handled the same way in Postgres; I haven't decided whether this is simply different database behavior or a bug somewhere. Anyway I found a workaround for that too.; - I may have messed up and branched from `master` in my fork by mistake, and in any case I'm definitely out of sync with your `develop`. Do you have a preferred workflow to bring my branch up to date, i.e. to minimize the mess in the Git history? (Despite using Git daily I'm still not totally sure what ""best practice"" is.). At this point I can at least run a workflow using Postgres, minus call caching. I'm going to be focusing on completing and testing this in the next couple of weeks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402
https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402:591,Modifiability,refactor,refactoring,591,"Since it's been a month I thought I'd post an update. Main items:; - In general I'm having to make a lot more changes to the Scala code than I expected due to queries being written in a way that Postgres doesn't like. (This isn't a criticism, more of a heads-up.) Nothing functional, just refactoring.; - The way `Blob` is handled in Slick+Postgres turns out to be a massive pain. I'm not sure if Slick is lazy-loading these fields or I just don't understand how it works under the hood, but the workaround is that the blobs need to be accessed as part of a transaction, which involved some refactoring of downstream processing.; - Semi-related question: is there a reason why the entire contents of the `importsZip` need to be stored in the database? This quickly leads to an enormous METADATA_ENTRY table - possibly because I have call caching turned on, I haven't checked whether this is the cause yet.; - The auto-incremented fields that are `Option[Long]` in the data model can't be handled the same way in Postgres; I haven't decided whether this is simply different database behavior or a bug somewhere. Anyway I found a workaround for that too.; - I may have messed up and branched from `master` in my fork by mistake, and in any case I'm definitely out of sync with your `develop`. Do you have a preferred workflow to bring my branch up to date, i.e. to minimize the mess in the Git history? (Despite using Git daily I'm still not totally sure what ""best practice"" is.). At this point I can at least run a workflow using Postgres, minus call caching. I'm going to be focusing on completing and testing this in the next couple of weeks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402
https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402:411,Performance,load,loading,411,"Since it's been a month I thought I'd post an update. Main items:; - In general I'm having to make a lot more changes to the Scala code than I expected due to queries being written in a way that Postgres doesn't like. (This isn't a criticism, more of a heads-up.) Nothing functional, just refactoring.; - The way `Blob` is handled in Slick+Postgres turns out to be a massive pain. I'm not sure if Slick is lazy-loading these fields or I just don't understand how it works under the hood, but the workaround is that the blobs need to be accessed as part of a transaction, which involved some refactoring of downstream processing.; - Semi-related question: is there a reason why the entire contents of the `importsZip` need to be stored in the database? This quickly leads to an enormous METADATA_ENTRY table - possibly because I have call caching turned on, I haven't checked whether this is the cause yet.; - The auto-incremented fields that are `Option[Long]` in the data model can't be handled the same way in Postgres; I haven't decided whether this is simply different database behavior or a bug somewhere. Anyway I found a workaround for that too.; - I may have messed up and branched from `master` in my fork by mistake, and in any case I'm definitely out of sync with your `develop`. Do you have a preferred workflow to bring my branch up to date, i.e. to minimize the mess in the Git history? (Despite using Git daily I'm still not totally sure what ""best practice"" is.). At this point I can at least run a workflow using Postgres, minus call caching. I'm going to be focusing on completing and testing this in the next couple of weeks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402
https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402:536,Security,access,accessed,536,"Since it's been a month I thought I'd post an update. Main items:; - In general I'm having to make a lot more changes to the Scala code than I expected due to queries being written in a way that Postgres doesn't like. (This isn't a criticism, more of a heads-up.) Nothing functional, just refactoring.; - The way `Blob` is handled in Slick+Postgres turns out to be a massive pain. I'm not sure if Slick is lazy-loading these fields or I just don't understand how it works under the hood, but the workaround is that the blobs need to be accessed as part of a transaction, which involved some refactoring of downstream processing.; - Semi-related question: is there a reason why the entire contents of the `importsZip` need to be stored in the database? This quickly leads to an enormous METADATA_ENTRY table - possibly because I have call caching turned on, I haven't checked whether this is the cause yet.; - The auto-incremented fields that are `Option[Long]` in the data model can't be handled the same way in Postgres; I haven't decided whether this is simply different database behavior or a bug somewhere. Anyway I found a workaround for that too.; - I may have messed up and branched from `master` in my fork by mistake, and in any case I'm definitely out of sync with your `develop`. Do you have a preferred workflow to bring my branch up to date, i.e. to minimize the mess in the Git history? (Despite using Git daily I'm still not totally sure what ""best practice"" is.). At this point I can at least run a workflow using Postgres, minus call caching. I'm going to be focusing on completing and testing this in the next couple of weeks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402
https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402:1603,Testability,test,testing,1603,"Since it's been a month I thought I'd post an update. Main items:; - In general I'm having to make a lot more changes to the Scala code than I expected due to queries being written in a way that Postgres doesn't like. (This isn't a criticism, more of a heads-up.) Nothing functional, just refactoring.; - The way `Blob` is handled in Slick+Postgres turns out to be a massive pain. I'm not sure if Slick is lazy-loading these fields or I just don't understand how it works under the hood, but the workaround is that the blobs need to be accessed as part of a transaction, which involved some refactoring of downstream processing.; - Semi-related question: is there a reason why the entire contents of the `importsZip` need to be stored in the database? This quickly leads to an enormous METADATA_ENTRY table - possibly because I have call caching turned on, I haven't checked whether this is the cause yet.; - The auto-incremented fields that are `Option[Long]` in the data model can't be handled the same way in Postgres; I haven't decided whether this is simply different database behavior or a bug somewhere. Anyway I found a workaround for that too.; - I may have messed up and branched from `master` in my fork by mistake, and in any case I'm definitely out of sync with your `develop`. Do you have a preferred workflow to bring my branch up to date, i.e. to minimize the mess in the Git history? (Despite using Git daily I'm still not totally sure what ""best practice"" is.). At this point I can at least run a workflow using Postgres, minus call caching. I'm going to be focusing on completing and testing this in the next couple of weeks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402
https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402:1056,Usability,simpl,simply,1056,"Since it's been a month I thought I'd post an update. Main items:; - In general I'm having to make a lot more changes to the Scala code than I expected due to queries being written in a way that Postgres doesn't like. (This isn't a criticism, more of a heads-up.) Nothing functional, just refactoring.; - The way `Blob` is handled in Slick+Postgres turns out to be a massive pain. I'm not sure if Slick is lazy-loading these fields or I just don't understand how it works under the hood, but the workaround is that the blobs need to be accessed as part of a transaction, which involved some refactoring of downstream processing.; - Semi-related question: is there a reason why the entire contents of the `importsZip` need to be stored in the database? This quickly leads to an enormous METADATA_ENTRY table - possibly because I have call caching turned on, I haven't checked whether this is the cause yet.; - The auto-incremented fields that are `Option[Long]` in the data model can't be handled the same way in Postgres; I haven't decided whether this is simply different database behavior or a bug somewhere. Anyway I found a workaround for that too.; - I may have messed up and branched from `master` in my fork by mistake, and in any case I'm definitely out of sync with your `develop`. Do you have a preferred workflow to bring my branch up to date, i.e. to minimize the mess in the Git history? (Despite using Git daily I'm still not totally sure what ""best practice"" is.). At this point I can at least run a workflow using Postgres, minus call caching. I'm going to be focusing on completing and testing this in the next couple of weeks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402
https://github.com/broadinstitute/cromwell/issues/4777#issuecomment-478074579:512,Modifiability,config,config,512,"Hi, look at the [`database.db` stanza](https://github.com/broadinstitute/cromwell/blob/40a0608a629976aca75ddf7f7adafc0fbe8bb399/cromwell.examples.conf#L800):. You want to set the following:; ```; database.db {; numThreads =20; minThreads = 20; maxThreads =20; minConnections = 20; maxConnections = 20; }; ```. For reference and guidance on these nubmers, [look at the `forConfig` Docs for Slick](http://slick.lightbend.com/doc/3.2.3/api/index.html#slick.jdbc.JdbcBackend$DatabaseFactoryDef@forConfig(path:String,config:com.typesafe.config.Config,driver:java.sql.Driver,classLoader:ClassLoader):JdbcBackend.this.Database).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4777#issuecomment-478074579
https://github.com/broadinstitute/cromwell/issues/4777#issuecomment-478074579:532,Modifiability,config,config,532,"Hi, look at the [`database.db` stanza](https://github.com/broadinstitute/cromwell/blob/40a0608a629976aca75ddf7f7adafc0fbe8bb399/cromwell.examples.conf#L800):. You want to set the following:; ```; database.db {; numThreads =20; minThreads = 20; maxThreads =20; minConnections = 20; maxConnections = 20; }; ```. For reference and guidance on these nubmers, [look at the `forConfig` Docs for Slick](http://slick.lightbend.com/doc/3.2.3/api/index.html#slick.jdbc.JdbcBackend$DatabaseFactoryDef@forConfig(path:String,config:com.typesafe.config.Config,driver:java.sql.Driver,classLoader:ClassLoader):JdbcBackend.this.Database).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4777#issuecomment-478074579
https://github.com/broadinstitute/cromwell/issues/4777#issuecomment-478074579:328,Usability,guid,guidance,328,"Hi, look at the [`database.db` stanza](https://github.com/broadinstitute/cromwell/blob/40a0608a629976aca75ddf7f7adafc0fbe8bb399/cromwell.examples.conf#L800):. You want to set the following:; ```; database.db {; numThreads =20; minThreads = 20; maxThreads =20; minConnections = 20; maxConnections = 20; }; ```. For reference and guidance on these nubmers, [look at the `forConfig` Docs for Slick](http://slick.lightbend.com/doc/3.2.3/api/index.html#slick.jdbc.JdbcBackend$DatabaseFactoryDef@forConfig(path:String,config:com.typesafe.config.Config,driver:java.sql.Driver,classLoader:ClassLoader):JdbcBackend.this.Database).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4777#issuecomment-478074579
https://github.com/broadinstitute/cromwell/issues/4803#issuecomment-480398885:57,Usability,guid,guide,57,"Perhaps then this would work better in a troubleshooting guide than the ""getting yourself a MySQL database section""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4803#issuecomment-480398885
https://github.com/broadinstitute/cromwell/pull/4811#issuecomment-481412176:12,Usability,clear,clearly,12,"@aednichols clearly, ; > it results from confusion between Latin cedere (“give up, yield”) and sedere (“to sit”). and not just ""oops, typo""... (cf: https://en.wiktionary.org/wiki/supercede#English)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4811#issuecomment-481412176
https://github.com/broadinstitute/cromwell/issues/4820#issuecomment-481837931:335,Usability,UX,UX,335,"The ""scatterAtxyz""s refer to implicit subworkflows that Cromwell creates to allow nested scatters (previously our only answer was ""write your own sub-workflow with the scatter in it). To find the call level data you'd go to the awkwardly-named subworkflow and expand it, just like any other subworkflow in a scatter. I agree there's a UX gap here - maybe our tooling can be smart and re-collapse the ""scatterAtxyz"" subworkflow into the same scope as the original workflow's calls?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4820#issuecomment-481837931
https://github.com/broadinstitute/cromwell/issues/4820#issuecomment-481838405:167,Usability,clear,clear,167,"I think it would be nice to be able to name a scatter. If I could `scatter (foo in bar) as baz { ... }` and have `baz` be what we see in metadata and timing, it would clear up a lot of confusion. Falling back to `line_char` would be ok if there's at least the option to specify a name.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4820#issuecomment-481838405
https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685:131,Availability,error,error,131,"Refinement update:. We are going to solve the first two points by adding a sort of `input_errors` map with input names as keys and error(s) as values. The absence of `errors` and presence of `input_errors` indicates the DA case where WDL is good and inputs bad. ~The last issue will split off into soliciting community feedback from non-workbench users and writing a nicer wrapper endpoint that's more compatible with curl (it is Adam's opinion, potentailly poorly supported, that this is hard right now)~ see https://github.com/broadinstitute/cromwell/issues/4892",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685
https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685:167,Availability,error,errors,167,"Refinement update:. We are going to solve the first two points by adding a sort of `input_errors` map with input names as keys and error(s) as values. The absence of `errors` and presence of `input_errors` indicates the DA case where WDL is good and inputs bad. ~The last issue will split off into soliciting community feedback from non-workbench users and writing a nicer wrapper endpoint that's more compatible with curl (it is Adam's opinion, potentailly poorly supported, that this is hard right now)~ see https://github.com/broadinstitute/cromwell/issues/4892",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685
https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685:11,Deployability,update,update,11,"Refinement update:. We are going to solve the first two points by adding a sort of `input_errors` map with input names as keys and error(s) as values. The absence of `errors` and presence of `input_errors` indicates the DA case where WDL is good and inputs bad. ~The last issue will split off into soliciting community feedback from non-workbench users and writing a nicer wrapper endpoint that's more compatible with curl (it is Adam's opinion, potentailly poorly supported, that this is hard right now)~ see https://github.com/broadinstitute/cromwell/issues/4892",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685
https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685:373,Integrability,wrap,wrapper,373,"Refinement update:. We are going to solve the first two points by adding a sort of `input_errors` map with input names as keys and error(s) as values. The absence of `errors` and presence of `input_errors` indicates the DA case where WDL is good and inputs bad. ~The last issue will split off into soliciting community feedback from non-workbench users and writing a nicer wrapper endpoint that's more compatible with curl (it is Adam's opinion, potentailly poorly supported, that this is hard right now)~ see https://github.com/broadinstitute/cromwell/issues/4892",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685
https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685:319,Usability,feedback,feedback,319,"Refinement update:. We are going to solve the first two points by adding a sort of `input_errors` map with input names as keys and error(s) as values. The absence of `errors` and presence of `input_errors` indicates the DA case where WDL is good and inputs bad. ~The last issue will split off into soliciting community feedback from non-workbench users and writing a nicer wrapper endpoint that's more compatible with curl (it is Adam's opinion, potentailly poorly supported, that this is hard right now)~ see https://github.com/broadinstitute/cromwell/issues/4892",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685
https://github.com/broadinstitute/cromwell/pull/4825#issuecomment-500586533:1195,Integrability,message,message,1195,"I need this ability to label Google VMs for resource tracking, but have been thus far unable to have a VM labelled correctly. The jobs submit and run, but the labels do not show up. . From the documentation here (https://cromwell.readthedocs.io/en/develop/wf_options/Google/), it's not clear where the google-specific options are added, so I tried the following: ; ```; {; ""default_runtime_attributes"":{; ""zones"":""us-east1-b"", ; ""google_labels"": {""custom-label"":""custom-value""}; }; }; ```; I submit (Cromwell v42) with:; ```; curl -X POST ""<CROMWELL URL>/api/workflows/v1"" \; -H ""accept: application/json"" \; -H ""Content-Type: multipart/form-data"" \; -F ""workflowSource=@main.wdl"" ; -F ""workflowInputs=@inputs.json"" \; -F ""workflowOptions=@options.json"" \; -F ""workflowType=WDL"" \; -F ""workflowTypeVersion=draft-2""; ```. That submits/runs fine, but when I check the VM that spins up, I only see the two labels of `cromwell-workflow-id` and `wdl-task-name`. If I change the options JSON to anything else, e.g.; ```; {; ""default_runtime_attributes"":{""zones"":""us-east1-b""},; ""google_labels"": {""custom-label"":""custom-value""}; }; ```; then it fails to submit, returning:; ```; {; ""status"": ""fail"",; ""message"": ""Error(s): Invalid workflow options provided: Unsupported key/value pair in WorkflowOptions: google_labels -> {\""custom-label\"":\""custom-value\""}""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4825#issuecomment-500586533
https://github.com/broadinstitute/cromwell/pull/4825#issuecomment-500586533:286,Usability,clear,clear,286,"I need this ability to label Google VMs for resource tracking, but have been thus far unable to have a VM labelled correctly. The jobs submit and run, but the labels do not show up. . From the documentation here (https://cromwell.readthedocs.io/en/develop/wf_options/Google/), it's not clear where the google-specific options are added, so I tried the following: ; ```; {; ""default_runtime_attributes"":{; ""zones"":""us-east1-b"", ; ""google_labels"": {""custom-label"":""custom-value""}; }; }; ```; I submit (Cromwell v42) with:; ```; curl -X POST ""<CROMWELL URL>/api/workflows/v1"" \; -H ""accept: application/json"" \; -H ""Content-Type: multipart/form-data"" \; -F ""workflowSource=@main.wdl"" ; -F ""workflowInputs=@inputs.json"" \; -F ""workflowOptions=@options.json"" \; -F ""workflowType=WDL"" \; -F ""workflowTypeVersion=draft-2""; ```. That submits/runs fine, but when I check the VM that spins up, I only see the two labels of `cromwell-workflow-id` and `wdl-task-name`. If I change the options JSON to anything else, e.g.; ```; {; ""default_runtime_attributes"":{""zones"":""us-east1-b""},; ""google_labels"": {""custom-label"":""custom-value""}; }; ```; then it fails to submit, returning:; ```; {; ""status"": ""fail"",; ""message"": ""Error(s): Invalid workflow options provided: Unsupported key/value pair in WorkflowOptions: google_labels -> {\""custom-label\"":\""custom-value\""}""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4825#issuecomment-500586533
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:126,Modifiability,config,configured,126,"I have validated that the log file for each submitted workflow does not get closed when Cromwell is run in server mode and is configured with `workflow-log-temporary: false` and the workflow does not specify the `final_workflow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:7,Security,validat,validated,7,"I have validated that the log file for each submitted workflow does not get closed when Cromwell is run in server mode and is configured with `workflow-log-temporary: false` and the workflow does not specify the `final_workflow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:26,Testability,log,log,26,"I have validated that the log file for each submitted workflow does not get closed when Cromwell is run in server mode and is configured with `workflow-log-temporary: false` and the workflow does not specify the `final_workflow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:152,Testability,log,log-temporary,152,"I have validated that the log file for each submitted workflow does not get closed when Cromwell is run in server mode and is configured with `workflow-log-temporary: false` and the workflow does not specify the `final_workflow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:257,Testability,test,tested,257,"I have validated that the log file for each submitted workflow does not get closed when Cromwell is run in server mode and is configured with `workflow-log-temporary: false` and the workflow does not specify the `final_workflow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:689,Testability,log,logs,689,"I have validated that the log file for each submitted workflow does not get closed when Cromwell is run in server mode and is configured with `workflow-log-temporary: false` and the workflow does not specify the `final_workflow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:734,Testability,log,log-temporary,734,"I have validated that the log file for each submitted workflow does not get closed when Cromwell is run in server mode and is configured with `workflow-log-temporary: false` and the workflow does not specify the `final_workflow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:942,Testability,log,log,942,"I have validated that the log file for each submitted workflow does not get closed when Cromwell is run in server mode and is configured with `workflow-log-temporary: false` and the workflow does not specify the `final_workflow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:1107,Testability,log,logs,1107,"does not get closed when Cromwell is run in server mode and is configured with `workflow-log-temporary: false` and the workflow does not specify the `final_workflow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.43bcbc56-3596-45b7-9f1d-c00dc36defd4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:1158,Testability,log,log,1158,"nal_workflow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.43bcbc56-3596-45b7-9f1d-c00dc36defd4.log; java 33951 cromwellbuild 40w REG 8,1 1340 533089 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.5b4c1033-3dfe-470c-bba4-5d31f2120948",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:1259,Testability,log,logs,1259,"nal_workflow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.43bcbc56-3596-45b7-9f1d-c00dc36defd4.log; java 33951 cromwellbuild 40w REG 8,1 1340 533089 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.5b4c1033-3dfe-470c-bba4-5d31f2120948",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:1310,Testability,log,log,1310,"ow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.43bcbc56-3596-45b7-9f1d-c00dc36defd4.log; java 33951 cromwellbuild 40w REG 8,1 1340 533089 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.5b4c1033-3dfe-470c-bba4-5d31f2120948.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:1410,Testability,log,logs,1410,"ow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.43bcbc56-3596-45b7-9f1d-c00dc36defd4.log; java 33951 cromwellbuild 40w REG 8,1 1340 533089 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.5b4c1033-3dfe-470c-bba4-5d31f2120948.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:1461,Testability,log,log,1461,"ow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.43bcbc56-3596-45b7-9f1d-c00dc36defd4.log; java 33951 cromwellbuild 40w REG 8,1 1340 533089 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.5b4c1033-3dfe-470c-bba4-5d31f2120948.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:1561,Testability,log,logs,1561,"ow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.43bcbc56-3596-45b7-9f1d-c00dc36defd4.log; java 33951 cromwellbuild 40w REG 8,1 1340 533089 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.5b4c1033-3dfe-470c-bba4-5d31f2120948.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:1612,Testability,log,log,1612,"ow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.43bcbc56-3596-45b7-9f1d-c00dc36defd4.log; java 33951 cromwellbuild 40w REG 8,1 1340 533089 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.5b4c1033-3dfe-470c-bba4-5d31f2120948.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:1712,Testability,log,logs,1712,"ow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.43bcbc56-3596-45b7-9f1d-c00dc36defd4.log; java 33951 cromwellbuild 40w REG 8,1 1340 533089 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.5b4c1033-3dfe-470c-bba4-5d31f2120948.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:1763,Testability,log,log,1763,"ow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.43bcbc56-3596-45b7-9f1d-c00dc36defd4.log; java 33951 cromwellbuild 40w REG 8,1 1340 533089 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.5b4c1033-3dfe-470c-bba4-5d31f2120948.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:1863,Testability,log,logs,1863,"ow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.43bcbc56-3596-45b7-9f1d-c00dc36defd4.log; java 33951 cromwellbuild 40w REG 8,1 1340 533089 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.5b4c1033-3dfe-470c-bba4-5d31f2120948.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:1914,Testability,log,log,1914,"ow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.43bcbc56-3596-45b7-9f1d-c00dc36defd4.log; java 33951 cromwellbuild 40w REG 8,1 1340 533089 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.5b4c1033-3dfe-470c-bba4-5d31f2120948.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:2014,Testability,log,logs,2014,"ow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.43bcbc56-3596-45b7-9f1d-c00dc36defd4.log; java 33951 cromwellbuild 40w REG 8,1 1340 533089 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.5b4c1033-3dfe-470c-bba4-5d31f2120948.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:2065,Testability,log,log,2065,"ow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.43bcbc56-3596-45b7-9f1d-c00dc36defd4.log; java 33951 cromwellbuild 40w REG 8,1 1340 533089 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.5b4c1033-3dfe-470c-bba4-5d31f2120948.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:2166,Testability,log,logs,2166,"ow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.43bcbc56-3596-45b7-9f1d-c00dc36defd4.log; java 33951 cromwellbuild 40w REG 8,1 1340 533089 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.5b4c1033-3dfe-470c-bba4-5d31f2120948.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:2217,Testability,log,log,2217,"ow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.43bcbc56-3596-45b7-9f1d-c00dc36defd4.log; java 33951 cromwellbuild 40w REG 8,1 1340 533089 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.5b4c1033-3dfe-470c-bba4-5d31f2120948.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273:840,Usability,simpl,simple,840,"I have validated that the log file for each submitted workflow does not get closed when Cromwell is run in server mode and is configured with `workflow-log-temporary: false` and the workflow does not specify the `final_workflow_log_dir` option. I also have tested that Tony's fix above resolves the problem. While using a workflow options file is a workaround, it's not a general solution for debugging failed workflows submitted by users who did not include the options file to begin with. Even worse, the bug results in a file handle leak in Cromwell server. . Repro steps:. ```; # cromwell.conf; include required(classpath(""application"")); workflow-options {; # When true, per workflow logs will be deleted after copying; workflow-log-temporary: false; }; ```. Run: `java -Dconfig.file=cromwell.conf -jar cromwell.jar server` . Submit a simple Hello World .wdl file to /api/workflows (submit a couple times see the leak). We see that many log file handles remain unclosed:; `sudo lsof -p $(pidof java)`. ```; java 33951 cromwellbuild 33w REG 8,1 117 533080 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.11dbb9e7-10e3-48dc-b36f-0b7e35941ce3.log; java 33951 cromwellbuild 34w REG 8,1 1301 525688 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.0ed80fab-afd1-4783-9b27-409b75f0f2f7.log; java 33951 cromwellbuild 35w REG 8,1 117 533081 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.ef428f60-f8c3-4456-a3ea-bea63486881a.log; java 33951 cromwellbuild 36w REG 8,1 117 533085 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.4df4069e-0ef0-43ba-8bfb-37413d7ed229.log; java 33951 cromwellbuild 37w REG 8,1 117 533086 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.48e5c4ff-c067-465d-820b-2b41cb74ef31.log; java 33951 cromwellbuild 38w REG 8,1 117 533087 /home/cromwellbuild/cromwell/cromwell-workflow-logs/workflow.05bcd5a7-3924-4ba0-ab39-57bc09716abb.log; java 33951 cromwellbuild 39w REG 8,1 117 533088 /home/cromwellbuild/cromwell/cromw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4826#issuecomment-499023273
https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-483794865:355,Security,hash,hash,355,"From the docs, emphasis mine:. >After a successful complete request, the parts no longer exist. Your complete multipart upload request must include the upload ID and a list of both part numbers and corresponding ETag values. Amazon S3 response includes an ETag that uniquely identifies the combined object data. **This ETag will not necessarily be an MD5 hash of the object data**. [Source](https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html). Thinking out loud: Do we have to recreate the original multipart upload to get the same ETag deterministically? Or otherwise how can we get the combined ETag deterministically?. If not, we can use [UploadPartCopyRequest](http://aws-java-sdk-javadoc.s3-website-us-west-2.amazonaws.com/latest/software/amazon/awssdk/services/s3/model/UploadPartCopyRequest.html) to accomplish this [here](https://github.com/broadinstitute/cromwell/blob/b8aa5e1eee730dcd3edc2c8ff0cf0144127a3208/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L433). [Example multipart copy using old SDK](https://docs.aws.amazon.com/AmazonS3/latest/dev/CopyingObjctsUsingLLJavaMPUapi.html); [Migration guide from 1.1 to 2.0 (to interpret above in 2.0)](https://github.com/aws/aws-sdk-java-v2/blob/master/docs/LaunchChangelog.md#41-s3-changes)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-483794865
https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-483794865:1146,Usability,guid,guide,1146,"From the docs, emphasis mine:. >After a successful complete request, the parts no longer exist. Your complete multipart upload request must include the upload ID and a list of both part numbers and corresponding ETag values. Amazon S3 response includes an ETag that uniquely identifies the combined object data. **This ETag will not necessarily be an MD5 hash of the object data**. [Source](https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html). Thinking out loud: Do we have to recreate the original multipart upload to get the same ETag deterministically? Or otherwise how can we get the combined ETag deterministically?. If not, we can use [UploadPartCopyRequest](http://aws-java-sdk-javadoc.s3-website-us-west-2.amazonaws.com/latest/software/amazon/awssdk/services/s3/model/UploadPartCopyRequest.html) to accomplish this [here](https://github.com/broadinstitute/cromwell/blob/b8aa5e1eee730dcd3edc2c8ff0cf0144127a3208/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L433). [Example multipart copy using old SDK](https://docs.aws.amazon.com/AmazonS3/latest/dev/CopyingObjctsUsingLLJavaMPUapi.html); [Migration guide from 1.1 to 2.0 (to interpret above in 2.0)](https://github.com/aws/aws-sdk-java-v2/blob/master/docs/LaunchChangelog.md#41-s3-changes)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-483794865
https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-484685791:294,Testability,log,logic,294,"We've implemented a fix for the Cromwell-side of things to work w/ AWS CLI default settings for S3. These default settings are good for up to 83 GiB. The old limit was 5 GiB. . At 83 GiB the AWS CLI has to alter the default part size from 8 MiB to something larger and it's not clear what that logic is. ## What we want to do from here forward. The proxy should use larger part sizes all the time, ideally the largest part size of 5 GiB. Then w/ AWS limit of 10K parts we have a new limit of ~ 53 TiB. We attempted to alter the proxy to use this part size and set the threshold to 5 GB before using multipart and it broke the proxy. In order to find out what's happening we need to investigate proxy logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-484685791
https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-484685791:700,Testability,log,logs,700,"We've implemented a fix for the Cromwell-side of things to work w/ AWS CLI default settings for S3. These default settings are good for up to 83 GiB. The old limit was 5 GiB. . At 83 GiB the AWS CLI has to alter the default part size from 8 MiB to something larger and it's not clear what that logic is. ## What we want to do from here forward. The proxy should use larger part sizes all the time, ideally the largest part size of 5 GiB. Then w/ AWS limit of 10K parts we have a new limit of ~ 53 TiB. We attempted to alter the proxy to use this part size and set the threshold to 5 GB before using multipart and it broke the proxy. In order to find out what's happening we need to investigate proxy logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-484685791
https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-484685791:278,Usability,clear,clear,278,"We've implemented a fix for the Cromwell-side of things to work w/ AWS CLI default settings for S3. These default settings are good for up to 83 GiB. The old limit was 5 GiB. . At 83 GiB the AWS CLI has to alter the default part size from 8 MiB to something larger and it's not clear what that logic is. ## What we want to do from here forward. The proxy should use larger part sizes all the time, ideally the largest part size of 5 GiB. Then w/ AWS limit of 10K parts we have a new limit of ~ 53 TiB. We attempted to alter the proxy to use this part size and set the threshold to 5 GB before using multipart and it broke the proxy. In order to find out what's happening we need to investigate proxy logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-484685791
https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-530844950:284,Usability,clear,clear,284,"I think two things here were premature: ; a) Me saying ""we've implemented"" as the branch was not merged.; b) Marking this ticket as ""Done"" . As this happened 6 months ago, I'm a little hazy on details but as I noted above the code we wrote ""broke the proxy,"" and apparently it wasn't clear why as I noted an investigation is in order. I will do my best to find that branch and surface it here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-530844950
https://github.com/broadinstitute/cromwell/issues/4842#issuecomment-487684113:410,Usability,clear,clear,410,"@gemmalam @danbills @mcovarr Sorry to bother you here, but I couldn't get a definite answer on the support forum ( https://gatkforums.broadinstitute.org/wdl/discussion/23421/multiple-cromwell-instances-sharing-the-same-database ). What is Cromwell's current level of support for multiple cromwell instances sharing the same MySQL database? From the forum, it seems that it works; but readthedocs don't make it clear, and the existence of the horicromtal label suggests that support for this is still being developed? Thanks for any info!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4842#issuecomment-487684113
https://github.com/broadinstitute/cromwell/pull/4878#issuecomment-486661322:28,Usability,feedback,feedback,28,Thanks @cjllanwarne for the feedback. I have moved the examples to the docs and referenced the docs in the changelog.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4878#issuecomment-486661322
https://github.com/broadinstitute/cromwell/pull/4893#issuecomment-487052380:18,Usability,simpl,simpler,18,@mcovarr now with simpler fix,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4893#issuecomment-487052380
https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-487106360:134,Usability,feedback,feedback,134,Thank you for the PR! The Cromwell team is unfortunately a bit slammed this week but we hope to get back to you with more substantial feedback next week. 🙂,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-487106360
https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703:708,Integrability,synchroniz,synchronized,708,"@aednichols . I implemented the changes. The lock should have a negligible performance impact. It only locks when cached-copy strategy is used, threads which use other strategies are not blocked by this. (This was already true before the changes). A dictionary is used to keep track which files are being copied within the process. Filesystem lock files are too slow for this because during scatters cromwell creates a lot of threads that need exactly the same file at the same time. . A lock file is now used so other cromwell processes know the file is being copied. The chances of two processes needing the same file at exactly the same time is negligible, so lock files are fast enough here. Due to the `synchronized` lock, there can not be race conditions where more than one thread modifies the dictionary and/or creates the lock file at the same time. (The lock is absolutely necessary for cached-copy to work). The amount of time spent in the lock per thread is negligible. . I implemented a waitOnCopy function which lets a thread wait for the locks in the dictionary on the filesystem to clear. This structure allows a lot of paths to be copied to the cache at the same time. So the copying is still multithreaded, maintaining high performance. Are these changes satisfactory?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703
https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703:75,Performance,perform,performance,75,"@aednichols . I implemented the changes. The lock should have a negligible performance impact. It only locks when cached-copy strategy is used, threads which use other strategies are not blocked by this. (This was already true before the changes). A dictionary is used to keep track which files are being copied within the process. Filesystem lock files are too slow for this because during scatters cromwell creates a lot of threads that need exactly the same file at the same time. . A lock file is now used so other cromwell processes know the file is being copied. The chances of two processes needing the same file at exactly the same time is negligible, so lock files are fast enough here. Due to the `synchronized` lock, there can not be race conditions where more than one thread modifies the dictionary and/or creates the lock file at the same time. (The lock is absolutely necessary for cached-copy to work). The amount of time spent in the lock per thread is negligible. . I implemented a waitOnCopy function which lets a thread wait for the locks in the dictionary on the filesystem to clear. This structure allows a lot of paths to be copied to the cache at the same time. So the copying is still multithreaded, maintaining high performance. Are these changes satisfactory?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703
https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703:114,Performance,cache,cached-copy,114,"@aednichols . I implemented the changes. The lock should have a negligible performance impact. It only locks when cached-copy strategy is used, threads which use other strategies are not blocked by this. (This was already true before the changes). A dictionary is used to keep track which files are being copied within the process. Filesystem lock files are too slow for this because during scatters cromwell creates a lot of threads that need exactly the same file at the same time. . A lock file is now used so other cromwell processes know the file is being copied. The chances of two processes needing the same file at exactly the same time is negligible, so lock files are fast enough here. Due to the `synchronized` lock, there can not be race conditions where more than one thread modifies the dictionary and/or creates the lock file at the same time. (The lock is absolutely necessary for cached-copy to work). The amount of time spent in the lock per thread is negligible. . I implemented a waitOnCopy function which lets a thread wait for the locks in the dictionary on the filesystem to clear. This structure allows a lot of paths to be copied to the cache at the same time. So the copying is still multithreaded, maintaining high performance. Are these changes satisfactory?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703
https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703:745,Performance,race condition,race conditions,745,"@aednichols . I implemented the changes. The lock should have a negligible performance impact. It only locks when cached-copy strategy is used, threads which use other strategies are not blocked by this. (This was already true before the changes). A dictionary is used to keep track which files are being copied within the process. Filesystem lock files are too slow for this because during scatters cromwell creates a lot of threads that need exactly the same file at the same time. . A lock file is now used so other cromwell processes know the file is being copied. The chances of two processes needing the same file at exactly the same time is negligible, so lock files are fast enough here. Due to the `synchronized` lock, there can not be race conditions where more than one thread modifies the dictionary and/or creates the lock file at the same time. (The lock is absolutely necessary for cached-copy to work). The amount of time spent in the lock per thread is negligible. . I implemented a waitOnCopy function which lets a thread wait for the locks in the dictionary on the filesystem to clear. This structure allows a lot of paths to be copied to the cache at the same time. So the copying is still multithreaded, maintaining high performance. Are these changes satisfactory?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703
https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703:897,Performance,cache,cached-copy,897,"@aednichols . I implemented the changes. The lock should have a negligible performance impact. It only locks when cached-copy strategy is used, threads which use other strategies are not blocked by this. (This was already true before the changes). A dictionary is used to keep track which files are being copied within the process. Filesystem lock files are too slow for this because during scatters cromwell creates a lot of threads that need exactly the same file at the same time. . A lock file is now used so other cromwell processes know the file is being copied. The chances of two processes needing the same file at exactly the same time is negligible, so lock files are fast enough here. Due to the `synchronized` lock, there can not be race conditions where more than one thread modifies the dictionary and/or creates the lock file at the same time. (The lock is absolutely necessary for cached-copy to work). The amount of time spent in the lock per thread is negligible. . I implemented a waitOnCopy function which lets a thread wait for the locks in the dictionary on the filesystem to clear. This structure allows a lot of paths to be copied to the cache at the same time. So the copying is still multithreaded, maintaining high performance. Are these changes satisfactory?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703
https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703:1162,Performance,cache,cache,1162,"@aednichols . I implemented the changes. The lock should have a negligible performance impact. It only locks when cached-copy strategy is used, threads which use other strategies are not blocked by this. (This was already true before the changes). A dictionary is used to keep track which files are being copied within the process. Filesystem lock files are too slow for this because during scatters cromwell creates a lot of threads that need exactly the same file at the same time. . A lock file is now used so other cromwell processes know the file is being copied. The chances of two processes needing the same file at exactly the same time is negligible, so lock files are fast enough here. Due to the `synchronized` lock, there can not be race conditions where more than one thread modifies the dictionary and/or creates the lock file at the same time. (The lock is absolutely necessary for cached-copy to work). The amount of time spent in the lock per thread is negligible. . I implemented a waitOnCopy function which lets a thread wait for the locks in the dictionary on the filesystem to clear. This structure allows a lot of paths to be copied to the cache at the same time. So the copying is still multithreaded, maintaining high performance. Are these changes satisfactory?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703
https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703:1242,Performance,perform,performance,1242,"@aednichols . I implemented the changes. The lock should have a negligible performance impact. It only locks when cached-copy strategy is used, threads which use other strategies are not blocked by this. (This was already true before the changes). A dictionary is used to keep track which files are being copied within the process. Filesystem lock files are too slow for this because during scatters cromwell creates a lot of threads that need exactly the same file at the same time. . A lock file is now used so other cromwell processes know the file is being copied. The chances of two processes needing the same file at exactly the same time is negligible, so lock files are fast enough here. Due to the `synchronized` lock, there can not be race conditions where more than one thread modifies the dictionary and/or creates the lock file at the same time. (The lock is absolutely necessary for cached-copy to work). The amount of time spent in the lock per thread is negligible. . I implemented a waitOnCopy function which lets a thread wait for the locks in the dictionary on the filesystem to clear. This structure allows a lot of paths to be copied to the cache at the same time. So the copying is still multithreaded, maintaining high performance. Are these changes satisfactory?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703
https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703:1098,Usability,clear,clear,1098,"@aednichols . I implemented the changes. The lock should have a negligible performance impact. It only locks when cached-copy strategy is used, threads which use other strategies are not blocked by this. (This was already true before the changes). A dictionary is used to keep track which files are being copied within the process. Filesystem lock files are too slow for this because during scatters cromwell creates a lot of threads that need exactly the same file at the same time. . A lock file is now used so other cromwell processes know the file is being copied. The chances of two processes needing the same file at exactly the same time is negligible, so lock files are fast enough here. Due to the `synchronized` lock, there can not be race conditions where more than one thread modifies the dictionary and/or creates the lock file at the same time. (The lock is absolutely necessary for cached-copy to work). The amount of time spent in the lock per thread is negligible. . I implemented a waitOnCopy function which lets a thread wait for the locks in the dictionary on the filesystem to clear. This structure allows a lot of paths to be copied to the cache at the same time. So the copying is still multithreaded, maintaining high performance. Are these changes satisfactory?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703
https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488296313:95,Availability,avail,available,95,"Thanks for the update! We will assign reviewers to give proper feedback, as soon as someone is available.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488296313
https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488296313:15,Deployability,update,update,15,"Thanks for the update! We will assign reviewers to give proper feedback, as soon as someone is available.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488296313
https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488296313:63,Usability,feedback,feedback,63,"Thanks for the update! We will assign reviewers to give proper feedback, as soon as someone is available.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488296313
https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488310510:128,Usability,feedback,feedback,128,"@gemmalam Could you remove the ""back with originator"" label? This PR has already returned from the originator. I am waiting for feedback. So if this stays with ""back with originator"" we will be in waiting on eachother deadlock :slightly_smiling_face: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488310510
https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-495676140:97,Availability,avail,available,97,"> Thanks for the update! We will assign reviewers to give proper feedback, as soon as someone is available. @aednichols could you give an indication when the feedback will be coming?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-495676140
https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-495676140:17,Deployability,update,update,17,"> Thanks for the update! We will assign reviewers to give proper feedback, as soon as someone is available. @aednichols could you give an indication when the feedback will be coming?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-495676140
https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-495676140:65,Usability,feedback,feedback,65,"> Thanks for the update! We will assign reviewers to give proper feedback, as soon as someone is available. @aednichols could you give an indication when the feedback will be coming?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-495676140
https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-495676140:158,Usability,feedback,feedback,158,"> Thanks for the update! We will assign reviewers to give proper feedback, as soon as someone is available. @aednichols could you give an indication when the feedback will be coming?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-495676140
https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488569744:127,Usability,clear,clearer,127,One of my former colleagues (ffinfo) added this feature because we needed it on our HPC. With your documentation it looks much clearer. Thanks a lot!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488569744
https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505586193:29,Availability,error,error,29,"It's a dependency resolution error, which seems surprising. I cleared Travis cache and restarted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505586193
https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505586193:7,Integrability,depend,dependency,7,"It's a dependency resolution error, which seems surprising. I cleared Travis cache and restarted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505586193
https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505586193:77,Performance,cache,cache,77,"It's a dependency resolution error, which seems surprising. I cleared Travis cache and restarted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505586193
https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505586193:62,Usability,clear,cleared,62,"It's a dependency resolution error, which seems surprising. I cleared Travis cache and restarted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505586193
https://github.com/broadinstitute/cromwell/pull/4922#issuecomment-488566852:227,Performance,cache,cached-copy,227,"Speaking as an external contributor: I like this contributing guide. It is short and to the point. Great work!. One question: what if I as an external contributor am interested in maintaining some of the code? For instance the cached-copy localization (#4900 ) is a small self-contained piece of code. Our institute will use that all the time. So I don't mind to fix the bugs in that part of the code. Since WDL is a bigger community than broad, it would be nice if some (self-contained) parts of the code can be maintained by the community as well. A good example would be code for backends that broad doesn't use (as much). EDIT: #4919 would be another great example of code contributed and possibly maintained by the community.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4922#issuecomment-488566852
https://github.com/broadinstitute/cromwell/pull/4922#issuecomment-488566852:62,Usability,guid,guide,62,"Speaking as an external contributor: I like this contributing guide. It is short and to the point. Great work!. One question: what if I as an external contributor am interested in maintaining some of the code? For instance the cached-copy localization (#4900 ) is a small self-contained piece of code. Our institute will use that all the time. So I don't mind to fix the bugs in that part of the code. Since WDL is a bigger community than broad, it would be nice if some (self-contained) parts of the code can be maintained by the community as well. A good example would be code for backends that broad doesn't use (as much). EDIT: #4919 would be another great example of code contributed and possibly maintained by the community.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4922#issuecomment-488566852
https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620:1082,Deployability,pipeline,pipelines,1082,ing [#4947](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=desc) into [develop](https://codecov.io/gh/broadinstitute/cromwell/commit/26085f5833cee3c9091d391102d5d0885a2b7d71?src=pr&el=desc) will **increase** coverage by `2.22%`.; > The diff coverage is `84.61%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #4947 +/- ##; ===========================================; + Coverage 62.61% 64.84% +2.22% ; ===========================================; Files 1015 1015 ; Lines 25904 25914 +10 ; Branches 811 829 +18 ; ===========================================; + Hits 16221 16804 +583 ; + Misses 9683 9110 -573; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [.../google/pipelines/v2alpha1/api/ActionBuilder.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvYXBpL0FjdGlvbkJ1aWxkZXIuc2NhbGE=) | `82.4% <84.61%> (+20.5%)` | :arrow_up: |; | [.../scala/cromiam/webservice/EngineRouteSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-Q3JvbUlBTS9zcmMvbWFpbi9zY2FsYS9jcm9taWFtL3dlYnNlcnZpY2UvRW5naW5lUm91dGVTdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...expression/renaming/BinaryOperatorEvaluators.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvbmV3LWJhc2Uvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmFzZS93ZGxvbTJ3b20vZXhwcmVzc2lvbi9yZW5hbWluZy9CaW5hcnlPcGVyYXRvckV2YWx1YXRvcnMuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...l/services/womtool/models/WomTypeJsonSupport.scala](https://code,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620
https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620:4534,Deployability,update,update,4534,"titute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL3BhY2thZ2Uuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...ool/src/main/scala/womtool/validate/Validate.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d29tdG9vbC9zcmMvbWFpbi9zY2FsYS93b210b29sL3ZhbGlkYXRlL1ZhbGlkYXRlLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...king/expression/files/BiscayneFileEvaluators.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvbGlua2luZy9leHByZXNzaW9uL2ZpbGVzL0Jpc2NheW5lRmlsZUV2YWx1YXRvcnMuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...in/scala/cromwell/backend/impl/bcs/BcsDocker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvYmNzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2JhY2tlbmQvaW1wbC9iY3MvQmNzRG9ja2VyLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...in/scala/cromwell/services/metadata/metadata.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c2VydmljZXMvc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvc2VydmljZXMvbWV0YWRhdGEvbWV0YWRhdGEuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | ... and [645 more](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=footer). Last update [26085f5...01c37f1](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620
https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620:2333,Modifiability,config,config,2333,vYXBpL0FjdGlvbkJ1aWxkZXIuc2NhbGE=) | `82.4% <84.61%> (+20.5%)` | :arrow_up: |; | [.../scala/cromiam/webservice/EngineRouteSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-Q3JvbUlBTS9zcmMvbWFpbi9zY2FsYS9jcm9taWFtL3dlYnNlcnZpY2UvRW5naW5lUm91dGVTdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...expression/renaming/BinaryOperatorEvaluators.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvbmV3LWJhc2Uvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmFzZS93ZGxvbTJ3b20vZXhwcmVzc2lvbi9yZW5hbWluZy9CaW5hcnlPcGVyYXRvckV2YWx1YXRvcnMuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...l/services/womtool/models/WomTypeJsonSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c2VydmljZXMvc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvc2VydmljZXMvd29tdG9vbC9tb2RlbHMvV29tVHlwZUpzb25TdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...end/impl/sfs/config/CpuDeclarationValidation.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvc2ZzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2JhY2tlbmQvaW1wbC9zZnMvY29uZmlnL0NwdURlY2xhcmF0aW9uVmFsaWRhdGlvbi5zY2FsYQ==) | `0% <0%> (-100%)` | :arrow_down: |; | [...aft2/src/main/scala/wdl/draft2/model/package.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL3BhY2thZ2Uuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...ool/src/main/scala/womtool/validate/Validate.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d29tdG9vbC9zcmMvbWFpbi9zY2FsYS93b210b29sL3ZhbGlkYXRlL1ZhbGlkYXRlLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...king/expression/files/BiscayneFileEvaluators.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvc3JjL21haW4vc2NhbGEvd2RsL3RyY,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620
https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620:2925,Security,validat,validate,2925,XhwcmVzc2lvbi9yZW5hbWluZy9CaW5hcnlPcGVyYXRvckV2YWx1YXRvcnMuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...l/services/womtool/models/WomTypeJsonSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c2VydmljZXMvc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvc2VydmljZXMvd29tdG9vbC9tb2RlbHMvV29tVHlwZUpzb25TdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...end/impl/sfs/config/CpuDeclarationValidation.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvc2ZzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2JhY2tlbmQvaW1wbC9zZnMvY29uZmlnL0NwdURlY2xhcmF0aW9uVmFsaWRhdGlvbi5zY2FsYQ==) | `0% <0%> (-100%)` | :arrow_down: |; | [...aft2/src/main/scala/wdl/draft2/model/package.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL3BhY2thZ2Uuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...ool/src/main/scala/womtool/validate/Validate.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d29tdG9vbC9zcmMvbWFpbi9zY2FsYS93b210b29sL3ZhbGlkYXRlL1ZhbGlkYXRlLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...king/expression/files/BiscayneFileEvaluators.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvbGlua2luZy9leHByZXNzaW9uL2ZpbGVzL0Jpc2NheW5lRmlsZUV2YWx1YXRvcnMuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...in/scala/cromwell/backend/impl/bcs/BcsDocker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvYmNzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2JhY2tlbmQvaW1wbC9iY3MvQmNzRG9ja2VyLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...in/scala/cromwell/services/metadata/metadata.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c2VydmljZXMvc3JjL21ha,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620
https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620:4296,Usability,learn,learn,4296,"titute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL3BhY2thZ2Uuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...ool/src/main/scala/womtool/validate/Validate.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d29tdG9vbC9zcmMvbWFpbi9zY2FsYS93b210b29sL3ZhbGlkYXRlL1ZhbGlkYXRlLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...king/expression/files/BiscayneFileEvaluators.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvbGlua2luZy9leHByZXNzaW9uL2ZpbGVzL0Jpc2NheW5lRmlsZUV2YWx1YXRvcnMuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...in/scala/cromwell/backend/impl/bcs/BcsDocker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvYmNzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2JhY2tlbmQvaW1wbC9iY3MvQmNzRG9ja2VyLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...in/scala/cromwell/services/metadata/metadata.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c2VydmljZXMvc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvc2VydmljZXMvbWV0YWRhdGEvbWV0YWRhdGEuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | ... and [645 more](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=footer). Last update [26085f5...01c37f1](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620
https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493606960:210,Performance,cache,cache,210,"Are you positive? I'm seeing a `Compilation failed` near the end. If `compile` alone doesn't surface the issue, try `test:compile`. There is a very small chance this is caused by caching - or rather inadequate cache invalidation - so I cleared Travis's cache on this build and restarted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493606960
https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493606960:253,Performance,cache,cache,253,"Are you positive? I'm seeing a `Compilation failed` near the end. If `compile` alone doesn't surface the issue, try `test:compile`. There is a very small chance this is caused by caching - or rather inadequate cache invalidation - so I cleared Travis's cache on this build and restarted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493606960
https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493606960:117,Testability,test,test,117,"Are you positive? I'm seeing a `Compilation failed` near the end. If `compile` alone doesn't surface the issue, try `test:compile`. There is a very small chance this is caused by caching - or rather inadequate cache invalidation - so I cleared Travis's cache on this build and restarted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493606960
https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493606960:236,Usability,clear,cleared,236,"Are you positive? I'm seeing a `Compilation failed` near the end. If `compile` alone doesn't surface the issue, try `test:compile`. There is a very small chance this is caused by caching - or rather inadequate cache invalidation - so I cleared Travis's cache on this build and restarted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493606960
https://github.com/broadinstitute/cromwell/issues/4979#issuecomment-494152187:56,Security,secur,security,56,"""This is not deemed to be a critical issue (yet) from a security perspective"". however. ""we should make sure to clear this up when we get a chance""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979#issuecomment-494152187
https://github.com/broadinstitute/cromwell/issues/4979#issuecomment-494152187:112,Usability,clear,clear,112,"""This is not deemed to be a critical issue (yet) from a security perspective"". however. ""we should make sure to clear this up when we get a chance""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979#issuecomment-494152187
https://github.com/broadinstitute/cromwell/pull/5003#issuecomment-496824583:206,Usability,clear,clear,206,@cjllanwarne Thanks for your great suggestions.; The change has been brought back to one line of code. And 7 lines of comments to explain why that line is the way it is... Hopefully the last line will make clear why `79` is an appropriate exit code for this use case.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5003#issuecomment-496824583
https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499248766:211,Usability,clear,clear,211,"I guess what we call these attributes doesn't really matter so much as, we have to accept one of these worlds:; * The spec defines a maximum memory value, above which Cromwell will never go; * The spec makes it clear that memory increasing will sometimes be required to complete tasks when they specify ""average"" `memory` attributes; * Cromwell will be able to run WDLs which will not run anywhere else... and thus we would have to be very strict in policing our ""best practices"" WDL to makes sure it can be run on other engines",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499248766
https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499374511:143,Energy Efficiency,schedul,scheduler,143,"> May I get a review on the design, but not the scala?. Wow thanks for adding this. This is exactly what we need on our cluster. Sometimes the scheduler aggressively kills jobs based on VMEM (instead of actual mem). So retrying with upping the memory requirements is a nice way to circumvent this annoying issue. (Instead of using insane memory requirements to make sure it passes in 99% of the cases). As for the design. I would add; 1. A number of attempts configurable parameter in the config. A sane default would be 1. Meaning that this feature will not be used by default, for reasons elaborated on later.; 2. A factor with which the memory is increased on each attempt. So if the factor is 1.5 > Attempt 1 will be 1.5^0 = 1 times the memory, Attempt 2 1.5^1 = 1.5, Attempt 3 = 1.5^2 = 2.25. A sane default here would be 2 I guess. As for @cjllanwarne's concerns:. > The spec defines a maximum memory value, above which Cromwell will never go; ; I think the spec just states the value that should be given to whatever backend. But semantics aside, I guess that means the same as saying it is the maximum. > Cromwell will be able to run WDLs which will not run anywhere else... and thus we would have to be very strict in policing our ""best practices"" WDL to makes sure it can be run on other engines. This is a very good reason to not enable this feature by default. But since there are very good reasons for having this feature, having it as a configurable option will be very very nice. Let the user decide how they want to treat their memory requirements. That is the most user-friendly way to do. This is why I think a sane default for the number of attempts should be 1 (i.e. no retries).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499374511
https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499374511:459,Modifiability,config,configurable,459,"> May I get a review on the design, but not the scala?. Wow thanks for adding this. This is exactly what we need on our cluster. Sometimes the scheduler aggressively kills jobs based on VMEM (instead of actual mem). So retrying with upping the memory requirements is a nice way to circumvent this annoying issue. (Instead of using insane memory requirements to make sure it passes in 99% of the cases). As for the design. I would add; 1. A number of attempts configurable parameter in the config. A sane default would be 1. Meaning that this feature will not be used by default, for reasons elaborated on later.; 2. A factor with which the memory is increased on each attempt. So if the factor is 1.5 > Attempt 1 will be 1.5^0 = 1 times the memory, Attempt 2 1.5^1 = 1.5, Attempt 3 = 1.5^2 = 2.25. A sane default here would be 2 I guess. As for @cjllanwarne's concerns:. > The spec defines a maximum memory value, above which Cromwell will never go; ; I think the spec just states the value that should be given to whatever backend. But semantics aside, I guess that means the same as saying it is the maximum. > Cromwell will be able to run WDLs which will not run anywhere else... and thus we would have to be very strict in policing our ""best practices"" WDL to makes sure it can be run on other engines. This is a very good reason to not enable this feature by default. But since there are very good reasons for having this feature, having it as a configurable option will be very very nice. Let the user decide how they want to treat their memory requirements. That is the most user-friendly way to do. This is why I think a sane default for the number of attempts should be 1 (i.e. no retries).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499374511
https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499374511:489,Modifiability,config,config,489,"> May I get a review on the design, but not the scala?. Wow thanks for adding this. This is exactly what we need on our cluster. Sometimes the scheduler aggressively kills jobs based on VMEM (instead of actual mem). So retrying with upping the memory requirements is a nice way to circumvent this annoying issue. (Instead of using insane memory requirements to make sure it passes in 99% of the cases). As for the design. I would add; 1. A number of attempts configurable parameter in the config. A sane default would be 1. Meaning that this feature will not be used by default, for reasons elaborated on later.; 2. A factor with which the memory is increased on each attempt. So if the factor is 1.5 > Attempt 1 will be 1.5^0 = 1 times the memory, Attempt 2 1.5^1 = 1.5, Attempt 3 = 1.5^2 = 2.25. A sane default here would be 2 I guess. As for @cjllanwarne's concerns:. > The spec defines a maximum memory value, above which Cromwell will never go; ; I think the spec just states the value that should be given to whatever backend. But semantics aside, I guess that means the same as saying it is the maximum. > Cromwell will be able to run WDLs which will not run anywhere else... and thus we would have to be very strict in policing our ""best practices"" WDL to makes sure it can be run on other engines. This is a very good reason to not enable this feature by default. But since there are very good reasons for having this feature, having it as a configurable option will be very very nice. Let the user decide how they want to treat their memory requirements. That is the most user-friendly way to do. This is why I think a sane default for the number of attempts should be 1 (i.e. no retries).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499374511
https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499374511:1451,Modifiability,config,configurable,1451,"> May I get a review on the design, but not the scala?. Wow thanks for adding this. This is exactly what we need on our cluster. Sometimes the scheduler aggressively kills jobs based on VMEM (instead of actual mem). So retrying with upping the memory requirements is a nice way to circumvent this annoying issue. (Instead of using insane memory requirements to make sure it passes in 99% of the cases). As for the design. I would add; 1. A number of attempts configurable parameter in the config. A sane default would be 1. Meaning that this feature will not be used by default, for reasons elaborated on later.; 2. A factor with which the memory is increased on each attempt. So if the factor is 1.5 > Attempt 1 will be 1.5^0 = 1 times the memory, Attempt 2 1.5^1 = 1.5, Attempt 3 = 1.5^2 = 2.25. A sane default here would be 2 I guess. As for @cjllanwarne's concerns:. > The spec defines a maximum memory value, above which Cromwell will never go; ; I think the spec just states the value that should be given to whatever backend. But semantics aside, I guess that means the same as saying it is the maximum. > Cromwell will be able to run WDLs which will not run anywhere else... and thus we would have to be very strict in policing our ""best practices"" WDL to makes sure it can be run on other engines. This is a very good reason to not enable this feature by default. But since there are very good reasons for having this feature, having it as a configurable option will be very very nice. Let the user decide how they want to treat their memory requirements. That is the most user-friendly way to do. This is why I think a sane default for the number of attempts should be 1 (i.e. no retries).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499374511
https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499374511:1582,Usability,user-friendly,user-friendly,1582,"> May I get a review on the design, but not the scala?. Wow thanks for adding this. This is exactly what we need on our cluster. Sometimes the scheduler aggressively kills jobs based on VMEM (instead of actual mem). So retrying with upping the memory requirements is a nice way to circumvent this annoying issue. (Instead of using insane memory requirements to make sure it passes in 99% of the cases). As for the design. I would add; 1. A number of attempts configurable parameter in the config. A sane default would be 1. Meaning that this feature will not be used by default, for reasons elaborated on later.; 2. A factor with which the memory is increased on each attempt. So if the factor is 1.5 > Attempt 1 will be 1.5^0 = 1 times the memory, Attempt 2 1.5^1 = 1.5, Attempt 3 = 1.5^2 = 2.25. A sane default here would be 2 I guess. As for @cjllanwarne's concerns:. > The spec defines a maximum memory value, above which Cromwell will never go; ; I think the spec just states the value that should be given to whatever backend. But semantics aside, I guess that means the same as saying it is the maximum. > Cromwell will be able to run WDLs which will not run anywhere else... and thus we would have to be very strict in policing our ""best practices"" WDL to makes sure it can be run on other engines. This is a very good reason to not enable this feature by default. But since there are very good reasons for having this feature, having it as a configurable option will be very very nice. Let the user decide how they want to treat their memory requirements. That is the most user-friendly way to do. This is why I think a sane default for the number of attempts should be 1 (i.e. no retries).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499374511
https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-500442583:23,Usability,feedback,feedback,23,Closing as I have some feedback to make changes!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-500442583
https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502138379:60,Usability,clear,clear,60,"@ruchim I'll keep the convo here so it's centralized. To be clear, you're asking about making sure what happens if the SA does not have the appropriate role attached to their permissions? If not, I don't understand the question (specifically the use of ""scope"" in this context)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502138379
https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502892778:208,Availability,error,error,208,"It would be nice to have some more documentation about this. When I first logged in this morning, I couldn't access the board, so I tried creating an account and that also failed initially for an _unexpected error, please try again later_ sort of thing. . Also, what board do we create Cromwell issues under? My best guess is `Jira Support` and that's where I created my issue: [Cromwell (server) loses ability to poll some workflows](https://broadworkbench.atlassian.net/browse/JS-34), but all of the other issues aren't really Cromwell related. A ""query"" field might also be useful. . These are the boards currently on Jira:; - `Batch Analysis`; - `Cloud Accounts`; - `Data-repo`; - `DevOps`; - `DSP-ELT Backlog`; - `Interactive Analysis`; - `Jira Support`; - `New Project`; - `PERF`; - `PRODUCTION`; - `QA`; - `SAND-NG`; - `SANDBOX`; - `SUPPORT`; - `TERRA ROADMAP`; - `TerraUI`; - `User Metrics`; - `UX`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502892778
https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502892778:109,Security,access,access,109,"It would be nice to have some more documentation about this. When I first logged in this morning, I couldn't access the board, so I tried creating an account and that also failed initially for an _unexpected error, please try again later_ sort of thing. . Also, what board do we create Cromwell issues under? My best guess is `Jira Support` and that's where I created my issue: [Cromwell (server) loses ability to poll some workflows](https://broadworkbench.atlassian.net/browse/JS-34), but all of the other issues aren't really Cromwell related. A ""query"" field might also be useful. . These are the boards currently on Jira:; - `Batch Analysis`; - `Cloud Accounts`; - `Data-repo`; - `DevOps`; - `DSP-ELT Backlog`; - `Interactive Analysis`; - `Jira Support`; - `New Project`; - `PERF`; - `PRODUCTION`; - `QA`; - `SAND-NG`; - `SANDBOX`; - `SUPPORT`; - `TERRA ROADMAP`; - `TerraUI`; - `User Metrics`; - `UX`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502892778
https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502892778:74,Testability,log,logged,74,"It would be nice to have some more documentation about this. When I first logged in this morning, I couldn't access the board, so I tried creating an account and that also failed initially for an _unexpected error, please try again later_ sort of thing. . Also, what board do we create Cromwell issues under? My best guess is `Jira Support` and that's where I created my issue: [Cromwell (server) loses ability to poll some workflows](https://broadworkbench.atlassian.net/browse/JS-34), but all of the other issues aren't really Cromwell related. A ""query"" field might also be useful. . These are the boards currently on Jira:; - `Batch Analysis`; - `Cloud Accounts`; - `Data-repo`; - `DevOps`; - `DSP-ELT Backlog`; - `Interactive Analysis`; - `Jira Support`; - `New Project`; - `PERF`; - `PRODUCTION`; - `QA`; - `SAND-NG`; - `SANDBOX`; - `SUPPORT`; - `TERRA ROADMAP`; - `TerraUI`; - `User Metrics`; - `UX`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502892778
https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502892778:903,Usability,UX,UX,903,"It would be nice to have some more documentation about this. When I first logged in this morning, I couldn't access the board, so I tried creating an account and that also failed initially for an _unexpected error, please try again later_ sort of thing. . Also, what board do we create Cromwell issues under? My best guess is `Jira Support` and that's where I created my issue: [Cromwell (server) loses ability to poll some workflows](https://broadworkbench.atlassian.net/browse/JS-34), but all of the other issues aren't really Cromwell related. A ""query"" field might also be useful. . These are the boards currently on Jira:; - `Batch Analysis`; - `Cloud Accounts`; - `Data-repo`; - `DevOps`; - `DSP-ELT Backlog`; - `Interactive Analysis`; - `Jira Support`; - `New Project`; - `PERF`; - `PRODUCTION`; - `QA`; - `SAND-NG`; - `SANDBOX`; - `SUPPORT`; - `TERRA ROADMAP`; - `TerraUI`; - `User Metrics`; - `UX`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502892778
https://github.com/broadinstitute/cromwell/pull/5038#issuecomment-505242680:151,Usability,clear,clear,151,"> It would be nice to document how to run this - I'm guessing it's running one of the shell scripts?. @aednichols -- I had that thought, but it wasn't clear where they would go. I see three options; 1.) main readme; 2.) separate readme (CLIENT.md); 3.) readthedocs site. Which is the appropriate path for docs like this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5038#issuecomment-505242680
https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509805758:4346,Deployability,update,update,4346,"yYy9tYWluL3NjYWxhL2N3bC9Xb3JrZmxvdy5zY2FsYQ==) | `95.52% <100%> (+7.46%)` | :arrow_up: |; | [...ain/scala/wdl/transforms/base/wdlom2wom/Util.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvbmV3LWJhc2Uvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmFzZS93ZGxvbTJ3b20vVXRpbC5zY2FsYQ==) | `100% <100%> (ø)` | :arrow_up: |; | [...dlom2wom/WdlDraft2WomWorkflowDefinitionMaker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvZHJhZnQyL3NyYy9tYWluL3NjYWxhL3dkbC90cmFuc2Zvcm1zL2RyYWZ0Mi93ZGxvbTJ3b20vV2RsRHJhZnQyV29tV29ya2Zsb3dEZWZpbml0aW9uTWFrZXIuc2NhbGE=) | `87.5% <75%> (-12.5%)` | :arrow_down: |; | [...m2wom/WdlDraft2WomCommandTaskDefinitionMaker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvZHJhZnQyL3NyYy9tYWluL3NjYWxhL3dkbC90cmFuc2Zvcm1zL2RyYWZ0Mi93ZGxvbTJ3b20vV2RsRHJhZnQyV29tQ29tbWFuZFRhc2tEZWZpbml0aW9uTWFrZXIuc2NhbGE=) | `95.23% <75%> (-4.77%)` | :arrow_down: |; | [.../scala/cromiam/webservice/EngineRouteSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-Q3JvbUlBTS9zcmMvbWFpbi9zY2FsYS9jcm9taWFtL3dlYnNlcnZpY2UvRW5naW5lUm91dGVTdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | ... and [626 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=footer). Last update [da601c8...ae566b9](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509805758
https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509805758:4108,Usability,learn,learn,4108,"yYy9tYWluL3NjYWxhL2N3bC9Xb3JrZmxvdy5zY2FsYQ==) | `95.52% <100%> (+7.46%)` | :arrow_up: |; | [...ain/scala/wdl/transforms/base/wdlom2wom/Util.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvbmV3LWJhc2Uvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmFzZS93ZGxvbTJ3b20vVXRpbC5zY2FsYQ==) | `100% <100%> (ø)` | :arrow_up: |; | [...dlom2wom/WdlDraft2WomWorkflowDefinitionMaker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvZHJhZnQyL3NyYy9tYWluL3NjYWxhL3dkbC90cmFuc2Zvcm1zL2RyYWZ0Mi93ZGxvbTJ3b20vV2RsRHJhZnQyV29tV29ya2Zsb3dEZWZpbml0aW9uTWFrZXIuc2NhbGE=) | `87.5% <75%> (-12.5%)` | :arrow_down: |; | [...m2wom/WdlDraft2WomCommandTaskDefinitionMaker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvZHJhZnQyL3NyYy9tYWluL3NjYWxhL3dkbC90cmFuc2Zvcm1zL2RyYWZ0Mi93ZGxvbTJ3b20vV2RsRHJhZnQyV29tQ29tbWFuZFRhc2tEZWZpbml0aW9uTWFrZXIuc2NhbGE=) | `95.23% <75%> (-4.77%)` | :arrow_down: |; | [.../scala/cromiam/webservice/EngineRouteSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-Q3JvbUlBTS9zcmMvbWFpbi9zY2FsYS9jcm9taWFtL3dlYnNlcnZpY2UvRW5naW5lUm91dGVTdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | ... and [626 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=footer). Last update [da601c8...ae566b9](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509805758
https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510955574:17,Usability,feedback,feedback,17,Closing based on feedback.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510955574
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509638449:35,Modifiability,variab,variables,35,"I also wouldn't be sparse with the variables, for some future user coming to read this, I would use `--exclusive` instead of `-x` and then `--unlock` instead of `-u` so it's explicitly clear.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509638449
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509638449:185,Usability,clear,clear,185,"I also wouldn't be sparse with the variables, for some future user coming to read this, I would use `--exclusive` instead of `-x` and then `--unlock` instead of `-u` so it's explicitly clear.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509638449
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:225,Availability,avail,available,225,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:402,Availability,avail,available,402,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:899,Modifiability,variab,variables,899,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:692,Performance,cache,cached,692,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:721,Performance,cache,cache,721,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:846,Performance,cache,cache,846,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:815,Safety,avoid,avoid,815,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:1041,Usability,clear,clear,1041,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517265945:93,Modifiability,variab,variables,93,"Would it be possible to use realistic paths, perhaps the `SINGULARITY_CACHEDIR`? And to make variables explicitly clear (and in quotes), maybe ""${docker_subbed}.sif"" instead of $docker_subbed.sif. The goal would be to have a general script that can work for most, correct?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517265945
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517265945:114,Usability,clear,clear,114,"Would it be possible to use realistic paths, perhaps the `SINGULARITY_CACHEDIR`? And to make variables explicitly clear (and in quotes), maybe ""${docker_subbed}.sif"" instead of $docker_subbed.sif. The goal would be to have a general script that can work for most, correct?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517265945
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685:301,Availability,down,down,301,"The main reason was that I don't have a native environment with python3 + the latest openssl that was usable with the github auth requirements. I have python3 + openssl that I've built, but need xmlsec which segfaulted when I used the centos6 version with the hand-built python environment. I started down the path of building xmlsec which didn't immediately work properly so wanted to stop spiraling down the rathole of building chained dependencies.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685:401,Availability,down,down,401,"The main reason was that I don't have a native environment with python3 + the latest openssl that was usable with the github auth requirements. I have python3 + openssl that I've built, but need xmlsec which segfaulted when I used the centos6 version with the hand-built python environment. I started down the path of building xmlsec which didn't immediately work properly so wanted to stop spiraling down the rathole of building chained dependencies.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685:438,Integrability,depend,dependencies,438,"The main reason was that I don't have a native environment with python3 + the latest openssl that was usable with the github auth requirements. I have python3 + openssl that I've built, but need xmlsec which segfaulted when I used the centos6 version with the hand-built python environment. I started down the path of building xmlsec which didn't immediately work properly so wanted to stop spiraling down the rathole of building chained dependencies.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685:102,Usability,usab,usable,102,"The main reason was that I don't have a native environment with python3 + the latest openssl that was usable with the github auth requirements. I have python3 + openssl that I've built, but need xmlsec which segfaulted when I used the centos6 version with the hand-built python environment. I started down the path of building xmlsec which didn't immediately work properly so wanted to stop spiraling down the rathole of building chained dependencies.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685
https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511471851:46,Testability,log,logs,46,"@ichengchang are we confident that INFO-level logs show up in Kibana?. Should be very easy to check, simply identify a single INFO entry in Kibana that came from Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511471851
https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511471851:101,Usability,simpl,simply,101,"@ichengchang are we confident that INFO-level logs show up in Kibana?. Should be very easy to check, simply identify a single INFO entry in Kibana that came from Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511471851
https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251:4537,Deployability,update,update,4537,"e#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | [...ll/engine/workflow/WorkflowDockerLookupActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS93b3JrZmxvdy9Xb3JrZmxvd0RvY2tlckxvb2t1cEFjdG9yLnNjYWxh) | `95.34% <0%> (+1.16%)` | :arrow_up: |; | [...cle/execution/callcaching/CallCacheDiffActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS93b3JrZmxvdy9saWZlY3ljbGUvZXhlY3V0aW9uL2NhbGxjYWNoaW5nL0NhbGxDYWNoZURpZmZBY3Rvci5zY2FsYQ==) | `96.38% <0%> (+1.2%)` | :arrow_up: |; | [...scala/cromwell/languages/util/ImportResolver.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-bGFuZ3VhZ2VGYWN0b3JpZXMvbGFuZ3VhZ2UtZmFjdG9yeS1jb3JlL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2xhbmd1YWdlcy91dGlsL0ltcG9ydFJlc29sdmVyLnNjYWxh) | `98.7% <0%> (+1.29%)` | :arrow_up: |; | [...src/main/scala/wdl/draft2/model/WdlNamespace.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL1dkbE5hbWVzcGFjZS5zY2FsYQ==) | `92.37% <0%> (+1.34%)` | :arrow_up: |; | ... and [402 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=footer). Last update [8055dad...803ebd8](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251
https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251:4299,Usability,learn,learn,4299,"e#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | [...ll/engine/workflow/WorkflowDockerLookupActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS93b3JrZmxvdy9Xb3JrZmxvd0RvY2tlckxvb2t1cEFjdG9yLnNjYWxh) | `95.34% <0%> (+1.16%)` | :arrow_up: |; | [...cle/execution/callcaching/CallCacheDiffActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS93b3JrZmxvdy9saWZlY3ljbGUvZXhlY3V0aW9uL2NhbGxjYWNoaW5nL0NhbGxDYWNoZURpZmZBY3Rvci5zY2FsYQ==) | `96.38% <0%> (+1.2%)` | :arrow_up: |; | [...scala/cromwell/languages/util/ImportResolver.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-bGFuZ3VhZ2VGYWN0b3JpZXMvbGFuZ3VhZ2UtZmFjdG9yeS1jb3JlL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2xhbmd1YWdlcy91dGlsL0ltcG9ydFJlc29sdmVyLnNjYWxh) | `98.7% <0%> (+1.29%)` | :arrow_up: |; | [...src/main/scala/wdl/draft2/model/WdlNamespace.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL1dkbE5hbWVzcGFjZS5zY2FsYQ==) | `92.37% <0%> (+1.34%)` | :arrow_up: |; | ... and [402 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=footer). Last update [8055dad...803ebd8](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251
https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119:237,Availability,error,error-reference,237,"# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@9ec815d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `95.12%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #5086 +/- ##; ==========================================; Coverage ? 78.36% ; ==========================================; Files ? 1038 ; Lines ? 26695 ; Branches ? 887 ; ==========================================; Hits ? 20920 ; Misses ? 5775 ; Partials ? 0; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [core/src/main/scala/cromwell/util/JsonEditor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC91dGlsL0pzb25FZGl0b3Iuc2NhbGE=) | `95.12% <95.12%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=footer). Last update [9ec815d...d2705e2](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119
https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119:1600,Deployability,update,update,1600,"# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@9ec815d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `95.12%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #5086 +/- ##; ==========================================; Coverage ? 78.36% ; ==========================================; Files ? 1038 ; Lines ? 26695 ; Branches ? 887 ; ==========================================; Hits ? 20920 ; Misses ? 5775 ; Partials ? 0; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [core/src/main/scala/cromwell/util/JsonEditor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC91dGlsL0pzb25FZGl0b3Iuc2NhbGE=) | `95.12% <95.12%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=footer). Last update [9ec815d...d2705e2](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119
https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119:185,Usability,learn,learn,185,"# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@9ec815d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `95.12%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #5086 +/- ##; ==========================================; Coverage ? 78.36% ; ==========================================; Files ? 1038 ; Lines ? 26695 ; Branches ? 887 ; ==========================================; Hits ? 20920 ; Misses ? 5775 ; Partials ? 0; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [core/src/main/scala/cromwell/util/JsonEditor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC91dGlsL0pzb25FZGl0b3Iuc2NhbGE=) | `95.12% <95.12%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=footer). Last update [9ec815d...d2705e2](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119
https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119:1362,Usability,learn,learn,1362,"# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@9ec815d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `95.12%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #5086 +/- ##; ==========================================; Coverage ? 78.36% ; ==========================================; Files ? 1038 ; Lines ? 26695 ; Branches ? 887 ; ==========================================; Hits ? 20920 ; Misses ? 5775 ; Partials ? 0; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [core/src/main/scala/cromwell/util/JsonEditor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC91dGlsL0pzb25FZGl0b3Iuc2NhbGE=) | `95.12% <95.12%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=footer). Last update [9ec815d...d2705e2](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366:78,Security,validat,validateRunArguments,78,"Hi @geoffjentry - ; The changes that I pushed are related to the test of the `validateRunArguments` method. Since I changed him, I had to change the tests. I did not notice this test initially.; This bothers me a bit because I did not expect the behavior of the method to change when processing WDL files. But in the end, now this method just processes the files in the same way as in server mode. Moreover, maybe the new behavior makes sense. For example, `validation.get.workflowUrl` indeed should be `None`, since we are supplying workflow without any `url`.; Just in case, I ran simple WDL workflows and didn't encounter any issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366:458,Security,validat,validation,458,"Hi @geoffjentry - ; The changes that I pushed are related to the test of the `validateRunArguments` method. Since I changed him, I had to change the tests. I did not notice this test initially.; This bothers me a bit because I did not expect the behavior of the method to change when processing WDL files. But in the end, now this method just processes the files in the same way as in server mode. Moreover, maybe the new behavior makes sense. For example, `validation.get.workflowUrl` indeed should be `None`, since we are supplying workflow without any `url`.; Just in case, I ran simple WDL workflows and didn't encounter any issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366:65,Testability,test,test,65,"Hi @geoffjentry - ; The changes that I pushed are related to the test of the `validateRunArguments` method. Since I changed him, I had to change the tests. I did not notice this test initially.; This bothers me a bit because I did not expect the behavior of the method to change when processing WDL files. But in the end, now this method just processes the files in the same way as in server mode. Moreover, maybe the new behavior makes sense. For example, `validation.get.workflowUrl` indeed should be `None`, since we are supplying workflow without any `url`.; Just in case, I ran simple WDL workflows and didn't encounter any issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366:149,Testability,test,tests,149,"Hi @geoffjentry - ; The changes that I pushed are related to the test of the `validateRunArguments` method. Since I changed him, I had to change the tests. I did not notice this test initially.; This bothers me a bit because I did not expect the behavior of the method to change when processing WDL files. But in the end, now this method just processes the files in the same way as in server mode. Moreover, maybe the new behavior makes sense. For example, `validation.get.workflowUrl` indeed should be `None`, since we are supplying workflow without any `url`.; Just in case, I ran simple WDL workflows and didn't encounter any issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366:178,Testability,test,test,178,"Hi @geoffjentry - ; The changes that I pushed are related to the test of the `validateRunArguments` method. Since I changed him, I had to change the tests. I did not notice this test initially.; This bothers me a bit because I did not expect the behavior of the method to change when processing WDL files. But in the end, now this method just processes the files in the same way as in server mode. Moreover, maybe the new behavior makes sense. For example, `validation.get.workflowUrl` indeed should be `None`, since we are supplying workflow without any `url`.; Just in case, I ran simple WDL workflows and didn't encounter any issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366:583,Usability,simpl,simple,583,"Hi @geoffjentry - ; The changes that I pushed are related to the test of the `validateRunArguments` method. Since I changed him, I had to change the tests. I did not notice this test initially.; This bothers me a bit because I did not expect the behavior of the method to change when processing WDL files. But in the end, now this method just processes the files in the same way as in server mode. Moreover, maybe the new behavior makes sense. For example, `validation.get.workflowUrl` indeed should be `None`, since we are supplying workflow without any `url`.; Just in case, I ran simple WDL workflows and didn't encounter any issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-555218927:29,Usability,simpl,simple,29,Resolved conflicts (it was a simple union of branch changes and develop changes),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-555218927
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:2296,Deployability,pipeline,pipelines,2296,%)` | :arrow_down: |; | [...ala/wdl/draft2/model/WdlSyntaxErrorFormatter.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL1dkbFN5bnRheEVycm9yRm9ybWF0dGVyLnNjYWxh) | `70.19% <0%> (-0.67%)` | :arrow_down: |; | [.../scala/cromwell/database/slick/SlickDatabase.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-ZGF0YWJhc2Uvc3FsL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2RhdGFiYXNlL3NsaWNrL1NsaWNrRGF0YWJhc2Uuc2NhbGE=) | `84.78% <0%> (-0.64%)` | :arrow_down: |; | [...a1/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvUGlwZWxpbmVzQXBpQXN5bmNCYWNrZW5kSm9iRXhlY3V0aW9uQWN0b3Iuc2NhbGE=) | `6.84% <0%> (-0.2%)` | :arrow_down: |; | [...d/google/pipelines/v2alpha1/api/Localization.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (ø)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (ø)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:2635,Deployability,pipeline,pipelines,2635,slick/SlickDatabase.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-ZGF0YWJhc2Uvc3FsL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2RhdGFiYXNlL3NsaWNrL1NsaWNrRGF0YWJhc2Uuc2NhbGE=) | `84.78% <0%> (-0.64%)` | :arrow_down: |; | [...a1/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvUGlwZWxpbmVzQXBpQXN5bmNCYWNrZW5kSm9iRXhlY3V0aW9uQWN0b3Iuc2NhbGE=) | `6.84% <0%> (-0.2%)` | :arrow_down: |; | [...d/google/pipelines/v2alpha1/api/Localization.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (ø)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (ø)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96% <0%> (+0.39%)` | :arrow_up: |; | [...n/scala/cromwell/core/path/BetterFileMethods.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9jb3JlL3BhdGgvQmV0dGVyRmlsZU1ldGhvZHMuc2NhbGE=) | `30.1% <0%> (+1.02%)` | :arrow_up: |; | [...wl/src/main/scala/cwl/ExpressionInter,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:4363,Deployability,update,update,4363,"JhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (ø)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (ø)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96% <0%> (+0.39%)` | :arrow_up: |; | [...n/scala/cromwell/core/path/BetterFileMethods.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9jb3JlL3BhdGgvQmV0dGVyRmlsZU1ldGhvZHMuc2NhbGE=) | `30.1% <0%> (+1.02%)` | :arrow_up: |; | [...wl/src/main/scala/cwl/ExpressionInterpolator.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | ... and [301 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=footer). Last update [242206f...e9ad3d7](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:4125,Usability,learn,learn,4125,"JhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (ø)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (ø)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96% <0%> (+0.39%)` | :arrow_up: |; | [...n/scala/cromwell/core/path/BetterFileMethods.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9jb3JlL3BhdGgvQmV0dGVyRmlsZU1ldGhvZHMuc2NhbGE=) | `30.1% <0%> (+1.02%)` | :arrow_up: |; | [...wl/src/main/scala/cwl/ExpressionInterpolator.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | ... and [301 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=footer). Last update [242206f...e9ad3d7](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842
https://github.com/broadinstitute/cromwell/pull/5135#issuecomment-524451332:30,Usability,feedback,feedback,30,"Hi @cjllanwarne !; Thanks for feedback!; My main point is that `BigDecimal` supports operations with `Float` (without rounding) ""out of the box"", while `Long` and `BigInteger` does not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5135#issuecomment-524451332
https://github.com/broadinstitute/cromwell/pull/5135#issuecomment-525469779:177,Usability,simpl,simply,177,"Oh! I thought the discussion was about using the Java type for implementation, not proposing it to literally be part of WDL. My $0.02 w/ my OpenWDL hat on is that I'd prefer to simply have fairly generic descriptions of a integer and float type and let implementations choose what they think is best. But that's best held for a discussion over there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5135#issuecomment-525469779
https://github.com/broadinstitute/cromwell/pull/5171#issuecomment-533183374:19,Modifiability,refactor,refactored,19,"@danbills @kshakir refactored to reflect the new shape of https://www.lucidchart.com/invitations/accept/495747cc-4eeb-4a49-97c2-5545d2411a93. In brief:. * The decider is outside of the HMSA itself, to preserve responsiveness while deciding ""where do I send this read request""; * There is a new regulator layer between the HMSA and the read decider (so that we only choose once per identical read request)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5171#issuecomment-533183374
https://github.com/broadinstitute/cromwell/pull/5171#issuecomment-533183374:210,Usability,responsiv,responsiveness,210,"@danbills @kshakir refactored to reflect the new shape of https://www.lucidchart.com/invitations/accept/495747cc-4eeb-4a49-97c2-5545d2411a93. In brief:. * The decider is outside of the HMSA itself, to preserve responsiveness while deciding ""where do I send this read request""; * There is a new regulator layer between the HMSA and the read decider (so that we only choose once per identical read request)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5171#issuecomment-533183374
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509:123,Integrability,depend,depend,123,"@cjllanwarne Sorry for the delay in response, I was offline over the Xmas break.; ; > Question: Does the new test actually depend on connecting to external AWS resources? . The short answer is no, it doesn't depend on actually connecting to AWS resources. . However it does require the AWS_REGION environment variable to be set so that the SDK initializes. So if it is run without AWS_REGION the test fails, if it is run with AWS_REGION it works (but doesn't actually connect to AWS). Assuming that your regular CI builds don't set the AWS env variables, it is probably simplest to leave it tagged as an AwsTest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509:208,Integrability,depend,depend,208,"@cjllanwarne Sorry for the delay in response, I was offline over the Xmas break.; ; > Question: Does the new test actually depend on connecting to external AWS resources? . The short answer is no, it doesn't depend on actually connecting to AWS resources. . However it does require the AWS_REGION environment variable to be set so that the SDK initializes. So if it is run without AWS_REGION the test fails, if it is run with AWS_REGION it works (but doesn't actually connect to AWS). Assuming that your regular CI builds don't set the AWS env variables, it is probably simplest to leave it tagged as an AwsTest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509:309,Modifiability,variab,variable,309,"@cjllanwarne Sorry for the delay in response, I was offline over the Xmas break.; ; > Question: Does the new test actually depend on connecting to external AWS resources? . The short answer is no, it doesn't depend on actually connecting to AWS resources. . However it does require the AWS_REGION environment variable to be set so that the SDK initializes. So if it is run without AWS_REGION the test fails, if it is run with AWS_REGION it works (but doesn't actually connect to AWS). Assuming that your regular CI builds don't set the AWS env variables, it is probably simplest to leave it tagged as an AwsTest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509:544,Modifiability,variab,variables,544,"@cjllanwarne Sorry for the delay in response, I was offline over the Xmas break.; ; > Question: Does the new test actually depend on connecting to external AWS resources? . The short answer is no, it doesn't depend on actually connecting to AWS resources. . However it does require the AWS_REGION environment variable to be set so that the SDK initializes. So if it is run without AWS_REGION the test fails, if it is run with AWS_REGION it works (but doesn't actually connect to AWS). Assuming that your regular CI builds don't set the AWS env variables, it is probably simplest to leave it tagged as an AwsTest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509:109,Testability,test,test,109,"@cjllanwarne Sorry for the delay in response, I was offline over the Xmas break.; ; > Question: Does the new test actually depend on connecting to external AWS resources? . The short answer is no, it doesn't depend on actually connecting to AWS resources. . However it does require the AWS_REGION environment variable to be set so that the SDK initializes. So if it is run without AWS_REGION the test fails, if it is run with AWS_REGION it works (but doesn't actually connect to AWS). Assuming that your regular CI builds don't set the AWS env variables, it is probably simplest to leave it tagged as an AwsTest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509:396,Testability,test,test,396,"@cjllanwarne Sorry for the delay in response, I was offline over the Xmas break.; ; > Question: Does the new test actually depend on connecting to external AWS resources? . The short answer is no, it doesn't depend on actually connecting to AWS resources. . However it does require the AWS_REGION environment variable to be set so that the SDK initializes. So if it is run without AWS_REGION the test fails, if it is run with AWS_REGION it works (but doesn't actually connect to AWS). Assuming that your regular CI builds don't set the AWS env variables, it is probably simplest to leave it tagged as an AwsTest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509:570,Usability,simpl,simplest,570,"@cjllanwarne Sorry for the delay in response, I was offline over the Xmas break.; ; > Question: Does the new test actually depend on connecting to external AWS resources? . The short answer is no, it doesn't depend on actually connecting to AWS resources. . However it does require the AWS_REGION environment variable to be set so that the SDK initializes. So if it is run without AWS_REGION the test fails, if it is run with AWS_REGION it works (but doesn't actually connect to AWS). Assuming that your regular CI builds don't set the AWS env variables, it is probably simplest to leave it tagged as an AwsTest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509
https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:244,Deployability,integrat,integrations,244,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633
https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:244,Integrability,integrat,integrations,244,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633
https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:101,Testability,test,tests,101,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633
https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:198,Testability,test,tests,198,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633
https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:223,Testability,test,tests,223,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633
https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:257,Testability,test,testing,257,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633
https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:296,Testability,test,tests,296,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633
https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:64,Usability,undo,undoing,64,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:53,Deployability,configurat,configuration,53,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:107,Deployability,update,updated,107,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:119,Deployability,configurat,configuration,119,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:265,Deployability,update,updated,265,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:273,Deployability,configurat,configuration,273,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:53,Modifiability,config,configuration,53,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:119,Modifiability,config,configuration,119,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:273,Modifiability,config,configuration,273,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:100,Usability,simpl,simply,100,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892
https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554031085:288,Usability,clear,clear,288,"![image](https://user-images.githubusercontent.com/4853242/68887215-9b8a5380-06e6-11ea-9870-8fd701773573.png); I'm still not sure what are `Execution store` and `Value store` from the picture above. Color coding suggests that they are `connected components`, but the meaning is still not clear to me. Also, maybe `connected component` is not the best term in this case, since its strong ties with graph theory (unless this is exactly what it means here)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554031085
https://github.com/broadinstitute/cromwell/pull/5267#issuecomment-551305427:490,Usability,simpl,simplest,490,"@grsterin the change from date to ID came up during a tech talk - it's because in our production database with several million rows, a timestamp comparison based query was extremely slow (~15s) whereas an ID based query (which are ordered, and indexed) was extremely fast (~0.04s). There probably *are* clever ways to work around that, but since we'll be the ones setting this value, and since we'll only need to do the calculation once, this slight change in the feature was chosen as the simplest way forward.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5267#issuecomment-551305427
https://github.com/broadinstitute/cromwell/pull/5267#issuecomment-551387140:567,Deployability,update,update,567,"> @grsterin the change from date to ID came up during a tech talk - it's because in our production database with several million rows, a timestamp comparison based query was extremely slow (~15s) whereas an ID based query (which are ordered, and indexed) was extremely fast (~0.04s).; > ; > There probably _are_ clever ways to work around that, but since we'll be the ones setting this value, and since we'll only need to do the calculation once, this slight change in the feature was chosen as the simplest way forward. Sounds good. But in this case, I'd suggest to update ticket description in order to eliminate discrepancy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5267#issuecomment-551387140
https://github.com/broadinstitute/cromwell/pull/5267#issuecomment-551387140:499,Usability,simpl,simplest,499,"> @grsterin the change from date to ID came up during a tech talk - it's because in our production database with several million rows, a timestamp comparison based query was extremely slow (~15s) whereas an ID based query (which are ordered, and indexed) was extremely fast (~0.04s).; > ; > There probably _are_ clever ways to work around that, but since we'll be the ones setting this value, and since we'll only need to do the calculation once, this slight change in the feature was chosen as the simplest way forward. Sounds good. But in this case, I'd suggest to update ticket description in order to eliminate discrepancy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5267#issuecomment-551387140
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:867,Availability,error,errors,867,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:1985,Deployability,pipeline,pipeline,1985,"ached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that has run multiple times, using call-caching and sometimes with slightly different settings: **85 GB** in files for the database.); MySQL performs much better with cromwell, but is infeasible to use in a per-project, per-user fashion. [I am working on solving the problem using SQLite](https://github.com/broadinstitute/cromwell/issues/5490) but there is NO ETA, and I don't know if I will ever get it to work. Still it is worth trying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:432,Performance,cache,cached,432,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:501,Performance,cache,cache,501,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:1134,Performance,cache,cached,1134," files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that has run multiple times, using call-caching and sometimes with slightly different settings: **85 GB** in files for the database.); MyS",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:1638,Performance,perform,performing,1638,"ached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that has run multiple times, using call-caching and sometimes with slightly different settings: **85 GB** in files for the database.); MySQL performs much better with cromwell, but is infeasible to use in a per-project, per-user fashion. [I am working on solving the problem using SQLite](https://github.com/broadinstitute/cromwell/issues/5490) but there is NO ETA, and I don't know if I will ever get it to work. Still it is worth trying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:2135,Performance,perform,performs,2135,"ached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that has run multiple times, using call-caching and sometimes with slightly different settings: **85 GB** in files for the database.); MySQL performs much better with cromwell, but is infeasible to use in a per-project, per-user fashion. [I am working on solving the problem using SQLite](https://github.com/broadinstitute/cromwell/issues/5490) but there is NO ETA, and I don't know if I will ever get it to work. Still it is worth trying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:115,Safety,timeout,timeouts,115,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:1542,Safety,timeout,timeout,1542,"ached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that has run multiple times, using call-caching and sometimes with slightly different settings: **85 GB** in files for the database.); MySQL performs much better with cromwell, but is infeasible to use in a per-project, per-user fashion. [I am working on solving the problem using SQLite](https://github.com/broadinstitute/cromwell/issues/5490) but there is NO ETA, and I don't know if I will ever get it to work. Still it is worth trying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:317,Usability,guid,guide,317,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:684,Usability,clear,clear,684,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:728,Usability,guid,guide,728,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/pull/5278#issuecomment-558290125:31,Usability,pause,pause,31,"@cjllanwarne, @mcovarr, please pause further reviews for now. I don't like current implementation and I think that previous implementation actually may have been better. This needs additional discussion.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5278#issuecomment-558290125
https://github.com/broadinstitute/cromwell/pull/5278#issuecomment-558475452:33,Usability,pause,pause,33,"> @cjllanwarne, @mcovarr, please pause further reviews for now. I don't like current implementation and I think that previous implementation actually may have been better. This needs additional discussion. Moved handling of `preemptible` and `maxRetries` attributes from Engine to backend's `StandardAsyncExecutionActor`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5278#issuecomment-558475452
https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176:1228,Deployability,release,release,1228,"@cjllanwarne Thanks for the clarification. I was already wondering why you would negate your own well-written namespace code with a single line... Anyway I created a pull request on the spec here: https://github.com/openwdl/wdl/pull/347, your feedback would be much appreciated. Here's to hoping that it gets unanimously approved :crossed_fingers: . @geoffjentry yes, the Cromwell team has a lot of influence on the spec by implementing or not implementing things. I can understand the temptation to use this for ""the greater good"" :wink: . But I am quite happy that the Cromwell developers chose to be in touch with the community and aggressively implement the development spec in the development version of Cromwell. This allows us to see how certain spec changes turn out *before* they get implemented in production. In this case I came across this when I was testing the code for #5312 and found that I could not set my resource requirements for BWA anymore (in BioWDL all tasks default to the least number of cores needed, and sometimes you want to override this for more power). Since BWA was nested in a subworkflow this turned out not to be possible. So now we can fix the spec and Cromwell before this ever gets into a release. I think it is great work by the Cromwell team. It can't always be easy to follow the spec that closely.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176
https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176:1077,Energy Efficiency,power,power,1077,"@cjllanwarne Thanks for the clarification. I was already wondering why you would negate your own well-written namespace code with a single line... Anyway I created a pull request on the spec here: https://github.com/openwdl/wdl/pull/347, your feedback would be much appreciated. Here's to hoping that it gets unanimously approved :crossed_fingers: . @geoffjentry yes, the Cromwell team has a lot of influence on the spec by implementing or not implementing things. I can understand the temptation to use this for ""the greater good"" :wink: . But I am quite happy that the Cromwell developers chose to be in touch with the community and aggressively implement the development spec in the development version of Cromwell. This allows us to see how certain spec changes turn out *before* they get implemented in production. In this case I came across this when I was testing the code for #5312 and found that I could not set my resource requirements for BWA anymore (in BioWDL all tasks default to the least number of cores needed, and sometimes you want to override this for more power). Since BWA was nested in a subworkflow this turned out not to be possible. So now we can fix the spec and Cromwell before this ever gets into a release. I think it is great work by the Cromwell team. It can't always be easy to follow the spec that closely.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176
https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176:863,Testability,test,testing,863,"@cjllanwarne Thanks for the clarification. I was already wondering why you would negate your own well-written namespace code with a single line... Anyway I created a pull request on the spec here: https://github.com/openwdl/wdl/pull/347, your feedback would be much appreciated. Here's to hoping that it gets unanimously approved :crossed_fingers: . @geoffjentry yes, the Cromwell team has a lot of influence on the spec by implementing or not implementing things. I can understand the temptation to use this for ""the greater good"" :wink: . But I am quite happy that the Cromwell developers chose to be in touch with the community and aggressively implement the development spec in the development version of Cromwell. This allows us to see how certain spec changes turn out *before* they get implemented in production. In this case I came across this when I was testing the code for #5312 and found that I could not set my resource requirements for BWA anymore (in BioWDL all tasks default to the least number of cores needed, and sometimes you want to override this for more power). Since BWA was nested in a subworkflow this turned out not to be possible. So now we can fix the spec and Cromwell before this ever gets into a release. I think it is great work by the Cromwell team. It can't always be easy to follow the spec that closely.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176
https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176:243,Usability,feedback,feedback,243,"@cjllanwarne Thanks for the clarification. I was already wondering why you would negate your own well-written namespace code with a single line... Anyway I created a pull request on the spec here: https://github.com/openwdl/wdl/pull/347, your feedback would be much appreciated. Here's to hoping that it gets unanimously approved :crossed_fingers: . @geoffjentry yes, the Cromwell team has a lot of influence on the spec by implementing or not implementing things. I can understand the temptation to use this for ""the greater good"" :wink: . But I am quite happy that the Cromwell developers chose to be in touch with the community and aggressively implement the development spec in the development version of Cromwell. This allows us to see how certain spec changes turn out *before* they get implemented in production. In this case I came across this when I was testing the code for #5312 and found that I could not set my resource requirements for BWA anymore (in BioWDL all tasks default to the least number of cores needed, and sometimes you want to override this for more power). Since BWA was nested in a subworkflow this turned out not to be possible. So now we can fix the spec and Cromwell before this ever gets into a release. I think it is great work by the Cromwell team. It can't always be easy to follow the spec that closely.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:385,Availability,down,down,385,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:105,Performance,queue,queue,105,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:711,Performance,perform,perform,711,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:797,Performance,queue,queue,797,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:302,Safety,abort,abort,302,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:511,Safety,abort,abort,511,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:551,Safety,abort,abort,551,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:613,Safety,abort,abort,613,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:226,Usability,guid,guide,226,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/pull/5382#issuecomment-577410441:64,Usability,simpl,simpler,64,"Yeah, for once I talked myself out of a regex, this is just way simpler to assess correctness on",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5382#issuecomment-577410441
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370199:82,Modifiability,config,configure,82,"We use singularity images too, are you following the Cromwell Containers guide to configure singularity: https://cromwell.readthedocs.io/en/stable/tutorials/Containers/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370199
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370199:73,Usability,guid,guide,73,"We use singularity images too, are you following the Cromwell Containers guide to configure singularity: https://cromwell.readthedocs.io/en/stable/tutorials/Containers/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370199
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047374011:349,Performance,cache,cache,349,"> 是的，很酷，当您指定一个 docker 时，Cromwell 必须解析映像摘要才能使调用缓存正常工作。; > ; > 如果 Cromwell 无法在线找到 docker，或者您的代理阻止了 Cromwell，或者该映像不在线，则映像摘要无法正确解析，并且调用缓存处于禁用状态。; > ; > 这里有一些进一步的上下文： #6140. I also learned this from the official documents. Our cluster individual cannot use Docker, nor can it be connected to the Internet, so we have to choose between mirroring and Call-cache. Thank you very much for your answer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047374011
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047374011:176,Usability,learn,learned,176,"> 是的，很酷，当您指定一个 docker 时，Cromwell 必须解析映像摘要才能使调用缓存正常工作。; > ; > 如果 Cromwell 无法在线找到 docker，或者您的代理阻止了 Cromwell，或者该映像不在线，则映像摘要无法正确解析，并且调用缓存处于禁用状态。; > ; > 这里有一些进一步的上下文： #6140. I also learned this from the official documents. Our cluster individual cannot use Docker, nor can it be connected to the Internet, so we have to choose between mirroring and Call-cache. Thank you very much for your answer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047374011
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:494,Deployability,update,updates,494,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:679,Energy Efficiency,efficient,efficient,679,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:730,Performance,queue,queue,730,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:937,Performance,perform,performance,937,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:116,Security,hash,hash,116,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:279,Security,hash,hash,279,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:28,Testability,test,tests,28,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:330,Usability,undo,undo,330,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436
https://github.com/broadinstitute/cromwell/pull/5427#issuecomment-591014236:66,Usability,learn,learned,66,"Thank you! This is a nice contrib on its own, and in reading it I learned of Oliver which I did not know before.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5427#issuecomment-591014236
https://github.com/broadinstitute/cromwell/pull/5432#issuecomment-591472639:338,Usability,learn,learn,338,"At 10am...; > created 9 hours ago. 😮 !. This is awesome, I really like this (the making things better with a new-to-cromwell technology, not just the 1am coding!). Bravo!. Today looks pretty busy but maybe a cross team tech talk about what this framework is, how we can use it, and what this PR is doing would be appropriate? I'd love to learn more at a high level as well as at an ""on the ground, code details"" level.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5432#issuecomment-591472639
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-623283470:24,Usability,feedback,feedback,24,Thanks everyone for the feedback and the merging!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-623283470
https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-785271100:178,Usability,clear,clear,178,"Hi all,. unless I'm wrong, I've got the same problem ( > 5 minutes for a Hello-World ) https://gist.github.com/lindenb/d89e69f31e1bc5d390dc043b48d651d9. I'm new to WDL. It's not clear to me if the `server` mode is the only 'true' way to run a WDL workflow (?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-785271100
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735:489,Deployability,release,release,489,"@aednichols Thanks again for allowing me push access to the repo. I can not test all the backends manually so it is good that I can access the CI environment and see if the bug fix turned out well. What is the formal process of getting this bug under the Cromwell team's attention? I have made a JIRA issue. Should I put it on the sprint? I also ask this for #5456 which is a really simple fix. I am not in great haste getting a review, but I want to ensure these fixes end up in the next release of Cromwell. These bugs are now actively blocking BioWDL development as our CI always uses a mainline version of Cromwell. (Usually the latest, but we are already actively excluding 49 because of the relative outputs bug). . By no means I want to push the Cromwell team in reviewing these fixes right now, but if you could give me some procedure that would make sure these are reviewed before the next release is out, that would give me some peace of mind. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735:899,Deployability,release,release,899,"@aednichols Thanks again for allowing me push access to the repo. I can not test all the backends manually so it is good that I can access the CI environment and see if the bug fix turned out well. What is the formal process of getting this bug under the Cromwell team's attention? I have made a JIRA issue. Should I put it on the sprint? I also ask this for #5456 which is a really simple fix. I am not in great haste getting a review, but I want to ensure these fixes end up in the next release of Cromwell. These bugs are now actively blocking BioWDL development as our CI always uses a mainline version of Cromwell. (Usually the latest, but we are already actively excluding 49 because of the relative outputs bug). . By no means I want to push the Cromwell team in reviewing these fixes right now, but if you could give me some procedure that would make sure these are reviewed before the next release is out, that would give me some peace of mind. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735:46,Security,access,access,46,"@aednichols Thanks again for allowing me push access to the repo. I can not test all the backends manually so it is good that I can access the CI environment and see if the bug fix turned out well. What is the formal process of getting this bug under the Cromwell team's attention? I have made a JIRA issue. Should I put it on the sprint? I also ask this for #5456 which is a really simple fix. I am not in great haste getting a review, but I want to ensure these fixes end up in the next release of Cromwell. These bugs are now actively blocking BioWDL development as our CI always uses a mainline version of Cromwell. (Usually the latest, but we are already actively excluding 49 because of the relative outputs bug). . By no means I want to push the Cromwell team in reviewing these fixes right now, but if you could give me some procedure that would make sure these are reviewed before the next release is out, that would give me some peace of mind. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735:132,Security,access,access,132,"@aednichols Thanks again for allowing me push access to the repo. I can not test all the backends manually so it is good that I can access the CI environment and see if the bug fix turned out well. What is the formal process of getting this bug under the Cromwell team's attention? I have made a JIRA issue. Should I put it on the sprint? I also ask this for #5456 which is a really simple fix. I am not in great haste getting a review, but I want to ensure these fixes end up in the next release of Cromwell. These bugs are now actively blocking BioWDL development as our CI always uses a mainline version of Cromwell. (Usually the latest, but we are already actively excluding 49 because of the relative outputs bug). . By no means I want to push the Cromwell team in reviewing these fixes right now, but if you could give me some procedure that would make sure these are reviewed before the next release is out, that would give me some peace of mind. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735:76,Testability,test,test,76,"@aednichols Thanks again for allowing me push access to the repo. I can not test all the backends manually so it is good that I can access the CI environment and see if the bug fix turned out well. What is the formal process of getting this bug under the Cromwell team's attention? I have made a JIRA issue. Should I put it on the sprint? I also ask this for #5456 which is a really simple fix. I am not in great haste getting a review, but I want to ensure these fixes end up in the next release of Cromwell. These bugs are now actively blocking BioWDL development as our CI always uses a mainline version of Cromwell. (Usually the latest, but we are already actively excluding 49 because of the relative outputs bug). . By no means I want to push the Cromwell team in reviewing these fixes right now, but if you could give me some procedure that would make sure these are reviewed before the next release is out, that would give me some peace of mind. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735:383,Usability,simpl,simple,383,"@aednichols Thanks again for allowing me push access to the repo. I can not test all the backends manually so it is good that I can access the CI environment and see if the bug fix turned out well. What is the formal process of getting this bug under the Cromwell team's attention? I have made a JIRA issue. Should I put it on the sprint? I also ask this for #5456 which is a really simple fix. I am not in great haste getting a review, but I want to ensure these fixes end up in the next release of Cromwell. These bugs are now actively blocking BioWDL development as our CI always uses a mainline version of Cromwell. (Usually the latest, but we are already actively excluding 49 because of the relative outputs bug). . By no means I want to push the Cromwell team in reviewing these fixes right now, but if you could give me some procedure that would make sure these are reviewed before the next release is out, that would give me some peace of mind. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735
https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527:724,Availability,error,errors,724,"### Progress update . I made a fix in liquibase and it was pulled. As a result I can now successfully create a correct cromwell database in SQLite. (At least I think so). There are some problems where SQLite has different types. (I.e. a timestamp is a text field, there is no separate TIMESTAMP column type only TEXT). This causes issues mainly in the testing, but I should be able to resolve this. There are some issues during the testing where Slick (?) or some other part does not seem to recognize foreign keys, primary keys and unique constraints, despite these being clearly there when I look at them with sqlitebrowser. This will require some more digging. As of yet running cromwell in server mode still spawns some errors when using a sqlite database, so I guess it will take some time before I have everything figured out. . Pinging @aednichols so at least someone in the Cromwell team knows this is ongoing :slightly_smiling_face: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527
https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527:13,Deployability,update,update,13,"### Progress update . I made a fix in liquibase and it was pulled. As a result I can now successfully create a correct cromwell database in SQLite. (At least I think so). There are some problems where SQLite has different types. (I.e. a timestamp is a text field, there is no separate TIMESTAMP column type only TEXT). This causes issues mainly in the testing, but I should be able to resolve this. There are some issues during the testing where Slick (?) or some other part does not seem to recognize foreign keys, primary keys and unique constraints, despite these being clearly there when I look at them with sqlitebrowser. This will require some more digging. As of yet running cromwell in server mode still spawns some errors when using a sqlite database, so I guess it will take some time before I have everything figured out. . Pinging @aednichols so at least someone in the Cromwell team knows this is ongoing :slightly_smiling_face: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527
https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527:352,Testability,test,testing,352,"### Progress update . I made a fix in liquibase and it was pulled. As a result I can now successfully create a correct cromwell database in SQLite. (At least I think so). There are some problems where SQLite has different types. (I.e. a timestamp is a text field, there is no separate TIMESTAMP column type only TEXT). This causes issues mainly in the testing, but I should be able to resolve this. There are some issues during the testing where Slick (?) or some other part does not seem to recognize foreign keys, primary keys and unique constraints, despite these being clearly there when I look at them with sqlitebrowser. This will require some more digging. As of yet running cromwell in server mode still spawns some errors when using a sqlite database, so I guess it will take some time before I have everything figured out. . Pinging @aednichols so at least someone in the Cromwell team knows this is ongoing :slightly_smiling_face: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527
https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527:432,Testability,test,testing,432,"### Progress update . I made a fix in liquibase and it was pulled. As a result I can now successfully create a correct cromwell database in SQLite. (At least I think so). There are some problems where SQLite has different types. (I.e. a timestamp is a text field, there is no separate TIMESTAMP column type only TEXT). This causes issues mainly in the testing, but I should be able to resolve this. There are some issues during the testing where Slick (?) or some other part does not seem to recognize foreign keys, primary keys and unique constraints, despite these being clearly there when I look at them with sqlitebrowser. This will require some more digging. As of yet running cromwell in server mode still spawns some errors when using a sqlite database, so I guess it will take some time before I have everything figured out. . Pinging @aednichols so at least someone in the Cromwell team knows this is ongoing :slightly_smiling_face: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527
https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527:573,Usability,clear,clearly,573,"### Progress update . I made a fix in liquibase and it was pulled. As a result I can now successfully create a correct cromwell database in SQLite. (At least I think so). There are some problems where SQLite has different types. (I.e. a timestamp is a text field, there is no separate TIMESTAMP column type only TEXT). This causes issues mainly in the testing, but I should be able to resolve this. There are some issues during the testing where Slick (?) or some other part does not seem to recognize foreign keys, primary keys and unique constraints, despite these being clearly there when I look at them with sqlitebrowser. This will require some more digging. As of yet running cromwell in server mode still spawns some errors when using a sqlite database, so I guess it will take some time before I have everything figured out. . Pinging @aednichols so at least someone in the Cromwell team knows this is ongoing :slightly_smiling_face: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527
https://github.com/broadinstitute/cromwell/pull/5493#issuecomment-619068749:90,Usability,feedback,feedback,90,"@rhpvorderman very much appreciated, thank you!. We're not python experts so this kind of feedback is extremely useful!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5493#issuecomment-619068749
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492:397,Availability,redundant,redundant,397,"@cjllanwarne @aednichols Sorry for my late reaction, I was on holiday last week. I have removed the forInput variable entirely thanks to @cjllanwarne's feedback. Instead I created the `makeInputSpecificFunctions` in the `IoFunctionSet` trait so every backend can use it. I then overrided it in the sfsBackend to return another class with a different postmapping. This made the `forInput` variable redundant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492:109,Modifiability,variab,variable,109,"@cjllanwarne @aednichols Sorry for my late reaction, I was on holiday last week. I have removed the forInput variable entirely thanks to @cjllanwarne's feedback. Instead I created the `makeInputSpecificFunctions` in the `IoFunctionSet` trait so every backend can use it. I then overrided it in the sfsBackend to return another class with a different postmapping. This made the `forInput` variable redundant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492:388,Modifiability,variab,variable,388,"@cjllanwarne @aednichols Sorry for my late reaction, I was on holiday last week. I have removed the forInput variable entirely thanks to @cjllanwarne's feedback. Instead I created the `makeInputSpecificFunctions` in the `IoFunctionSet` trait so every backend can use it. I then overrided it in the sfsBackend to return another class with a different postmapping. This made the `forInput` variable redundant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492:397,Safety,redund,redundant,397,"@cjllanwarne @aednichols Sorry for my late reaction, I was on holiday last week. I have removed the forInput variable entirely thanks to @cjllanwarne's feedback. Instead I created the `makeInputSpecificFunctions` in the `IoFunctionSet` trait so every backend can use it. I then overrided it in the sfsBackend to return another class with a different postmapping. This made the `forInput` variable redundant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492:152,Usability,feedback,feedback,152,"@cjllanwarne @aednichols Sorry for my late reaction, I was on holiday last week. I have removed the forInput variable entirely thanks to @cjllanwarne's feedback. Instead I created the `makeInputSpecificFunctions` in the `IoFunctionSet` trait so every backend can use it. I then overrided it in the sfsBackend to return another class with a different postmapping. This made the `forInput` variable redundant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492
https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-632668244:404,Energy Efficiency,monitor,monitor,404,"If my question isnt clear enough, let me expand some. I need to set AWS_BATCH_JOB_ATTEMPTS because AWS will terminate my jobs in the middle of them if my spot instance request is outbid (usually the only higher bids are on demand prices). AWS_BATCH_JOB_ATTEMPTS will allow me to tell aws that when I job is stopped for that reason, it will restart the job automatically without me needing to continually monitor it, Is there any way to do this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-632668244
https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-632668244:20,Usability,clear,clear,20,"If my question isnt clear enough, let me expand some. I need to set AWS_BATCH_JOB_ATTEMPTS because AWS will terminate my jobs in the middle of them if my spot instance request is outbid (usually the only higher bids are on demand prices). AWS_BATCH_JOB_ATTEMPTS will allow me to tell aws that when I job is stopped for that reason, it will restart the job automatically without me needing to continually monitor it, Is there any way to do this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-632668244
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628453342:423,Performance,load,load,423,"@vsoch and @illusional I have processed your comments. I made it more clear why the `--containall` flag is so important and I dropped the list of stuff that Singularity does without the flag. Instead I focused on the way Singularity affects reproducibility and how this can be prevented by the containall flag. I have also removed any references to a particular version of Singularity, and provided alternatives for module load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628453342
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628453342:70,Usability,clear,clear,70,"@vsoch and @illusional I have processed your comments. I made it more clear why the `--containall` flag is so important and I dropped the list of stuff that Singularity does without the flag. Instead I focused on the way Singularity affects reproducibility and how this can be prevented by the containall flag. I have also removed any references to a particular version of Singularity, and provided alternatives for module load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628453342
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541:147,Availability,echo,echo,147,"For further clarification. There are two `exec` statements in the config. The `exec` statement **before** the submit command will execute a simple echo command. If singularity notices the image is not there it will pull it to `SINGULARITY_CACHEDIR`. ; Then the job is submitted and the `exec` statement **in** the submit command is executed. If `SINGULARITY_CACHEDIR` is on a shared filesystem the image will already be present. The image will *not* be pulled by the execution node and the job executes right away. Using `exec` instead of `pull` means that Singularity will decide where the image goes, and not the user. This is quite useful as Singularity has all the functionality to make a functional image cache already built-in. This way we don't have to hack it together in bash, which is always the less preferable option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541:66,Modifiability,config,config,66,"For further clarification. There are two `exec` statements in the config. The `exec` statement **before** the submit command will execute a simple echo command. If singularity notices the image is not there it will pull it to `SINGULARITY_CACHEDIR`. ; Then the job is submitted and the `exec` statement **in** the submit command is executed. If `SINGULARITY_CACHEDIR` is on a shared filesystem the image will already be present. The image will *not* be pulled by the execution node and the job executes right away. Using `exec` instead of `pull` means that Singularity will decide where the image goes, and not the user. This is quite useful as Singularity has all the functionality to make a functional image cache already built-in. This way we don't have to hack it together in bash, which is always the less preferable option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541:710,Performance,cache,cache,710,"For further clarification. There are two `exec` statements in the config. The `exec` statement **before** the submit command will execute a simple echo command. If singularity notices the image is not there it will pull it to `SINGULARITY_CACHEDIR`. ; Then the job is submitted and the `exec` statement **in** the submit command is executed. If `SINGULARITY_CACHEDIR` is on a shared filesystem the image will already be present. The image will *not* be pulled by the execution node and the job executes right away. Using `exec` instead of `pull` means that Singularity will decide where the image goes, and not the user. This is quite useful as Singularity has all the functionality to make a functional image cache already built-in. This way we don't have to hack it together in bash, which is always the less preferable option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541:140,Usability,simpl,simple,140,"For further clarification. There are two `exec` statements in the config. The `exec` statement **before** the submit command will execute a simple echo command. If singularity notices the image is not there it will pull it to `SINGULARITY_CACHEDIR`. ; Then the job is submitted and the `exec` statement **in** the submit command is executed. If `SINGULARITY_CACHEDIR` is on a shared filesystem the image will already be present. The image will *not* be pulled by the execution node and the job executes right away. Using `exec` instead of `pull` means that Singularity will decide where the image goes, and not the user. This is quite useful as Singularity has all the functionality to make a functional image cache already built-in. This way we don't have to hack it together in bash, which is always the less preferable option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054:61,Availability,down,down,61,"Since it's still crickets chirping upstream, and quay.io was down again yesterday, I decided to work a bit on this permanent cache thing. Nobody likes bash scripts. But are python scripts okay? I was wondering what you think about this python script @TMiguelT, @illusional, @vsoch ? The PR where it is created is [here](https://github.com/biowdl/singularity-permanent-cache/pull/1). It is open for feedback. The python script:; - Pulls images to a single location based on environment or command line flag.; - Uses `singularity pull` as the backend.; - Returns the location of the image to stdout.; - Checks if an image is present in the cache. If so, it returns it and does not use `singularity pull` in that case. It does not require any internet connection in that case.; - Utilizes a filelock to prevent cache corruption. It uses flock to do this.; - Has no dependencies. Only a modern version of python is needed (3.5 or higher).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054:862,Integrability,depend,dependencies,862,"Since it's still crickets chirping upstream, and quay.io was down again yesterday, I decided to work a bit on this permanent cache thing. Nobody likes bash scripts. But are python scripts okay? I was wondering what you think about this python script @TMiguelT, @illusional, @vsoch ? The PR where it is created is [here](https://github.com/biowdl/singularity-permanent-cache/pull/1). It is open for feedback. The python script:; - Pulls images to a single location based on environment or command line flag.; - Uses `singularity pull` as the backend.; - Returns the location of the image to stdout.; - Checks if an image is present in the cache. If so, it returns it and does not use `singularity pull` in that case. It does not require any internet connection in that case.; - Utilizes a filelock to prevent cache corruption. It uses flock to do this.; - Has no dependencies. Only a modern version of python is needed (3.5 or higher).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054:125,Performance,cache,cache,125,"Since it's still crickets chirping upstream, and quay.io was down again yesterday, I decided to work a bit on this permanent cache thing. Nobody likes bash scripts. But are python scripts okay? I was wondering what you think about this python script @TMiguelT, @illusional, @vsoch ? The PR where it is created is [here](https://github.com/biowdl/singularity-permanent-cache/pull/1). It is open for feedback. The python script:; - Pulls images to a single location based on environment or command line flag.; - Uses `singularity pull` as the backend.; - Returns the location of the image to stdout.; - Checks if an image is present in the cache. If so, it returns it and does not use `singularity pull` in that case. It does not require any internet connection in that case.; - Utilizes a filelock to prevent cache corruption. It uses flock to do this.; - Has no dependencies. Only a modern version of python is needed (3.5 or higher).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054:368,Performance,cache,cache,368,"Since it's still crickets chirping upstream, and quay.io was down again yesterday, I decided to work a bit on this permanent cache thing. Nobody likes bash scripts. But are python scripts okay? I was wondering what you think about this python script @TMiguelT, @illusional, @vsoch ? The PR where it is created is [here](https://github.com/biowdl/singularity-permanent-cache/pull/1). It is open for feedback. The python script:; - Pulls images to a single location based on environment or command line flag.; - Uses `singularity pull` as the backend.; - Returns the location of the image to stdout.; - Checks if an image is present in the cache. If so, it returns it and does not use `singularity pull` in that case. It does not require any internet connection in that case.; - Utilizes a filelock to prevent cache corruption. It uses flock to do this.; - Has no dependencies. Only a modern version of python is needed (3.5 or higher).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054:638,Performance,cache,cache,638,"Since it's still crickets chirping upstream, and quay.io was down again yesterday, I decided to work a bit on this permanent cache thing. Nobody likes bash scripts. But are python scripts okay? I was wondering what you think about this python script @TMiguelT, @illusional, @vsoch ? The PR where it is created is [here](https://github.com/biowdl/singularity-permanent-cache/pull/1). It is open for feedback. The python script:; - Pulls images to a single location based on environment or command line flag.; - Uses `singularity pull` as the backend.; - Returns the location of the image to stdout.; - Checks if an image is present in the cache. If so, it returns it and does not use `singularity pull` in that case. It does not require any internet connection in that case.; - Utilizes a filelock to prevent cache corruption. It uses flock to do this.; - Has no dependencies. Only a modern version of python is needed (3.5 or higher).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054:808,Performance,cache,cache,808,"Since it's still crickets chirping upstream, and quay.io was down again yesterday, I decided to work a bit on this permanent cache thing. Nobody likes bash scripts. But are python scripts okay? I was wondering what you think about this python script @TMiguelT, @illusional, @vsoch ? The PR where it is created is [here](https://github.com/biowdl/singularity-permanent-cache/pull/1). It is open for feedback. The python script:; - Pulls images to a single location based on environment or command line flag.; - Uses `singularity pull` as the backend.; - Returns the location of the image to stdout.; - Checks if an image is present in the cache. If so, it returns it and does not use `singularity pull` in that case. It does not require any internet connection in that case.; - Utilizes a filelock to prevent cache corruption. It uses flock to do this.; - Has no dependencies. Only a modern version of python is needed (3.5 or higher).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054:398,Usability,feedback,feedback,398,"Since it's still crickets chirping upstream, and quay.io was down again yesterday, I decided to work a bit on this permanent cache thing. Nobody likes bash scripts. But are python scripts okay? I was wondering what you think about this python script @TMiguelT, @illusional, @vsoch ? The PR where it is created is [here](https://github.com/biowdl/singularity-permanent-cache/pull/1). It is open for feedback. The python script:; - Pulls images to a single location based on environment or command line flag.; - Uses `singularity pull` as the backend.; - Returns the location of the image to stdout.; - Checks if an image is present in the cache. If so, it returns it and does not use `singularity pull` in that case. It does not require any internet connection in that case.; - Utilizes a filelock to prevent cache corruption. It uses flock to do this.; - Has no dependencies. Only a modern version of python is needed (3.5 or higher).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054
https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284:260,Availability,error,error,260,"@illusional The key part was this:; ```; runtime-attributes = """"""; Int cpu = 1; String? memory; """"""; ```; plus passing `memory` to the submit command - which in my case is a wrapper script that can interpret strings like ""4GB"". After another hour of trial-and-error I did finally get a working config using `memory_gb` in the config file instead of `memory`. But I have to say, a colleague and I read the documentation on memory repeatedly and we're still confused about what it's *trying* to say and how that relates to what actually happens. (In particular, it is not clear that you can't pass `memory` directly to non-SFS backends.) The fact that both `memory` and `memory_gb` can be task runtime parameters, yet one will presumably be overridden anyway, seems unwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284
https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284:174,Integrability,wrap,wrapper,174,"@illusional The key part was this:; ```; runtime-attributes = """"""; Int cpu = 1; String? memory; """"""; ```; plus passing `memory` to the submit command - which in my case is a wrapper script that can interpret strings like ""4GB"". After another hour of trial-and-error I did finally get a working config using `memory_gb` in the config file instead of `memory`. But I have to say, a colleague and I read the documentation on memory repeatedly and we're still confused about what it's *trying* to say and how that relates to what actually happens. (In particular, it is not clear that you can't pass `memory` directly to non-SFS backends.) The fact that both `memory` and `memory_gb` can be task runtime parameters, yet one will presumably be overridden anyway, seems unwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284
https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284:294,Modifiability,config,config,294,"@illusional The key part was this:; ```; runtime-attributes = """"""; Int cpu = 1; String? memory; """"""; ```; plus passing `memory` to the submit command - which in my case is a wrapper script that can interpret strings like ""4GB"". After another hour of trial-and-error I did finally get a working config using `memory_gb` in the config file instead of `memory`. But I have to say, a colleague and I read the documentation on memory repeatedly and we're still confused about what it's *trying* to say and how that relates to what actually happens. (In particular, it is not clear that you can't pass `memory` directly to non-SFS backends.) The fact that both `memory` and `memory_gb` can be task runtime parameters, yet one will presumably be overridden anyway, seems unwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284
https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284:326,Modifiability,config,config,326,"@illusional The key part was this:; ```; runtime-attributes = """"""; Int cpu = 1; String? memory; """"""; ```; plus passing `memory` to the submit command - which in my case is a wrapper script that can interpret strings like ""4GB"". After another hour of trial-and-error I did finally get a working config using `memory_gb` in the config file instead of `memory`. But I have to say, a colleague and I read the documentation on memory repeatedly and we're still confused about what it's *trying* to say and how that relates to what actually happens. (In particular, it is not clear that you can't pass `memory` directly to non-SFS backends.) The fact that both `memory` and `memory_gb` can be task runtime parameters, yet one will presumably be overridden anyway, seems unwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284
https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284:570,Usability,clear,clear,570,"@illusional The key part was this:; ```; runtime-attributes = """"""; Int cpu = 1; String? memory; """"""; ```; plus passing `memory` to the submit command - which in my case is a wrapper script that can interpret strings like ""4GB"". After another hour of trial-and-error I did finally get a working config using `memory_gb` in the config file instead of `memory`. But I have to say, a colleague and I read the documentation on memory repeatedly and we're still confused about what it's *trying* to say and how that relates to what actually happens. (In particular, it is not clear that you can't pass `memory` directly to non-SFS backends.) The fact that both `memory` and `memory_gb` can be task runtime parameters, yet one will presumably be overridden anyway, seems unwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:42,Deployability,configurat,configuration,42,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:851,Deployability,update,update,851,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:42,Modifiability,config,configuration,42,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:894,Modifiability,config,config,894,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:65,Security,access,access,65,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:269,Security,access,access,269,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:330,Security,access,access,330,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:550,Security,audit,audit,550,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:366,Usability,usab,usable,366,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:668,Usability,usab,usable,668,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-663231595:72,Integrability,depend,dependencies,72,">The Guava in Cromwell appears to have it though, could add that to the dependencies. I think it'd be better/simpler to use Java 9 for this app rather than include another lib in the jar",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-663231595
https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-663231595:109,Usability,simpl,simpler,109,">The Guava in Cromwell appears to have it though, could add that to the dependencies. I think it'd be better/simpler to use Java 9 for this app rather than include another lib in the jar",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-663231595
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386:758,Availability,echo,echo,758,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386:991,Availability,error,error,991,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386:216,Testability,test,teste,216,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386:1022,Usability,simpl,simple,1022,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386:1209,Usability,simpl,simply,1209,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:2828,Availability,error,error,2828,"nfig {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:2883,Availability,failure,failure,2883,"nfig {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:3262,Availability,error,error,3262," Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:3522,Availability,error,error,3522,"his allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:4798,Availability,error,error,4798," failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.list is not supported for this resource.; ```; No luck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1223,Deployability,configurat,configuration,1223,"ons in Google Cloud. The admin that set up my account also had no idea what `compute.zones.list` mean. I don't see this in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A refere",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1383,Deployability,configurat,configuration,1383,"tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, n",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:4564,Deployability,configurat,configuration,4564," failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.list is not supported for this resource.; ```; No luck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1223,Modifiability,config,configuration,1223,"ons in Google Cloud. The admin that set up my account also had no idea what `compute.zones.list` mean. I don't see this in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A refere",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1383,Modifiability,config,configuration,1383,"tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, n",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1803,Modifiability,config,config,1803,"ding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:4564,Modifiability,config,configuration,4564," failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.list is not supported for this resource.; ```; No luck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:2136,Security,access,access,2136,"orage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:4941,Security,access,access,4941," failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.list is not supported for this resource.; ```; No luck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:2802,Testability,log,logic,2802,"nfig {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:3573,Testability,log,logs,3573,"service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:3730,Testability,log,log,3730,"service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:3752,Testability,log,log,3752,"or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:119,Usability,learn,learn,119,"The problem is that when I started to use Cromwell I was under the impression that I would get shielded from having to learn all the complexities of a given backend and Cromwell would take care of it. I have zero familiarity with permissions in Google Cloud. The admin that set up my account also had no idea what `compute.zones.list` mean. I don't see this in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; //",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:956,Usability,simpl,simply,956,"The problem is that when I started to use Cromwell I was under the impression that I would get shielded from having to learn all the complexities of a given backend and Cromwell would take care of it. I have zero familiarity with permissions in Google Cloud. The admin that set up my account also had no idea what `compute.zones.list` mean. I don't see this in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; //",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:1503,Availability,error,error,1503,"engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_M",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:130,Deployability,configurat,configuration,130,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:510,Deployability,configurat,configuration,510,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:575,Deployability,configurat,configuration,575,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:753,Deployability,configurat,configuration,753,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:822,Deployability,configurat,configuration,822,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:130,Modifiability,config,configuration,130,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:510,Modifiability,config,configuration,510,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:575,Modifiability,config,configuration,575,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:753,Modifiability,config,configuration,753,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:822,Modifiability,config,configuration,822,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:1028,Modifiability,config,config,1028,"ibing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:2157,Modifiability,config,config,2157,"rovided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... As for Terra, I have used it quite a bit for the last week but, and I am not alone in saying this, Terra is not a good environment for development of new WDLs. For example, just to upload a new WDL for testing it takes so many steps. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:2709,Security,access,access,2709,"t least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... As for Terra, I have used it quite a bit for the last week but, and I am not alone in saying this, Terra is not a good environment for development of new WDLs. For example, just to upload a new WDL for testing it takes so many steps. Maybe once you have a polished WDL it is great for users with less technical expertise. But I want to use Cromwell to develop and test new WDLs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:2923,Security,access,access,2923,"t least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... As for Terra, I have used it quite a bit for the last week but, and I am not alone in saying this, Terra is not a good environment for development of new WDLs. For example, just to upload a new WDL for testing it takes so many steps. Maybe once you have a polished WDL it is great for users with less technical expertise. But I want to use Cromwell to develop and test new WDLs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:2988,Security,access,access,2988,"t least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... As for Terra, I have used it quite a bit for the last week but, and I am not alone in saying this, Terra is not a good environment for development of new WDLs. For example, just to upload a new WDL for testing it takes so many steps. Maybe once you have a polished WDL it is great for users with less technical expertise. But I want to use Cromwell to develop and test new WDLs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:975,Testability,log,logs,975,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:1112,Testability,log,logs,1112,"ibing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:1213,Testability,log,log,1213,"[page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:1384,Testability,log,logs,1384,"I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LO",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:3271,Testability,test,testing,3271,"t least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... As for Terra, I have used it quite a bit for the last week but, and I am not alone in saying this, Terra is not a good environment for development of new WDLs. For example, just to upload a new WDL for testing it takes so many steps. Maybe once you have a polished WDL it is great for users with less technical expertise. But I want to use Cromwell to develop and test new WDLs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:3433,Testability,test,test,3433,"t least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... As for Terra, I have used it quite a bit for the last week but, and I am not alone in saying this, Terra is not a good environment for development of new WDLs. For example, just to upload a new WDL for testing it takes so many steps. Maybe once you have a polished WDL it is great for users with less technical expertise. But I want to use Cromwell to develop and test new WDLs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:181,Usability,simpl,simple,181,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:2905,Usability,intuit,intuitive,2905,"t least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... As for Terra, I have used it quite a bit for the last week but, and I am not alone in saying this, Terra is not a good environment for development of new WDLs. For example, just to upload a new WDL for testing it takes so many steps. Maybe once you have a polished WDL it is great for users with less technical expertise. But I want to use Cromwell to develop and test new WDLs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:52,Deployability,configurat,configuration,52,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:52,Modifiability,config,configuration,52,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:927,Modifiability,config,configured,927,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:494,Security,access,access,494,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:559,Security,access,access,559,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:889,Security,access,access,889,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:1312,Security,access,access,1312,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:103,Usability,simpl,simple,103,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:476,Usability,intuit,intuitive,476,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:638,Availability,error,errors,638,"Since this post was about the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) being quite broken, here a few points:. 1) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; #Create a new service account called ""MyServiceAccount"", and from the output of the command, take the email address that was generated; EMAIL=$(gcloud beta iam service-accounts create MyServiceAccount --description ""to run cromwell"" --display-name ""cromwell service account"" --format json | jq '.email' | sed -e 's/\""//g'); ```; does not work. It errors out with:; ```; ERROR: (gcloud.beta.iam.service-accounts.create) argument NAME: Bad value [MyServiceAccount]: Service account name must be between 6 and 30 characters (inclusive), must begin with a lowercase letter, and consist of lowercase alphanumeric characters that can be separated by hyphens.; ```; I believe `MyServiceAccount` needs to change to `my-service-account` (similarly to how it is used [here](https://cromwell.readthedocs.io/en/stable/backends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:1662,Availability,error,errors,1662,"service-accounts.create) argument NAME: Bad value [MyServiceAccount]: Service account name must be between 6 and 30 characters (inclusive), must begin with a lowercase letter, and consist of lowercase alphanumeric characters that can be separated by hyphens.; ```; I believe `MyServiceAccount` needs to change to `my-service-account` (similarly to how it is used [here](https://cromwell.readthedocs.io/en/stable/backends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:1395,Deployability,pipeline,pipelinesRunner,1395,"that was generated; EMAIL=$(gcloud beta iam service-accounts create MyServiceAccount --description ""to run cromwell"" --display-name ""cromwell service account"" --format json | jq '.email' | sed -e 's/\""//g'); ```; does not work. It errors out with:; ```; ERROR: (gcloud.beta.iam.service-accounts.create) argument NAME: Bad value [MyServiceAccount]: Service account name must be between 6 and 30 characters (inclusive), must begin with a lowercase letter, and consist of lowercase alphanumeric characters that can be separated by hyphens.; ```; I believe `MyServiceAccount` needs to change to `my-service-account` (similarly to how it is used [here](https://cromwell.readthedocs.io/en/stable/backends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutori",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2084,Deployability,pipeline,pipelinesRunner,2084,"ackends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cro",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2567,Deployability,configurat,configuration,2567,"g to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleCon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2663,Deployability,configurat,configuration,2663,"g to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleCon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2791,Deployability,configurat,configuration,2791,"int-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleConfigurationSpec.scala) it should be defined as:; ```; {; name = ""user-account""; scheme = ""user_account""; user = ""me""; secrets-file = ""/very/secret/file.txt""; data-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2567,Modifiability,config,configuration,2567,"g to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleCon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2663,Modifiability,config,configuration,2663,"g to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleCon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2791,Modifiability,config,configuration,2791,"int-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleConfigurationSpec.scala) it should be defined as:; ```; {; name = ""user-account""; scheme = ""user_account""; user = ""me""; secrets-file = ""/very/secret/file.txt""; data-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2914,Modifiability,config,configured,2914,"m-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleConfigurationSpec.scala) it should be defined as:; ```; {; name = ""user-account""; scheme = ""user_account""; user = ""me""; secrets-file = ""/very/secret/file.txt""; data-store-dir = ""/where/the/data/at""; }; ```; But I was not able to get it to work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2594,Security,authoriz,authorization,2594,"g to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleCon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2935,Security,authoriz,authorization,2935,"m-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleConfigurationSpec.scala) it should be defined as:; ```; {; name = ""user-account""; scheme = ""user_account""; user = ""me""; secrets-file = ""/very/secret/file.txt""; data-store-dir = ""/where/the/data/at""; }; ```; But I was not able to get it to work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:3574,Testability,test,test,3574,"m-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleConfigurationSpec.scala) it should be defined as:; ```; {; name = ""user-account""; scheme = ""user_account""; user = ""me""; secrets-file = ""/very/secret/file.txt""; data-store-dir = ""/where/the/data/at""; }; ```; But I was not able to get it to work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:1986,Usability,clear,clearly,1986,"y-service-account` (similarly to how it is used [here](https://cromwell.readthedocs.io/en/stable/backends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it ve",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2450,Usability,guid,guides,2450,"bjects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further ex",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/pull/5724#issuecomment-671513896:101,Usability,simpl,simple,101,But you could probably also make a list of all the files in a gs directory and their crc32c's with a simple gsutil loop. I thought we wanted to make sure we get the files which are actually on the disk image?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5724#issuecomment-671513896
https://github.com/broadinstitute/cromwell/pull/5724#issuecomment-671534634:517,Availability,down,download,517,"> But you could probably also make a list of all the files in a gs directory and their crc32c's with a simple gsutil loop. I thought we wanted to make sure we get the files which are actually on the disk image?. The problem with this value is that it should be >= than size of the disk from which you created the image, not just the sum of file sizes. So the whole image/manifest creation algorithm is like this:. 1. You use gsutil to see bucket size; 2. You create empty disk of the size >= than bucket size; 3. You download files to that disk and create image from it.; 4. You run manifest creator app against that disk feeding it with the disk size.; 5. You delete the disk.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5724#issuecomment-671534634
https://github.com/broadinstitute/cromwell/pull/5724#issuecomment-671534634:103,Usability,simpl,simple,103,"> But you could probably also make a list of all the files in a gs directory and their crc32c's with a simple gsutil loop. I thought we wanted to make sure we get the files which are actually on the disk image?. The problem with this value is that it should be >= than size of the disk from which you created the image, not just the sum of file sizes. So the whole image/manifest creation algorithm is like this:. 1. You use gsutil to see bucket size; 2. You create empty disk of the size >= than bucket size; 3. You download files to that disk and create image from it.; 4. You run manifest creator app against that disk feeding it with the disk size.; 5. You delete the disk.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5724#issuecomment-671534634
https://github.com/broadinstitute/cromwell/pull/5784#issuecomment-677038991:72,Modifiability,config,configs,72,"Yeah I thought it was weird too, I don't have it in any of my container configs. - `${script} is your full path, eg: `/Users/michael/path/to/execution/PIPELINE_NAME/<guid>/execution,; - `{docker_script}` is rebased from execution as `/cromwell-execution/PIPELINE_NAME/<guid>/execution`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5784#issuecomment-677038991
https://github.com/broadinstitute/cromwell/pull/5784#issuecomment-677038991:166,Usability,guid,guid,166,"Yeah I thought it was weird too, I don't have it in any of my container configs. - `${script} is your full path, eg: `/Users/michael/path/to/execution/PIPELINE_NAME/<guid>/execution,; - `{docker_script}` is rebased from execution as `/cromwell-execution/PIPELINE_NAME/<guid>/execution`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5784#issuecomment-677038991
https://github.com/broadinstitute/cromwell/pull/5784#issuecomment-677038991:269,Usability,guid,guid,269,"Yeah I thought it was weird too, I don't have it in any of my container configs. - `${script} is your full path, eg: `/Users/michael/path/to/execution/PIPELINE_NAME/<guid>/execution,; - `{docker_script}` is rebased from execution as `/cromwell-execution/PIPELINE_NAME/<guid>/execution`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5784#issuecomment-677038991
https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-756439506:304,Usability,simpl,simply,304,"Hi @nvanaja, thanks for your PR. Since you've got a few PRs open, we've added you as a collaborator so you can make branches directly in our repo. This enables our CI to run automatically for you (we are careful with it because it runs workflows in the cloud with real money). To take advantage of this, simply push a new branch to the broadinstitute/cromwell repo and create a new PR, for this one as well as https://github.com/broadinstitute/cromwell/pull/6058. Thank you again for your contributions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-756439506
https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-759700421:313,Usability,simpl,simply,313,"> Hi @nvanaja, thanks for your PR. Since you've got a few PRs open, we've added you as a collaborator so you can make branches directly in our repo. This enables our CI to run automatically for you (we are careful with it because it runs workflows in the cloud with real money).; > ; > To take advantage of this, simply push a new branch to the broadinstitute/cromwell repo and create a new PR, for this one as >well as #6058. Hi @aednichols, #6058 includes these changes also. So I don't need this PR anymore. #6058 will do.; I'll close this. ; Do I still need to create a new branch for #6058? ; Thanks,; Vanaja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-759700421
https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082:131,Modifiability,parameteriz,parameterized,131,"Thanks for the feedback. Can you elaborate more on the need to be able to; run a container as privileged?. It could (in theory) be parameterized if required but it seems hazardous to; have this be the default. On Tue, Jan 5, 2021 at 5:42 PM Shane Canon <notifications@github.com> wrote:. > We have a similar need. This overlaps a little with #4579; > <https://github.com/broadinstitute/cromwell/issues/4579>. It would be; > useful if the submit-docker was parameterized similar to how it is for some; > of the other backends.; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-754946394>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ELZQ5HVOSV2JRMIH3LSYOIVRANCNFSM4RQDAKPQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082
https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082:456,Modifiability,parameteriz,parameterized,456,"Thanks for the feedback. Can you elaborate more on the need to be able to; run a container as privileged?. It could (in theory) be parameterized if required but it seems hazardous to; have this be the default. On Tue, Jan 5, 2021 at 5:42 PM Shane Canon <notifications@github.com> wrote:. > We have a similar need. This overlaps a little with #4579; > <https://github.com/broadinstitute/cromwell/issues/4579>. It would be; > useful if the submit-docker was parameterized similar to how it is for some; > of the other backends.; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-754946394>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ELZQ5HVOSV2JRMIH3LSYOIVRANCNFSM4RQDAKPQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082
https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082:170,Safety,hazard,hazardous,170,"Thanks for the feedback. Can you elaborate more on the need to be able to; run a container as privileged?. It could (in theory) be parameterized if required but it seems hazardous to; have this be the default. On Tue, Jan 5, 2021 at 5:42 PM Shane Canon <notifications@github.com> wrote:. > We have a similar need. This overlaps a little with #4579; > <https://github.com/broadinstitute/cromwell/issues/4579>. It would be; > useful if the submit-docker was parameterized similar to how it is for some; > of the other backends.; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-754946394>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ELZQ5HVOSV2JRMIH3LSYOIVRANCNFSM4RQDAKPQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082
https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082:15,Usability,feedback,feedback,15,"Thanks for the feedback. Can you elaborate more on the need to be able to; run a container as privileged?. It could (in theory) be parameterized if required but it seems hazardous to; have this be the default. On Tue, Jan 5, 2021 at 5:42 PM Shane Canon <notifications@github.com> wrote:. > We have a similar need. This overlaps a little with #4579; > <https://github.com/broadinstitute/cromwell/issues/4579>. It would be; > useful if the submit-docker was parameterized similar to how it is for some; > of the other backends.; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-754946394>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ELZQ5HVOSV2JRMIH3LSYOIVRANCNFSM4RQDAKPQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082
https://github.com/broadinstitute/cromwell/issues/5966#issuecomment-714180309:124,Deployability,update,update,124,"Update:. Issue is that we were updating the resource requests and cromwell was seeing this as a new run, is there anyway to update the resource requests while allowing cromwell to resume the call-caching?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5966#issuecomment-714180309
https://github.com/broadinstitute/cromwell/issues/5966#issuecomment-714180309:180,Usability,resume,resume,180,"Update:. Issue is that we were updating the resource requests and cromwell was seeing this as a new run, is there anyway to update the resource requests while allowing cromwell to resume the call-caching?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5966#issuecomment-714180309
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:315,Availability,down,down,315,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:799,Deployability,pipeline,pipeline,799,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:872,Deployability,pipeline,pipeline,872,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:897,Performance,concurren,concurrently,897,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:25,Usability,feedback,feedback,25,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:270,Usability,pause,pause,270,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:463,Usability,simpl,simple,463,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:313,Availability,down,down,313,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:775,Deployability,pipeline,pipeline,775,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:845,Deployability,pipeline,pipeline,845,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:870,Performance,concurren,concurrently,870,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:26,Usability,feedback,feedback,26,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:269,Usability,pause,pause,269,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:460,Usability,simpl,simple,460,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:508,Availability,down,down,508,"Yes, the script epilogue is exactly where the change should be. The script; is generated by AwsBatchJob.scala. On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isn’t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > … <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/u",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:1006,Deployability,pipeline,pipeline,1006,"Yes, the script epilogue is exactly where the change should be. The script; is generated by AwsBatchJob.scala. On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isn’t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > … <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/u",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:1079,Deployability,pipeline,pipeline,1079," On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isn’t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > … <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA; > .; >; > Hi Mark,; >; > Thanks for your reply.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:1104,Performance,concurren,concurrently,1104," On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isn’t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > … <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA; > .; >; > Hi Mark,; >; > Thanks for your reply.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:209,Usability,feedback,feedback,209,"Yes, the script epilogue is exactly where the change should be. The script; is generated by AwsBatchJob.scala. On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isn’t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > … <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/u",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:461,Usability,pause,pause,461,"Yes, the script epilogue is exactly where the change should be. The script; is generated by AwsBatchJob.scala. On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isn’t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > … <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/u",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:661,Usability,simpl,simple,661,"Yes, the script epilogue is exactly where the change should be. The script; is generated by AwsBatchJob.scala. On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isn’t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > … <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/u",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:590,Availability,down,down,590,"However, AWS Batch backend ignores script-epilogue as unrecognized. Do you have any suggestions?. > Yes, the script epilogue is exactly where the change should be. The script is generated by AwsBatchJob.scala; > […](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark … <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probabl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:1064,Deployability,pipeline,pipeline,1064,"pt-epilogue as unrecognized. Do you have any suggestions?. > Yes, the script epilogue is exactly where the change should be. The script is generated by AwsBatchJob.scala; > […](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark … <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:1134,Deployability,pipeline,pipeline,1134,"he script is generated by AwsBatchJob.scala; > […](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark … <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:1159,Performance,concurren,concurrently,1159,"he script is generated by AwsBatchJob.scala; > […](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark … <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:303,Usability,feedback,feedback,303,"However, AWS Batch backend ignores script-epilogue as unrecognized. Do you have any suggestions?. > Yes, the script epilogue is exactly where the change should be. The script is generated by AwsBatchJob.scala; > […](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark … <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probabl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:546,Usability,pause,pause,546,"However, AWS Batch backend ignores script-epilogue as unrecognized. Do you have any suggestions?. > Yes, the script epilogue is exactly where the change should be. The script is generated by AwsBatchJob.scala; > […](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark … <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probabl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:737,Usability,simpl,simple,737,"However, AWS Batch backend ignores script-epilogue as unrecognized. Do you have any suggestions?. > Yes, the script epilogue is exactly where the change should be. The script is generated by AwsBatchJob.scala; > […](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark … <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probabl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:976,Availability,down,down,976,"The clean up instructions to delete the files should go in here; https://github.com/broadinstitute/cromwell/blob/bda6d9043de866b1542a58555dcb9c6070a7c7b5/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L164. After the delocalization happens. On Wed, Oct 28, 2020 at 9:42 PM Luyu <notifications@github.com> wrote:. > However, AWS Batch backend ignores script-epilogue as unrecognized. Do you; > have any suggestions?; >; > Yes, the script epilogue is exactly where the change should be. The script; > is generated by AwsBatchJob.scala; > … <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:1467,Deployability,pipeline,pipeline,1467,"stions?; >; > Yes, the script epilogue is exactly where the change should be. The script; > is generated by AwsBatchJob.scala; > … <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:1540,Deployability,pipeline,pipeline,1540," <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974> <#5974; > <https://github.c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:1565,Performance,concurren,concurrently,1565," <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974> <#5974; > <https://github.c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:680,Usability,feedback,feedback,680,"The clean up instructions to delete the files should go in here; https://github.com/broadinstitute/cromwell/blob/bda6d9043de866b1542a58555dcb9c6070a7c7b5/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L164. After the delocalization happens. On Wed, Oct 28, 2020 at 9:42 PM Luyu <notifications@github.com> wrote:. > However, AWS Batch backend ignores script-epilogue as unrecognized. Do you; > have any suggestions?; >; > Yes, the script epilogue is exactly where the change should be. The script; > is generated by AwsBatchJob.scala; > … <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:932,Usability,pause,pause,932,"The clean up instructions to delete the files should go in here; https://github.com/broadinstitute/cromwell/blob/bda6d9043de866b1542a58555dcb9c6070a7c7b5/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L164. After the delocalization happens. On Wed, Oct 28, 2020 at 9:42 PM Luyu <notifications@github.com> wrote:. > However, AWS Batch backend ignores script-epilogue as unrecognized. Do you; > have any suggestions?; >; > Yes, the script epilogue is exactly where the change should be. The script; > is generated by AwsBatchJob.scala; > … <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:1129,Usability,simpl,simple,1129,"555dcb9c6070a7c7b5/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L164. After the delocalization happens. On Wed, Oct 28, 2020 at 9:42 PM Luyu <notifications@github.com> wrote:. > However, AWS Batch backend ignores script-epilogue as unrecognized. Do you; > have any suggestions?; >; > Yes, the script epilogue is exactly where the change should be. The script; > is generated by AwsBatchJob.scala; > … <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:481,Deployability,update,updated,481,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:737,Deployability,pipeline,pipeline,737,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:509,Modifiability,config,config,509,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:218,Testability,test,testing,218,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:558,Testability,test,testing,558,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:625,Testability,test,testing,625,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:823,Testability,test,test,823,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:839,Testability,test,tests,839,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:429,Usability,clear,clearly,429,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627
https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-756438492:304,Usability,simpl,simply,304,"Hi @myazinn, thanks for your PR. Since you've got a few PRs open, we've added you as a collaborator so you can make branches directly in our repo. This enables our CI to run automatically for you (we are careful with it because it runs workflows in the cloud with real money). To take advantage of this, simply push a new branch to the `broadinstitute/cromwell` repo and create a new PR, for this one as well as https://github.com/broadinstitute/cromwell/pull/6072 and https://github.com/broadinstitute/cromwell/pull/6081. Thank you again for your contributions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-756438492
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754228301:40,Usability,feedback,feedback,40,"Thanks @cjllanwarne, I've actioned your feedback - I'll let you _Resolve conversation_.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754228301
https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308:90,Availability,failure,failure,90,"To clear up my previous comment, it does indeed look like coverage was just an `sbt` test failure issue. Though I'm not clear on why the tests were failing. 🤷🤔:insert_flaky_test_emoji:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308
https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308:85,Testability,test,test,85,"To clear up my previous comment, it does indeed look like coverage was just an `sbt` test failure issue. Though I'm not clear on why the tests were failing. 🤷🤔:insert_flaky_test_emoji:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308
https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308:137,Testability,test,tests,137,"To clear up my previous comment, it does indeed look like coverage was just an `sbt` test failure issue. Though I'm not clear on why the tests were failing. 🤷🤔:insert_flaky_test_emoji:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308
https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308:3,Usability,clear,clear,3,"To clear up my previous comment, it does indeed look like coverage was just an `sbt` test failure issue. Though I'm not clear on why the tests were failing. 🤷🤔:insert_flaky_test_emoji:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308
https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308:120,Usability,clear,clear,120,"To clear up my previous comment, it does indeed look like coverage was just an `sbt` test failure issue. Though I'm not clear on why the tests were failing. 🤷🤔:insert_flaky_test_emoji:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308
https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-921173810:156,Usability,feedback,feedback,156,"@aednichols In that case is there an alternative implementation path that would be acceptable? I am not wedded to this way of doing it, but I've heard zero feedback on what to do instead and I'm reluctant to start coding anything new until I know whether it will actually be reviewed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-921173810
https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484:494,Availability,avail,available,494,"Hi Jon,. This isn't directly supported right now. You could change the; CloudFormation template so that your customized mount is mounted to the EC2; nodes. It is also possible to mount EFS directly into an AWS Batch; container through the job definition but that would require changes in; Cromwell's AWS Batch backend. Using EFS with containers for Cromwell; workflows is something we are investigating but there are some stress tests; that we need to do at scale to see if sufficient IOPs are available. On Wed, May 5, 2021 at 11:29 AM microbioticajon ***@***.***>; wrote:. > Hi Guys,; >; > This is more of a question/request than a bug report. Apologies if this is; > not the place to ask.; >; > Im trying to run Cromwell with an AWS backend. A number of our workflows; > make extensive use of very large reference files. To avoid localising the; > same huge file over and over (wasting time and space) I want to copy these; > reference files to an additional volume during batch node initialisation; > and mount to each container (rather than using File arguments I would use a; > simple String argument to prevent localisation - I appreciate this is a; > hack). I am already doing this with a different pipeline framework with; > some success, however it requires the JobDefinition to specify the mount; > locations between the node(host) and job container; >; > Is it possible to provide additional mount/volume instructions to the aws; > batch backend in the cromwell.conf?; >; > If this is possible, I cannot see any specific examples in the Cromwell; > docs. If this is not currently possible, could I request adding the ability; > to define additional mount points as a feature request??; >; > Kind Regards,; > Jon; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6334>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EI7XBOPHMWSYW3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484
https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484:1207,Deployability,pipeline,pipeline,1207,"ted right now. You could change the; CloudFormation template so that your customized mount is mounted to the EC2; nodes. It is also possible to mount EFS directly into an AWS Batch; container through the job definition but that would require changes in; Cromwell's AWS Batch backend. Using EFS with containers for Cromwell; workflows is something we are investigating but there are some stress tests; that we need to do at scale to see if sufficient IOPs are available. On Wed, May 5, 2021 at 11:29 AM microbioticajon ***@***.***>; wrote:. > Hi Guys,; >; > This is more of a question/request than a bug report. Apologies if this is; > not the place to ask.; >; > Im trying to run Cromwell with an AWS backend. A number of our workflows; > make extensive use of very large reference files. To avoid localising the; > same huge file over and over (wasting time and space) I want to copy these; > reference files to an additional volume during batch node initialisation; > and mount to each container (rather than using File arguments I would use a; > simple String argument to prevent localisation - I appreciate this is a; > hack). I am already doing this with a different pipeline framework with; > some success, however it requires the JobDefinition to specify the mount; > locations between the node(host) and job container; >; > Is it possible to provide additional mount/volume instructions to the aws; > batch backend in the cromwell.conf?; >; > If this is possible, I cannot see any specific examples in the Cromwell; > docs. If this is not currently possible, could I request adding the ability; > to define additional mount points as a feature request??; >; > Kind Regards,; > Jon; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6334>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EI7XBOPHMWSYW3EDFTTMFP7HANCNFSM44FGDSRQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484
https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484:827,Safety,avoid,avoid,827,"Hi Jon,. This isn't directly supported right now. You could change the; CloudFormation template so that your customized mount is mounted to the EC2; nodes. It is also possible to mount EFS directly into an AWS Batch; container through the job definition but that would require changes in; Cromwell's AWS Batch backend. Using EFS with containers for Cromwell; workflows is something we are investigating but there are some stress tests; that we need to do at scale to see if sufficient IOPs are available. On Wed, May 5, 2021 at 11:29 AM microbioticajon ***@***.***>; wrote:. > Hi Guys,; >; > This is more of a question/request than a bug report. Apologies if this is; > not the place to ask.; >; > Im trying to run Cromwell with an AWS backend. A number of our workflows; > make extensive use of very large reference files. To avoid localising the; > same huge file over and over (wasting time and space) I want to copy these; > reference files to an additional volume during batch node initialisation; > and mount to each container (rather than using File arguments I would use a; > simple String argument to prevent localisation - I appreciate this is a; > hack). I am already doing this with a different pipeline framework with; > some success, however it requires the JobDefinition to specify the mount; > locations between the node(host) and job container; >; > Is it possible to provide additional mount/volume instructions to the aws; > batch backend in the cromwell.conf?; >; > If this is possible, I cannot see any specific examples in the Cromwell; > docs. If this is not currently possible, could I request adding the ability; > to define additional mount points as a feature request??; >; > Kind Regards,; > Jon; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6334>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EI7XBOPHMWSYW3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484
https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484:429,Testability,test,tests,429,"Hi Jon,. This isn't directly supported right now. You could change the; CloudFormation template so that your customized mount is mounted to the EC2; nodes. It is also possible to mount EFS directly into an AWS Batch; container through the job definition but that would require changes in; Cromwell's AWS Batch backend. Using EFS with containers for Cromwell; workflows is something we are investigating but there are some stress tests; that we need to do at scale to see if sufficient IOPs are available. On Wed, May 5, 2021 at 11:29 AM microbioticajon ***@***.***>; wrote:. > Hi Guys,; >; > This is more of a question/request than a bug report. Apologies if this is; > not the place to ask.; >; > Im trying to run Cromwell with an AWS backend. A number of our workflows; > make extensive use of very large reference files. To avoid localising the; > same huge file over and over (wasting time and space) I want to copy these; > reference files to an additional volume during batch node initialisation; > and mount to each container (rather than using File arguments I would use a; > simple String argument to prevent localisation - I appreciate this is a; > hack). I am already doing this with a different pipeline framework with; > some success, however it requires the JobDefinition to specify the mount; > locations between the node(host) and job container; >; > Is it possible to provide additional mount/volume instructions to the aws; > batch backend in the cromwell.conf?; >; > If this is possible, I cannot see any specific examples in the Cromwell; > docs. If this is not currently possible, could I request adding the ability; > to define additional mount points as a feature request??; >; > Kind Regards,; > Jon; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6334>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EI7XBOPHMWSYW3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484
https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484:1084,Usability,simpl,simple,1084,"Hi Jon,. This isn't directly supported right now. You could change the; CloudFormation template so that your customized mount is mounted to the EC2; nodes. It is also possible to mount EFS directly into an AWS Batch; container through the job definition but that would require changes in; Cromwell's AWS Batch backend. Using EFS with containers for Cromwell; workflows is something we are investigating but there are some stress tests; that we need to do at scale to see if sufficient IOPs are available. On Wed, May 5, 2021 at 11:29 AM microbioticajon ***@***.***>; wrote:. > Hi Guys,; >; > This is more of a question/request than a bug report. Apologies if this is; > not the place to ask.; >; > Im trying to run Cromwell with an AWS backend. A number of our workflows; > make extensive use of very large reference files. To avoid localising the; > same huge file over and over (wasting time and space) I want to copy these; > reference files to an additional volume during batch node initialisation; > and mount to each container (rather than using File arguments I would use a; > simple String argument to prevent localisation - I appreciate this is a; > hack). I am already doing this with a different pipeline framework with; > some success, however it requires the JobDefinition to specify the mount; > locations between the node(host) and job container; >; > Is it possible to provide additional mount/volume instructions to the aws; > batch backend in the cromwell.conf?; >; > If this is possible, I cannot see any specific examples in the Cromwell; > docs. If this is not currently possible, could I request adding the ability; > to define additional mount points as a feature request??; >; > Kind Regards,; > Jon; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6334>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EI7XBOPHMWSYW3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484
https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010:93,Availability,avail,available,93,"@KevinDuringWork, I guess thanks to you for making N2D, one of the best CPU families on GCP, available on Terra. T2D (AMD Milan) now offers the best price-to-performance ratio. I'm still new to software development. Could you add support to T2D or guide me on how I can accomplish this? thank you very much!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010
https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010:158,Performance,perform,performance,158,"@KevinDuringWork, I guess thanks to you for making N2D, one of the best CPU families on GCP, available on Terra. T2D (AMD Milan) now offers the best price-to-performance ratio. I'm still new to software development. Could you add support to T2D or guide me on how I can accomplish this? thank you very much!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010
https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010:248,Usability,guid,guide,248,"@KevinDuringWork, I guess thanks to you for making N2D, one of the best CPU families on GCP, available on Terra. T2D (AMD Milan) now offers the best price-to-performance ratio. I'm still new to software development. Could you add support to T2D or guide me on how I can accomplish this? thank you very much!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:769,Availability,down,download,769,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:158,Energy Efficiency,charge,charges,158,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:280,Energy Efficiency,charge,charges,280,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:401,Energy Efficiency,charge,charges,401,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:485,Energy Efficiency,charge,charges,485,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:143,Safety,risk,risk,143,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:1305,Usability,guid,guidelines,1305,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:16,Availability,echo,echo,16,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:382,Availability,down,downloading,382,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:178,Modifiability,variab,variable,178,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:217,Modifiability,config,config,217,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:272,Modifiability,config,config,272,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:857,Modifiability,config,configure,857,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:279,Performance,cache,cache,279,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:336,Performance,cache,cached,336,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:411,Performance,cache,cache,411,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:554,Performance,perform,perform,554,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:965,Performance,cache,cached,965,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:162,Usability,simpl,simply,162,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-1737682362:684,Usability,simpl,simple,684,"> Hi, I'm wondering what's the status here. We are bit by this and we really want to use Directory type because that saves a lot of troubles.; > ; > Without careful check, I'm wondering if it's just failed wom check and the String to Directory conversion actually works in Cromwell? If so, I'm wondering if [this code](https://github.com/broadinstitute/cromwell/blob/develop/wom/src/main/scala/wom/types/WomPrimitiveType.scala#L8-L9) is relevant and could partially fix the problem (not the `sub` function I think) if `WomUnlistedDirectoryType` is added to `WomStringType` coercion targets. Sorry for the random @ but may I get some eyes from contributors here? Just to see whether a simple fix on `coercionMap` is meaningful and helpful? Happy to contribute with PR. Maybe @aednichols ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-1737682362
https://github.com/broadinstitute/cromwell/pull/6641#issuecomment-1006911032:64,Safety,avoid,avoid,64,"There are a few follow ups that I will make new tickets for, to avoid delaying the merge on this PR:. 1. Renaming the method to processEventsAndEmitWarnings would make it more clear what's going on; 2. Accounting for metadata generated at the parent or root workflows level separately from contributions from their subworkflows",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6641#issuecomment-1006911032
https://github.com/broadinstitute/cromwell/pull/6641#issuecomment-1006911032:176,Usability,clear,clear,176,"There are a few follow ups that I will make new tickets for, to avoid delaying the merge on this PR:. 1. Renaming the method to processEventsAndEmitWarnings would make it more clear what's going on; 2. Accounting for metadata generated at the parent or root workflows level separately from contributions from their subworkflows",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6641#issuecomment-1006911032
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:62,Availability,error,errors,62,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:234,Availability,failure,failure,234,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:429,Availability,error,error,429,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:1242,Availability,error,error,1242,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:905,Deployability,update,update,905,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:1021,Deployability,update,update,1021,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:399,Energy Efficiency,reduce,reduce,399,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:107,Performance,cache,cache,107,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:264,Performance,cache,cache,264,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:799,Performance,cache,cache,799,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:322,Security,access,access,322,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:229,Testability,test,test,229,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:588,Testability,test,test,588,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:98,Usability,clear,clearing,98,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:687,Usability,feedback,feedback,687,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:787,Usability,clear,clear,787,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022339247:106,Testability,log,logic,106,"We discussed this and decided to go forward with this change. There isn't an easy way to change the retry logic on a per-test-format basis in Centaur, and adding one would be nontrivial. This change makes the behavior clear to future test-spelunkers and will be easy to walk back if it turns out the retries were useful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022339247
https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022339247:121,Testability,test,test-format,121,"We discussed this and decided to go forward with this change. There isn't an easy way to change the retry logic on a per-test-format basis in Centaur, and adding one would be nontrivial. This change makes the behavior clear to future test-spelunkers and will be easy to walk back if it turns out the retries were useful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022339247
https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022339247:234,Testability,test,test-spelunkers,234,"We discussed this and decided to go forward with this change. There isn't an easy way to change the retry logic on a per-test-format basis in Centaur, and adding one would be nontrivial. This change makes the behavior clear to future test-spelunkers and will be easy to walk back if it turns out the retries were useful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022339247
https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022339247:218,Usability,clear,clear,218,"We discussed this and decided to go forward with this change. There isn't an easy way to change the retry logic on a per-test-format basis in Centaur, and adding one would be nontrivial. This change makes the behavior clear to future test-spelunkers and will be easy to walk back if it turns out the retries were useful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022339247
https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142:882,Modifiability,refactor,refactoring,882,"@jgainerdewar Your guess was correct--Prometheus/Stackdriver labels have no concept of order, so that information is lost when the InstrumentationPath is used to derive that sort of name. The StatsD form effectively looses the additional ""what's a high variant part and what isn't"" info while the Prometheus form looses the ordering of those parts. This is normal in Prometheus-land and all the query systems are built on that assumption. . There'll definitely be tests for the Prometheus code when it's added to clearly demonstrate that--this is sorta a weird one because I want to lay the groundwork without actually adding full Prometheus support yet. [This test](https://github.com/broadinstitute/cromwell/pull/6681/files#diff-65d7248a0c8799434124fe6d53023d0d8ac3492d640bbe4510801d2981e0903fR42) and a few others check that the groundwork is working, it's just a bit of a weird refactoring until the Prometheus code takes advantage of it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142
https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142:464,Testability,test,tests,464,"@jgainerdewar Your guess was correct--Prometheus/Stackdriver labels have no concept of order, so that information is lost when the InstrumentationPath is used to derive that sort of name. The StatsD form effectively looses the additional ""what's a high variant part and what isn't"" info while the Prometheus form looses the ordering of those parts. This is normal in Prometheus-land and all the query systems are built on that assumption. . There'll definitely be tests for the Prometheus code when it's added to clearly demonstrate that--this is sorta a weird one because I want to lay the groundwork without actually adding full Prometheus support yet. [This test](https://github.com/broadinstitute/cromwell/pull/6681/files#diff-65d7248a0c8799434124fe6d53023d0d8ac3492d640bbe4510801d2981e0903fR42) and a few others check that the groundwork is working, it's just a bit of a weird refactoring until the Prometheus code takes advantage of it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142
https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142:661,Testability,test,test,661,"@jgainerdewar Your guess was correct--Prometheus/Stackdriver labels have no concept of order, so that information is lost when the InstrumentationPath is used to derive that sort of name. The StatsD form effectively looses the additional ""what's a high variant part and what isn't"" info while the Prometheus form looses the ordering of those parts. This is normal in Prometheus-land and all the query systems are built on that assumption. . There'll definitely be tests for the Prometheus code when it's added to clearly demonstrate that--this is sorta a weird one because I want to lay the groundwork without actually adding full Prometheus support yet. [This test](https://github.com/broadinstitute/cromwell/pull/6681/files#diff-65d7248a0c8799434124fe6d53023d0d8ac3492d640bbe4510801d2981e0903fR42) and a few others check that the groundwork is working, it's just a bit of a weird refactoring until the Prometheus code takes advantage of it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142
https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142:513,Usability,clear,clearly,513,"@jgainerdewar Your guess was correct--Prometheus/Stackdriver labels have no concept of order, so that information is lost when the InstrumentationPath is used to derive that sort of name. The StatsD form effectively looses the additional ""what's a high variant part and what isn't"" info while the Prometheus form looses the ordering of those parts. This is normal in Prometheus-land and all the query systems are built on that assumption. . There'll definitely be tests for the Prometheus code when it's added to clearly demonstrate that--this is sorta a weird one because I want to lay the groundwork without actually adding full Prometheus support yet. [This test](https://github.com/broadinstitute/cromwell/pull/6681/files#diff-65d7248a0c8799434124fe6d53023d0d8ac3492d640bbe4510801d2981e0903fR42) and a few others check that the groundwork is working, it's just a bit of a weird refactoring until the Prometheus code takes advantage of it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142
https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1171664673:40,Usability,learn,learn,40,"Thanks for the followup, interesting to learn you found a workaround. I'd be curious to see whether there is a simpler workaround involving a change to the directory you run Cromwell from. When I run with local Docker, Cromwell puts `cromwell-executions` at the same path as the executable, i.e. if I run Cromwell from; ```; /Users/anichols/Projects/cromwell; ```; I get a files at paths like; ```; /Users/anichols/Projects/cromwell/cromwell-executions/three_step/ce6a6385-a8d6-4532-9aa4-d2eacdd89f5b/call-cgrep/execution/rc; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1171664673
https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1171664673:111,Usability,simpl,simpler,111,"Thanks for the followup, interesting to learn you found a workaround. I'd be curious to see whether there is a simpler workaround involving a change to the directory you run Cromwell from. When I run with local Docker, Cromwell puts `cromwell-executions` at the same path as the executable, i.e. if I run Cromwell from; ```; /Users/anichols/Projects/cromwell; ```; I get a files at paths like; ```; /Users/anichols/Projects/cromwell/cromwell-executions/three_step/ce6a6385-a8d6-4532-9aa4-d2eacdd89f5b/call-cgrep/execution/rc; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1171664673
https://github.com/broadinstitute/cromwell/pull/6769#issuecomment-1138542829:17,Usability,simpl,simply,17,"I ran a job that simply sleeps for 120 seconds, then requested the timing diagram (""Before the fix""). Then I restarted Cromwell with the fix, and requested the timing diagram again (""After the fix""). Before the fix:; <img width=""940"" alt=""before fix"" src=""https://user-images.githubusercontent.com/5531017/170490489-f892596e-8770-41db-83df-2f6a8d0b859b.png"">. After the fix:; <img width=""940"" alt=""after fix"" src=""https://user-images.githubusercontent.com/5531017/170490551-d71fc7d8-c78d-4daa-ad11-d10a4a23db66.png"">. In the ""after the fix"" image, the purple bar is ""cromwell starting overhead"", and lasts about 60 seconds. The maroon bar is ""RunningJob"", and also last about 60 seconds. It seems like the Maroon bar should have also been ""RunningJob"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6769#issuecomment-1138542829
https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030:189,Availability,error,errors,189,"NOTE FOR REVIEWERS: I was hoping to get feedback for how to best implement `successHandler` located in `CromwellApiService.scala`. Passing it in the `askSubmit` function gives a variety of errors on github and in code. Put it before `case Success(w)` and I get an error in Github because of mismatched states. Put the handler anywhere else in the function and it's unreachable. Currently, only the `errorHandler` is being used in the `askSubmit` function. Do I need to implement the `successHandler` in the function? Or can I leave it as is?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030
https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030:264,Availability,error,error,264,"NOTE FOR REVIEWERS: I was hoping to get feedback for how to best implement `successHandler` located in `CromwellApiService.scala`. Passing it in the `askSubmit` function gives a variety of errors on github and in code. Put it before `case Success(w)` and I get an error in Github because of mismatched states. Put the handler anywhere else in the function and it's unreachable. Currently, only the `errorHandler` is being used in the `askSubmit` function. Do I need to implement the `successHandler` in the function? Or can I leave it as is?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030
https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030:399,Availability,error,errorHandler,399,"NOTE FOR REVIEWERS: I was hoping to get feedback for how to best implement `successHandler` located in `CromwellApiService.scala`. Passing it in the `askSubmit` function gives a variety of errors on github and in code. Put it before `case Success(w)` and I get an error in Github because of mismatched states. Put the handler anywhere else in the function and it's unreachable. Currently, only the `errorHandler` is being used in the `askSubmit` function. Do I need to implement the `successHandler` in the function? Or can I leave it as is?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030
https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030:40,Usability,feedback,feedback,40,"NOTE FOR REVIEWERS: I was hoping to get feedback for how to best implement `successHandler` located in `CromwellApiService.scala`. Passing it in the `askSubmit` function gives a variety of errors on github and in code. Put it before `case Success(w)` and I get an error in Github because of mismatched states. Put the handler anywhere else in the function and it's unreachable. Currently, only the `errorHandler` is being used in the `askSubmit` function. Do I need to implement the `successHandler` in the function? Or can I leave it as is?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030
https://github.com/broadinstitute/cromwell/issues/6827#issuecomment-2153074225:138,Deployability,deploy,deployment,138,At Fred Hutch we're using Github container registries and guiding people who are new to WDL and Cromwell to use them. They do work on our deployment of Cromwell but I can confirm that no tasks are ever call caching hits with ghcr.io containers. https://github.com/getwilds/wilds-docker-library,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6827#issuecomment-2153074225
https://github.com/broadinstitute/cromwell/issues/6827#issuecomment-2153074225:58,Usability,guid,guiding,58,At Fred Hutch we're using Github container registries and guiding people who are new to WDL and Cromwell to use them. They do work on our deployment of Cromwell but I can confirm that no tasks are ever call caching hits with ghcr.io containers. https://github.com/getwilds/wilds-docker-library,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6827#issuecomment-2153074225
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:623,Availability,error,error,623,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:980,Availability,error,error,980,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1449,Availability,error,error,1449,"ng(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(maybe_tsv)])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(maybe_tsv)'. Expected 'File' but got 'String?'_. In ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:263,Modifiability,variab,variable,263,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:291,Modifiability,variab,variable,291,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:314,Modifiability,variab,variable,314,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:354,Modifiability,variab,variable,354,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:389,Modifiability,variab,variable,389,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:428,Modifiability,variab,variable,428,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:830,Modifiability,variab,variable,830,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1119,Modifiability,variab,variable,1119,"ell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback val",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1387,Modifiability,variab,variable,1387,"ype(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(may",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1908,Modifiability,variab,variable,1908,"r, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(maybe_tsv)])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(maybe_tsv)'. Expected 'File' but got 'String?'_. In other words -- it is unnecessarily complicated to work with optionals, and the workarounds that do exist appear to be inconsistent. It seems this could all be sidestepped by having ""if X exists, do something to X"" logic. (I want to make clear I'm coming at this from a perspective of someone who uses WDL heavily, mostly in the context of Terra/Cromwell, and wants wider adoption. From my experience, oddities like this are serious obstacles for both newcome",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:48,Testability,test,testing,48,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1242,Testability,log,logical,1242,"ype(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(may",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:2640,Testability,log,logic,2640," do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(maybe_tsv)])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(maybe_tsv)'. Expected 'File' but got 'String?'_. In other words -- it is unnecessarily complicated to work with optionals, and the workarounds that do exist appear to be inconsistent. It seems this could all be sidestepped by having ""if X exists, do something to X"" logic. (I want to make clear I'm coming at this from a perspective of someone who uses WDL heavily, mostly in the context of Terra/Cromwell, and wants wider adoption. From my experience, oddities like this are serious obstacles for both newcomers and heavy users like myself.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1731,Usability,intuit,intuitive,1731,"o the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(maybe_tsv)])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(maybe_tsv)'. Expected 'File' but got 'String?'_. In other words -- it is unnecessarily complicated to work with optionals, and the workarounds that do exist appear to be inconsistent. It seems this could all be sidestepped by having ""if X exists, do something to X"" logic. (I want to make clear I'm coming at this from a perspective of someone who uses W",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1826,Usability,intuit,intuitive,1826,"r, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(maybe_tsv)])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(maybe_tsv)'. Expected 'File' but got 'String?'_. In other words -- it is unnecessarily complicated to work with optionals, and the workarounds that do exist appear to be inconsistent. It seems this could all be sidestepped by having ""if X exists, do something to X"" logic. (I want to make clear I'm coming at this from a perspective of someone who uses WDL heavily, mostly in the context of Terra/Cromwell, and wants wider adoption. From my experience, oddities like this are serious obstacles for both newcome",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:2663,Usability,clear,clear,2663," do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(maybe_tsv)])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(maybe_tsv)'. Expected 'File' but got 'String?'_. In other words -- it is unnecessarily complicated to work with optionals, and the workarounds that do exist appear to be inconsistent. It seems this could all be sidestepped by having ""if X exists, do something to X"" logic. (I want to make clear I'm coming at this from a perspective of someone who uses WDL heavily, mostly in the context of Terra/Cromwell, and wants wider adoption. From my experience, oddities like this are serious obstacles for both newcomers and heavy users like myself.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245900745:16,Usability,feedback,feedback,16,"Thanks for your feedback. Since this is more of a WDL spec item than a Cromwell one, I suggest creating a new [Issue](https://github.com/openwdl/wdl/issues), [PR](https://github.com/openwdl/wdl/pulls), or [Discussion](https://github.com/openwdl/wdl/discussions) over at OpenWDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245900745
https://github.com/broadinstitute/cromwell/pull/6954#issuecomment-1332204180:390,Security,access,access,390,"I did some cleanup on this yesterday. Things I was planning to do and didn't have time for due to prod incident:; * Check all language to be sure it's really clear when we're dealing with WSM auth tokens and when we're dealing with blob SAS tokens.; * Either a lot more comments throughout or one large comment with pointers throughout, to clarify the different paths we could take to blob access and when they're useful, what they mean.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6954#issuecomment-1332204180
https://github.com/broadinstitute/cromwell/pull/6954#issuecomment-1332204180:158,Usability,clear,clear,158,"I did some cleanup on this yesterday. Things I was planning to do and didn't have time for due to prod incident:; * Check all language to be sure it's really clear when we're dealing with WSM auth tokens and when we're dealing with blob SAS tokens.; * Either a lot more comments throughout or one large comment with pointers throughout, to clarify the different paths we could take to blob access and when they're useful, what they mean.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6954#issuecomment-1332204180
https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1351914943:157,Deployability,pipeline,pipeline,157,"@aofarrel Thanks a lot! This actually worked just fine till I hit the memory wall of 200Gb of RAM. In fact, my Docker launches a tool invoking a _Snakemake_ pipeline for genome inference, the fourth step of which requires 200Gb of memory. Now, prior to your suggestion my `Docker Engine` was running with 20Gb of RAM, I then pushed it to 120. This helped to go through the 3rd step of the _Snakemake_ pipeline, requiring 100Gb of memory and where previously my WDL run was terminated, still the entire script cannot complete its execution due to memory requirement. With that said, I might keep this issue open for a bit longer maybe someone can relate to this and, most importantly, someone without the high memory demand for the tool I'm using might actually get the job done with this simple workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1351914943
https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1351914943:401,Deployability,pipeline,pipeline,401,"@aofarrel Thanks a lot! This actually worked just fine till I hit the memory wall of 200Gb of RAM. In fact, my Docker launches a tool invoking a _Snakemake_ pipeline for genome inference, the fourth step of which requires 200Gb of memory. Now, prior to your suggestion my `Docker Engine` was running with 20Gb of RAM, I then pushed it to 120. This helped to go through the 3rd step of the _Snakemake_ pipeline, requiring 100Gb of memory and where previously my WDL run was terminated, still the entire script cannot complete its execution due to memory requirement. With that said, I might keep this issue open for a bit longer maybe someone can relate to this and, most importantly, someone without the high memory demand for the tool I'm using might actually get the job done with this simple workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1351914943
https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1351914943:788,Usability,simpl,simple,788,"@aofarrel Thanks a lot! This actually worked just fine till I hit the memory wall of 200Gb of RAM. In fact, my Docker launches a tool invoking a _Snakemake_ pipeline for genome inference, the fourth step of which requires 200Gb of memory. Now, prior to your suggestion my `Docker Engine` was running with 20Gb of RAM, I then pushed it to 120. This helped to go through the 3rd step of the _Snakemake_ pipeline, requiring 100Gb of memory and where previously my WDL run was terminated, still the entire script cannot complete its execution due to memory requirement. With that said, I might keep this issue open for a bit longer maybe someone can relate to this and, most importantly, someone without the high memory demand for the tool I'm using might actually get the job done with this simple workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1351914943
https://github.com/broadinstitute/cromwell/pull/7153#issuecomment-1577103887:152,Testability,test,testing,152,"I'm a bit wary of turning `simpleLooksParseable` into too much of an actual parser but it looks like we already have that functionality implemented, so testing is a no brainer 👍",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7153#issuecomment-1577103887
https://github.com/broadinstitute/cromwell/pull/7153#issuecomment-1577103887:27,Usability,simpl,simpleLooksParseable,27,"I'm a bit wary of turning `simpleLooksParseable` into too much of an actual parser but it looks like we already have that functionality implemented, so testing is a no brainer 👍",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7153#issuecomment-1577103887
https://github.com/broadinstitute/cromwell/issues/7171#issuecomment-1623729173:35,Usability,undo,undocumented,35,As mentioned in Slack archiving is undocumented/untested/unsupported on self-hosted instances. I'll leave the issue open on the off chance somebody actually got it working.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7171#issuecomment-1623729173
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648047643:147,Integrability,message,message,147,Google Auth is handled by Cromwell cloud support. Not the supported backend it appears. @aednichols - how is the service account provided? The log message is not clear. https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/main/scala/cromwell/cloudsupport/gcp/auth/GoogleAuthMode.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648047643
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648047643:143,Testability,log,log,143,Google Auth is handled by Cromwell cloud support. Not the supported backend it appears. @aednichols - how is the service account provided? The log message is not clear. https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/main/scala/cromwell/cloudsupport/gcp/auth/GoogleAuthMode.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648047643
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648047643:162,Usability,clear,clear,162,Google Auth is handled by Cromwell cloud support. Not the supported backend it appears. @aednichols - how is the service account provided? The log message is not clear. https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/main/scala/cromwell/cloudsupport/gcp/auth/GoogleAuthMode.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648047643
https://github.com/broadinstitute/cromwell/issues/7217#issuecomment-1717897693:60,Performance,perform,performing,60,"Per prior user feedback,; > [HSQLDB has got to be the worst performing embedded database designed in the history of mankind.](https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757). and we do not recommend it, like, at all, unless your use case is tiny. Likewise, `run` mode is; > [Appropriate for prototyping or demo use on a user's local machine. Features are limited and the web API is not supported.](https://cromwell.readthedocs.io/en/stable/Modes/). For real workflows – certainly anything running more than 1 hour – the path is `server` mode with a daemon-type relational DB like PostgreSQL or MySQL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7217#issuecomment-1717897693
https://github.com/broadinstitute/cromwell/issues/7217#issuecomment-1717897693:15,Usability,feedback,feedback,15,"Per prior user feedback,; > [HSQLDB has got to be the worst performing embedded database designed in the history of mankind.](https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757). and we do not recommend it, like, at all, unless your use case is tiny. Likewise, `run` mode is; > [Appropriate for prototyping or demo use on a user's local machine. Features are limited and the web API is not supported.](https://cromwell.readthedocs.io/en/stable/Modes/). For real workflows – certainly anything running more than 1 hour – the path is `server` mode with a daemon-type relational DB like PostgreSQL or MySQL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7217#issuecomment-1717897693
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506:46,Deployability,continuous,continuous,46,"I hear you @patmagee. Cromwell has had a SaaS continuous development model for a few years now, with new code going to Terra daily. We learned that most standalone Cromwell users at the Broad upgrade infrequently, such as every 6-12 months. Thus, we committed put in the effort for two ""shrink-wrapped"" releases per year so we can balance SaaS with standalone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506:192,Deployability,upgrade,upgrade,192,"I hear you @patmagee. Cromwell has had a SaaS continuous development model for a few years now, with new code going to Terra daily. We learned that most standalone Cromwell users at the Broad upgrade infrequently, such as every 6-12 months. Thus, we committed put in the effort for two ""shrink-wrapped"" releases per year so we can balance SaaS with standalone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506:303,Deployability,release,releases,303,"I hear you @patmagee. Cromwell has had a SaaS continuous development model for a few years now, with new code going to Terra daily. We learned that most standalone Cromwell users at the Broad upgrade infrequently, such as every 6-12 months. Thus, we committed put in the effort for two ""shrink-wrapped"" releases per year so we can balance SaaS with standalone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506:294,Integrability,wrap,wrapped,294,"I hear you @patmagee. Cromwell has had a SaaS continuous development model for a few years now, with new code going to Terra daily. We learned that most standalone Cromwell users at the Broad upgrade infrequently, such as every 6-12 months. Thus, we committed put in the effort for two ""shrink-wrapped"" releases per year so we can balance SaaS with standalone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506:135,Usability,learn,learned,135,"I hear you @patmagee. Cromwell has had a SaaS continuous development model for a few years now, with new code going to Terra daily. We learned that most standalone Cromwell users at the Broad upgrade infrequently, such as every 6-12 months. Thus, we committed put in the effort for two ""shrink-wrapped"" releases per year so we can balance SaaS with standalone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506
https://github.com/broadinstitute/cromwell/issues/7249#issuecomment-1858364933:158,Safety,safe,safety,158,"Hi, thanks for the reply. I was using `version development` which I had assumed meant it had at least the 1.1 features (including `None`). I agree using some safety checks and `select_first` can work, but often using the above paradigm with `None` can be a lot clearer/easier to work with in code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7249#issuecomment-1858364933
https://github.com/broadinstitute/cromwell/issues/7249#issuecomment-1858364933:261,Usability,clear,clearer,261,"Hi, thanks for the reply. I was using `version development` which I had assumed meant it had at least the 1.1 features (including `None`). I agree using some safety checks and `select_first` can work, but often using the above paradigm with `None` can be a lot clearer/easier to work with in code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7249#issuecomment-1858364933
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2104444317:77,Usability,feedback,feedback,77,"@aednichols I have addressed all my TODO items, it would be nice to get some feedback. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2104444317
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709:964,Availability,error,error,964,"Due to the activity noise, the comments are hidden, I'll post here for better visibility. > Request grouping. Originally, this was created because we hoped that Google had an alternative to Batch requests, by now, Google has confirmed that there is no way to do that. These are some notes from our internal discussions:; 1. The code becomes way simpler if this grouping gets removed.; 2. We have not checked the potential implications on creating a batch client for every request, or, reusing the same client for the application's lifecycle.; 3. Grouping requests could allow us to eventually implement streaming like fs2/akka-stream, which could allow us to throttle the requests, still, if Cromwell already does this in another layer, this becomes unnecessary. Given that the current code has been tested so many times, my suggestion is to keep the grouping and potentially remove it in another iteration. > Error codes. Google has confirmed that there are more error codes than what the grpc response provides, still, these can be found at the job events, hence, they need to be parsed from the strings (PAPI does something similar). But, this has not been done in this PR which is why I have removed a lot of code that is not necessary. In a following PR, we should implement part of this in order to handle preemption errors. See https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709:1323,Availability,error,errors,1323,"Due to the activity noise, the comments are hidden, I'll post here for better visibility. > Request grouping. Originally, this was created because we hoped that Google had an alternative to Batch requests, by now, Google has confirmed that there is no way to do that. These are some notes from our internal discussions:; 1. The code becomes way simpler if this grouping gets removed.; 2. We have not checked the potential implications on creating a batch client for every request, or, reusing the same client for the application's lifecycle.; 3. Grouping requests could allow us to eventually implement streaming like fs2/akka-stream, which could allow us to throttle the requests, still, if Cromwell already does this in another layer, this becomes unnecessary. Given that the current code has been tested so many times, my suggestion is to keep the grouping and potentially remove it in another iteration. > Error codes. Google has confirmed that there are more error codes than what the grpc response provides, still, these can be found at the job events, hence, they need to be parsed from the strings (PAPI does something similar). But, this has not been done in this PR which is why I have removed a lot of code that is not necessary. In a following PR, we should implement part of this in order to handle preemption errors. See https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709:659,Performance,throttle,throttle,659,"Due to the activity noise, the comments are hidden, I'll post here for better visibility. > Request grouping. Originally, this was created because we hoped that Google had an alternative to Batch requests, by now, Google has confirmed that there is no way to do that. These are some notes from our internal discussions:; 1. The code becomes way simpler if this grouping gets removed.; 2. We have not checked the potential implications on creating a batch client for every request, or, reusing the same client for the application's lifecycle.; 3. Grouping requests could allow us to eventually implement streaming like fs2/akka-stream, which could allow us to throttle the requests, still, if Cromwell already does this in another layer, this becomes unnecessary. Given that the current code has been tested so many times, my suggestion is to keep the grouping and potentially remove it in another iteration. > Error codes. Google has confirmed that there are more error codes than what the grpc response provides, still, these can be found at the job events, hence, they need to be parsed from the strings (PAPI does something similar). But, this has not been done in this PR which is why I have removed a lot of code that is not necessary. In a following PR, we should implement part of this in order to handle preemption errors. See https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709:800,Testability,test,tested,800,"Due to the activity noise, the comments are hidden, I'll post here for better visibility. > Request grouping. Originally, this was created because we hoped that Google had an alternative to Batch requests, by now, Google has confirmed that there is no way to do that. These are some notes from our internal discussions:; 1. The code becomes way simpler if this grouping gets removed.; 2. We have not checked the potential implications on creating a batch client for every request, or, reusing the same client for the application's lifecycle.; 3. Grouping requests could allow us to eventually implement streaming like fs2/akka-stream, which could allow us to throttle the requests, still, if Cromwell already does this in another layer, this becomes unnecessary. Given that the current code has been tested so many times, my suggestion is to keep the grouping and potentially remove it in another iteration. > Error codes. Google has confirmed that there are more error codes than what the grpc response provides, still, these can be found at the job events, hence, they need to be parsed from the strings (PAPI does something similar). But, this has not been done in this PR which is why I have removed a lot of code that is not necessary. In a following PR, we should implement part of this in order to handle preemption errors. See https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709:345,Usability,simpl,simpler,345,"Due to the activity noise, the comments are hidden, I'll post here for better visibility. > Request grouping. Originally, this was created because we hoped that Google had an alternative to Batch requests, by now, Google has confirmed that there is no way to do that. These are some notes from our internal discussions:; 1. The code becomes way simpler if this grouping gets removed.; 2. We have not checked the potential implications on creating a batch client for every request, or, reusing the same client for the application's lifecycle.; 3. Grouping requests could allow us to eventually implement streaming like fs2/akka-stream, which could allow us to throttle the requests, still, if Cromwell already does this in another layer, this becomes unnecessary. Given that the current code has been tested so many times, my suggestion is to keep the grouping and potentially remove it in another iteration. > Error codes. Google has confirmed that there are more error codes than what the grpc response provides, still, these can be found at the job events, hence, they need to be parsed from the strings (PAPI does something similar). But, this has not been done in this PR which is why I have removed a lot of code that is not necessary. In a following PR, we should implement part of this in order to handle preemption errors. See https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709
https://github.com/broadinstitute/cromwell/issues/7465#issuecomment-2203351311:124,Usability,resume,resume,124,"No, every workflow gets a new ID. You can re-submit with the same parameters and call caching, if enabled, should act as a ""resume"" function and re-use existing results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7465#issuecomment-2203351311
https://github.com/broadinstitute/cromwell/issues/7489#issuecomment-2299775834:202,Testability,log,logic,202,Hi @kachulis - this behavior is expected with GCP Batch as the persistent disk gets remounted to the replacement VM. There is not a way to specify a new disk on the Google Cloud side so please add some logic to your workflow to clear out files if present if that is needed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7489#issuecomment-2299775834
https://github.com/broadinstitute/cromwell/issues/7489#issuecomment-2299775834:228,Usability,clear,clear,228,Hi @kachulis - this behavior is expected with GCP Batch as the persistent disk gets remounted to the replacement VM. There is not a way to specify a new disk on the Google Cloud side so please add some logic to your workflow to clear out files if present if that is needed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7489#issuecomment-2299775834
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287238131:76,Testability,log,logs,76,> I will ask Google if there is something to propagate additional labels to logs. Thank you! This would be a big help in making the GCP Batch backend user-friendly at scale.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287238131
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287238131:150,Usability,user-friendly,user-friendly,150,> I will ask Google if there is something to propagate additional labels to logs. Thank you! This would be a big help in making the GCP Batch backend user-friendly at scale.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287238131
https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204:122,Deployability,release,release,122,"> Since this is a big change to the way we start jobs, I'm wondering if we want to include an ""off switch"" in the initial release. If we discovered a problem with this behavior and want to quickly revert to the old behavior, can we do that by setting the config to a 0 minute threshold? Should we build in an enabled flag for this behavior in config?. @jgainerdewar yes I had thought about that and setting the config to 0 should work. But I like your suggestion about having an actual config value like `enabled` instead which should make it more clear. Use the `enabled` flag would also be better so that if it is set to false the `JobTokenDsispenserActor` won't ask `GroupMetricsActor` about exhausted groups at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204
https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204:255,Modifiability,config,config,255,"> Since this is a big change to the way we start jobs, I'm wondering if we want to include an ""off switch"" in the initial release. If we discovered a problem with this behavior and want to quickly revert to the old behavior, can we do that by setting the config to a 0 minute threshold? Should we build in an enabled flag for this behavior in config?. @jgainerdewar yes I had thought about that and setting the config to 0 should work. But I like your suggestion about having an actual config value like `enabled` instead which should make it more clear. Use the `enabled` flag would also be better so that if it is set to false the `JobTokenDsispenserActor` won't ask `GroupMetricsActor` about exhausted groups at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204
https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204:343,Modifiability,config,config,343,"> Since this is a big change to the way we start jobs, I'm wondering if we want to include an ""off switch"" in the initial release. If we discovered a problem with this behavior and want to quickly revert to the old behavior, can we do that by setting the config to a 0 minute threshold? Should we build in an enabled flag for this behavior in config?. @jgainerdewar yes I had thought about that and setting the config to 0 should work. But I like your suggestion about having an actual config value like `enabled` instead which should make it more clear. Use the `enabled` flag would also be better so that if it is set to false the `JobTokenDsispenserActor` won't ask `GroupMetricsActor` about exhausted groups at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204
https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204:411,Modifiability,config,config,411,"> Since this is a big change to the way we start jobs, I'm wondering if we want to include an ""off switch"" in the initial release. If we discovered a problem with this behavior and want to quickly revert to the old behavior, can we do that by setting the config to a 0 minute threshold? Should we build in an enabled flag for this behavior in config?. @jgainerdewar yes I had thought about that and setting the config to 0 should work. But I like your suggestion about having an actual config value like `enabled` instead which should make it more clear. Use the `enabled` flag would also be better so that if it is set to false the `JobTokenDsispenserActor` won't ask `GroupMetricsActor` about exhausted groups at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204
https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204:486,Modifiability,config,config,486,"> Since this is a big change to the way we start jobs, I'm wondering if we want to include an ""off switch"" in the initial release. If we discovered a problem with this behavior and want to quickly revert to the old behavior, can we do that by setting the config to a 0 minute threshold? Should we build in an enabled flag for this behavior in config?. @jgainerdewar yes I had thought about that and setting the config to 0 should work. But I like your suggestion about having an actual config value like `enabled` instead which should make it more clear. Use the `enabled` flag would also be better so that if it is set to false the `JobTokenDsispenserActor` won't ask `GroupMetricsActor` about exhausted groups at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204
https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204:548,Usability,clear,clear,548,"> Since this is a big change to the way we start jobs, I'm wondering if we want to include an ""off switch"" in the initial release. If we discovered a problem with this behavior and want to quickly revert to the old behavior, can we do that by setting the config to a 0 minute threshold? Should we build in an enabled flag for this behavior in config?. @jgainerdewar yes I had thought about that and setting the config to 0 should work. But I like your suggestion about having an actual config value like `enabled` instead which should make it more clear. Use the `enabled` flag would also be better so that if it is set to false the `JobTokenDsispenserActor` won't ask `GroupMetricsActor` about exhausted groups at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204
https://github.com/broadinstitute/cromwell/pull/7533#issuecomment-2334841724:75,Security,audit,audit,75,"Oh, and the next part of the plan is to delete the buckets to simplify the audit. That's the real purpose here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7533#issuecomment-2334841724
https://github.com/broadinstitute/cromwell/pull/7533#issuecomment-2334841724:62,Usability,simpl,simplify,62,"Oh, and the next part of the plan is to delete the buckets to simplify the audit. That's the real purpose here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7533#issuecomment-2334841724
https://github.com/broadinstitute/cromwell/pull/7545#issuecomment-2357254944:152,Testability,test,tests,152,"@aednichols @mcovarr : Please let us know if you have any feedback on this PR. I would like to confirm this is in the right direction before working on tests and getting this ready for merging. Alternatively, please let me know if I should reach out to someone else for review.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7545#issuecomment-2357254944
https://github.com/broadinstitute/cromwell/pull/7545#issuecomment-2357254944:58,Usability,feedback,feedback,58,"@aednichols @mcovarr : Please let us know if you have any feedback on this PR. I would like to confirm this is in the right direction before working on tests and getting this ready for merging. Alternatively, please let me know if I should reach out to someone else for review.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7545#issuecomment-2357254944
